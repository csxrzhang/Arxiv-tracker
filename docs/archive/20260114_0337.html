<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-14 03:37</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260114_0337</div>
    <div class="row"><div class="card">
<div class="title">Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning with Self-Recovery for Real-World Manipulation</div>
<div class="meta-line">Authors: Huanyu Li, Kun Lei, Sheng Zang, Kaizhe Hu, Yongyuan Liang, Bo An, Xiaoli Li, Huazhe Xu</div>
<div class="meta-line">First: 2026-01-12T18:53:11+00:00 · Latest: 2026-01-12T18:53:11+00:00</div>
<div class="meta-line">Comments: Project page: https://failure-aware-rl.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07821v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07821v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://failure-aware-rl.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Post-training algorithms based on deep reinforcement learning can push the limits of robotic models for specific objectives, such as generalizability, accuracy, and robustness. However, Intervention-requiring Failures (IR Failures) (e.g., a robot spilling water or breaking fragile glass) during real-world exploration happen inevitably, hindering the practical deployment of such a paradigm. To tackle this, we introduce Failure-Aware Offline-to-Online Reinforcement Learning (FARL), a new paradigm minimizing failures during real-world reinforcement learning. We create FailureBench, a benchmark that incorporates common failure scenarios requiring human intervention, and propose an algorithm that integrates a world-model-based safety critic and a recovery policy trained offline to prevent failures during online exploration. Extensive simulation and real-world experiments demonstrate the effectiveness of FARL in significantly reducing IR Failures while improving performance and generalization during online reinforcement learning post-training. FARL reduces IR Failures by 73.1% while elevating performance by 11.3% on average during real-world RL post-training. Videos and code are available at https://failure-aware-rl.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>故障感知强化学习：具备自我恢复能力的可靠离线至在线强化学习在现实世界操控中的应用</div>
<div class="mono" style="margin-top:8px">基于深度强化学习的训练后算法能够针对特定目标（如泛化性、精确性和鲁棒性）突破机器人模型的性能极限。然而，现实世界探索过程中难免发生需要人工干预的故障（如机器人打翻水杯或打碎玻璃），阻碍了该范式的实际部署。为此，我们提出故障感知离线至在线强化学习（FARL）这一新范式，旨在最小化现实世界强化学习过程中的故障发生率。我们构建了包含常见需人工干预故障场景的基准测试集FailureBench，并提出一种集成基于世界模型的安全评判器与离线训练恢复策略的算法，以预防在线探索期间的故障。大量仿真和现实世界实验表明，FARL能显著减少需干预故障（平均降低73.1%），同时在训练后在线强化学习中提升性能（平均提高11.3%）与泛化能力。视频与代码详见项目网站。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the practical challenge of Intervention-requiring Failures during real-world robotic reinforcement learning, which hinder deployment. The method introduces Failure-Aware RL, a paradigm that integrates a world-model-based safety critic and an offline-trained recovery policy to minimize failures. Main experimental results on the proposed FailureBench show that FARL reduces such failures by 73.1% and improves performance by 11.3% on average in real-world post-training.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现实世界机器人强化学习中需要人工干预的失败问题，这类问题阻碍了实际部署。方法上提出了故障感知强化学习新范式，它整合了基于世界模型的安全评估器和离线训练的恢复策略以防止失败。在提出的FailureBench上的主要实验结果表明，FARL在现实世界后训练中将此类失败减少了73.1%，平均性能提升了11.3%。</div>
</details>
</div>
<div class="card">
<div class="title">Near-Real-Time Resource Slicing for QoS Optimization in 5G O-RAN using Deep Reinforcement Learning</div>
<div class="meta-line">Authors: Peihao Yan, Jie Lu, Huacheng Zeng, Y. Thomas Hou</div>
<div class="meta-line">Venue: P. Yan, J. Lu, H. Zeng and Y. Thomas Hou, &quot;Near-Real-Time Resource Slicing for QoS Optimization in 5G O-RAN Using Deep Reinforcement Learning,&quot; in IEEE Transactions on Networking, vol. 34, pp. 1596-1611, 2026</div>
<div class="meta-line">First: 2025-09-17T18:20:04+00:00 · Latest: 2026-01-12T18:11:14+00:00</div>
<div class="meta-line">Comments: Published in: IEEE Transactions on Networking</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.14343v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.14343v2">PDF</a> · <a href="https://github.com/xslice-5G/code">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-Radio Access Network (O-RAN) has become an important paradigm for 5G and beyond radio access networks. This paper presents an xApp called xSlice for the Near-Real-Time (Near-RT) RAN Intelligent Controller (RIC) of 5G O-RANs. xSlice is an online learning algorithm that adaptively adjusts MAC-layer resource allocation in response to dynamic network states, including time-varying wireless channel conditions, user mobility, traffic fluctuations, and changes in user demand. To address these network dynamics, we first formulate the Quality-of-Service (QoS) optimization problem as a regret minimization problem by quantifying the QoS demands of all traffic sessions through weighting their throughput, latency, and reliability. We then develop a deep reinforcement learning (DRL) framework that utilizes an actor-critic model to combine the advantages of both value-based and policy-based updating methods. A graph convolutional network (GCN) is incorporated as a component of the DRL framework for graph embedding of RAN data, enabling xSlice to handle a dynamic number of traffic sessions. We have implemented xSlice on an O-RAN testbed with 10 smartphones and conducted extensive experiments to evaluate its performance in realistic scenarios. Experimental results show that xSlice can reduce performance regret by 67% compared to the state-of-the-art solutions. Source code is available at https://github.com/xslice-5G/code.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度强化学习的5G O-RAN近实时资源切片服务质量优化</div>
<div class="mono" style="margin-top:8px">开放无线接入网（O-RAN）已成为5G及未来无线接入网络的重要范式。本文提出了一种名为xSlice的xApp，用于5G O-RAN的近实时无线接入网络智能控制器。xSlice是一种在线学习算法，能够根据动态网络状态自适应调整MAC层资源分配，这些状态包括时变无线信道条件、用户移动性、流量波动及用户需求变化。为应对这些网络动态，我们首先通过加权所有流量会话的吞吐量、时延和可靠性来量化其服务质量需求，从而将服务质量优化问题建模为遗憾最小化问题。随后，我们开发了一个深度强化学习框架，该框架采用演员-评论家模型，结合了基于值和基于策略的更新方法的优势。图卷积网络作为深度强化学习框架的组成部分，用于无线接入网络数据的图嵌入，使xSlice能够处理动态数量的流量会话。我们在配备10部智能手机的O-RAN测试平台上实现了xSlice，并进行了大量实验以评估其在真实场景中的性能。实验结果表明，与现有先进解决方案相比，xSlice能将性能遗憾降低67%。源代码发布于：https://github.com/xslice-5G/code。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to manage dynamic network conditions like varying channels and user mobility in 5G O-RAN, this paper introduces xSlice, an xApp for the Near-RT RIC that optimizes QoS via online learning. The method formulates QoS optimization as a regret minimization problem, weighting throughput, latency, and reliability, and employs a deep reinforcement learning framework with an actor-critic model and graph convolutional network to handle dynamic traffic sessions. Experimental results from a testbed with 10 smartphones demonstrate that xSlice reduces performance regret by 67% compared to state-of-the-art solutions.</div>
<div class="mono" style="margin-top:8px">本文针对5G O-RAN中时变信道、用户移动性等动态网络条件，提出了名为xSlice的xApp，用于近实时RAN智能控制器，通过在线学习优化服务质量。方法将服务质量优化问题构建为遗憾最小化问题，加权考虑吞吐量、时延和可靠性，并采用结合行动者-评论家模型与图卷积网络的深度强化学习框架，以处理动态流量会话。在包含10部智能手机的O-RAN测试平台上进行的实验表明，xSlice相比现有最优方案能将性能遗憾降低67%。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning</div>
<div class="meta-line">Authors: Wei Fang, James Glass</div>
<div class="meta-line">First: 2026-01-12T17:58:39+00:00 · Latest: 2026-01-12T17:58:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07782v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07782v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM agents operating over massive, dynamic tool libraries rely on effective retrieval, yet standard single-shot dense retrievers struggle with complex requests. These failures primarily stem from the disconnect between abstract user goals and technical documentation, and the limited capacity of fixed-size embeddings to model combinatorial tool compositions. To address these challenges, we propose TOOLQP, a lightweight framework that models retrieval as iterative query planning. Instead of single-shot matching, TOOLQP decomposes instructions into sub-tasks and dynamically generates queries to interact with the retriever, effectively bridging the semantic gap by targeting the specific sub-tasks required for composition. We train TOOLQP using synthetic query trajectories followed by optimization via Reinforcement Learning with Verifiable Rewards (RLVR). Experiments demonstrate that TOOLQP achieves state-of-the-art performance, exhibiting superior zero-shot generalization, robustness across diverse retrievers, and significant improvements in downstream agentic execution.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越单次检索：基于查询规划的多步骤工具检索方法</div>
<div class="mono" style="margin-top:8px">在大规模动态工具库中运行的LLM智能体依赖高效检索，但标准的单次密集检索器难以处理复杂请求。这些失败主要源于抽象用户目标与技术文档之间的语义鸿沟，以及固定维度嵌入在建模组合工具结构时的能力局限。为解决这些问题，我们提出TOOLQP轻量级框架，将检索建模为迭代式查询规划。TOOLQP不再采用单次匹配，而是将指令分解为子任务，动态生成查询与检索器交互，通过针对组合所需的特定子任务有效弥合语义差距。我们使用合成查询轨迹训练TOOLQP，并通过可验证奖励强化学习（RLVR）进行优化。实验表明TOOLQP实现了最先进的性能，展现出卓越的零样本泛化能力、跨检索器的鲁棒性，以及下游智能体执行的显著提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that large language model agents using extensive and dynamic tool libraries require effective retrieval, but standard single-shot dense retrievers often fail with complex instructions due to a semantic gap between user goals and technical documentation, and the limited ability of fixed embeddings to represent combinatorial tool compositions. To address this, the method introduces TOOLQP, a lightweight framework that models retrieval as iterative query planning, decomposing instructions into sub-tasks and dynamically generating queries to interact with the retriever, thereby bridging the semantic gap by targeting specific sub-tasks needed for composition; it is trained using synthetic query trajectories followed by optimization via Reinforcement Learning with Verifiable Rewards (RLVR). The main experimental results show that TOOLQP achieves state-of-the-art performance, demonstrating superior zero-shot generalization, robustness across diverse retrievers, and significant improvements in downstream agentic execution.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，使用大规模动态工具库的大型语言模型代理需要有效的检索，但标准的单次密集检索器在处理复杂指令时往往失败，这源于用户抽象目标与技术文档之间的语义鸿沟，以及固定大小嵌入在建模组合工具构成时的能力有限。为解决这些问题，方法提出了TOOLQP，一个轻量级框架，将检索建模为迭代式查询规划，通过将指令分解为子任务并动态生成查询与检索器交互，从而针对组合所需的具体子任务来弥合语义差距；该方法使用合成查询轨迹进行训练，并通过可验证奖励的强化学习进行优化。主要实验结果表明，TOOLQP实现了最先进的性能，展现出卓越的零样本泛化能力、跨不同检索器的鲁棒性，以及在下游代理执行中的显著提升。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to bin: differentiable and Bayesian optimization for multi-dimensional discriminants in high-energy physics</div>
<div class="meta-line">Authors: Johannes Erdmann, Nitish Kumar Kasaraguppe, Florian Mausolf</div>
<div class="meta-line">First: 2026-01-12T17:40:45+00:00 · Latest: 2026-01-12T17:40:45+00:00</div>
<div class="meta-line">Comments: 13 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07756v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07756v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Categorizing events using discriminant observables is central to many high-energy physics analyses. Yet, bin boundaries are often chosen by hand. A simple, popular choice is to apply argmax projections of multi-class scores and equidistant binning of one-dimensional discriminants. We propose a binning optimization for signal significance directly in multi-dimensional discriminants. We use a Gaussian Mixture Model (GMM) to define flexible bin boundary shapes for multi-class scores, while in one dimension (binary classification) we move bin boundaries directly. On this binning model, we study two optimization strategies: a differentiable and a Bayesian optimization approach. We study two toy setups: a binary classification and a three-class problem with two signals and backgrounds. In the one-dimensional case, both approaches achieve similar gains in signal sensitivity compared to equidistant binnings for a given number of bins. In the multi-dimensional case, the GMM-based binning defines sensitive categories as well, with the differentiable approach performing best. We show that, in particular for limited separability of the signal processes, our approach outperforms argmax classification even with optimized binning in the one-dimensional projections. Both methods are released as lightweight Python plugins intended for straightforward integration into existing analyses.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习分箱：高能物理中多维判别式的可微与贝叶斯优化方法</div>
<div class="mono" style="margin-top:8px">基于判别观测量的事件分类是高能物理分析的核心环节，但分箱边界常依赖人工设定。常见做法是对多类得分进行argmax投影后对一维判别式进行等距分箱。本文提出直接在多维判别式上优化信号显著性的分箱方法：对多类得分采用高斯混合模型定义灵活的分箱边界形状，对一维情形（二分类）则直接移动分箱边界。基于该分箱模型，研究两种优化策略：可微优化与贝叶斯优化。通过二分类及双信号-本底的三分类两个示例验证：在一维场景中，两种方法在给定箱数下均较等距分箱获得相近的信号灵敏度提升；多维场景中，基于高斯混合模型的分箱同样能构建敏感类别，其中可微优化表现最佳。研究表明，尤其在信号过程可分性有限时，本方法优于对一维投影进行优化分箱的argmax分类。两种方法均以轻量级Python插件形式发布，便于集成至现有分析流程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the common practice of manually choosing bin boundaries for discriminant observables in high-energy physics analyses, which often relies on simplistic one-dimensional projections, this paper proposes a method to optimize binning directly in multi-dimensional space to maximize signal significance. The method employs a Gaussian Mixture Model (GMM) to define flexible bin shapes for multi-class scores and directly adjusts bin boundaries in one-dimensional cases, with optimization performed via differentiable and Bayesian approaches. Experimental results on toy setups, including binary and three-class problems, show that both optimization strategies improve signal sensitivity over equidistant binning in one dimension, while in multi-dimensional scenarios, the GMM-based binning with differentiable optimization performs best, particularly outperforming argmax classification when signal separability is limited.</div>
<div class="mono" style="margin-top:8px">针对高能物理分析中通常手动选择判别变量分箱边界、且多依赖简单一维投影的现状，本文提出一种直接在多维空间中优化分箱以最大化信号显著性的方法。该方法采用高斯混合模型（GMM）为多类得分定义灵活的分箱形状，在一维情况下直接调整边界，并通过可微分优化和贝叶斯优化两种策略进行优化。在包括二分类和三分类问题的玩具设置上的实验结果表明，两种优化策略在一维情况下均比等距分箱提升了信号灵敏度；而在多维场景中，基于GMM的分箱结合可微分优化表现最佳，尤其在信号可分离性有限时，其性能优于基于一维投影的argmax分类方法。</div>
</details>
</div>
<div class="card">
<div class="title">MiST: Understanding the Role of Mid-Stage Scientific Training in Developing Chemical Reasoning Models</div>
<div class="meta-line">Authors: Andres M Bran, Tong Xie, Shai Pranesh, Jeffrey Meng, Xuan Vu Nguyen, Jeremy Goumaz, David Ming Segura, Ruizhi Xu, Dongzhan Zhou, Wenjie Zhang, Bram Hoex, Philippe Schwaller</div>
<div class="meta-line">First: 2025-12-24T15:15:18+00:00 · Latest: 2026-01-12T17:32:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21231v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.21231v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models can develop reasoning capabilities through online fine-tuning with rule-based rewards. However, recent studies reveal a critical constraint: reinforcement learning succeeds only when the base model already assigns non-negligible probability to correct answers -- a property we term &#x27;latent solvability&#x27;. This work investigates the emergence of chemical reasoning capabilities and what these prerequisites mean for chemistry. We identify two necessary conditions for RL-based chemical reasoning: 1) Symbolic competence, and 2) Latent chemical knowledge. We propose mid-stage scientific training (MiST): a set of mid-stage training techniques to satisfy these, including data-mixing with SMILES/CIF-aware pre-processing, continued pre-training on 2.9B tokens, and supervised fine-tuning on 1B tokens. These steps raise the latent-solvability score on 3B and 7B models by up to 1.8x, and enable RL to lift top-1 accuracy from 10.9 to 63.9% on organic reaction naming, and from 40.6 to 67.4% on inorganic material generation. Similar results are observed for other challenging chemical tasks, while producing interpretable reasoning traces. Our results define clear prerequisites for chemical reasoning training and highlight the broader role of mid-stage training in unlocking reasoning capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MiST：理解中期科学训练在化学推理模型发展中的作用</div>
<div class="mono" style="margin-top:8px">大型语言模型可通过基于规则的奖励进行在线微调来发展推理能力。然而，近期研究揭示了一个关键限制：仅当基础模型已对正确答案分配了不可忽略的概率时，强化学习才能成功——这一特性我们称为“潜在可解性”。本研究探讨了化学推理能力的涌现及其先决条件对化学领域的意义。我们确定了基于强化学习的化学推理的两个必要条件：1）符号能力，2）潜在化学知识。我们提出了中期科学训练（MiST）：一套满足这些条件的中期训练技术，包括结合SMILES/CIF感知预处理的数据混合、对29亿词元的持续预训练，以及对10亿词元的有监督微调。这些步骤将30亿和70亿参数模型的潜在可解性分数提升至多1.8倍，并使强化学习在有机反应命名任务中的Top-1准确率从10.9%提升至63.9%，在无机材料生成任务中从40.6%提升至67.4%。在其他具有挑战性的化学任务中也观察到类似结果，同时生成可解释的推理轨迹。我们的研究明确了化学推理训练的必备前提，并凸显了中期训练在解锁推理能力方面的广泛作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work is motivated by the observation that reinforcement learning (RL) for chemical reasoning in large language models requires the base model to already assign some probability to correct answers, a property termed &#x27;latent solvability&#x27;. To establish this prerequisite, the authors propose Mid-Stage Scientific Training (MiST), a method involving data-mixing with chemical-aware pre-processing (SMILES/CIF), continued pre-training on 2.9B tokens, and supervised fine-tuning on 1B tokens. The main experimental results show that MiST increases the latent-solvability score by up to 1.8x for 3B and 7B models, enabling RL to significantly improve top-1 accuracy from 10.9% to 63.9% on organic reaction naming and from 40.6% to 67.4% on inorganic material generation, with similar gains on other challenging chemical tasks while producing interpretable reasoning traces.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于发现，基于强化学习训练大语言模型进行化学推理时，要求基础模型已对正确答案赋予一定的概率，这一特性被称为&#x27;潜在可解性&#x27;。为满足这一前提，作者提出了中期科学训练方法，该方法包括采用化学感知预处理的数据混合、基于29亿词元的继续预训练和基于10亿词元的监督微调。主要实验结果表明，该方法将3B和7B模型的潜在可解性分数提升了最高1.8倍，从而使强化学习能够将有机反应命名的Top-1准确率从10.9%大幅提升至63.9%，将无机材料生成的准确率从40.6%提升至67.4%，在其他具有挑战性的化学任务上也观察到类似提升，同时能产生可解释的推理轨迹。</div>
</details>
</div>
<div class="card">
<div class="title">Hiking in the Wild: A Scalable Perceptive Parkour Framework for Humanoids</div>
<div class="meta-line">Authors: Shaoting Zhu, Ziwen Zhuang, Mengjie Zhao, Kun-Ying Lee, Hang Zhao</div>
<div class="meta-line">First: 2026-01-12T16:50:50+00:00 · Latest: 2026-01-12T16:50:50+00:00</div>
<div class="meta-line">Comments: Project Page: https://project-instinct.github.io/hiking-in-the-wild</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07718v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07718v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://project-instinct.github.io/hiking-in-the-wild">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Achieving robust humanoid hiking in complex, unstructured environments requires transitioning from reactive proprioception to proactive perception. However, integrating exteroception remains a significant challenge: mapping-based methods suffer from state estimation drift; for instance, LiDAR-based methods do not handle torso jitter well. Existing end-to-end approaches often struggle with scalability and training complexity; specifically, some previous works using virtual obstacles are implemented case-by-case. In this work, we present \textit{Hiking in the Wild}, a scalable, end-to-end parkour perceptive framework designed for robust humanoid hiking. To ensure safety and training stability, we introduce two key mechanisms: a foothold safety mechanism combining scalable \textit{Terrain Edge Detection} with \textit{Foot Volume Points} to prevent catastrophic slippage on edges, and a \textit{Flat Patch Sampling} strategy that mitigates reward hacking by generating feasible navigation targets. Our approach utilizes a single-stage reinforcement learning scheme, mapping raw depth inputs and proprioception directly to joint actions, without relying on external state estimation. Extensive field experiments on a full-size humanoid demonstrate that our policy enables robust traversal of complex terrains at speeds up to 2.5 m/s. The training and deployment code is open-sourced to facilitate reproducible research and deployment on real robots with minimal hardware modifications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>野外徒步：面向人形机器人的可扩展感知跑酷框架</div>
<div class="mono" style="margin-top:8px">在复杂非结构化环境中实现稳健的人形机器人徒步，需要从反应性本体感知转向主动性感知。然而，整合外部感知仍是重大挑战：基于地图的方法存在状态估计漂移问题，例如基于激光雷达的方法难以有效处理躯干抖动。现有端到端方法常受限于可扩展性和训练复杂性；特别是先前采用虚拟障碍物的研究多为个案实现。本文提出《野外徒步》，一个为稳健人形徒步设计的可扩展端到端感知跑酷框架。为确保安全与训练稳定性，我们引入两项关键机制：结合可扩展《地形边缘检测》与《足部体积点》的立足点安全机制以防止边缘灾难性滑移，以及通过生成可行导航目标来缓解奖励作弊的《平坦区块采样》策略。该方法采用单阶段强化学习方案，将原始深度输入与本体感知直接映射为关节动作，无需依赖外部状态估计。在全尺寸人形机器人上的大量实地实验表明，我们的策略能以最高2.5米/秒的速度稳健穿越复杂地形。训练与部署代码已开源，以促进可复现研究及在最小硬件改动下的真实机器人部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to enable robust humanoid hiking in complex, unstructured environments by moving beyond reactive proprioception to integrate proactive perception, addressing challenges like state estimation drift in mapping-based methods and the scalability issues of existing end-to-end approaches. The method introduces a scalable, end-to-end parkour perceptive framework called &#x27;Hiking in the Wild&#x27;, which employs a single-stage reinforcement learning scheme that maps raw depth and proprioceptive inputs directly to joint actions, avoiding external state estimation; key innovations include a foothold safety mechanism combining Terrain Edge Detection with Foot Volume Points to prevent slippage, and a Flat Patch Sampling strategy to mitigate reward hacking by generating feasible navigation targets. The main experimental results from extensive field tests on a full-size humanoid show that the policy achieves robust traversal of complex terrains at speeds up to 2.5 m/s, with the training and deployment code being open-sourced to support reproducible research and real-robot deployment with minimal hardware modifications.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过从反应性本体感知转向主动性感知，使双足人形机器人能在复杂、非结构化环境中实现稳健的徒步行走，以解决基于地图的方法中状态估计漂移以及现有端到端方法可扩展性不足等挑战。方法上，提出了一个名为&#x27;Hiking in the Wild&#x27;的可扩展端到端跑酷感知框架，采用单阶段强化学习方案，将原始深度和本体感知输入直接映射到关节动作，无需依赖外部状态估计；关键创新包括结合地形边缘检测与足部体积点的立足点安全机制以防止边缘滑倒，以及通过平坦区域采样策略生成可行导航目标来缓解奖励欺骗。主要实验结果表明，在完整尺寸人形机器人上的大量野外测试中，该策略能以高达2.5米/秒的速度稳健穿越复杂地形，且训练与部署代码已开源，以支持可重复研究及对真实机器人的最小硬件修改部署。</div>
</details>
</div>
<div class="card">
<div class="title">Modern Neuromorphic AI: From Intra-Token to Inter-Token Processing</div>
<div class="meta-line">Authors: Osvaldo Simeone</div>
<div class="meta-line">First: 2026-01-01T07:38:07+00:00 · Latest: 2026-01-12T15:47:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00245v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.00245v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid growth of artificial intelligence (AI) has brought novel data processing and generative capabilities but also escalating energy requirements. This challenge motivates renewed interest in neuromorphic computing principles, which promise brain-like efficiency through discrete and sparse activations, recurrent dynamics, and non-linear feedback. In fact, modern AI architectures increasingly embody neuromorphic principles through heavily quantized activations, state-space dynamics, and sparse attention mechanisms. This paper elaborates on the connections between neuromorphic models, state-space models, and transformer architectures through the lens of the distinction between intra-token processing and inter-token processing. Most early work on neuromorphic AI was based on spiking neural networks (SNNs) for intra-token processing, i.e., for transformations involving multiple channels, or features, of the same vector input, such as the pixels of an image. In contrast, more recent research has explored how neuromorphic principles can be leveraged to design efficient inter-token processing methods, which selectively combine different information elements depending on their contextual relevance. Implementing associative memorization mechanisms, these approaches leverage state-space dynamics or sparse self-attention. Along with a systematic presentation of modern neuromorphic AI models through the lens of intra-token and inter-token processing, training methodologies for neuromorphic AI models are also reviewed. These range from surrogate gradients leveraging parallel convolutional processing to local learning rules based on reinforcement learning mechanisms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>现代神经形态人工智能：从令牌内处理到令牌间处理</div>
<div class="mono" style="margin-top:8px">人工智能的快速发展带来了新颖的数据处理和生成能力，但也伴随着不断攀升的能耗需求。这一挑战重新激发了人们对神经形态计算原理的兴趣，该原理通过离散稀疏激活、循环动力学和非线性反馈，有望实现类脑的高效计算。事实上，现代人工智能架构正通过重度量化激活、状态空间动力学和稀疏注意力机制，日益体现神经形态原理。本文通过区分令牌内处理与令牌间处理的视角，详细阐述了神经形态模型、状态空间模型与Transformer架构之间的关联。早期神经形态人工智能研究大多基于脉冲神经网络进行令牌内处理，即对同一向量输入（如图像像素）的多通道或特征进行变换。相比之下，近期研究探索了如何利用神经形态原理设计高效的令牌间处理方法，这些方法能根据上下文相关性选择性地组合不同的信息元素。通过实现关联记忆机制，这些方法利用了状态空间动力学或稀疏自注意力。本文不仅通过令牌内与令牌间处理的视角系统介绍了现代神经形态人工智能模型，还综述了相关训练方法，涵盖从利用并行卷积处理的替代梯度法到基于强化学习机制的局部学习规则。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the escalating energy demands of conventional AI, this paper explores neuromorphic computing principles as a pathway to brain-like efficiency, highlighting their embodiment in modern architectures through quantized activations and sparse mechanisms. The method introduces a conceptual framework distinguishing intra-token processing, historically addressed by spiking neural networks for multi-channel vector transformations like images, from inter-token processing, which uses state-space dynamics or sparse attention to contextually combine information elements. Experimental results and reviews indicate that these neuromorphic approaches, supported by training methods such as surrogate gradients and local learning rules, enable efficient associative memorization and selective information integration, bridging models like transformers with neuromorphic efficiency gains.</div>
<div class="mono" style="margin-top:8px">本文针对传统人工智能不断增长的能耗挑战，探讨了神经形态计算原理如何通过离散稀疏激活和反馈机制实现类脑高效能，并指出现代架构已通过量化激活和稀疏注意力等体现了这些原则。方法上，论文提出了一个区分帧内处理与帧间处理的概念框架：帧内处理早期多由脉冲神经网络实现，用于如图像等多通道向量转换；而帧间处理则利用状态空间动态或稀疏自注意力，根据上下文相关性选择性地整合信息元素。实验与综述表明，这些神经形态方法结合代理梯度等训练技术，能够实现高效的关联记忆和选择性信息处理，从而将Transformer等模型与神经形态能效优势相连接。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning for Micro-Level Claims Reserving</div>
<div class="meta-line">Authors: Benjamin Avanzi, Ronald Richman, Bernard Wong, Mario Wüthrich, Yagebu Xie</div>
<div class="meta-line">First: 2026-01-12T15:17:18+00:00 · Latest: 2026-01-12T15:17:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07637v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07637v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Outstanding claim liabilities are revised repeatedly as claims develop, yet most modern reserving models are trained as one-shot predictors and typically learn only from settled claims. We formulate individual claims reserving as a claim-level Markov decision process in which an agent sequentially updates outstanding claim liability (OCL) estimates over development, using continuous actions and a reward design that balances accuracy with stable reserve revisions. A key advantage of this reinforcement learning (RL) approach is that it can learn from all observed claim trajectories, including claims that remain open at valuation, thereby avoiding the reduced sample size and selection effects inherent in supervised methods trained on ultimate outcomes only. We also introduce practical components needed for actuarial use -- initialisation of new claims, temporally consistent tuning via a rolling-settlement scheme, and an importance-weighting mechanism to mitigate portfolio-level underestimation driven by the rarity of large claims. On CAS and SPLICE synthetic general insurance datasets, the proposed Soft Actor-Critic implementation delivers competitive claim-level accuracy and strong aggregate OCL performance, particularly for the immature claim segments that drive most of the liability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>微观理赔准备金评估的强化学习方法</div>
<div class="mono" style="margin-top:8px">未决赔款负债随理赔进展反复调整，但多数现代准备金模型仅作为单次预测器训练，且通常仅从已结案赔案中学习。本研究将个体理赔准备金评估构建为理赔层面的马尔可夫决策过程：智能体通过连续动作和平衡预测精度与准备金调整稳定性的奖励设计，在理赔进展中动态更新未决赔款负债估计。该强化学习方法的关键优势在于能利用所有观测到的理赔轨迹（包括评估时仍处于开放状态的赔案），从而避免仅基于最终结果训练的监督方法固有的样本缩减和选择偏差问题。研究还引入了精算实务所需的组件——新赔案的初始化方法、通过滚动结案方案实现时间一致性调优，以及缓解大额赔案稀缺性导致组合层面低估的重要性加权机制。在CAS与SPLICE综合财产保险数据集上的实验表明，基于软演员-评论家算法的实施方案在理赔层面精度与聚合未决赔款负债预测方面均表现优异，尤其对构成主要负债的未成熟赔案段具有显著优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the limitations of traditional one-shot reserving models that only learn from settled claims, which reduces sample size and introduces selection bias. The method formulates individual claims reserving as a Markov decision process, using reinforcement learning with continuous actions and a reward function that balances accuracy and revision stability, allowing learning from all claim trajectories including open claims. Experimental results on CAS and SPLICE synthetic datasets show that the Soft Actor-Critic implementation achieves competitive claim-level accuracy and strong aggregate outstanding liability performance, especially for immature claims that constitute most of the liability.</div>
<div class="mono" style="margin-top:8px">本文的动机在于传统一次性准备金模型仅从已结案索赔中学习，导致样本量减少和选择偏差的问题。该方法将个体索赔准备金建模为马尔可夫决策过程，采用强化学习配合连续动作和平衡准确性与修订稳定性的奖励设计，从而能够从包括未结案索赔在内的所有索赔轨迹中学习。在CAS和SPLICE合成数据集上的实验结果表明，所提出的Soft Actor-Critic方法实现了具有竞争力的索赔级别准确性，并在总未决负债方面表现优异，尤其对于构成大部分负债的未成熟索赔部分。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Stable and Effective Reinforcement Learning for Mixture-of-Experts</div>
<div class="meta-line">Authors: Di Zhang, Xun Wu, Shaohan Huang, Lingjie Jiang, Yaru Hao, Li Dong, Zewen Chi, Zhifang Sui, Furu Wei</div>
<div class="meta-line">First: 2025-10-27T05:47:48+00:00 · Latest: 2026-01-12T15:14:47+00:00</div>
<div class="meta-line">Comments: Added additional experiments, improved analysis, and fixed minor issues</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.23027v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.23027v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in reinforcement learning (RL) have substantially improved the training of large-scale language models, leading to significant gains in generation quality and reasoning ability. However, most existing research focuses on dense models, while RL training for Mixture-of-Experts (MoE) architectures remains underexplored. To address the instability commonly observed in MoE training, we propose a novel router-aware approach to optimize importance sampling (IS) weights in off-policy RL. Specifically, we design a rescaling strategy guided by router logits, which effectively reduces gradient variance and mitigates training divergence. Experimental results demonstrate that our method significantly improves both the convergence stability and the final performance of MoE models, highlighting the potential of RL algorithmic innovations tailored to MoE architectures and providing a promising direction for efficient training of large-scale expert models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向混合专家模型的稳定高效强化学习研究</div>
<div class="mono" style="margin-top:8px">强化学习的最新进展显著提升了大语言模型的训练效果，在生成质量与推理能力上取得重要突破。然而现有研究多集中于稠密模型，针对混合专家架构的强化学习训练仍探索不足。为解决MoE训练中常见的不稳定问题，本文提出一种新颖的路由器感知方法，用于优化离线策略强化学习中的重要性采样权重。具体而言，我们设计了一种基于路由器逻辑值的重缩放策略，可有效降低梯度方差并缓解训练发散。实验结果表明，该方法显著提升了MoE模型的收敛稳定性与最终性能，凸显了针对MoE架构定制强化学习算法的潜力，为大规模专家模型的高效训练提供了可行方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the instability in reinforcement learning (RL) training for Mixture-of-Experts (MoE) models, which is underexplored compared to dense models. The method introduces a router-aware approach that optimizes importance sampling weights via a rescaling strategy guided by router logits to reduce gradient variance and prevent training divergence. Experiments show the proposed technique significantly enhances both convergence stability and final performance of MoE models, indicating a promising direction for efficient large-scale expert model training.</div>
<div class="mono" style="margin-top:8px">本文针对混合专家模型中强化学习训练不稳定的问题展开研究，该问题相较于稠密模型尚未得到充分探索。方法上提出了一种路由器感知的新方法，通过基于路由器逻辑的重新缩放策略来优化重要性采样权重，从而降低梯度方差并防止训练发散。实验结果表明，该技术显著提高了混合专家模型的收敛稳定性和最终性能，为高效训练大规模专家模型提供了一个有前景的方向。</div>
</details>
</div>
<div class="card">
<div class="title">NeoAMT: Neologism-Aware Agentic Machine Translation with Reinforcement Learning</div>
<div class="meta-line">Authors: Zhongtao Miao, Kaiyan Zhao, Masaaki Nagata, Yoshimasa Tsuruoka</div>
<div class="meta-line">First: 2026-01-07T10:49:00+00:00 · Latest: 2026-01-12T15:01:07+00:00</div>
<div class="meta-line">Comments: Fixed typos in Table 1, Figure 7 and Section 4.2: regex -&gt; exact. Refined the caption of Table 3</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03790v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03790v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neologism-aware machine translation aims to translate source sentences containing neologisms into target languages. This field remains underexplored compared with general machine translation (MT). In this paper, we propose an agentic framework, NeoAMT, for neologism-aware machine translation using a Wiktionary search tool. Specifically, we first create a new dataset for neologism-aware machine translation and develop a search tool based on Wiktionary. The new dataset covers 16 languages and 75 translation directions and is derived from approximately 10 million records of an English Wiktionary dump. The retrieval corpus of the search tool is also constructed from around 3 million cleaned records of the Wiktionary dump. We then use it for training the translation agent with reinforcement learning (RL) and evaluating the accuracy of neologism-aware machine translation. Based on this, we also propose an RL training framework that contains a novel reward design and an adaptive rollout generation approach by leveraging &quot;translation difficulty&quot; to further improve the translation quality of translation agents using our search tool.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NeoAMT：基于强化学习的新词感知智能机器翻译框架</div>
<div class="mono" style="margin-top:8px">新词感知机器翻译旨在将包含新词的源语句翻译为目标语言。与通用机器翻译相比，该领域研究尚不充分。本文提出一种基于维基词典检索工具的智能框架NeoAMT，用于新词感知机器翻译。具体而言，我们首先构建了覆盖16种语言、75个翻译方向的新词感知机器翻译数据集，该数据集源自约1000万条英语维基词典记录；同时基于约300万条清洗后的维基词典记录构建检索工具语料库。随后利用该工具，通过强化学习训练翻译智能体，并评估新词感知机器翻译的准确性。在此基础上，进一步提出包含创新奖励机制与自适应推演生成方法的强化学习训练框架，通过量化“翻译难度”来提升使用本检索工具的翻译智能体的译文质量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the underexplored challenge of translating neologisms in machine translation by proposing NeoAMT, an agentic framework that integrates a Wiktionary-based search tool. The method involves creating a new multilingual dataset from English Wiktionary and a retrieval corpus, then training a translation agent using reinforcement learning with a novel reward design and adaptive rollout generation based on translation difficulty. Experimental results demonstrate improved translation accuracy for neologisms across 16 languages and 75 translation directions, validating the effectiveness of the framework.</div>
<div class="mono" style="margin-top:8px">本文针对机器翻译中未被充分探索的新词翻译问题，提出了NeoAMT这一基于维基词典搜索工具的智能体框架。该方法通过从英语维基词典构建新的多语言数据集和检索语料库，并利用强化学习训练翻译智能体，其中设计了新颖的奖励机制和基于翻译难度的自适应生成策略。实验结果表明，该框架在16种语言和75个翻译方向上有效提升了新词翻译的准确性，验证了其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">GRPO with State Mutations: Improving LLM-Based Hardware Test Plan Generation</div>
<div class="meta-line">Authors: Dimple Vijay Kochar, Nathaniel Pinckney, Guan-Ting Liu, Chia-Tung Ho, Chenhui Deng, Haoxing Ren, Brucek Khailany</div>
<div class="meta-line">First: 2026-01-12T14:42:42+00:00 · Latest: 2026-01-12T14:42:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07593v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07593v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">RTL design often relies heavily on ad-hoc testbench creation early in the design cycle. While large language models (LLMs) show promise for RTL code generation, their ability to reason about hardware specifications and generate targeted test plans remains largely unexplored. We present the first systematic study of LLM reasoning capabilities for RTL verification stimuli generation, establishing a two-stage framework that decomposes test plan generation from testbench execution. Our benchmark reveals that state-of-the-art models, including DeepSeek-R1 and Claude-4.0-Sonnet, achieve only 15.7-21.7% success rates on generating stimuli that pass golden RTL designs. To improve LLM generated stimuli, we develop a comprehensive training methodology combining supervised fine-tuning with a novel reinforcement learning approach, GRPO with State Mutation (GRPO-SMu), which enhances exploration by varying input mutations. Our approach leverages a tree-based branching mutation strategy to construct training data comprising equivalent and mutated trees, moving beyond linear mutation approaches to provide rich learning signals. Training on this curated dataset, our 7B parameter model achieves a 33.3% golden test pass rate and a 13.9% mutation detection rate, representing a 17.6% absolute improvement over baseline and outperforming much larger general-purpose models. These results demonstrate that specialized training methodologies can significantly enhance LLM reasoning capabilities for hardware verification tasks, establishing a foundation for automated sub-unit testing in semiconductor design workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于状态突变的GRPO：改进基于大语言模型的硬件测试计划生成</div>
<div class="mono" style="margin-top:8px">RTL设计在设计周期早期通常严重依赖临时测试平台创建。虽然大语言模型在RTL代码生成方面展现出潜力，但其理解硬件规格并生成针对性测试计划的能力仍待探索。我们首次系统研究了大语言模型在RTL验证激励生成中的推理能力，建立了一个将测试计划生成与测试平台执行解耦的两阶段框架。基准测试表明，包括DeepSeek-R1和Claude-4.0-Sonnet在内的前沿模型，在生成能通过黄金RTL设计的激励方面成功率仅为15.7-21.7%。为改进大语言模型生成的激励，我们开发了结合监督微调与新型强化学习方法（带状态突变的GRPO，即GRPO-SMu）的综合训练方法，通过输入突变增强探索能力。该方法采用基于树的分支突变策略构建包含等价树与突变树的训练数据，突破线性突变方法以提供丰富的学习信号。基于此定制数据集训练的7B参数模型实现了33.3%的黄金测试通过率和13.9%的突变检测率，较基线绝对提升17.6%，并优于规模更大的通用模型。这些结果表明，专业化训练方法能显著增强大语言模型在硬件验证任务中的推理能力，为半导体设计流程中的自动化子单元测试奠定基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the underexplored potential of large language models (LLMs) for generating targeted hardware test plans from RTL specifications, this paper introduces a systematic two-stage framework to decompose test plan generation from testbench execution. The method develops a comprehensive training approach combining supervised fine-tuning with a novel reinforcement learning technique called GRPO with State Mutation (GRPO-SMu), which employs a tree-based branching mutation strategy to enhance exploration and create rich training data. Experimental results show that a 7B parameter model trained with this methodology achieves a 33.3% pass rate on golden tests and a 13.9% mutation detection rate, marking a 17.6% absolute improvement over baseline models and outperforming much larger general-purpose LLMs, thereby demonstrating significant enhancement in LLM reasoning for hardware verification tasks.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型在根据硬件RTL规范生成针对性测试计划方面潜力未充分开发的现状，提出了一种系统性的两阶段框架，将测试计划生成与测试平台执行解耦。方法上，结合了监督微调与一种名为GRPO状态突变的新型强化学习技术，采用基于树的分支突变策略来增强探索并构建丰富的训练数据。实验结果表明，采用此方法训练的70亿参数模型在黄金测试通过率达到33.3%，突变检测率为13.9%，相比基线模型绝对提升17.6%，且性能优于规模大得多的通用模型，这证明了专业化训练方法能显著提升LLM在硬件验证任务中的推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Large Language Models for Physics Instrument Design</div>
<div class="meta-line">Authors: Sara Zoccheddu, Shah Rukh Qasim, Patrick Owen, Nicola Serra</div>
<div class="meta-line">First: 2026-01-12T14:30:54+00:00 · Latest: 2026-01-12T14:30:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07580v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07580v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the use of large language models (LLMs) for physics instrument design and compare their performance to reinforcement learning (RL). Using only prompting, LLMs are given task constraints and summaries of prior high-scoring designs and propose complete detector configurations, which we evaluate with the same simulators and reward functions used in RL-based optimization. Although RL yields stronger final designs, we find that modern LLMs consistently generate valid, resource-aware, and physically meaningful configurations that draw on broad pretrained knowledge of detector design principles and particle--matter interactions, despite having no task-specific training. Based on this result, as a first step toward hybrid design workflows, we explore pairing the LLMs with a dedicated trust region optimizer, serving as a precursor to future pipelines in which LLMs propose and structure design hypotheses while RL performs reward-driven optimization. Based on these experiments, we argue that LLMs are well suited as meta-planners: they can design and orchestrate RL-based optimization studies, define search strategies, and coordinate multiple interacting components within a unified workflow. In doing so, they point toward automated, closed-loop instrument design in which much of the human effort required to structure and supervise optimization can be reduced.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型在物理仪器设计中的应用</div>
<div class="mono" style="margin-top:8px">本研究探讨大语言模型（LLMs）在物理仪器设计中的应用，并将其性能与强化学习（RL）进行比较。仅通过提示，LLMs接收任务约束和先前高分设计的摘要，提出完整的探测器配置，并使用与RL优化相同的模拟器和奖励函数进行评估。尽管RL能产生更优的最终设计，但现代LLMs无需任务特定训练，即可基于探测器设计原理和粒子-物质相互作用的广泛预训练知识，持续生成有效、资源感知且物理意义明确的配置。基于此结果，作为混合设计工作流程的第一步，我们探索将LLMs与专用信任区域优化器结合，作为未来流程的雏形：LLMs提出并构建设计假设，而RL执行奖励驱动的优化。实验表明，LLMs适合作为元规划器：它们能设计和协调基于RL的优化研究、定义搜索策略，并在统一工作流中协调多个交互组件。这为实现自动化闭环仪器设计指明了方向，可大幅减少人工在构建和监督优化过程中的投入。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the application of large language models (LLMs) to automate physics instrument design, motivated by the need to reduce human effort in structuring and supervising optimization processes. The method involves prompting LLMs with task constraints and summaries of high-scoring designs to generate complete detector configurations, which are then evaluated using simulators and reward functions comparable to those in reinforcement learning (RL) optimization. Experimental results show that while RL produces superior final designs, LLMs consistently generate valid, resource-aware, and physically meaningful configurations without task-specific training, leveraging their broad pretrained knowledge. The study further explores hybrid workflows where LLMs act as meta-planners to propose design hypotheses and orchestrate RL optimization, suggesting a pathway toward automated, closed-loop instrument design.</div>
<div class="mono" style="margin-top:8px">本文研究利用大语言模型（LLMs）自动化物理仪器设计，旨在减少人工在优化过程中所需的结构化与监督工作。方法上，通过向LLMs提供任务约束和先前高分设计的摘要，使其生成完整的探测器配置，并使用与强化学习（RL）优化相同的模拟器和奖励函数进行评估。实验结果表明，尽管RL能产生更优的最终设计，但LLMs无需任务特定训练即可持续生成有效、资源感知且物理意义明确的配置，这得益于其广泛的预训练知识。研究进一步探索了混合工作流程，其中LLMs作为元规划器提出设计假设并协调RL优化，为自动化闭环仪器设计指明了方向。</div>
</details>
</div>
<div class="card">
<div class="title">Can Reasoning Help Large Language Models Capture Human Annotator Disagreement?</div>
<div class="meta-line">Authors: Jingwei Ni, Yu Fan, Vilém Zouhar, Donya Rooein, Alexander Hoyle, Mrinmaya Sachan, Markus Leippold, Dirk Hovy, Elliott Ash</div>
<div class="meta-line">First: 2025-06-24T09:49:26+00:00 · Latest: 2026-01-12T13:46:04+00:00</div>
<div class="meta-line">Comments: EACL 2026 Main</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.19467v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.19467v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Variation in human annotation (i.e., disagreements) is common in NLP, often reflecting important information like task subjectivity and sample ambiguity. Modeling this variation is important for applications that are sensitive to such information. Although RLVR-style reasoning (Reinforcement Learning with Verifiable Rewards) has improved Large Language Model (LLM) performance on many tasks, it remains unclear whether such reasoning enables LLMs to capture informative variation in human annotation. In this work, we evaluate the influence of different reasoning settings on LLM disagreement modeling. We systematically evaluate each reasoning setting across model sizes, distribution expression methods, and steering methods, resulting in 60 experimental setups across 3 tasks. Surprisingly, our results show that RLVR-style reasoning degrades performance in disagreement modeling, while naive Chain-of-Thought (CoT) reasoning improves the performance of RLHF LLMs (RL from human feedback). These findings underscore the potential risk of replacing human annotators with reasoning LLMs, especially when disagreements are important.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>推理能否帮助大语言模型捕捉人类标注者的分歧？</div>
<div class="mono" style="margin-top:8px">人类标注中的差异（即分歧）在自然语言处理中普遍存在，常反映任务主观性和样本歧义等重要信息。对此类信息敏感的应用中，建模这种差异至关重要。尽管基于可验证奖励的强化学习式推理提升了大语言模型在多项任务上的表现，但此类推理是否有助于LLM捕捉人类标注中有信息量的差异仍不明确。本研究评估了不同推理设置对LLM分歧建模的影响，系统考察了模型规模、分布表达方法和引导方法，在3个任务上构建了60种实验配置。出人意料的是，结果显示RLVR式推理会降低分歧建模性能，而朴素的思维链推理却能提升基于人类反馈强化学习的LLM表现。这些发现凸显了用推理型LLM替代人类标注者的潜在风险，尤其在分歧具有重要意义时。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates whether reasoning techniques, specifically RLVR-style and Chain-of-Thought (CoT), help large language models (LLMs) capture human annotator disagreements, which reflect task subjectivity and sample ambiguity. The motivation is to assess if such reasoning improves modeling of annotation variation, important for sensitive applications. The method involves systematically evaluating different reasoning settings across model sizes, distribution expression methods, and steering methods, totaling 60 experimental setups on three tasks. The main experimental results surprisingly show that RLVR-style reasoning degrades performance in disagreement modeling, while naive CoT reasoning improves it for RLHF LLMs, highlighting risks in replacing human annotators with reasoning LLMs when disagreements are informative.</div>
<div class="mono" style="margin-top:8px">本文研究推理技术，特别是RLVR风格和思维链（CoT），是否有助于大语言模型捕捉人类标注者的分歧，这些分歧反映了任务的主观性和样本的模糊性。动机在于评估此类推理是否能改进对标注变异的建模，这对敏感应用至关重要。方法包括系统评估不同推理设置，涵盖模型大小、分布表达方法和引导方法，在三个任务上共进行了60个实验设置。主要实验结果出人意料地表明，RLVR风格推理会降低分歧建模的性能，而朴素的CoT推理则能提升RLHF大语言模型的性能，这突显了在分歧信息重要时用推理大语言模型替代人类标注者的潜在风险。</div>
</details>
</div>
<div class="card">
<div class="title">Stagewise Reinforcement Learning and the Geometry of the Regret Landscape</div>
<div class="meta-line">Authors: Chris Elliott, Einar Urdshals, David Quarel, Matthew Farrugia-Roberts, Daniel Murfet</div>
<div class="meta-line">First: 2026-01-12T13:25:21+00:00 · Latest: 2026-01-12T13:25:21+00:00</div>
<div class="meta-line">Comments: 50 pages, 14 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07524v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07524v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Singular learning theory characterizes Bayesian learning as an evolving tradeoff between accuracy and complexity, with transitions between qualitatively different solutions as sample size increases. We extend this theory to deep reinforcement learning, proving that the concentration of the generalized posterior over policies is governed by the local learning coefficient (LLC), an invariant of the geometry of the regret function. This theory predicts that Bayesian phase transitions in reinforcement learning should proceed from simple policies with high regret to complex policies with low regret. We verify this prediction empirically in a gridworld environment exhibiting stagewise policy development: phase transitions over SGD training manifest as &quot;opposing staircases&quot; where regret decreases sharply while the LLC increases. Notably, the LLC detects phase transitions even when estimated on a subset of states where the policies appear identical in terms of regret, suggesting it captures changes in the underlying algorithm rather than just performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>阶段性强化学习与遗憾函数的几何景观</div>
<div class="mono" style="margin-top:8px">奇异学习理论将贝叶斯学习描述为准确性与复杂性之间随样本量增加而演变的权衡过程，其间存在不同质解之间的转换。我们将该理论拓展至深度强化学习，证明策略广义后验的集中性由遗憾函数几何的不变量——局部学习系数（LLC）所主导。该理论预测强化学习中的贝叶斯相变应从高遗憾的简单策略向低遗憾的复杂策略演进。我们在呈现阶段性策略发展的网格世界环境中实证验证了这一预测：随机梯度下降训练中的相变表现为“反向阶梯”现象，即遗憾急剧下降的同时LLC上升。值得注意的是，即使在策略遗憾表现相同的状态子集上估计，LLC仍能检测到相变，表明其捕捉的是底层算法的本质变化而非仅性能指标。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by extending singular learning theory, which describes Bayesian learning as a tradeoff between accuracy and complexity with phase transitions, to the domain of deep reinforcement learning (RL). The method involves proving that the concentration of the generalized posterior over policies is governed by a geometric invariant called the local learning coefficient (LLC), derived from the regret function&#x27;s geometry. The main experimental results, validated in a gridworld environment, show stagewise policy development where phase transitions during SGD training appear as &quot;opposing staircases&quot;: regret decreases sharply while the LLC increases, and the LLC detects these transitions even in state subsets where policies have identical regret, indicating it captures algorithmic changes beyond mere performance.</div>
<div class="mono" style="margin-top:8px">本文的动机是将奇异学习理论——该理论将贝叶斯学习描述为准确性与复杂性之间随样本量增加而发生相变的权衡——扩展到深度强化学习领域。方法上，研究证明了策略的广义后验集中性由遗憾函数几何的一个不变量，即局部学习系数（LLC）所主导。主要实验结果在网格世界环境中得到验证，显示了阶段性策略发展：SGD训练中的相变表现为“反向阶梯”，即遗憾急剧下降而LLC上升；值得注意的是，LLC甚至在策略遗憾表现相同的状态子集中也能检测到相变，表明其捕捉的是底层算法的变化，而不仅仅是性能差异。</div>
</details>
</div>
<div class="card">
<div class="title">SegDAC: Improving Visual Reinforcement Learning by Extracting Dynamic Object-Centric Representations from Pretrained Vision Models</div>
<div class="meta-line">Authors: Alexandre Brown, Glen Berseth</div>
<div class="meta-line">First: 2025-08-12T20:16:54+00:00 · Latest: 2026-01-12T13:21:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.09325v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.09325v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://segdac.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual reinforcement learning (RL) is challenging due to the need to extract useful representations from high-dimensional inputs while learning effective control from sparse and noisy rewards. Although large perception models exist, integrating them effectively into RL for visual generalization and improved sample efficiency remains difficult. We propose SegDAC, a Segmentation-Driven Actor-Critic method. SegDAC uses Segment Anything (SAM) for object-centric decomposition and YOLO-World to ground the image segmentation process via text inputs. It includes a novel transformer-based architecture that supports a dynamic number of segments at each time step and effectively learns which segments to focus on using online RL, without using human labels. By evaluating SegDAC over a challenging visual generalization benchmark using Maniskill3, which covers diverse manipulation tasks under strong visual perturbations, we demonstrate that SegDAC achieves significantly better visual generalization, doubling prior performance on the hardest setting and matching or surpassing prior methods in sample efficiency across all evaluated tasks. Project Page: https://segdac.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SegDAC：通过从预训练视觉模型中提取动态以对象为中心的表示来改进视觉强化学习</div>
<div class="mono" style="margin-top:8px">视觉强化学习（RL）面临挑战，因为需要从高维输入中提取有用表示，同时从稀疏且嘈杂的奖励中学习有效控制。尽管存在大型感知模型，但将其有效整合到RL中以实现视觉泛化和提高样本效率仍然困难。我们提出SegDAC，一种分割驱动的行动者-评论家方法。SegDAC使用Segment Anything（SAM）进行以对象为中心的分解，并利用YOLO-World通过文本输入实现图像分割过程的语义落地。该方法包含一种新颖的基于Transformer的架构，支持每个时间步动态数量的分割片段，并通过在线RL有效学习应关注哪些片段，无需使用人工标注。通过在Maniskill3构建的具有挑战性的视觉泛化基准上进行评估（该基准涵盖强视觉扰动下的多样化操作任务），我们证明SegDAC实现了显著更好的视觉泛化能力，在最困难设置下将先前性能提升一倍，并在所有评估任务中达到或超越先前方法的样本效率。项目页面：https://segdac.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge in visual reinforcement learning of extracting useful representations from high-dimensional inputs for effective control under sparse rewards. The proposed method, SegDAC, integrates pretrained vision models by using Segment Anything (SAM) for object-centric decomposition and YOLO-World for text-grounded segmentation, coupled with a novel transformer architecture that dynamically selects relevant segments during online RL without human labels. Experimental results on the Maniskill3 benchmark show that SegDAC significantly improves visual generalization, doubling prior performance in the hardest setting and matching or surpassing existing methods in sample efficiency across diverse manipulation tasks.</div>
<div class="mono" style="margin-top:8px">本文针对视觉强化学习中从高维输入提取有效表征以在稀疏奖励下实现高效控制的挑战，提出SegDAC方法。该方法整合预训练视觉模型，利用Segment Anything（SAM）进行以物体为中心的分解，并通过YOLO-World实现基于文本的语义分割，同时采用一种新颖的Transformer架构，能够在在线强化学习过程中动态选择相关片段而无需人工标注。在Maniskill3基准测试上的实验结果表明，SegDAC显著提升了视觉泛化能力，在最困难场景下性能翻倍，并在所有评估任务中达到或超越了现有方法的样本效率。</div>
</details>
</div>
<div class="card">
<div class="title">Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions</div>
<div class="meta-line">Authors: Yongqi Li, Hao Lang, Tieyun Qian, Yongbin Li</div>
<div class="meta-line">First: 2026-01-12T13:13:24+00:00 · Latest: 2026-01-12T13:13:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07516v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07516v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models are increasingly employed as multimodal conversational agents (MCAs) for diverse conversational tasks. Recently, reinforcement learning (RL) has been widely explored for adapting MCAs to various human-AI interaction scenarios. Despite showing great enhancement in generalization performance, fine-tuning MCAs via RL still faces challenges in handling the extremely large text token space. To address this, we learn a compact latent action space for RL fine-tuning instead. Specifically, we adopt the learning from observation mechanism to construct the codebook for the latent action space, where future observations are leveraged to estimate current latent actions that could further be used to reconstruct future observations. However, the scarcity of paired image-text data hinders learning a codebook with sufficient coverage. Thus, we leverage both paired image-text data and text-only data to construct the latent action space, using a cross-modal projector for transforming text embeddings into image-text embeddings. We initialize the cross-modal projector on paired image-text data, and further train it on massive text-only data with a novel cycle consistency loss to enhance its robustness. We show that our latent action based method outperforms competitive baselines on two conversation tasks across various RL algorithms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于覆盖增强隐式动作的多模态对话智能体控制</div>
<div class="mono" style="margin-top:8px">视觉语言模型正日益作为多模态对话智能体应用于多样化对话任务。近期，强化学习被广泛探索用于使多模态对话智能体适应不同的人机交互场景。尽管在泛化性能上展现出显著提升，但通过强化学习微调多模态对话智能体仍面临处理极大文本标记空间的挑战。为此，我们转而学习紧凑的隐式动作空间进行强化学习微调。具体而言，采用观察学习机制构建隐式动作空间的码本，利用未来观测值估计当前隐式动作，该动作可进一步用于重建未来观测值。然而，配对图文数据的稀缺性阻碍了学习具有充分覆盖度的码本。因此，我们同时利用配对图文数据与纯文本数据构建隐式动作空间，通过跨模态投影器将文本嵌入转换为图文嵌入。我们在配对图文数据上初始化跨模态投影器，并基于新型循环一致性损失在大量纯文本数据上进一步训练以增强其鲁棒性。实验表明，基于隐式动作的方法在两种对话任务中均优于多种强化学习算法的基线模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the challenge of fine-tuning multimodal conversational agents (MCAs) via reinforcement learning (RL), which struggles with the large text token space. The method involves learning a compact latent action space for RL by constructing a codebook using a learning-from-observation mechanism, where future observations estimate current latent actions to reconstruct those observations; to enhance coverage, it leverages both paired image-text and text-only data with a cross-modal projector trained via cycle consistency loss. The main experimental results show that this latent action-based approach outperforms competitive baselines on two conversation tasks across various RL algorithms.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决通过强化学习微调多模态对话代理时面临的大规模文本标记空间挑战。方法上，通过从观察中学习的机制构建潜在动作空间的码本，利用未来观察估计当前潜在动作以重建观察；为提升覆盖度，结合配对图像-文本和纯文本数据，使用跨模态投影器并通过循环一致性损失增强其鲁棒性。主要实验结果表明，该基于潜在动作的方法在两种对话任务上优于多种强化学习算法的基线模型。</div>
</details>
</div>
<div class="card">
<div class="title">Graph Inference Towards ICD Coding</div>
<div class="meta-line">Authors: Xiaoxiao Deng</div>
<div class="meta-line">First: 2026-01-12T12:51:21+00:00 · Latest: 2026-01-12T12:51:21+00:00</div>
<div class="meta-line">Comments: 6 pages, 2 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07496v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07496v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated ICD coding involves assigning standardized diagnostic codes to clinical narratives. The vast label space and extreme class imbalance continue to challenge precise prediction. To address these issues, LabGraph is introduced -- a unified framework that reformulates ICD coding as a graph generation task. By combining adversarial domain adaptation, graph-based reinforcement learning, and perturbation regularization, LabGraph effectively enhances model robustness and generalization. In addition, a label graph discriminator dynamically evaluates each generated code, providing adaptive reward feedback during training. Experiments on benchmark datasets demonstrate that LabGraph consistently outperforms previous approaches on micro-F1, micro-AUC, and P@K.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向ICD编码的图推理方法</div>
<div class="mono" style="margin-top:8px">自动化ICD编码旨在为临床叙述分配标准化诊断代码。庞大的标签空间与极端的类别不平衡仍是精准预测的挑战。为此，本文提出LabGraph——一个将ICD编码重构为图生成任务的统一框架。该框架融合对抗性领域适应、基于图的强化学习与扰动正则化技术，有效提升了模型的鲁棒性与泛化能力。此外，标签图判别器能动态评估每个生成的代码，在训练过程中提供自适应奖励反馈。在基准数据集上的实验表明，LabGraph在micro-F1、micro-AUC和P@K指标上均持续优于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to improve automated ICD coding, which assigns diagnostic codes to clinical narratives, by addressing challenges like the vast label space and extreme class imbalance. The method introduces LabGraph, a unified framework that reformulates ICD coding as a graph generation task, combining adversarial domain adaptation, graph-based reinforcement learning, and perturbation regularization to enhance robustness and generalization, with a label graph discriminator providing adaptive reward feedback. Main experimental results on benchmark datasets show that LabGraph consistently outperforms previous approaches in metrics such as micro-F1, micro-AUC, and P@K.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过解决标签空间庞大和类别极度不平衡等挑战，改进自动化ICD编码，即向临床叙述分配标准化诊断代码。方法上提出了LabGraph，这是一个将ICD编码重新定义为图生成任务的统一框架，结合了对抗性领域适应、基于图的强化学习和扰动正则化以增强鲁棒性和泛化能力，并使用标签图判别器提供自适应奖励反馈。在基准数据集上的主要实验结果表明，LabGraph在micro-F1、micro-AUC和P@K等指标上持续优于先前方法。</div>
</details>
</div>
<div class="card">
<div class="title">Bridging the Linguistic Divide: A Survey on Leveraging Large Language Models for Machine Translation</div>
<div class="meta-line">Authors: Baban Gain, Dibyanayan Bandyopadhyay, Asif Ekbal, Trilok Nath Singh</div>
<div class="meta-line">First: 2025-04-02T17:26:40+00:00 · Latest: 2026-01-12T12:46:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.01919v4">Abs</a> · <a href="https://arxiv.org/pdf/2504.01919v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) are rapidly reshaping machine translation (MT), particularly by introducing instruction-following, in-context learning, and preference-based alignment into what has traditionally been a supervised encoder-decoder paradigm. This survey provides a comprehensive and up-to-date overview of how LLMs are being leveraged for MT across data regimes, languages, and application settings. We systematically analyze prompting-based methods, parameter-efficient and full fine-tuning strategies, synthetic data generation, preference-based optimization, and reinforcement learning with human and weakly supervised feedback. Special attention is given to low-resource translation, where we examine the roles of synthetic data quality, diversity, and preference signals, as well as the limitations of current RLHF pipelines. We further review recent advances in Mixture-of-Experts models, MT-focused LLMs, and multilingual alignment, highlighting trade-offs between scalability, specialization, and accessibility. Beyond sentence-level translation, we survey emerging document-level and discourse-aware MT methods with LLMs, showing that most approaches extend sentence-level pipelines through structured context selection, post-editing, or reranking rather than requiring fundamentally new data regimes or architectures. Finally, we discuss LLM-based evaluation, its strengths and biases, and its role alongside learned metrics. Overall, this survey positions LLM-based MT as an evolution of traditional MT systems, where gains increasingly depend on data quality, preference alignment, and context utilization rather than scale alone, and outlines open challenges for building robust, inclusive, and controllable translation systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>弥合语言鸿沟：基于大语言模型的机器翻译研究综述</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）正迅速重塑机器翻译（MT）领域，尤其通过引入指令跟随、上下文学习和基于偏好的对齐机制，对传统的有监督编码器-解码器范式形成革新。本综述全面且及时地概述了LLMs在不同数据体系、语言和应用场景中如何赋能机器翻译。我们系统分析了基于提示的方法、参数高效与全参数微调策略、合成数据生成、基于偏好的优化，以及结合人类反馈与弱监督反馈的强化学习。特别关注低资源翻译场景，深入探讨合成数据的质量、多样性与偏好信号的作用，以及当前RLHF流程的局限性。进一步回顾了专家混合模型、面向MT的专用LLMs和多语言对齐的最新进展，揭示可扩展性、专业性与可访问性之间的权衡关系。除句子级翻译外，本文还综述了新兴的文档级与语篇感知的LLM翻译方法，指出多数方法通过结构化上下文选择、后编辑或重排序来扩展句子级流程，而非依赖全新的数据体系或架构。最后，我们讨论了基于LLM的评估方法及其优势与偏差，以及其与学习型指标的协同作用。总体而言，本综述将基于LLM的机器翻译定位为传统MT系统的演进方向，其性能提升日益依赖于数据质量、偏好对齐和上下文利用而非单纯规模扩张，并展望了构建鲁棒、包容且可控的翻译系统所面临的开放挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This survey is motivated by the rapid integration of Large Language Models (LLMs) into machine translation, which introduces capabilities like instruction-following and in-context learning to the traditional supervised paradigm. The method involves a systematic analysis of prompting, fine-tuning strategies, synthetic data generation, and preference-based optimization, with special attention to low-resource settings and emerging areas like document-level translation. The main experimental results and findings highlight that gains in LLM-based translation increasingly depend on data quality, preference alignment, and context utilization rather than scale alone, while also outlining challenges in robustness and inclusivity.</div>
<div class="mono" style="margin-top:8px">本综述的动机在于大型语言模型（LLM）正快速重塑机器翻译领域，将指令跟随、上下文学习等能力引入传统的监督式编码器-解码器范式。方法上，系统分析了提示工程、参数高效与全参数微调、合成数据生成、基于偏好的优化等技术，并特别关注低资源翻译及文档级翻译等新兴方向。主要实验结果表明，基于LLM的翻译效果提升越来越依赖于数据质量、偏好对齐和上下文利用而非单纯模型规模，同时指出了在构建鲁棒、包容和可控翻译系统方面存在的开放挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Puzzle it Out: Local-to-Global World Model for Offline Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Sijia li, Xinran Li, Shibo Chen, Jun Zhang</div>
<div class="meta-line">First: 2026-01-12T12:17:11+00:00 · Latest: 2026-01-12T12:17:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07463v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07463v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline multi-agent reinforcement learning (MARL) aims to solve cooperative decision-making problems in multi-agent systems using pre-collected datasets. Existing offline MARL methods primarily constrain training within the dataset distribution, resulting in overly conservative policies that struggle to generalize beyond the support of the data. While model-based approaches offer a promising solution by expanding the original dataset with synthetic data generated from a learned world model, the high dimensionality, non-stationarity, and complexity of multi-agent systems make it challenging to accurately estimate the transitions and reward functions in offline MARL. Given the difficulty of directly modeling joint dynamics, we propose a local-to-global (LOGO) world model, a novel framework that leverages local predictions-which are easier to estimate-to infer global state dynamics, thus improving prediction accuracy while implicitly capturing agent-wise dependencies. Using the trained world model, we generate synthetic data to augment the original dataset, expanding the effective state-action space. To ensure reliable policy learning, we further introduce an uncertainty-aware sampling mechanism that adaptively weights synthetic data by prediction uncertainty, reducing approximation error propagation to policies. In contrast to conventional ensemble-based methods, our approach requires only an additional encoder for uncertainty estimation, significantly reducing computational overhead while maintaining accuracy. Extensive experiments across 8 scenarios against 8 baselines demonstrate that our method surpasses state-of-the-art baselines on standard offline MARL benchmarks, establishing a new model-based baseline for generalizable offline multi-agent learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>拼图求解：面向离线多智能体强化学习的局部到全局世界模型</div>
<div class="mono" style="margin-top:8px">离线多智能体强化学习（MARL）旨在利用预收集数据集解决多智能体系统中的协作决策问题。现有离线MARL方法主要将训练约束在数据集分布内，导致策略过于保守，难以泛化至数据支持范围之外。尽管基于模型的方法通过学习的世界模型生成合成数据以扩展原始数据集，提供了有前景的解决方案，但多智能体系统的高维性、非平稳性和复杂性使得准确估计离线MARL中的状态转移和奖励函数极具挑战。鉴于直接建模联合动力学十分困难，我们提出局部到全局（LOGO）世界模型，该新颖框架利用更易估计的局部预测来推断全局状态动力学，从而在隐式捕获智能体间依赖关系的同时提升预测精度。借助训练后的世界模型，我们生成合成数据以增强原始数据集，扩展有效的状态-动作空间。为确保策略学习的可靠性，我们进一步引入不确定性感知采样机制，通过预测不确定性自适应加权合成数据，减少近似误差向策略的传播。与传统的基于集成的方法相比，本方法仅需一个额外的编码器进行不确定性估计，在保持精度的同时显著降低计算开销。在8个场景中与8个基线方法的广泛实验表明，本方法在标准离线MARL基准测试中超越了现有最优基线，为可泛化的离线多智能体学习建立了新的基于模型基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the overly conservative policies in offline multi-agent reinforcement learning (MARL) that arise from strict dataset constraints, which limit generalization beyond the data support. The method introduces a local-to-global (LOGO) world model, which improves prediction accuracy by inferring global state dynamics from easier-to-estimate local predictions, implicitly capturing agent dependencies, and augments the dataset with synthetic data while using an uncertainty-aware sampling mechanism to weight synthetic data based on prediction uncertainty, reducing computational overhead compared to ensemble methods. Experimental results across 8 scenarios against 8 baselines show that this approach outperforms state-of-the-art methods on standard offline MARL benchmarks, establishing a new model-based baseline for generalizable learning.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决离线多智能体强化学习中因严格数据集约束导致的策略过于保守、难以泛化到数据支持范围之外的问题。方法上提出了一种局部到全局的世界模型，通过从易于估计的局部预测推断全局状态动态来提高预测准确性，隐含捕捉智能体间依赖关系，并利用合成数据增强原始数据集，同时引入不确定性感知采样机制根据预测不确定性自适应加权合成数据，相比集成方法降低了计算开销。实验结果表明，在8个场景中与8个基线方法对比，该方法在标准离线多智能体强化学习基准测试中超越了现有最优方法，为可泛化的离线多智能体学习建立了新的基于模型的基准。</div>
</details>
</div>
<div class="card">
<div class="title">SPEC-RL: Accelerating On-Policy Reinforcement Learning with Speculative Rollouts</div>
<div class="meta-line">Authors: Bingshuai Liu, Ante Wang, Zijun Min, Liang Yao, Haibo Zhang, Yang Liu, Xu Han, Peng Li, Anxiang Zeng, Jinsong Su</div>
<div class="meta-line">First: 2025-09-27T10:32:34+00:00 · Latest: 2026-01-12T11:06:46+00:00</div>
<div class="meta-line">Comments: fixed typos</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23232v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.23232v3">PDF</a> · <a href="https://github.com/ShopeeLLM/Spec-RL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) increasingly rely on reinforcement learning with verifiable rewards (RLVR) to elicit reliable chain-of-thought reasoning. However, the training process remains bottlenecked by the computationally expensive rollout stage. Existing acceleration methods-such as parallelization, objective- and data-driven modifications, and replay buffers-either incur diminishing returns, introduce bias, or overlook redundancy across iterations. We identify that rollouts from consecutive training epochs frequently share a large portion of overlapping segments, wasting computation. To address this, we propose SPEC-RL, a novel framework that integrates SPECulative decoding with the RL rollout process. SPEC-RL reuses prior trajectory segments as speculative prefixes and extends them via a draft-and-verify mechanism, avoiding redundant generation while ensuring policy consistency. Experiments on diverse math reasoning and generalization benchmarks, including AIME24, MATH-500, OlympiadBench, MMLU-STEM, and others, demonstrate that SPEC-RL reduces rollout time by 2-3x without compromising policy quality. As a purely rollout-stage enhancement, SPEC-RL integrates seamlessly with mainstream algorithms (e.g., PPO, GRPO, DAPO), offering a general and practical path to scale RLVR for large reasoning models. Our code is available at https://github.com/ShopeeLLM/Spec-RL</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPEC-RL：通过推测式轨迹展开加速同策略强化学习</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）日益依赖可验证奖励的强化学习（RLVR）来激发可靠的思维链推理。然而，训练过程仍受限于计算成本高昂的轨迹展开阶段。现有加速方法（如并行化、目标与数据驱动的修改、经验回放缓冲区）要么收益递减，要么引入偏差，或忽略了迭代间的冗余。我们发现连续训练周期的轨迹展开常存在大量重叠片段，造成计算浪费。为此，我们提出SPEC-RL——一种将推测式解码与RL轨迹展开相结合的新框架。该框架复用先前的轨迹片段作为推测前缀，通过起草-验证机制进行扩展，在确保策略一致性的同时避免冗余生成。在包括AIME24、MATH-500、OlympiadBench、MMLU-STEM等多样化数学推理与泛化基准测试中，实验表明SPEC-RL能在不降低策略质量的前提下将展开时间缩短2-3倍。作为纯轨迹展开阶段的增强方案，SPEC-RL可无缝集成主流算法（如PPO、GRPO、DAPO），为大规模推理模型的RLVR扩展提供了通用且实用的路径。代码已开源：https://github.com/ShopeeLLM/Spec-RL</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for SPEC-RL stems from the computational bottleneck in reinforcement learning with verifiable rewards (RLVR) for large language models, where traditional rollout acceleration methods face diminishing returns, bias, or inefficiency due to redundant trajectory segments across training epochs. The method introduces a speculative decoding approach integrated into RL rollouts, reusing prior trajectory segments as speculative prefixes and extending them through a draft-and-verify mechanism to avoid redundant generation while maintaining policy consistency. Experimental results on benchmarks like AIME24, MATH-500, and MMLU-STEM show that SPEC-RL reduces rollout time by 2-3 times without compromising policy quality, and it seamlessly integrates with algorithms such as PPO and GRPO as a general rollout-stage enhancement.</div>
<div class="mono" style="margin-top:8px">SPEC-RL的动机源于大型语言模型在可验证奖励强化学习中存在的计算瓶颈，传统加速方法因训练迭代间轨迹段冗余而面临收益递减、偏差或效率低下问题。该方法将推测性解码集成到强化学习rollout过程中，重用先前的轨迹段作为推测前缀，并通过草稿-验证机制进行扩展，以避免冗余生成并保持策略一致性。在AIME24、MATH-500和MMLU-STEM等基准测试中的实验结果表明，SPEC-RL将rollout时间减少了2-3倍，且不损害策略质量，并能作为通用的rollout阶段增强无缝集成到PPO、GRPO等主流算法中。</div>
</details>
</div>
<div class="card">
<div class="title">BiTrajDiff: Bidirectional Trajectory Generation with Diffusion Models for Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Yunpeng Qing, Yixiao Chi, Shuo Chen, Shunyu Liu, Kelu Yao, Sixu Lin, Litao Liu, Changqing Zou</div>
<div class="meta-line">First: 2025-06-06T05:41:33+00:00 · Latest: 2026-01-12T10:54:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.05762v4">Abs</a> · <a href="https://arxiv.org/pdf/2506.05762v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in offline Reinforcement Learning (RL) have proven that effective policy learning can benefit from imposing conservative constraints on pre-collected datasets. However, such static datasets often exhibit distribution bias, resulting in limited generalizability. To address this limitation, a straightforward solution is data augmentation (DA), which leverages generative models to enrich data distribution. Despite the promising results, current DA techniques focus solely on reconstructing future trajectories from given states, while ignoring the exploration of history transitions that reach them. This single-direction paradigm inevitably hinders the discovery of diverse behavior patterns, especially those leading to critical states that may have yielded high-reward outcomes. In this work, we introduce Bidirectional Trajectory Diffusion (BiTrajDiff), a novel DA framework for offline RL that models both future and history trajectories from any intermediate states. Specifically, we decompose the trajectory generation task into two independent yet complementary diffusion processes: one generating forward trajectories to predict future dynamics, and the other generating backward trajectories to trace essential history transitions.BiTrajDiff can efficiently leverage critical states as anchors to expand into potentially valuable yet underexplored regions of the state space, thereby facilitating dataset diversity. Extensive experiments on the D4RL benchmark suite demonstrate that BiTrajDiff achieves superior performance compared to other advanced DA methods across various offline RL backbones.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BiTrajDiff：基于扩散模型的双向轨迹生成用于离线强化学习</div>
<div class="mono" style="margin-top:8px">离线强化学习的最新进展表明，对预收集数据集施加保守约束有助于有效策略学习。然而，此类静态数据集常存在分布偏差，导致泛化能力受限。为解决此问题，一种直接方案是数据增强，即利用生成模型丰富数据分布。尽管现有方法效果显著，但当前数据增强技术仅关注从给定状态重构未来轨迹，而忽略了探索到达这些状态的历史转移。这种单向范式不可避免地限制了多样化行为模式的发现，尤其是那些可能导致高回报结果的关键状态。本文提出双向轨迹扩散，这是一种用于离线强化学习的新型数据增强框架，可从任意中间状态同时建模未来与历史轨迹。具体而言，我们将轨迹生成任务分解为两个独立且互补的扩散过程：一个生成前向轨迹以预测未来动态，另一个生成后向轨迹以追溯关键历史转移。BiTrajDiff能高效利用关键状态作为锚点，向状态空间中潜在有价值但未充分探索的区域扩展，从而提升数据集多样性。在D4RL基准测试集上的大量实验表明，相较于其他先进数据增强方法，BiTrajDiff在多种离线强化学习骨干模型上均取得更优性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces BiTrajDiff, a data augmentation framework for offline reinforcement learning motivated by the need to overcome distribution bias in static datasets, which limits policy generalizability. The method employs bidirectional diffusion models to generate both future and historical trajectories from any intermediate state, thereby expanding dataset diversity by exploring critical states and their surrounding transitions. Experimental results on the D4RL benchmark show that BiTrajDiff outperforms existing data augmentation techniques across multiple offline RL algorithms.</div>
<div class="mono" style="margin-top:8px">本文提出了BiTrajDiff，一种用于离线强化学习的数据增强框架，其动机是解决静态数据集中存在的分布偏差问题，该偏差限制了策略的泛化能力。该方法采用双向扩散模型，从任意中间状态生成未来和历史轨迹，从而通过探索关键状态及其周围转移来增加数据集的多样性。在D4RL基准测试上的实验结果表明，BiTrajDiff在多种离线RL算法中优于现有的数据增强方法。</div>
</details>
</div>
<div class="card">
<div class="title">Outcome-Grounded Advantage Reshaping for Fine-Grained Credit Assignment in Mathematical Reasoning</div>
<div class="meta-line">Authors: Ziheng Li, Liu Kang, Feng Xiao, Luxi Xing, Qingyi Si, Zhuoran Li, Weikang Gong, Deqing Yang, Yanghua Xiao, Hongcheng Guo</div>
<div class="meta-line">First: 2026-01-12T10:48:02+00:00 · Latest: 2026-01-12T10:48:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07408v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07408v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Group Relative Policy Optimization (GRPO) has emerged as a promising critic-free reinforcement learning paradigm for reasoning tasks. However, standard GRPO employs a coarse-grained credit assignment mechanism that propagates group-level rewards uniformly to to every token in a sequence, neglecting the varying contribution of individual reasoning steps. We address this limitation by introducing Outcome-grounded Advantage Reshaping (OAR), a fine-grained credit assignment mechanism that redistributes advantages based on how much each token influences the model&#x27;s final answer. We instantiate OAR via two complementary strategies: (1) OAR-P, which estimates outcome sensitivity through counterfactual token perturbations, serving as a high-fidelity attribution signal; (2) OAR-G, which uses an input-gradient sensitivity proxy to approximate the influence signal with a single backward pass. These importance signals are integrated with a conservative Bi-Level advantage reshaping scheme that suppresses low-impact tokens and boosts pivotal ones while preserving the overall advantage mass. Empirical results on extensive mathematical reasoning benchmarks demonstrate that while OAR-P sets the performance upper bound, OAR-G achieves comparable gains with negligible computational overhead, both significantly outperforming a strong GRPO baseline, pushing the boundaries of critic-free LLM reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于结果的优势重塑机制在数学推理中的细粒度信用分配研究</div>
<div class="mono" style="margin-top:8px">群体相对策略优化（GRPO）已成为推理任务中一种前景广阔的无评论者强化学习范式。然而，标准GRPO采用粗粒度信用分配机制，将群体层面的奖励均匀传播给序列中的每个标记，忽略了各推理步骤的差异性贡献。我们通过引入基于结果的优势重塑（OAR）机制来解决这一局限，该细粒度信用分配机制根据每个标记对模型最终答案的影响程度重新分配优势值。我们通过两种互补策略实现OAR：（1）OAR-P通过反事实标记扰动估计结果敏感性，作为高保真归因信号；（2）OAR-G使用输入梯度敏感性代理，通过单次反向传播近似影响信号。这些重要性信号与保守的双层优势重塑方案相结合，在保持总体优势质量的同时抑制低影响标记并增强关键标记。大量数学推理基准测试的实证结果表明：OAR-P设定了性能上限，而OAR-G以可忽略的计算开销实现了可比增益，两者均显著优于强GRPO基线，推动了无评论者大语言模型推理的边界。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the coarse-grained credit assignment in Group Relative Policy Optimization (GRPO) for reasoning tasks, which uniformly rewards all tokens and overlooks individual step contributions. The method introduces Outcome-grounded Advantage Reshaping (OAR), a fine-grained mechanism that redistributes advantages based on each token&#x27;s influence on the final answer, implemented via two strategies: OAR-P uses counterfactual token perturbations for high-fidelity attribution, while OAR-G employs an input-gradient proxy for efficient approximation, both integrated with a conservative Bi-Level reshaping scheme. Main experimental results on mathematical reasoning benchmarks show that OAR-P sets a performance upper bound, OAR-G achieves comparable gains with minimal computational cost, and both significantly outperform the GRPO baseline, advancing critic-free LLM reasoning.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决数学推理任务中组相对策略优化（GRPO）的粗粒度信用分配问题，该方法均匀奖励所有标记而忽略了个别推理步骤的贡献。方法上提出了基于结果的优势重塑（OAR），这是一种细粒度信用分配机制，根据每个标记对最终答案的影响重新分配优势，通过两种策略实现：OAR-P利用反事实标记扰动提供高保真归因信号，而OAR-G使用输入梯度敏感性代理以单次反向传播近似影响信号，两者均与保守的双层优势重塑方案结合。主要实验结果在广泛的数学推理基准测试中表明，OAR-P设定了性能上限，OAR-G以可忽略的计算开销实现了可比增益，两者均显著优于强GRPO基线，推动了无批评者LLM推理的边界。</div>
</details>
</div>
<div class="card">
<div class="title">MCP-ITP: An Automated Framework for Implicit Tool Poisoning in MCP</div>
<div class="meta-line">Authors: Ruiqi Li, Zhiqiang Wang, Yunhao Yao, Xiang-Yang Li</div>
<div class="meta-line">First: 2026-01-12T10:28:46+00:00 · Latest: 2026-01-12T10:28:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07395v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07395v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To standardize interactions between LLM-based agents and their environments, the Model Context Protocol (MCP) was proposed and has since been widely adopted. However, integrating external tools expands the attack surface, exposing agents to tool poisoning attacks. In such attacks, malicious instructions embedded in tool metadata are injected into the agent context during MCP registration phase, thereby manipulating agent behavior. Prior work primarily focuses on explicit tool poisoning or relied on manually crafted poisoned tools. In contrast, we focus on a particularly stealthy variant: implicit tool poisoning, where the poisoned tool itself remains uninvoked. Instead, the instructions embedded in the tool metadata induce the agent to invoke a legitimate but high-privilege tool to perform malicious operations. We propose MCP-ITP, the first automated and adaptive framework for implicit tool poisoning within the MCP ecosystem. MCP-ITP formulates poisoned tool generation as a black-box optimization problem and employs an iterative optimization strategy that leverages feedback from both an evaluation LLM and a detection LLM to maximize Attack Success Rate (ASR) while evading current detection mechanisms. Experimental results on the MCPTox dataset across 12 LLM agents demonstrate that MCP-ITP consistently outperforms the manually crafted baseline, achieving up to 84.2% ASR while suppressing the Malicious Tool Detection Rate (MDR) to as low as 0.3%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MCP-ITP：一种针对MCP中隐式工具投毒的自动化框架</div>
<div class="mono" style="margin-top:8px">为规范基于LLM的智能体与环境间的交互，模型上下文协议（MCP）被提出并已广泛采用。然而，集成外部工具扩展了攻击面，使智能体面临工具投毒攻击。在此类攻击中，恶意指令通过工具元数据在MCP注册阶段注入智能体上下文，从而操控其行为。现有研究主要关注显式工具投毒或依赖人工构造的投毒工具。相比之下，我们聚焦于一种更具隐蔽性的变体：隐式工具投毒——投毒工具本身未被调用，而是通过其元数据中嵌入的指令诱导智能体调用合法但高权限工具执行恶意操作。我们提出MCP-ITP，这是首个针对MCP生态系统的自动化自适应隐式工具投毒框架。该框架将投毒工具生成建模为黑盒优化问题，采用迭代优化策略，结合评估LLM和检测LLM的反馈，在规避现有检测机制的同时最大化攻击成功率（ASR）。在MCPTox数据集上对12个LLM智能体的实验表明，MCP-ITP始终优于人工构造的基线方法，最高可实现84.2%的ASR，同时将恶意工具检测率（MDR）压制至0.3%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the widespread adoption of the Model Context Protocol (MCP) for standardizing LLM agent-environment interactions and the expanded attack surface from external tool integration, this paper addresses the stealthy threat of implicit tool poisoning, where malicious metadata induces agents to misuse legitimate high-privilege tools without invoking the poisoned tool itself. The method introduces MCP-ITP, an automated framework that formulates poisoned tool generation as a black-box optimization problem, employing an iterative strategy with feedback from evaluation and detection LLMs to maximize attack success while evading detection. Experimental results on the MCPTox dataset across 12 LLM agents show that MCP-ITP outperforms manual baselines, achieving up to 84.2% Attack Success Rate while reducing Malicious Tool Detection Rate to as low as 0.3%.</div>
<div class="mono" style="margin-top:8px">本文的动机源于模型上下文协议（MCP）在标准化基于LLM的智能体与环境交互中的广泛应用，以及外部工具集成所扩大的攻击面，重点关注一种隐蔽的隐式工具投毒攻击，即恶意工具元数据诱导智能体误用合法高权限工具，而无需调用投毒工具本身。方法上提出了MCP-ITP，这是一个自动化框架，将投毒工具生成视为黑盒优化问题，采用迭代优化策略，结合评估和检测LLM的反馈，以在规避现有检测机制的同时最大化攻击成功率。在MCPTox数据集上对12个LLM智能体的实验结果表明，MCP-ITP持续优于手动构建的基线方法，攻击成功率最高达84.2%，同时将恶意工具检测率压制至最低0.3%。</div>
</details>
</div>
<div class="card">
<div class="title">On the Non-decoupling of Supervised Fine-tuning and Reinforcement Learning in Post-training</div>
<div class="meta-line">Authors: Xueyan Niu, Bo Bai, Wei Han, Weixi Zhang</div>
<div class="meta-line">First: 2026-01-12T10:14:09+00:00 · Latest: 2026-01-12T10:14:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07389v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07389v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Post-training of large language models routinely interleaves supervised fine-tuning (SFT) with reinforcement learning (RL). These two methods have different objectives: SFT minimizes the cross-entropy loss between model outputs and expert responses, while RL maximizes reward signals derived from human preferences or rule-based verifiers. Modern reasoning models have widely adopted the practice of alternating SFT and RL training. However, there is no theoretical account of whether they can be decoupled. We prove that decoupling is impossible in either order: (1) SFT-then-RL coupling: RL increases SFT loss under SFT optimality and (2) RL-then-SFT coupling: SFT lowers the reward achieved by RL. Experiments on Qwen3-0.6B confirm the predicted degradation, verifying that SFT and RL cannot be separated without loss of prior performance in the post-training</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>论后训练中监督微调与强化学习的不可解耦性</div>
<div class="mono" style="margin-top:8px">大语言模型的后训练通常交替进行监督微调（SFT）与强化学习（RL）。这两种方法目标不同：SFT最小化模型输出与专家响应间的交叉熵损失，而RL最大化基于人类偏好或规则验证器的奖励信号。现代推理模型广泛采用交替进行SFT与RL训练的做法，但二者能否解耦尚无理论依据。我们证明两种顺序均无法解耦：（1）先SFT后RL的耦合：在SFT最优条件下，RL会增加SFT损失；（2）先RL后SFT的耦合：SFT会降低RL已获得的奖励。基于Qwen3-0.6B的实验证实了预测的性能下降，验证了后训练中若分离SFT与RL将不可避免地损失已有性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the theoretical and practical interdependence of supervised fine-tuning (SFT) and reinforcement learning (RL) during the post-training of large language models, motivated by the common practice of alternating these methods despite their differing objectives—SFT minimizes cross-entropy loss on expert data, while RL maximizes external reward signals. The authors prove that these stages are fundamentally coupled and cannot be decoupled without performance loss: specifically, RL applied after SFT increases the SFT loss under optimal SFT conditions, and SFT applied after RL reduces the reward optimized by RL. Experimental validation using the Qwen3-0.6B model confirms the predicted performance degradation, demonstrating that separating SFT and RL in post-training pipelines compromises prior gains from each stage.</div>
<div class="mono" style="margin-top:8px">本文研究了大语言模型后训练中监督微调（SFT）与强化学习（RL）的理论与实践相互依赖性，其动机在于尽管两者目标不同——SFT最小化专家数据的交叉熵损失，而RL最大化外部奖励信号——但交替使用这两种方法已成为常见做法。作者证明这两个阶段本质上是耦合的，无法分离而不损失性能：具体而言，在SFT之后应用RL会在SFT最优条件下增加SFT损失，而在RL之后应用SFT则会降低RL所优化的奖励。使用Qwen3-0.6B模型进行的实验验证了预测的性能下降，表明在后训练流程中将SFT与RL分离会损害各自阶段先前取得的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">OpenTinker: Separating Concerns in Agentic Reinforcement Learning</div>
<div class="meta-line">Authors: Siqi Zhu, Jiaxuan You</div>
<div class="meta-line">First: 2026-01-12T09:57:46+00:00 · Latest: 2026-01-12T09:57:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07376v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07376v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce OpenTinker, an infrastructure for reinforcement learning (RL) of large language model (LLM) agents built around a separation of concerns across algorithm design, execution, and agent-environment interaction. Rather than relying on monolithic, end-to-end RL pipelines, OpenTinker decomposes agentic learning systems into lightweight, composable components with clearly defined abstraction boundaries. Users specify agents, environments, and interaction protocols, while inference and training are delegated to a managed execution runtime. OpenTinker introduces a centralized scheduler for managing training and inference workloads, including LoRA-based and full-parameter RL, supervised fine-tuning, and inference, over shared resources. We further discuss design principles for extending OpenTinker to multi-agent training. Finally, we present a set of RL use cases that demonstrate the effectiveness of the framework in practical agentic learning scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OpenTinker：智能体强化学习中的关注点分离架构</div>
<div class="mono" style="margin-top:8px">本文提出OpenTinker——一个围绕算法设计、执行与智能体-环境交互的关注点分离原则构建的大语言模型智能体强化学习基础设施。该框架摒弃传统端到端的单体式强化学习流程，将智能体学习系统解耦为具有明确定义抽象边界的轻量级可组合模块。用户仅需定义智能体、环境及交互协议，而推理与训练任务则由托管执行运行时统一调度。OpenTinker引入集中式调度器，可在共享资源上协调基于LoRA/全参数的强化学习、监督微调及推理等混合工作负载。本文进一步探讨了将该框架扩展至多智能体训练的设计原则，并通过系列强化学习应用案例验证了其在实践场景中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind OpenTinker is to address the complexity of monolithic, end-to-end pipelines in agentic reinforcement learning by proposing a modular infrastructure that separates concerns across algorithm design, execution, and agent-environment interaction. Its method involves decomposing learning systems into lightweight, composable components with clear abstraction boundaries, where users specify agents and environments while delegating inference and training to a managed runtime with a centralized scheduler for workloads like LoRA-based RL and supervised fine-tuning. The main experimental results demonstrate the framework&#x27;s effectiveness through a set of practical RL use cases, showing its capability in handling diverse agentic learning scenarios and extending to multi-agent training.</div>
<div class="mono" style="margin-top:8px">OpenTinker的动机是解决智能体强化学习中端到端整体式流程的复杂性，通过提出一种模块化基础设施，将算法设计、执行和智能体-环境交互的关注点分离。其方法是将学习系统分解为具有清晰抽象边界的轻量级可组合组件，用户指定智能体和环境，同时将推理和训练委托给一个托管运行时，该运行时配备集中式调度器来管理如基于LoRA的强化学习和监督微调等工作负载。主要实验结果通过一系列实际强化学习用例展示了该框架的有效性，证明了其处理多样化智能体学习场景并扩展至多智能体训练的能力。</div>
</details>
</div>
<div class="card">
<div class="title">GlobalRAG: Enhancing Global Reasoning in Multi-hop Question Answering via Reinforcement Learning</div>
<div class="meta-line">Authors: Jinchang Luo, Mingquan Cheng, Fan Wan, Ni Li, Xiaoling Xia, Shuangshuang Tian, Tingcheng Bian, Haiwei Wang, Haohuan Fu, Yan Tao</div>
<div class="meta-line">First: 2025-10-23T13:35:02+00:00 · Latest: 2026-01-12T09:03:57+00:00</div>
<div class="meta-line">Comments: 8 pages, 3 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.20548v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.20548v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning has recently shown promise in improving retrieval-augmented generation (RAG). Despite these advances, its effectiveness in multi-hop question answering (QA) remains limited by two fundamental limitations: (i) global planning absence to structure multi-step reasoning, and (ii) unfaithful execution, which hinders effective query formulation and consistent use of retrieved evidence. We propose GlobalRAG, a reinforcement learning framework designed to enhance global reasoning in multi-hop QA. GlobalRAG decomposes questions into subgoals, coordinates retrieval with reasoning, and refines evidence iteratively. To guide this process, we introduce Planning Quality Reward and SubGoal Completion Reward, which encourage coherent planning and reliable subgoal execution. In addition, a progressive weight annealing strategy balances process-oriented and outcome-based objectives. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that GlobalRAG significantly outperforms strong baselines while using only 8k training data (42% of the training data used by strong baselines), achieving average improvements of 14.2% in both EM and F1.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GlobalRAG：通过强化学习增强多跳问答中的全局推理能力</div>
<div class="mono" style="margin-top:8px">强化学习近期在改进检索增强生成（RAG）方面展现出潜力。然而，其在多跳问答（QA）中的有效性仍受限于两个根本问题：（i）缺乏构建多步推理的全局规划；（ii）执行过程不可靠，影响有效查询构建与检索证据的一致性使用。我们提出GlobalRAG——一个专为增强多跳QA全局推理设计的强化学习框架。该框架将问题分解为子目标，协调检索与推理过程，并迭代优化证据。为引导此过程，我们引入规划质量奖励和子目标完成奖励，以促进连贯规划与可靠执行。此外，渐进权重退火策略平衡了过程导向与结果导向的目标。在领域内和跨领域基准上的大量实验表明，GlobalRAG仅使用8k训练数据（仅为强基线训练数据的42%）即显著超越现有基线模型，在EM和F1指标上平均提升14.2%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses limitations in applying reinforcement learning to retrieval-augmented generation for multi-hop question answering, specifically the lack of global planning and unfaithful execution that hinder structured reasoning and consistent evidence use. It proposes GlobalRAG, a framework that decomposes questions into subgoals, coordinates retrieval with reasoning, and iteratively refines evidence using novel rewards for planning quality and subgoal completion, alongside a progressive weight annealing strategy to balance process and outcome objectives. Experimental results on in-domain and out-of-domain benchmarks show that GlobalRAG significantly outperforms strong baselines with only 8k training data, achieving average improvements of 14.2% in both EM and F1 scores.</div>
<div class="mono" style="margin-top:8px">该论文针对强化学习在检索增强生成中应用于多跳问答时的局限性，即缺乏全局规划和执行不忠实，这阻碍了结构化推理和证据的一致性使用。它提出了GlobalRAG框架，通过将问题分解为子目标、协调检索与推理、并利用规划质量奖励和子目标完成奖励迭代优化证据，同时采用渐进权重退火策略平衡过程与结果目标。在领域内和领域外基准测试上的实验结果表明，GlobalRAG仅使用8k训练数据就显著优于强基线，在EM和F1分数上平均提升了14.2%。</div>
</details>
</div>
<div class="card">
<div class="title">Segmental Advantage Estimation: Enhancing PPO for Long-Context LLM Training</div>
<div class="meta-line">Authors: Xue Gong, Qi Yi, Ziyuan Nan, Guanhua Huang, Kejiao Li, Yuhao Jiang, Ruibin Xiong, Zenan Xu, Jiaming Guo, Shaohui Peng, Bo Zhou</div>
<div class="meta-line">First: 2026-01-12T08:41:47+00:00 · Latest: 2026-01-12T08:41:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07320v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07320v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training Large Language Models (LLMs) for reasoning tasks is increasingly driven by Reinforcement Learning with Verifiable Rewards (RLVR), where Proximal Policy Optimization (PPO) provides a principled framework for stable policy updates. However, the practical application of PPO is hindered by unreliable advantage estimation in the sparse-reward RLVR regime. This issue arises because the sparse rewards in RLVR lead to inaccurate intermediate value predictions, which in turn introduce significant bias when aggregated at every token by Generalized Advantage Estimation (GAE). To address this, we introduce Segmental Advantage Estimation (SAE), which mitigates the bias that GAE can incur in RLVR. Our key insight is that aggregating $n$-step advantages at every token(as in GAE) is unnecessary and often introduces excessive bias, since individual tokens carry minimal information. Instead, SAE first partitions the generated sequence into coherent sub-segments using low-probability tokens as heuristic boundaries. It then selectively computes variance-reduced advantage estimates only from these information-rich segment transitions, effectively filtering out noise from intermediate tokens. Our experiments demonstrate that SAE achieves superior performance, with marked improvements in final scores, training stability, and sample efficiency. These gains are shown to be consistent across multiple model sizes, and a correlation analysis confirms that our proposed advantage estimator achieves a higher correlation with an approximate ground-truth advantage, justifying its superior performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分段优势估计：增强PPO在长上下文LLM训练中的性能</div>
<div class="mono" style="margin-top:8px">针对推理任务的大语言模型训练日益依赖可验证奖励的强化学习，其中近端策略优化为稳定策略更新提供了理论框架。然而，PPO在实际应用中被稀疏奖励RLVR机制中不可靠的优势估计所阻碍。该问题源于RLVR的稀疏奖励导致中间价值预测不准确，进而在广义优势估计逐词元聚合时引入显著偏差。为此，我们提出分段优势估计方法，以缓解GAE在RLVR中可能产生的偏差。核心洞见在于：逐词元聚合n步优势既非必要且常引入过度偏差，因为单个词元信息量有限。SAE首先使用低概率词元作为启发式边界，将生成序列划分为连贯子段，随后仅从这些信息丰富的段间转换选择性计算方差缩减的优势估计，有效滤除中间词元的噪声。实验表明SAE实现了更优性能，在最终得分、训练稳定性和样本效率方面均有显著提升。这些增益在不同模型规模中表现一致，相关性分析证实我们提出的优势估计器与近似真实优势具有更高相关性，佐证了其优越性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unreliable advantage estimation in Proximal Policy Optimization (PPO) when training Large Language Models for reasoning tasks with sparse verifiable rewards, which leads to biased policy updates. To mitigate this, the authors propose Segmental Advantage Estimation (SAE), a method that partitions generated sequences into sub-segments using low-probability tokens as boundaries and computes advantage estimates selectively at segment transitions rather than at every token, thereby reducing noise and bias. Experimental results show that SAE improves final performance scores, enhances training stability, and increases sample efficiency across various model sizes, with correlation analysis confirming its advantage estimates align better with ground-truth advantages.</div>
<div class="mono" style="margin-top:8px">本文针对在稀疏可验证奖励下训练大型语言模型进行推理任务时，近端策略优化中优势估计不可靠导致策略更新偏差的问题。作者提出了分段优势估计方法，该方法使用低概率词元作为启发式边界将生成序列划分为连贯子段，并选择性地在信息丰富的段转换处计算方差缩减的优势估计，从而过滤中间词元的噪声。实验结果表明，该方法在最终得分、训练稳定性和样本效率方面均取得显著提升，且在不同模型规模上表现一致，相关性分析证实其优势估计与近似真实优势具有更高相关性，验证了其优越性能。</div>
</details>
</div>
<div class="card">
<div class="title">VLM-CAD: VLM-Optimized Collaborative Agent Design Workflow for Analog Circuit Sizing</div>
<div class="meta-line">Authors: Guanyuan Pan, Yugui Lin, Tiansheng Zhou, Pietro Liò, Shuai Wang, Yaqi Wang</div>
<div class="meta-line">First: 2026-01-12T08:37:32+00:00 · Latest: 2026-01-12T08:37:32+00:00</div>
<div class="meta-line">Comments: 8 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07315v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07315v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Analog mixed-signal circuit sizing involves complex trade-offs within high-dimensional design spaces. Existing automatic analog circuit sizing approaches often underutilize circuit schematics and lack the explainability required for industry adoption. To tackle these challenges, we propose a Vision Language Model-optimized collaborative agent design workflow (VLM-CAD), which analyzes circuits, optimizes DC operating points, performs inference-based sizing and executes external sizing optimization. We integrate Image2Net to annotate circuit schematics and generate a structured JSON description for precise interpretation by Vision Language Models. Furthermore, we propose an Explainable Trust Region Bayesian Optimization method (ExTuRBO) that employs collaborative warm-starting from agent-generated seeds and offers dual-granularity sensitivity analysis for external sizing optimization, supporting a comprehensive final design report. Experiment results on amplifier sizing tasks using 180nm, 90nm, and 45nm Predictive Technology Models demonstrate that VLM-CAD effectively balances power and performance, achieving a 100% success rate in optimizing an amplifier with a complementary input and a class-AB output stage, while maintaining total runtime under 43 minutes across all experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VLM-CAD：面向模拟电路尺寸优化的视觉语言模型协同智能体设计流程</div>
<div class="mono" style="margin-top:8px">模拟混合信号电路尺寸优化涉及高维设计空间中的复杂权衡。现有自动优化方法常未充分利用电路原理图，且缺乏工业应用所需的可解释性。为此，我们提出一种视觉语言模型优化的协同智能体设计流程（VLM-CAD），通过分析电路、优化直流工作点、执行基于推理的尺寸优化及外部尺寸优化来应对这些挑战。我们集成Image2Net工具标注电路原理图并生成结构化JSON描述，供视觉语言模型精准解析。此外，提出可解释信任域贝叶斯优化方法（ExTuRBO），采用智能体生成种子的协同热启动策略，提供双粒度灵敏度分析以支持外部尺寸优化，并生成完整最终设计报告。基于180nm、90nm和45nm预测技术模型的放大器尺寸优化实验表明，VLM-CAD能有效平衡功耗与性能，在优化互补输入与AB类输出级放大器时实现100%成功率，且所有实验总运行时间均控制在43分钟内。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to better utilize circuit schematics and improve explainability in automatic analog circuit sizing, this paper introduces VLM-CAD, a collaborative agent workflow that integrates Vision Language Models (VLMs) to analyze circuits and optimize designs. The method employs Image2Net to annotate schematics into structured JSON for VLM interpretation and proposes ExTuRBO, an explainable Bayesian optimization technique with collaborative warm-starting and sensitivity analysis for external sizing. Experimental results on amplifier sizing across 180nm, 90nm, and 45nm technology nodes show that VLM-CAD successfully balances power and performance, achieving a 100% success rate for a specific amplifier topology while keeping total runtime under 43 minutes.</div>
<div class="mono" style="margin-top:8px">针对自动模拟电路尺寸设计中电路原理图利用不足和可解释性差的问题，本文提出了VLM-CAD，一种基于视觉语言模型优化的协同智能体设计流程。该方法通过Image2Net对原理图进行标注并生成结构化JSON描述供VLM解析，同时提出了ExTuRBO可解释信任域贝叶斯优化方法，利用智能体生成的种子进行协同热启动，并提供双重粒度灵敏度分析以支持外部尺寸优化。在180nm、90nm和45nm预测技术模型上的放大器尺寸实验表明，VLM-CAD能有效平衡功耗与性能，在优化一个具有互补输入和AB类输出级的放大器时实现了100%的成功率，且所有实验总运行时间控制在43分钟以内。</div>
</details>
</div>
<div class="card">
<div class="title">Heterogeneous Multi-Expert Reinforcement Learning for Long-Horizon Multi-Goal Tasks in Autonomous Forklifts</div>
<div class="meta-line">Authors: Yun Chen, Bowei Huang, Fan Guo, Kang Song</div>
<div class="meta-line">First: 2026-01-12T08:27:24+00:00 · Latest: 2026-01-12T08:27:24+00:00</div>
<div class="meta-line">Comments: 9 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07304v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07304v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous mobile manipulation in unstructured warehouses requires a balance between efficient large-scale navigation and high-precision object interaction. Traditional end-to-end learning approaches often struggle to handle the conflicting demands of these distinct phases. Navigation relies on robust decision-making over large spaces, while manipulation needs high sensitivity to fine local details. Forcing a single network to learn these different objectives simultaneously often causes optimization interference, where improving one task degrades the other. To address these limitations, we propose a Heterogeneous Multi-Expert Reinforcement Learning (HMER) framework tailored for autonomous forklifts. HMER decomposes long-horizon tasks into specialized sub-policies controlled by a Semantic Task Planner. This structure separates macro-level navigation from micro-level manipulation, allowing each expert to focus on its specific action space without interference. The planner coordinates the sequential execution of these experts, bridging the gap between task planning and continuous control. Furthermore, to solve the problem of sparse exploration, we introduce a Hybrid Imitation-Reinforcement Training Strategy. This method uses expert demonstrations to initialize the policy and Reinforcement Learning for fine-tuning. Experiments in Gazebo simulations show that HMER significantly outperforms sequential and end-to-end baselines. Our method achieves a task success rate of 94.2\% (compared to 62.5\% for baselines), reduces operation time by 21.4\%, and maintains placement error within 1.5 cm, validating its efficacy for precise material handling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向自主叉车长时域多目标任务的异构多专家强化学习</div>
<div class="mono" style="margin-top:8px">非结构化仓库中的自主移动操作需兼顾大规模高效导航与高精度物体交互。传统端到端学习方法常难以协调这两个阶段的冲突需求：导航依赖大范围空间的鲁棒决策，而操作则需对局部细节高度敏感。强制单一网络同时学习这些不同目标易导致优化干扰，即提升一项任务会损害另一项。为此，我们提出一种专为自主叉车设计的异构多专家强化学习框架。该框架通过语义任务规划器将长时域任务分解为专用子策略，实现宏观导航与微观操作的解耦，使各专家能专注其特定动作空间而无相互干扰。规划器协调专家间的顺序执行，弥合任务规划与连续控制间的鸿沟。此外，针对稀疏探索问题，我们引入混合模仿-强化训练策略，利用专家演示初始化策略并通过强化学习进行微调。Gazebo仿真实验表明，该框架显著优于序列化与端到端基线方法：任务成功率提升至94.2%（基线为62.5%），操作时间减少21.4%，放置误差保持在1.5厘米内，验证了其在精密物料搬运中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of autonomous mobile manipulation in warehouses, where traditional end-to-end learning struggles with the conflicting demands of large-scale navigation and fine-grained manipulation. The authors propose a Heterogeneous Multi-Expert Reinforcement Learning (HMER) framework that decomposes tasks into specialized sub-policies for navigation and manipulation, coordinated by a Semantic Task Planner, and employ a hybrid imitation-reinforcement training strategy to overcome sparse exploration. Experimental results in Gazebo simulations demonstrate that HMER achieves a 94.2% task success rate, reduces operation time by 21.4%, and maintains placement error within 1.5 cm, significantly outperforming baseline methods.</div>
<div class="mono" style="margin-top:8px">本文针对仓库环境中自主移动操作的挑战，即传统端到端学习方法难以平衡大规模导航与精细操作之间的冲突需求。作者提出了一种异构多专家强化学习框架，通过语义任务规划器将任务分解为导航与操作的专用子策略进行协调，并采用混合模仿-强化训练策略以解决稀疏探索问题。在Gazebo仿真实验中的结果表明，该方法实现了94.2%的任务成功率，减少了21.4%的操作时间，并将放置误差控制在1.5厘米以内，性能显著优于基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive Tool Use with Reinforcement Learning</div>
<div class="meta-line">Authors: Wenxun Wu, Yuanyang Li, Guhan Chen, Linyue Wang, Hongyang Chen</div>
<div class="meta-line">First: 2025-10-08T14:04:27+00:00 · Latest: 2026-01-12T08:14:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.07038v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.07038v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large language models (LLMs) have popularized test-time scaling, where models generate additional reasoning tokens before producing final answers. These approaches have demonstrated significant performance improvements on benchmarks involving mathematical reasoning. However, language models relying solely on direct inference still struggle with tasks demanding up-to-date knowledge or computational tools such as calculators and code interpreters for complex arithmetic operations. To overcome these limitations, we propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement learning framework that systematically integrates multi-hop reasoning with adaptive tool-calling capabilities. Our approach employs a modified version of Dynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm, which we adapt specifically for tool invocation scenarios, enabling models to dynamically interleave complex reasoning with on-demand tool usage (including search APIs and Python interpreters).
  To support this research, we introduce two new datasets: TAPO-easy-60K and TAPO-hard-18K, specifically designed to train and evaluate both fact-based reasoning and mathematical calculation capabilities. Our experiments on Qwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach, with both models achieving state-of-the-art performance on tasks requiring external knowledge and mathematical computation among methods with comparable parameters. Notably, TAPO achieves more efficient tool utilization than baseline methods while preventing excessive calls caused by reward hacking. These results highlight the significant potential of combining advanced reasoning with tool usage to enhance model performance in knowledge-intensive and computationally demanding tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>工具增强策略优化：推理与自适应工具使用与强化学习的协同</div>
<div class="mono" style="margin-top:8px">近期大语言模型（LLMs）的进展推动了测试时扩展的普及，即模型在生成最终答案前先产生额外的推理标记。这些方法在涉及数学推理的基准测试中展现出显著的性能提升。然而，仅依赖直接推理的语言模型在处理需要最新知识或计算工具（如计算器和代码解释器以执行复杂算术运算）的任务时仍面临困难。为克服这些限制，我们提出了工具增强策略优化（TAPO），这是一种新颖的强化学习框架，系统性地将多跳推理与自适应工具调用能力相结合。我们的方法采用动态采样策略优化（DAPO）的改进版本——一种近期开发的强化学习范式，并专门针对工具调用场景进行适配，使模型能够动态交织复杂推理与按需工具使用（包括搜索API和Python解释器）。为支持本研究，我们引入了两个新数据集：TAPO-easy-60K和TAPO-hard-18K，专门用于训练和评估基于事实的推理与数学计算能力。在Qwen2.5-3B和Qwen2.5-7B模型上的实验证明了我们方法的有效性，在参数量可比的方法中，两个模型在需要外部知识和数学计算的任务上均达到了最先进的性能。值得注意的是，TAPO相比基线方法实现了更高效的工具利用，同时避免了因奖励攻击导致的过度调用。这些结果凸显了将高级推理与工具使用相结合，在知识密集型和计算密集型任务中提升模型性能的巨大潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of large language models in handling tasks requiring current knowledge or computational tools, this paper introduces Tool-Augmented Policy Optimization (TAPO), a reinforcement learning framework that synergizes multi-hop reasoning with adaptive tool-calling. The method adapts Dynamic Sampling Policy Optimization to enable models to dynamically interleave reasoning with tool use, such as search APIs and Python interpreters, and is supported by two new datasets, TAPO-easy-60K and TAPO-hard-18K, for training and evaluation. Experimental results on Qwen2.5-3B and Qwen2.5-7B models show state-of-the-art performance on tasks needing external knowledge and mathematical computation, with more efficient tool utilization and prevention of excessive calls compared to baselines.</div>
<div class="mono" style="margin-top:8px">针对大语言模型在处理需要最新知识或计算工具的任务时存在的局限，本文提出了工具增强策略优化（TAPO），这是一个将多跳推理与自适应工具调用相结合的强化学习框架。该方法通过改进动态采样策略优化，使模型能够动态交错推理与工具使用（如搜索API和Python解释器），并引入了两个新数据集TAPO-easy-60K和TAPO-hard-18K用于训练和评估。在Qwen2.5-3B和Qwen2.5-7B模型上的实验结果表明，该方法在需要外部知识和数学计算的任务上达到了同类参数方法中的最优性能，同时实现了比基线方法更高效的工具利用，并避免了因奖励黑客行为导致的过度调用。</div>
</details>
</div>
<div class="card">
<div class="title">LRAS: Advanced Legal Reasoning with Agentic Search</div>
<div class="meta-line">Authors: Yujin Zhou, Chuxue Cao, Jinluan Yang, Lijun Wu, Conghui He, Sirui Han, Yike Guo</div>
<div class="meta-line">First: 2026-01-12T08:07:35+00:00 · Latest: 2026-01-12T08:07:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07296v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07296v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Reasoning Models (LRMs) have demonstrated exceptional logical capabilities in mathematical domains, their application to the legal field remains hindered by the strict requirements for procedural rigor and adherence to legal logic. Existing legal LLMs, which rely on &quot;closed-loop reasoning&quot; derived solely from internal parametric knowledge, frequently suffer from lack of self-awareness regarding their knowledge boundaries, leading to confident yet incorrect conclusions. To address this challenge, we present Legal Reasoning with Agentic Search (LRAS), the first framework designed to transition legal LLMs from static and parametric &quot;closed-loop thinking&quot; to dynamic and interactive &quot;Active Inquiry&quot;. By integrating Introspective Imitation Learning and Difficulty-aware Reinforcement Learning, LRAS enables LRMs to identify knowledge boundaries and handle legal reasoning complexity. Empirical results demonstrate that LRAS outperforms state-of-the-art baselines by 8.2-32\%, with the most substantial gains observed in tasks requiring deep reasoning with reliable knowledge. We will release our data and models for further exploration soon.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LRAS：基于智能体搜索的高级法律推理框架</div>
<div class="mono" style="margin-top:8px">尽管大型推理模型在数学领域展现出卓越的逻辑能力，但其在法律领域的应用仍受限于对程序严谨性和法律逻辑遵循的严格要求。现有依赖纯内部参数化知识进行&#x27;闭环推理&#x27;的法律大语言模型，常因缺乏对知识边界的自我认知而得出自信但错误的结论。为应对这一挑战，我们提出基于智能体搜索的法律推理框架，这是首个旨在将法律大语言模型从静态参数化的&#x27;闭环思维&#x27;转向动态交互式&#x27;主动质询&#x27;的框架。通过整合内省模仿学习与难度感知强化学习，该框架使大型推理模型能够识别知识边界并处理法律推理的复杂性。实证结果表明，该框架以8.2-32%的优势超越现有最优基线模型，在需要可靠知识深度推理的任务中提升最为显著。我们将很快发布相关数据与模型以供进一步探索。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of current legal large language models (LLMs) that rely on closed-loop reasoning and often produce confidently incorrect conclusions due to a lack of self-awareness about knowledge boundaries, this paper introduces the Legal Reasoning with Agentic Search (LRAS) framework. The method transitions models from static parametric reasoning to dynamic active inquiry by integrating introspective imitation learning and difficulty-aware reinforcement learning, enabling them to identify knowledge gaps and manage legal complexity. Experimental results show that LRAS outperforms state-of-the-art baselines by 8.2-32%, with the most significant improvements in tasks requiring deep, knowledge-reliable reasoning.</div>
<div class="mono" style="margin-top:8px">本文的动机在于现有法律大语言模型依赖闭环推理，缺乏对知识边界的自我认知，常导致自信但错误的结论。为此，研究提出了法律推理与主动搜索框架，通过整合内省模仿学习和难度感知强化学习，使模型从静态参数推理转向动态主动查询，以识别知识边界并处理法律推理复杂性。实验结果表明，该框架在基准测试中性能提升8.2-32%，尤其在需要深度可靠知识的推理任务中表现最为突出。</div>
</details>
</div>
<div class="card">
<div class="title">Simulated Annealing-based Candidate Optimization for Batch Acquisition Functions</div>
<div class="meta-line">Authors: Sk Md Ahnaf Akif Alvi, Raymundo Arróyave, Douglas Allaire</div>
<div class="meta-line">First: 2026-01-12T06:51:49+00:00 · Latest: 2026-01-12T06:51:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07258v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07258v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bayesian Optimization with multi-objective acquisition functions such as q-Expected Hypervolume Improvement (qEHVI) requires efficient candidate optimization to maximize acquisition function values. Traditional approaches rely on continuous optimization methods like Sequential Least Squares Programming (SLSQP) for candidate selection. However, these gradient-based methods can become trapped in local optima, particularly in complex or high-dimensional objective landscapes. This paper presents a simulated annealing-based approach for candidate optimization in batch acquisition functions as an alternative to conventional continuous optimization methods. We evaluate our simulated annealing approach against SLSQP across four benchmark multi-objective optimization problems: ZDT1 (30D, 2 objectives), DTLZ2 (7D, 3 objectives), Kursawe (3D, 2 objectives), and Latent-Aware (4D, 2 objectives). Our results demonstrate that simulated annealing consistently achieves superior hypervolume performance compared to SLSQP in most test functions. The improvement is particularly pronounced for DTLZ2 and Latent-Aware problems, where simulated annealing reaches significantly higher hypervolume values and maintains better convergence characteristics. The histogram analysis of objective space coverage further reveals that simulated annealing explores more diverse and optimal regions of the Pareto front. These findings suggest that metaheuristic optimization approaches like simulated annealing can provide more robust and effective candidate optimization for multi-objective Bayesian optimization, offering a promising alternative to traditional gradient-based methods for batch acquisition function optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于模拟退火的批量采集函数候选优化</div>
<div class="mono" style="margin-top:8px">使用多目标采集函数（如q-期望超体积改进）的贝叶斯优化需要高效的候选优化以最大化采集函数值。传统方法依赖连续优化方法（如序列最小二乘规划）进行候选选择，但这些基于梯度的方法易陷入局部最优，尤其在复杂或高维目标空间中。本文提出一种基于模拟退火的批量采集函数候选优化方法，以替代传统连续优化方法。我们在四个基准多目标优化问题（ZDT1、DTLZ2、Kursawe和Latent-Aware）上将模拟退火与序列最小二乘规划进行对比评估。结果表明，在多数测试函数中，模拟退火始终获得更优的超体积性能，尤其在DTLZ2和Latent-Aware问题上表现显著，能达到更高的超体积值并保持更好的收敛特性。目标空间覆盖的直方图分析进一步显示，模拟退火能探索帕累托前沿更多样且更优的区域。这些发现表明，模拟退火等元启发式优化方法可为多目标贝叶斯优化提供更稳健有效的候选优化，为批量采集函数优化提供了有前景的替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of gradient-based methods like SLSQP, which can get stuck in local optima when optimizing multi-objective acquisition functions such as qEHVI in Bayesian Optimization, this paper proposes using simulated annealing as an alternative for candidate optimization. The method applies simulated annealing to maximize acquisition function values in batch settings, evaluating it against SLSQP on four benchmark problems including ZDT1, DTLZ2, Kursawe, and Latent-Aware. Experimental results show that simulated annealing consistently outperforms SLSQP in hypervolume performance, especially on DTLZ2 and Latent-Aware, achieving higher hypervolume values, better convergence, and more diverse exploration of the Pareto front, indicating its robustness for multi-objective Bayesian Optimization.</div>
<div class="mono" style="margin-top:8px">针对传统梯度优化方法（如SLSQP）在优化多目标采集函数（如qEHVI）时易陷入局部最优的问题，本文提出使用模拟退火算法作为候选优化的替代方法。该方法将模拟退火应用于批量采集函数优化，并在ZDT1、DTLZ2、Kursawe和Latent-Aware四个基准问题上与SLSQP进行对比评估。实验结果表明，模拟退火在超体积性能上持续优于SLSQP，尤其在DTLZ2和Latent-Aware问题上表现突出，获得了更高的超体积值、更好的收敛性以及对帕累托前沿更广泛的探索，证明了其在多目标贝叶斯优化中的鲁棒性和有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Group Pattern Selection Optimization: Let LRMs Pick the Right Pattern for Reasoning</div>
<div class="meta-line">Authors: Hanbin Wang, Jingwei Song, Jinpeng Li, Fei Mi, Lifeng Shang</div>
<div class="meta-line">First: 2026-01-12T06:19:09+00:00 · Latest: 2026-01-12T06:19:09+00:00</div>
<div class="meta-line">Comments: 8 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07238v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07238v1">PDF</a> · <a href="https://github.com/wanghanbinpanda/GPSO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models (LRMs) exhibit diverse high-level reasoning patterns (e.g., direct solution, reflection-and-verification, and exploring multiple solutions), yet prevailing training recipes implicitly bias models toward a limited set of dominant patterns. Through a systematic analysis, we identify substantial accuracy variance across these patterns on mathematics and science benchmarks, revealing that a model&#x27;s default reasoning pattern is often sub-optimal for a given problem. To address this, we introduce Group Pattern Selection Optimization (GPSO), a reinforcement learning framework that extends GRPO by incorporating multi-pattern rollouts, verifier-guided optimal pattern selection per problem, and attention masking during optimization to prevent the leakage of explicit pattern suffixes into the learned policy. By exploring a portfolio of diverse reasoning strategies and optimizing the policy on the most effective ones, GPSO enables the model to internalize the mapping from problem characteristics to optimal reasoning patterns. Extensive experiments demonstrate that GPSO delivers consistent and substantial performance gains across various model backbones and benchmarks, effectively mitigating pattern sub-optimality and fostering more robust, adaptable reasoning. All data and codes are available at https://github.com/wanghanbinpanda/GPSO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>群体模式选择优化：让大型推理模型自主选择最佳推理模式</div>
<div class="mono" style="margin-top:8px">大型推理模型（LRMs）展现出多样化的高层推理模式（如直接求解、反思验证、多解探索），但主流训练方法会隐式地将模型偏向于有限的优势模式。通过系统分析，我们在数学与科学基准测试中发现不同模式间存在显著的准确率差异，表明模型默认的推理模式常非当前问题的最优解。为此，我们提出群体模式选择优化（GPSO）——一种强化学习框架，它在GRPO基础上融合了多模式推演、基于验证器的问题级最优模式选择，以及在优化过程中通过注意力掩码防止显式模式后缀泄露至学习策略。通过探索多样化推理策略组合并针对最有效策略优化策略，GPSO使模型能够内化从问题特征到最优推理模式的映射。大量实验表明，GPSO在不同模型架构与基准测试中均能带来持续显著的性能提升，有效缓解模式次优问题，促进更鲁棒、自适应的推理能力。所有数据与代码已开源：https://github.com/wanghanbinpanda/GPSO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue that large reasoning models (LRMs) are often biased toward a limited set of dominant reasoning patterns during training, which can be sub-optimal for specific problems, as evidenced by substantial accuracy variance across patterns on mathematics and science benchmarks. The authors introduce Group Pattern Selection Optimization (GPSO), a reinforcement learning framework that extends GRPO by incorporating multi-pattern rollouts, verifier-guided optimal pattern selection per problem, and attention masking to prevent pattern suffix leakage, enabling the model to learn a mapping from problem characteristics to optimal reasoning strategies. Experimental results demonstrate that GPSO delivers consistent and substantial performance gains across various model backbones and benchmarks, effectively mitigating pattern sub-optimality and fostering more robust, adaptable reasoning.</div>
<div class="mono" style="margin-top:8px">本文针对大型推理模型在训练中常偏向于有限的主导推理模式，导致对特定问题可能并非最优的问题，通过在数学和科学基准测试中观察到不同模式间显著的准确率差异而提出。作者引入了群体模式选择优化方法，这是一个强化学习框架，它扩展了GRPO，通过整合多模式展开、基于验证器的问题级最优模式选择以及防止模式后缀泄露的注意力掩码，使模型能够学习从问题特征到最优推理策略的映射。大量实验表明，该方法在各种模型主干和基准测试上均带来一致且显著的性能提升，有效缓解了模式次优性问题，并促进了更鲁棒、自适应的推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">ADPO: Anchored Direct Preference Optimization</div>
<div class="meta-line">Authors: Wang Zixian</div>
<div class="meta-line">First: 2025-10-21T05:53:13+00:00 · Latest: 2026-01-12T06:15:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.18913v7">Abs</a> · <a href="https://arxiv.org/pdf/2510.18913v7">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Anchored Direct Preference Optimization (ADPO), a policy alignment method derived from first principles of KL-regularized reinforcement learning. Unlike standard approaches that treat the reference policy merely as a regularizer, we show that the optimal policy in reinforcement learning from human feedback inherently operates in a differential coordinate system, optimizing relative advantage in the form of log ratios rather than absolute probabilities. ADPO explicitly parameterizes this optimal structure through anchored logits, effectively decoupling response quality from prior popularity and creating an implicit trust region through curvature scaling. We show that this formulation unifies supervised fine-tuning, reinforcement learning, and ranking-based objectives under a single geometric perspective. Theoretically, ADPO resolves the probability smearing problem of supervised fine-tuning while avoiding the mode-seeking instability characteristic of reverse-KL methods. Empirically, the listwise ranking variant of ADPO achieves state-of-the-art performance on reasoning tasks, outperforming GRPO by 30.9 percent on Qwen3-1.7B and demonstrating superior robustness under distribution shift.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ADPO：锚定直接偏好优化</div>
<div class="mono" style="margin-top:8px">本文提出锚定直接偏好优化（ADPO），一种基于KL正则化强化学习基本原理推导的策略对齐方法。不同于仅将参考策略作为正则项的标准方法，我们证明基于人类反馈的强化学习中最优策略本质上运行于微分坐标系中，通过对数比形式优化相对优势而非绝对概率。ADPO通过锚定逻辑参数显式建模这一最优结构，有效解耦响应质量与先验流行度，并通过曲率缩放构建隐式信任区域。该框架将监督微调、强化学习和基于排序的目标统一于单一几何视角。理论上，ADPO解决了监督微调的概率弥散问题，同时避免了反向KL方法固有的模式寻求不稳定性。实证表明，ADPO的列表排序变体在推理任务上达到最先进性能，在Qwen3-1.7B模型上超越GRPO达30.9%，并在分布偏移下表现出卓越的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Anchored Direct Preference Optimization (ADPO), motivated by the need to improve policy alignment by treating the reference policy as more than just a regularizer, instead deriving it from KL-regularized reinforcement learning principles. The method parameterizes the optimal policy using anchored logits, which operate in a differential coordinate system to optimize relative advantage via log ratios, thereby decoupling response quality from prior popularity and creating an implicit trust region through curvature scaling. Experimentally, ADPO achieves state-of-the-art results, with its listwise ranking variant outperforming GRPO by 30.9% on Qwen3-1.7B in reasoning tasks and showing superior robustness under distribution shift.</div>
<div class="mono" style="margin-top:8px">本文提出了锚定直接偏好优化（ADPO），其动机源于需要改进策略对齐方法，将参考策略视为不仅仅是正则化项，而是基于KL正则化强化学习原理推导而来。该方法通过锚定logits参数化最优策略，在微分坐标系中运作，通过对数比优化相对优势，从而将响应质量与先验流行度解耦，并通过曲率缩放创建隐式信任区域。实验结果表明，ADPO取得了最先进的性能，其列表排序变体在推理任务上比GRPO高出30.9%（基于Qwen3-1.7B模型），并在分布偏移下表现出更优的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Aligning Findings with Diagnosis: A Self-Consistent Reinforcement Learning Framework for Trustworthy Radiology Reporting</div>
<div class="meta-line">Authors: Kun Zhao, Siyuan Dai, Pan Wang, Jifeng Song, Hui Ji, Chenghua Lin, Liang Zhan, Haoteng Tang</div>
<div class="meta-line">First: 2026-01-06T14:17:44+00:00 · Latest: 2026-01-12T05:56:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03321v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03321v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) have shown strong potential for radiology report generation, yet their clinical translation is hindered by architectural heterogeneity and the prevalence of factual hallucinations. Standard supervised fine-tuning often fails to strictly align linguistic outputs with visual evidence, while existing reinforcement learning approaches struggle with either prohibitive computational costs or limited exploration. To address these challenges, we propose a comprehensive framework for self-consistent radiology report generation. First, we conduct a systematic evaluation to identify optimal vision encoder and LLM backbone configurations for medical imaging. Building on this foundation, we introduce a novel &quot;Reason-then-Summarize&quot; architecture optimized via Group Relative Policy Optimization (GRPO). This framework restructures generation into two distinct components: a think block for detailed findings and an answer block for structured disease labels. By utilizing a multi-dimensional composite reward function, we explicitly penalize logical discrepancies between the generated narrative and the final diagnosis. Extensive experiments on the MIMIC-CXR benchmark demonstrate that our method achieves state-of-the-art performance in clinical efficacy metrics and significantly reduces hallucinations compared to strong supervised baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>发现与诊断对齐：基于自洽强化学习的可信放射学报告生成框架</div>
<div class="mono" style="margin-top:8px">多模态大语言模型在放射学报告生成中展现出巨大潜力，但其临床转化受限于架构异构性和事实性幻觉的普遍存在。标准监督微调常难以严格对齐语言输出与视觉证据，而现有强化学习方法则面临计算成本过高或探索能力有限的问题。为此，我们提出一个自洽的放射学报告生成综合框架。首先，通过系统评估确定医学影像处理中最优的视觉编码器与大语言模型骨干配置。在此基础上，引入基于组相对策略优化的新型“先推理后总结”架构。该框架将生成过程重构为两个独立模块：用于详细发现的思考模块和用于结构化疾病标签的应答模块。通过采用多维复合奖励函数，我们显式惩罚生成叙述与最终诊断间的逻辑不一致性。在MIMIC-CXR基准上的大量实验表明，相较于强监督基线方法，本方法在临床效能指标上达到最先进水平，并显著减少了幻觉现象。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to reduce factual hallucinations and improve clinical trustworthiness in radiology report generation by Multimodal Large Language Models (MLLMs), this paper proposes a self-consistent reinforcement learning framework. The method involves a systematic evaluation to select optimal vision and language backbones, followed by a novel &quot;Reason-then-Summarize&quot; architecture optimized via Group Relative Policy Optimization (GRPO), which separates detailed findings generation from structured diagnosis and uses a composite reward to penalize logical inconsistencies. Experimental results on the MIMIC-CXR benchmark show the framework achieves state-of-the-art clinical efficacy and significantly reduces hallucinations compared to strong supervised baselines.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决多模态大语言模型在放射学报告生成中存在的幻觉问题及临床可信度挑战，提出了一个自洽的强化学习框架。方法上，首先系统评估并选择了最优的视觉编码器和语言模型主干，进而引入一种通过组相对策略优化训练的“先推理后总结”架构，该架构将详细发现生成与结构化疾病标签分离，并利用复合奖励函数惩罚叙述与诊断间的逻辑不一致。在MIMIC-CXR基准上的大量实验表明，该方法在临床效能指标上达到了最先进水平，并相比强大的监督基线显著减少了幻觉。</div>
</details>
</div>
<div class="card">
<div class="title">Consolidation or Adaptation? PRISM: Disentangling SFT and RL Data via Gradient Concentration</div>
<div class="meta-line">Authors: Yang Zhao, Yangou Ouyang, Xiao Ding, Hepeng Wang, Bibo Cai, Kai Xiong, Jinglong Gao, Zhouhao Sun, Li Du, Bing Qin, Ting Liu</div>
<div class="meta-line">First: 2026-01-12T05:43:20+00:00 · Latest: 2026-01-12T05:43:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07224v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07224v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Hybrid Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has become the standard paradigm for training LLM agents, effective mechanisms for data allocation between these stages remain largely underexplored. Current data arbitration strategies often rely on surface-level heuristics that fail to diagnose intrinsic learning needs. Since SFT targets pattern consolidation through imitation while RL drives structural adaptation via exploration, misaligning data with these functional roles causes severe optimization interference. We propose PRISM, a dynamics-aware framework grounded in Schema Theory that arbitrates data based on its degree of cognitive conflict with the model&#x27;s existing knowledge. By analyzing the spatial geometric structure of gradients, PRISM identifies data triggering high spatial concentration as high-conflict signals that require RL for structural restructuring. In contrast, data yielding diffuse updates is routed to SFT for efficient consolidation. Extensive experiments on WebShop and ALFWorld demonstrate that PRISM achieves a Pareto improvement, outperforming state-of-the-art hybrid methods while reducing computational costs by up to 3.22$\times$. Our findings suggest that disentangling data based on internal optimization regimes is crucial for scalable and robust agent alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>巩固还是适应？PRISM：通过梯度集中度解耦监督微调与强化学习数据</div>
<div class="mono" style="margin-top:8px">尽管监督微调（SFT）与强化学习（RL）结合的混合范式已成为训练大语言模型智能体的标准方法，但两阶段间有效的数据分配机制仍鲜有研究。当前数据仲裁策略多依赖表层启发式方法，难以诊断内在学习需求。由于SFT通过模仿实现模式巩固，而RL通过探索驱动结构适应，数据与这些功能角色的错配会导致严重的优化干扰。我们提出PRISM——一个基于图式理论、具备动态感知能力的框架，该框架根据数据与模型现有知识的认知冲突程度进行仲裁。通过分析梯度的空间几何结构，PRISM将触发高空间集中度的数据识别为高冲突信号，这类数据需通过RL进行结构重组；而产生分散更新的数据则被导向SFT以实现高效巩固。在WebShop和ALFWorld上的大量实验表明，PRISM实现了帕累托改进，在超越先进混合方法的同时将计算成本降低最高达3.22倍。我们的研究结果表明，基于内部优化机制解耦数据对于实现可扩展且稳健的智能体对齐至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the lack of effective data allocation mechanisms between Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) stages in training LLM agents, as current heuristics fail to align data with their distinct functional roles of consolidation versus adaptation. The method, PRISM, introduces a dynamics-aware framework based on Schema Theory that arbitrates data by analyzing the spatial concentration of gradients, routing high-conflict data requiring structural adaptation to RL and low-conflict data for consolidation to SFT. Experimental results on WebShop and ALFWorld show that PRISM achieves a Pareto improvement over state-of-the-art hybrid methods while reducing computational costs by up to 3.22 times, demonstrating the importance of disentangling data based on internal optimization regimes for scalable agent alignment.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，当前训练大语言模型智能体时，监督微调（SFT）和强化学习（RL）阶段之间缺乏有效的数据分配机制，现有启发式方法无法使数据与其巩固和适应的不同功能角色对齐。所提出的PRISM方法基于图式理论，通过分析梯度的空间集中度来仲裁数据，将需要结构重构的高冲突数据分配给RL，而将产生分散更新的低冲突数据路由给SFT以进行高效巩固。在WebShop和ALFWorld上的大量实验表明，PRISM实现了帕累托改进，优于最先进的混合方法，同时将计算成本降低了高达3.22倍，这证明了基于内部优化机制分离数据对于可扩展的智能体对齐至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">ToolBrain: A Flexible Reinforcement Learning Framework for Agentic Tools</div>
<div class="meta-line">Authors: Quy Minh Le, Minh Sao Khue Luu, Khanh-Tung Tran, Duc-Hai Nguyen, Hoang-Quoc-Viet Pham, Quan Le, Hoang Thanh Lam, Hoang D. Nguyen</div>
<div class="meta-line">First: 2025-09-24T16:01:05+00:00 · Latest: 2026-01-12T04:21:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.00023v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.00023v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Effective tool use is essential for agentic AI, yet training agents to utilize tools remains challenging due to manually designed rewards, limited training data, and poor multi-tool selection, resulting in slow adaptation, wasted computational resources, and suboptimal performance. We introduce ToolBrain, a lightweight and user-friendly framework for training tool use in agentic models with flexible reinforcement learning, thereby easing the barriers for researchers and practitioners to adapt LLM-based agents to specific domains. It supports a wide range of training strategies, including reinforcement learning algorithms such as GRPO and DPO, as well as supervised learning. ToolBrain enables custom reward callables directly on an agent&#x27;s execution traces or simply utilizes an automated LLM-as-a-judge system for reward generation. It is packed with useful capabilities, including knowledge distillation from large to small models, automatic task generation from tool descriptions, seamless tool retrieval, efficient fine-tuning pipelines with QLoRA through Unsloth, and quantized inference via bitsandbytes. We demonstrate ToolBrain through an Email Search Agent case study, showing measurable improvements in tool-use skills under a realistic workflow, while keeping the codebase simple and extensible. Our framework is publicly available at https://toolbrain.org/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ToolBrain：面向智能体工具的灵活强化学习框架</div>
<div class="mono" style="margin-top:8px">高效的工具使用对智能体AI至关重要，但由于手动设计奖励、训练数据有限及多工具选择能力不足，训练智能体使用工具仍面临挑战，导致适应缓慢、计算资源浪费和性能欠佳。我们推出ToolBrain，一个轻量级、用户友好的框架，通过灵活强化学习训练智能体模型中的工具使用，降低研究者和实践者将基于LLM的智能体适配特定领域的门槛。它支持广泛的训练策略，包括GRPO和DPO等强化学习算法以及监督学习。ToolBrain支持直接基于智能体执行轨迹自定义奖励函数，或直接利用自动化LLM-as-a-judge系统生成奖励。该框架集成了多项实用功能，包括从大模型到小模型的知识蒸馏、基于工具描述的自动任务生成、无缝工具检索、通过Unsloth与QLoRA实现的高效微调流程，以及基于bitsandbytes的量化推理。我们通过电子邮件搜索智能体案例展示ToolBrain，在真实工作流中实现了工具使用技能的显著提升，同时保持代码库简洁且可扩展。本框架已在https://toolbrain.org/公开提供。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind ToolBrain is to address the challenges in training AI agents for effective tool use, such as manual reward design and poor multi-tool selection, which lead to slow adaptation and suboptimal performance. The method introduces a lightweight, flexible reinforcement learning framework that supports various training strategies, including GRPO, DPO, and supervised learning, with features like custom reward callables, automated LLM-as-a-judge reward generation, knowledge distillation, and efficient fine-tuning via QLoRA. Experimental results from an Email Search Agent case study demonstrate measurable improvements in tool-use skills within realistic workflows, while maintaining a simple and extensible codebase.</div>
<div class="mono" style="margin-top:8px">ToolBrain的动机是解决训练AI智能体有效使用工具所面临的挑战，如手动设计奖励和多工具选择不佳，导致适应缓慢和性能欠佳。该方法提出了一个轻量级、灵活的强化学习框架，支持多种训练策略，包括GRPO、DPO和监督学习，具备自定义奖励函数、自动化LLM作为评判的奖励生成、知识蒸馏以及通过QLoRA进行高效微调等功能。通过电子邮件搜索智能体的案例研究，实验结果表明在现实工作流程中，工具使用技能得到了可衡量的提升，同时保持了代码库的简洁性和可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">Offline Meta-Reinforcement Learning with Flow-Based Task Inference and Adaptive Correction of Feature Overgeneralization</div>
<div class="meta-line">Authors: Min Wang, Xin Li, Mingzhong Wang, Hasnaa Bennis</div>
<div class="meta-line">First: 2026-01-12T03:16:07+00:00 · Latest: 2026-01-12T03:16:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07164v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07164v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline meta-reinforcement learning (OMRL) combines the strengths of learning from diverse datasets in offline RL with the adaptability to new tasks of meta-RL, promising safe and efficient knowledge acquisition by RL agents. However, OMRL still suffers extrapolation errors due to out-of-distribution (OOD) actions, compromised by broad task distributions and Markov Decision Process (MDP) ambiguity in meta-RL setups. Existing research indicates that the generalization of the $Q$ network affects the extrapolation error in offline RL. This paper investigates this relationship by decomposing the $Q$ value into feature and weight components, observing that while decomposition enhances adaptability and convergence in the case of high-quality data, it often leads to policy degeneration or collapse in complex tasks. We observe that decomposed $Q$ values introduce a large estimation bias when the feature encounters OOD samples, a phenomenon we term &#x27;&#x27;feature overgeneralization&#x27;&#x27;. To address this issue, we propose FLORA, which identifies OOD samples by modeling feature distributions and estimating their uncertainties. FLORA integrates a return feedback mechanism to adaptively adjust feature components. Furthermore, to learn precise task representations, FLORA explicitly models the complex task distribution using a chain of invertible transformations. We theoretically and empirically demonstrate that FLORA achieves rapid adaptation and meta-policy improvement compared to baselines across various environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于流式任务推断与特征过度泛化自适应校正的离线元强化学习</div>
<div class="mono" style="margin-top:8px">离线元强化学习（OMRL）结合了离线强化学习从多样化数据集中学习的优势与元强化学习对新任务的适应能力，有望使强化学习智能体实现安全高效的知识获取。然而，由于分布外（OOD）动作的存在，OMRL仍存在外推误差，这受到元强化学习设置中广泛的任务分布和马尔可夫决策过程（MDP）模糊性的影响。现有研究表明，$Q$网络的泛化能力会影响离线强化学习中的外推误差。本文通过将$Q$值分解为特征和权重分量来研究这一关系，发现虽然分解在高质量数据情况下能增强适应性和收敛性，但在复杂任务中常导致策略退化或崩溃。我们观察到，当特征遇到OOD样本时，分解后的$Q$值会引入较大的估计偏差，这一现象我们称之为“特征过度泛化”。为解决此问题，我们提出FLORA方法，通过建模特征分布并估计其不确定性来识别OOD样本。FLORA集成了回报反馈机制以自适应调整特征分量。此外，为学习精确的任务表示，FLORA使用可逆变换链显式建模复杂任务分布。我们从理论和实验上证明，与基线方法相比，FLORA在各种环境中实现了快速适应和元策略改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the problem of extrapolation errors in offline meta-reinforcement learning (OMRL), which arise from out-of-distribution actions and are exacerbated by feature overgeneralization when decomposing Q-values. The proposed method, FLORA, mitigates this by modeling feature distributions to identify OOD samples and using a return feedback mechanism to adaptively correct features, while also employing flow-based models to learn precise task representations from complex distributions. Experimental results demonstrate that FLORA achieves faster adaptation and improved meta-policy performance compared to baseline methods across various environments.</div>
<div class="mono" style="margin-top:8px">本文针对离线元强化学习中因分布外动作导致的泛化误差问题，指出Q值分解会引发特征过度泛化，从而损害策略性能。所提出的FLORA方法通过建模特征分布来识别分布外样本，并利用回报反馈机制自适应调整特征分量，同时使用基于流的模型从复杂任务分布中学习精确的任务表示。实验结果表明，与基线方法相比，FLORA在多种环境中实现了更快的适应速度和更优的元策略性能。</div>
</details>
</div>
<div class="card">
<div class="title">AscendKernelGen: A Systematic Study of LLM-Based Kernel Generation for Neural Processing Units</div>
<div class="meta-line">Authors: Xinzi Cao, Jianyang Zhai, Pengfei Li, Zhiheng Hu, Cen Yan, Bingxu Mu, Guanghuan Fang, Bin She, Jiayu Li, Yihan Su, Dongyang Tao, Xiansong Huang, Fan Xu, Feidiao Yang, Yao Lu, Chang-Dong Wang, Yutong Lu, Weicheng Xue, Bin Zhou, Yonghong Tian</div>
<div class="meta-line">First: 2026-01-12T03:12:58+00:00 · Latest: 2026-01-12T03:12:58+00:00</div>
<div class="meta-line">Comments: 33 pages,7 figures,16 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07160v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07160v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To meet the ever-increasing demand for computational efficiency, Neural Processing Units (NPUs) have become critical in modern AI infrastructure. However, unlocking their full potential requires developing high-performance compute kernels using vendor-specific Domain-Specific Languages (DSLs), a task that demands deep hardware expertise and is labor-intensive. While Large Language Models (LLMs) have shown promise in general code generation, they struggle with the strict constraints and scarcity of training data in the NPU domain. Our preliminary study reveals that state-of-the-art general-purpose LLMs fail to generate functional complex kernels for Ascend NPUs, yielding a near-zero success rate. To address these challenges, we propose AscendKernelGen, a generation-evaluation integrated framework for NPU kernel development. We introduce Ascend-CoT, a high-quality dataset incorporating chain-of-thought reasoning derived from real-world kernel implementations, and KernelGen-LM, a domain-adaptive model trained via supervised fine-tuning and reinforcement learning with execution feedback. Furthermore, we design NPUKernelBench, a comprehensive benchmark for assessing compilation, correctness, and performance across varying complexity levels. Experimental results demonstrate that our approach significantly bridges the gap between general LLMs and hardware-specific coding. Specifically, the compilation success rate on complex Level-2 kernels improves from 0% to 95.5% (Pass@10), while functional correctness achieves 64.3% compared to the baseline&#x27;s complete failure. These results highlight the critical role of domain-specific reasoning and rigorous evaluation in automating accelerator-aware code generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AscendKernelGen：基于大语言模型的神经处理器内核生成系统性研究</div>
<div class="mono" style="margin-top:8px">为满足日益增长的计算效率需求，神经处理器（NPU）已成为现代人工智能基础设施的关键组件。然而，要充分发挥其潜力，需要使用厂商特定的领域专用语言（DSL）开发高性能计算内核，这项任务既需要深厚的硬件专业知识，又极为耗时费力。尽管大语言模型（LLM）在通用代码生成方面展现出潜力，但在NPU领域严格的约束条件和训练数据稀缺的情况下仍面临困难。我们的初步研究表明，最先进的通用LLM无法为昇腾NPU生成可运行的复杂内核，成功率近乎为零。为应对这些挑战，我们提出AscendKernelGen——一个面向NPU内核开发的生成-评估一体化框架。我们构建了Ascend-CoT高质量数据集（包含从真实内核实现中提取的思维链推理过程），并通过监督微调和带执行反馈的强化学习训练出领域自适应模型KernelGen-LM。此外，我们设计了NPUKernelBench综合基准测试集，用于评估不同复杂度级别下的编译成功率、功能正确性和性能表现。实验结果表明，我们的方法显著缩小了通用LLM与硬件专用编程之间的差距：在复杂二级内核上，编译成功率从0%提升至95.5%（Pass@10），功能正确率达到64.3%，而基线模型完全失效。这些成果凸显了领域专用推理和严格评估在加速器感知代码自动生成中的关键作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to efficiently develop high-performance compute kernels for Neural Processing Units (NPUs) using vendor-specific Domain-Specific Languages, a task that is both expertise-intensive and hindered by the poor performance of general-purpose Large Language Models due to strict constraints and data scarcity, this paper proposes AscendKernelGen, an integrated generation-evaluation framework. The method introduces Ascend-CoT, a dataset with chain-of-thought reasoning from real implementations, and KernelGen-LM, a model fine-tuned with supervised learning and reinforcement learning using execution feedback, alongside the NPUKernelBench benchmark for evaluation. Experimental results show the approach dramatically improves performance over baselines, raising the compilation success rate for complex kernels from 0% to 95.5% and achieving 64.3% functional correctness where baselines entirely failed, underscoring the importance of domain-specific adaptation and rigorous testing.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决为神经处理器（NPU）使用厂商特定领域语言高效开发高性能计算内核的难题，该任务需要深厚硬件专业知识且通用大语言模型因严格约束和数据稀缺而表现不佳。为此，论文提出了AscendKernelGen，一个集生成与评估于一体的框架，其方法包括构建包含真实实现思维链的高质量数据集Ascend-CoT，通过监督微调和基于执行反馈的强化学习训练领域自适应模型KernelGen-LM，并设计用于评估的NPUKernelBench基准。主要实验结果表明，该方法显著超越了基线性能，将复杂内核的编译成功率从0%提升至95.5%，并在基线完全失败的情况下实现了64.3%的功能正确率，凸显了领域特定推理和严格评估在自动化加速器感知代码生成中的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">Finite-Time Analysis of Simultaneous Double Q-learning</div>
<div class="meta-line">Authors: Hyunjun Na, Donghwan Lee</div>
<div class="meta-line">First: 2024-06-14T11:47:25+00:00 · Latest: 2026-01-12T02:42:33+00:00</div>
<div class="meta-line">Comments: 31 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.09946v2">Abs</a> · <a href="https://arxiv.org/pdf/2406.09946v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">$Q$-learning is one of the most fundamental reinforcement learning (RL) algorithms. Despite its widespread success in various applications, it is prone to overestimation bias in the $Q$-learning update. To address this issue, double $Q$-learning employs two independent $Q$-estimators which are randomly selected and updated during the learning process. This paper proposes a modified double $Q$-learning, called simultaneous double $Q$-learning (SDQ), with its finite-time analysis. SDQ eliminates the need for random selection between the two $Q$-estimators, and this modification allows us to analyze double $Q$-learning through the lens of a novel switching system framework facilitating efficient finite-time analysis. Empirical studies demonstrate that SDQ converges faster than double $Q$-learning while retaining the ability to mitigate the maximization bias. Finally, we derive a finite-time expected error bound for SDQ.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>同步双Q学习的有限时间分析</div>
<div class="mono" style="margin-top:8px">Q学习是最基础的强化学习算法之一，尽管在各领域应用广泛，但其Q值更新过程易产生高估偏差。为应对此问题，双Q学习采用两个独立的Q估计器，在学习过程中随机选择并更新。本文提出一种改进的双Q学习方法——同步双Q学习（SDQ），并给出其有限时间分析。SDQ无需在两个Q估计器间随机选择，这一改进使我们能通过新型切换系统框架分析双Q学习，从而实现高效的有限时间分析。实验表明SDQ在保持缓解最大化偏差能力的同时，收敛速度优于双Q学习。最后，我们推导出SDQ的有限时间期望误差界。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the overestimation bias inherent in standard Q-learning, a fundamental reinforcement learning algorithm. To address this, the authors propose Simultaneous Double Q-learning (SDQ), a modified version of double Q-learning that eliminates the random selection between its two Q-estimators, instead updating them simultaneously. This modification enables a novel switching system framework for finite-time analysis. The main experimental results demonstrate that SDQ converges faster than traditional double Q-learning while still effectively mitigating the maximization bias, and the paper provides a formal finite-time expected error bound for the proposed algorithm.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决标准Q-learning这一强化学习基础算法中固有的高估偏差。为此，作者提出了同步双Q学习（SDQ），这是对双Q学习的一种改进，它取消了在两个Q估计器之间的随机选择，转而同步更新它们。这一修改使得能够通过一种新颖的切换系统框架进行有限时间分析。主要实验结果表明，SDQ比传统双Q学习收敛更快，同时仍能有效缓解最大化偏差，并且论文为该算法推导出了一个正式的有限时间期望误差界。</div>
</details>
</div>
<div class="card">
<div class="title">Rewarding Creativity: A Human-Aligned Generative Reward Model for Reinforcement Learning in Storytelling</div>
<div class="meta-line">Authors: Zhaoyan Li, Hang Lei, Yujia Wang, Lanbo Liu, Hao Liu, Liang Yu</div>
<div class="meta-line">First: 2026-01-12T02:39:47+00:00 · Latest: 2026-01-12T02:39:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07149v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07149v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Language Models (LLMs) can generate fluent text, producing high-quality creative stories remains challenging. Reinforcement Learning (RL) offers a promising solution but faces two critical obstacles: designing reliable reward signals for subjective storytelling quality and mitigating training instability. This paper introduces the Reinforcement Learning for Creative Storytelling (RLCS) framework to systematically address both challenges. First, we develop a Generative Reward Model (GenRM) that provides multi-dimensional analysis and explicit reasoning about story preferences, trained through supervised fine-tuning on demonstrations with reasoning chains distilled from strong teacher models, followed by GRPO-based refinement on expanded preference data. Second, we introduce an entropy-based reward shaping strategy that dynamically prioritizes learning on confident errors and uncertain correct predictions, preventing overfitting on already-mastered patterns. Experiments demonstrate that GenRM achieves 68\% alignment with human creativity judgments, and RLCS significantly outperforms strong baselines including Gemini-2.5-Pro in overall story quality. This work provides a practical pipeline for applying RL to creative domains, effectively navigating the dual challenges of reward modeling and training stability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>奖励创造力：一种面向人类对齐的生成式奖励模型用于故事创作中的强化学习</div>
<div class="mono" style="margin-top:8px">尽管大语言模型能够生成流畅文本，但创作高质量创意故事仍具挑战。强化学习提供了一种有前景的解决方案，但面临两大关键障碍：为具有主观性的故事质量设计可靠奖励信号，以及缓解训练不稳定性。本文提出创意故事创作强化学习框架，系统性地应对这两项挑战。首先，我们开发了一种生成式奖励模型，该模型提供对故事偏好多维度分析和显式推理，其训练过程包括：在带有从强教师模型蒸馏出的推理链的演示数据上进行监督微调，随后在扩展的偏好数据上基于GRPO方法进行精炼。其次，我们引入一种基于熵的奖励塑形策略，动态优先学习置信度高的错误和不确定的正确预测，防止对已掌握模式的过拟合。实验表明，该生成式奖励模型在人类创造力评判上达到68%的对齐度，且该框架在整体故事质量上显著优于包括Gemini-2.5-Pro在内的强基线模型。本工作为将强化学习应用于创意领域提供了实用流程，有效应对了奖励建模与训练稳定性的双重挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of generating high-quality creative stories with LLMs, where reinforcement learning is hindered by the difficulty of designing reliable reward signals for subjective quality and by training instability. The proposed RLCS framework introduces a Generative Reward Model trained with supervised fine-tuning on demonstrations containing reasoning chains and refined with GRPO on expanded preference data, alongside an entropy-based reward shaping strategy that dynamically focuses learning on confident errors and uncertain correct predictions to prevent overfitting. Experimental results show that the reward model achieves 68% alignment with human creativity judgments, and the RLCS framework outperforms strong baselines like Gemini-2.5-Pro in overall story quality, offering a practical pipeline for RL in creative domains.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型生成高质量创意故事的挑战，其中强化学习因难以设计主观质量的可靠奖励信号和训练不稳定性而受阻。提出的RLCS框架引入了一个生成式奖励模型，该模型通过在有推理链的演示上进行监督微调训练，并在扩展的偏好数据上使用GRPO进行精炼，同时结合一种基于熵的奖励塑造策略，动态优先学习自信的错误和不确定的正确预测以防止过拟合。实验结果表明，该奖励模型与人类创造力判断的对齐度达到68%，且RLCS框架在整体故事质量上显著优于包括Gemini-2.5-Pro在内的强基线，为创意领域的强化学习应用提供了实用流程。</div>
</details>
</div>
<div class="card">
<div class="title">Generating readily synthesizable small molecule fluorophore scaffolds with reinforcement learning</div>
<div class="meta-line">Authors: Ruhi Sayana, Kate Callon, Jennifer Xu, Jonathan Deutsch, Steven Chu, James Zou, John Janetzko, Rabindra V. Shivnaraine, Kyle Swanson</div>
<div class="meta-line">First: 2026-01-12T02:31:43+00:00 · Latest: 2026-01-12T02:31:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07145v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07145v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Developing new fluorophores for advanced imaging techniques requires exploring new chemical space. While generative AI approaches have shown promise in designing novel dye scaffolds, prior efforts often produced synthetically intractable candidates due to a lack of reaction constraints. Here, we developed SyntheFluor-RL, a generative AI model that employs known reaction libraries and molecular building blocks to create readily synthesizable fluorescent molecule scaffolds via reinforcement learning. To guide the generation of fluorophores, SyntheFluor-RL employs a scoring function built on multiple graph neural networks (GNNs) that predict key photophysical properties, including photoluminescence quantum yield, absorption, and emission wavelengths. These outputs are dynamically weighted and combined with a computed pi-conjugation score to prioritize candidates with desirable optical characteristics and synthetic feasibility. SyntheFluor-RL generated 11,590 candidate molecules, which were filtered to 19 structures predicted to possess dye-like properties. Of the 19 molecules, 14 were synthesized and 13 were experimentally confirmed. The top three were characterized, with the lead compound featuring a benzothiadiazole chromophore and exhibiting strong fluorescence (PLQY = 0.62), a large Stokes shift (97 nm), and a long excited-state lifetime (11.5 ns). These results demonstrate the effectiveness of SyntheFluor-RL in the identification of synthetically accessible fluorophores for further development.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用强化学习生成易于合成的小分子荧光团骨架</div>
<div class="mono" style="margin-top:8px">为先进成像技术开发新型荧光团需探索新的化学空间。尽管生成式AI方法在设计新型染料骨架方面展现出潜力，但先前研究常因缺乏反应约束而生成难以合成的候选分子。本研究开发了SyntheFluor-RL生成式AI模型，该模型利用已知反应库和分子构建模块，通过强化学习创建易于合成的荧光分子骨架。为引导荧光团生成，SyntheFluor-RL采用基于多个图神经网络（GNN）构建的评分函数，可预测光致发光量子产率、吸收与发射波长等关键光物理性质。这些输出经动态加权后与计算的π共轭评分结合，以优先筛选具有理想光学特性与合成可行性的候选分子。SyntheFluor-RL共生成11,590个候选分子，经筛选获得19个预测具有类染料特性的结构。其中14个分子被成功合成，13个获得实验验证。对前三名分子进行表征显示，先导化合物以苯并噻二唑为发色团，具备强荧光性（PLQY=0.62）、大斯托克斯位移（97 nm）及长激发态寿命（11.5 ns）。这些结果证明了SyntheFluor-RL在识别可合成荧光团以推进后续开发方面的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work was to address the challenge of generating novel fluorophores for advanced imaging that are not only optically promising but also synthetically feasible, as prior AI-generated designs often lacked practical reaction constraints. The method introduced SyntheFluor-RL, a generative AI model that uses reinforcement learning guided by known reaction libraries and molecular building blocks to ensure synthetic accessibility, alongside a scoring function based on graph neural networks to predict key photophysical properties like quantum yield and wavelengths. Experimental results showed that from 11,590 generated candidates, 19 were predicted as dye-like, with 14 synthesized and 13 experimentally confirmed; the top compound exhibited strong fluorescence, a large Stokes shift, and a long excited-state lifetime, validating the model&#x27;s effectiveness in producing readily synthesizable fluorophores.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决为先进成像技术开发新型荧光团时面临的挑战，即确保生成的分子不仅具有理想的光学特性，还需具备合成可行性，因为先前的人工智能生成设计常因缺乏反应约束而难以合成。方法上，研究提出了SyntheFluor-RL，这是一种基于强化学习的生成式人工智能模型，它利用已知反应库和分子构建块来保证合成可及性，并通过图神经网络构建的评分函数预测光物理性质，如量子产率和波长。实验结果表明，从生成的11,590个候选分子中，筛选出19个预测具有染料特性的结构，其中14个被合成，13个得到实验验证；最优化合物表现出强荧光、大斯托克斯位移和长激发态寿命，证明了该模型在识别可合成荧光团方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">ENTRA: Entropy-Based Redundancy Avoidance in Large Language Model Reasoning</div>
<div class="meta-line">Authors: Ruichu Cai, Haopeng Du, Qingwen Lin, Yutong Chen, Zijian Li, Boyan Xu</div>
<div class="meta-line">First: 2026-01-12T01:26:30+00:00 · Latest: 2026-01-12T01:26:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07123v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07123v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Reasoning Models (LRMs) often suffer from overthinking, generating unnecessarily long reasoning chains even for simple tasks. This leads to substantial computational overhead with limited performance gain, primarily due to redundant verification and repetitive generation. While prior work typically constrains output length or optimizes correctness, such coarse supervision fails to guide models toward concise yet accurate inference. In this paper, we propose ENTRA, an entropy-based training framework that suppresses redundant reasoning while preserving performance. ENTRA first estimates the token-level importance using a lightweight Bidirectional Importance Estimation (BIE) method, which accounts for both prediction confidence and forward influence. It then computes a redundancy reward based on the entropy of low-importance tokens, normalized by its theoretical upper bound, and optimizes this reward via reinforcement learning. Experiments on mathematical reasoning benchmarks demonstrate that ENTRA reduces output length by 37% to 53% with no loss-and in some cases, gains-in accuracy. Our approach offers a principled and efficient solution to reduce overthinking in LRMs, and provides a generalizable path toward redundancy-aware reasoning optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ENTRA：基于熵的大语言模型推理冗余规避方法</div>
<div class="mono" style="margin-top:8px">大型推理模型在处理简单任务时经常出现过度思考现象，生成不必要的冗长推理链，导致计算开销大幅增加而性能提升有限，主要源于冗余验证和重复生成。现有研究通常通过约束输出长度或优化正确性进行干预，但这类粗粒度监督难以引导模型实现简洁而准确的推理。本文提出ENTRA——一种基于熵的训练框架，在保持性能的同时抑制冗余推理。ENTRA首先通过轻量级双向重要性估计方法评估词元级重要性，该方法同时考虑预测置信度与前向影响力；随后基于低重要性词元的熵计算冗余奖励（经理论上界归一化），并通过强化学习优化该奖励。数学推理基准测试表明，ENTRA在保持准确率不损失（部分任务甚至提升）的前提下，将输出长度缩减37%至53%。该方法为减少大型推理模型的过度思考提供了原则性高效解决方案，并为冗余感知的推理优化开辟了可推广的技术路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue of overthinking in Large Reasoning Models, where they produce excessively long reasoning chains for simple tasks, leading to computational inefficiency without significant performance benefits. To mitigate this, the authors propose ENTRA, an entropy-based training framework that uses a lightweight Bidirectional Importance Estimation method to assess token-level importance based on prediction confidence and forward influence, then computes a redundancy reward from the entropy of low-importance tokens and optimizes it via reinforcement learning. Experimental results on mathematical reasoning benchmarks show that ENTRA reduces output length by 37% to 53% while maintaining or even improving accuracy, offering a principled approach to suppress redundant reasoning.</div>
<div class="mono" style="margin-top:8px">本文针对大型推理模型中的过度思考问题展开研究，该问题导致模型即使处理简单任务也会生成过长的推理链，造成计算资源浪费且性能提升有限。为解决此问题，作者提出了ENTRA，一种基于熵的训练框架，采用轻量级双向重要性估计方法，结合预测置信度和前向影响来评估令牌级重要性，然后基于低重要性令牌的熵计算冗余奖励，并通过强化学习进行优化。在数学推理基准测试上的实验结果表明，ENTRA能将输出长度减少37%至53%，同时保持甚至提升准确性，为抑制冗余推理提供了一种原则性方法。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Cloud Network Resilience via a Robust LLM-Empowered Multi-Agent Reinforcement Learning Framework</div>
<div class="meta-line">Authors: Yixiao Peng, Hao Hu, Feiyang Li, Xinye Cao, Yingchang Jiang, Jipeng Tang, Guoshun Nan, Yuling Liu</div>
<div class="meta-line">First: 2026-01-12T01:25:41+00:00 · Latest: 2026-01-12T01:25:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07122v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07122v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While virtualization and resource pooling empower cloud networks with structural flexibility and elastic scalability, they inevitably expand the attack surface and challenge cyber resilience. Reinforcement Learning (RL)-based defense strategies have been developed to optimize resource deployment and isolation policies under adversarial conditions, aiming to enhance system resilience by maintaining and restoring network availability. However, existing approaches lack robustness as they require retraining to adapt to dynamic changes in network structure, node scale, attack strategies, and attack intensity. Furthermore, the lack of Human-in-the-Loop (HITL) support limits interpretability and flexibility. To address these limitations, we propose CyberOps-Bots, a hierarchical multi-agent reinforcement learning framework empowered by Large Language Models (LLMs). Inspired by MITRE ATT&amp;CK&#x27;s Tactics-Techniques model, CyberOps-Bots features a two-layer architecture: (1) An upper-level LLM agent with four modules--ReAct planning, IPDRR-based perception, long-short term memory, and action/tool integration--performs global awareness, human intent recognition, and tactical planning; (2) Lower-level RL agents, developed via heterogeneous separated pre-training, execute atomic defense actions within localized network regions. This synergy preserves LLM adaptability and interpretability while ensuring reliable RL execution. Experiments on real cloud datasets show that, compared to state-of-the-art algorithms, CyberOps-Bots maintains network availability 68.5% higher and achieves a 34.7% jumpstart performance gain when shifting the scenarios without retraining. To our knowledge, this is the first study to establish a robust LLM-RL framework with HITL support for cloud defense. We will release our framework to the community, facilitating the advancement of robust and autonomous defense in cloud networks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于鲁棒性LLM赋能的层次化多智能体强化学习框架增强云网络韧性</div>
<div class="mono" style="margin-top:8px">虚拟化与资源池化技术虽赋予云网络结构灵活性与弹性扩展能力，却也扩大了攻击面并挑战网络韧性。基于强化学习的防御策略通过优化对抗环境下的资源部署与隔离策略，致力于维持并恢复网络可用性以增强系统韧性。然而，现有方法缺乏鲁棒性，需重新训练以适应网络结构、节点规模、攻击策略与强度的动态变化，且缺乏人机协同支持限制了可解释性与灵活性。为此，我们提出CyberOps-Bots——一个由大语言模型赋能的层次化多智能体强化学习框架。该框架受MITRE ATT&amp;CK战术-技术模型启发，采用双层架构：（1）上层LLM智能体通过反应式规划、基于IPDRR的感知、长短时记忆及行动/工具集成四大模块，实现全局态势感知、人类意图识别与战术规划；（2）下层RL智能体通过异构分离预训练开发，在局部网络区域执行原子化防御动作。这种协同机制在保持LLM适应性与可解释性的同时，确保了RL执行的可靠性。在真实云数据集上的实验表明，相较于前沿算法，CyberOps-Bots在网络可用性上保持68.5%的优势，并在无需重训练的场景切换中实现34.7%的跳跃式性能提升。据我们所知，这是首个构建具备人机协同支持的鲁棒性LLM-RL框架用于云防御的研究。我们将向社区开源该框架，以推动云网络鲁棒自主防御的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the lack of robustness and interpretability in existing Reinforcement Learning (RL)-based cloud network defense systems, which struggle to adapt to dynamic network and attack changes without retraining. The authors propose CyberOps-Bots, a hierarchical multi-agent framework that integrates a Large Language Model (LLM) for global planning and human intent recognition with lower-level RL agents for executing localized defense actions, inspired by the MITRE ATT&amp;CK model. Experimental results on real cloud datasets demonstrate that the framework maintains network availability 68.5% higher than state-of-the-art methods and achieves a 34.7% jumpstart performance gain in new scenarios without requiring retraining, establishing a robust LLM-RL system with human-in-the-loop support for cloud defense.</div>
<div class="mono" style="margin-top:8px">本文针对现有基于强化学习的云网络防御系统缺乏鲁棒性和可解释性、难以适应动态网络与攻击变化而需重新训练的问题，提出了一种名为CyberOps-Bots的分层多智能体框架。该框架受MITRE ATT&amp;CK模型启发，集成了大型语言模型进行全局规划与人类意图识别，并利用下层强化学习智能体执行局部防御动作。在真实云数据集上的实验表明，该框架相比最先进算法，能将网络可用性提高68.5%，并在未重新训练的场景转换中实现34.7%的启动性能提升，从而首次建立了一个具备人机交互支持的鲁棒LLM-RL云防御系统。</div>
</details>
</div>
<div class="card">
<div class="title">Reward-Preserving Attacks For Robust Reinforcement Learning</div>
<div class="meta-line">Authors: Lucas Schott, Elies Gherbi, Hatem Hajri, Sylvain Lamprier</div>
<div class="meta-line">First: 2026-01-12T01:14:03+00:00 · Latest: 2026-01-12T01:14:03+00:00</div>
<div class="meta-line">Comments: 19 pages, 6 figures, 4 algorithms, preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07118v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07118v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Adversarial robustness in RL is difficult because perturbations affect entire trajectories: strong attacks can break learning, while weak attacks yield little robustness, and the appropriate strength varies by state. We propose $α$-reward-preserving attacks, which adapt the strength of the adversary so that an $α$ fraction of the nominal-to-worst-case return gap remains achievable at each state. In deep RL, we use a gradient-based attack direction and learn a state-dependent magnitude $η\le η_{\mathcal B}$ selected via a critic $Q^π_α((s,a),η)$ trained off-policy over diverse radii. This adaptive tuning calibrates attack strength and, with intermediate $α$, improves robustness across radii while preserving nominal performance, outperforming fixed- and random-radius baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向鲁棒强化学习的奖励保持型攻击</div>
<div class="mono" style="margin-top:8px">强化学习中的对抗鲁棒性研究面临挑战，因为扰动会影响完整轨迹：强攻击会破坏学习过程，弱攻击则难以提升鲁棒性，且适宜的攻击强度随状态变化。我们提出α-奖励保持型攻击，通过动态调整对抗强度，使每个状态下仍能保持名义回报与最差情形回报差距的α比例。在深度强化学习中，我们采用基于梯度的攻击方向，并通过离线训练、覆盖多半径范围的评判器Q^π_α((s,a),η)学习状态相关的强度参数η≤η_ℬ。这种自适应调节机制能校准攻击强度，在中间值α设定下，可在保持名义性能的同时提升跨半径鲁棒性，其效果优于固定半径与随机半径基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of adversarial robustness in reinforcement learning, where perturbations impact entire trajectories and finding an appropriate attack strength is state-dependent. The authors propose α-reward-preserving attacks, which adapt the adversary&#x27;s strength to preserve a fraction α of the nominal-to-worst-case return gap at each state. In deep RL, they use a gradient-based attack direction with a state-dependent magnitude learned via an off-policy critic, enabling adaptive tuning. Experimental results show that this method, with intermediate α values, improves robustness across various perturbation radii while maintaining nominal performance, outperforming fixed- and random-radius baselines.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中的对抗鲁棒性挑战，其中扰动影响整个轨迹，且合适的攻击强度因状态而异。作者提出α-奖励保持攻击，通过调整对手强度，使每个状态保留名义与最坏情况回报差距的α比例。在深度强化学习中，他们采用基于梯度的攻击方向，并通过离策略训练的评论家学习状态相关的幅度，实现自适应调整。实验结果表明，该方法在中间α值下，能在不同扰动半径上提高鲁棒性，同时保持名义性能，优于固定和随机半径的基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Certifying Stability of Reinforcement Learning Policies using Generalized Lyapunov Functions</div>
<div class="meta-line">Authors: Kehan Long, Jorge Cortés, Nikolay Atanasov</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-16T07:36:40+00:00 · Latest: 2026-01-12T01:08:26+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.10947v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.10947v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Establishing stability certificates for closed-loop systems under reinforcement learning (RL) policies is essential to move beyond empirical performance and offer guarantees of system behavior. Classical Lyapunov methods require a strict stepwise decrease in the Lyapunov function but such certificates are difficult to construct for learned policies. The RL value function is a natural candidate but it is not well understood how it can be adapted for this purpose. To gain intuition, we first study the linear quadratic regulator (LQR) problem and make two key observations. First, a Lyapunov function can be obtained from the value function of an LQR policy by augmenting it with a residual term related to the system dynamics and stage cost. Second, the classical Lyapunov decrease requirement can be relaxed to a generalized Lyapunov condition requiring only decrease on average over multiple time steps. Using this intuition, we consider the nonlinear setting and formulate an approach to learn generalized Lyapunov functions by augmenting RL value functions with neural network residual terms. Our approach successfully certifies the stability of RL policies trained on Gymnasium and DeepMind Control benchmarks. We also extend our method to jointly train neural controllers and stability certificates using a multi-step Lyapunov loss, resulting in larger certified inner approximations of the region of attraction compared to the classical Lyapunov approach. Overall, our formulation enables stability certification for a broad class of systems with learned policies by making certificates easier to construct, thereby bridging classical control theory and modern learning-based methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用广义李雅普诺夫函数认证强化学习策略的稳定性</div>
<div class="mono" style="margin-top:8px">为强化学习策略下的闭环系统建立稳定性证书，对于超越经验性性能表现、提供系统行为保障至关重要。经典李雅普诺夫方法要求李雅普诺夫函数严格逐步递减，但此类证书难以针对学习得到的策略构建。强化学习价值函数是一个自然的候选对象，但其如何适用于此目的尚未得到充分理解。为获得直观认识，我们首先研究线性二次调节器问题，并得出两个关键观察：第一，通过将LQR策略的价值函数与系统动态和阶段成本相关的残差项结合，可构造出李雅普诺夫函数；第二，经典的李雅普诺夫递减要求可放宽为广义李雅普诺夫条件，仅需在多时间步长上平均递减。基于此思路，我们考虑非线性场景，提出通过为强化学习价值函数添加神经网络残差项来学习广义李雅普诺夫函数的方法。该方法成功认证了在Gymnasium和DeepMind Control基准测试中训练的强化学习策略的稳定性。我们还将该方法扩展至通过多步李雅普诺夫损失联合训练神经控制器与稳定性证书，相比经典李雅普诺夫方法，获得了更大范围的吸引域内近似认证。总体而言，我们的框架通过降低证书构建难度，实现了对广泛学习策略系统的稳定性认证，从而连接了经典控制理论与现代学习方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for formal stability guarantees in systems controlled by reinforcement learning (RL) policies, moving beyond empirical validation. The method develops generalized Lyapunov functions by augmenting the RL value function with a neural network residual term, relaxing the classical requirement to demand a decrease only on average over multiple steps. Experimental results on Gymnasium and DeepMind Control benchmarks show successful stability certification for trained RL policies, and an extended training approach yields larger certified regions of attraction compared to classical Lyapunov methods.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习策略控制的系统，提出了超越经验验证、提供形式化稳定性保证的需求。其方法通过用神经网络残差项增强强化学习价值函数，构建广义李雅普诺夫函数，将经典的李雅普诺夫递减要求放宽为仅需在多步上平均递减。在Gymnasium和DeepMind Control基准测试上的实验结果表明，该方法成功认证了已训练强化学习策略的稳定性，并且一种扩展的训练方法相比经典李雅普诺夫方法能产生更大的经认证的吸引域内近似。</div>
</details>
</div>
<div class="card">
<div class="title">MEDVISTAGYM: A Scalable Training Environment for Thinking with Medical Images via Tool-Integrated Reinforcement Learning</div>
<div class="meta-line">Authors: Meng Lu, Yuxing Lu, Yuchen Zhuang, Megan Mullins, Yang Xie, Guanghua Xiao, Charles Fleming, Wenqi Shi, Xuan Wang</div>
<div class="meta-line">First: 2026-01-12T00:11:10+00:00 · Latest: 2026-01-12T00:11:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07107v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07107v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision language models (VLMs) achieve strong performance on general image understanding but struggle to think with medical images, especially when performing multi-step reasoning through iterative visual interaction. Medical VLMs often rely on static visual embeddings and single-pass inference, preventing models from re-examining, verifying, or refining visual evidence during reasoning. While tool-integrated reasoning offers a promising path forward, open-source VLMs lack the training infrastructure to learn effective tool selection, invocation, and coordination in multi-modal medical reasoning. We introduce MedVistaGym, a scalable and interactive training environment that incentivizes tool-integrated visual reasoning for medical image analysis. MedVistaGym equips VLMs to determine when and which tools to invoke, localize task-relevant image regions, and integrate single or multiple sub-image evidence into interleaved multimodal reasoning within a unified, executable interface for agentic training. Using MedVistaGym, we train MedVistaGym-R1 to interleave tool use with agentic reasoning through trajectory sampling and end-to-end reinforcement learning. Across six medical VQA benchmarks, MedVistaGym-R1-8B exceeds comparably sized tool-augmented baselines by 19.10% to 24.21%, demonstrating that structured agentic training--not tool access alone--unlocks effective tool-integrated reasoning for medical image analysis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MEDVISTAGYM：基于工具集成强化学习的医学图像思维训练可扩展环境</div>
<div class="mono" style="margin-top:8px">视觉语言模型在通用图像理解上表现优异，但在医学图像的思维推理方面存在困难，尤其是在通过迭代视觉交互进行多步推理时。现有医学视觉语言模型多依赖静态视觉嵌入和单次推理，无法在推理过程中重新审视、验证或优化视觉证据。虽然工具集成推理提供了可行路径，但开源视觉语言模型缺乏训练基础设施来学习多模态医学推理中的工具选择、调用与协调。我们提出MedVistaGym——一个可扩展的交互式训练环境，通过激励工具集成的视觉推理来促进医学图像分析。该环境使视觉语言模型能够决策工具调用时机与类型、定位任务相关图像区域，并在统一可执行界面中将单/多子图像证据整合至交错式多模态推理中，以支持智能体训练。基于MedVistaGym，我们通过轨迹采样和端到端强化学习训练出MedVistaGym-R1模型，实现工具使用与智能体推理的交错执行。在六项医学视觉问答基准测试中，MedVistaGym-R1-8B以19.10%至24.21%的优势超越同规模工具增强基线模型，证明结构化智能体训练（而非单纯工具调用）能有效解锁医学图像分析中的工具集成推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that while vision language models (VLMs) excel at general image understanding, they struggle with the multi-step, iterative reasoning required for medical image analysis, as they typically rely on static visual embeddings and single-pass inference without the ability to re-examine evidence. To address this, the authors introduce MedVistaGym, a scalable training environment that uses tool-integrated reinforcement learning to train VLMs to dynamically select, invoke, and coordinate tools, localize relevant image regions, and integrate sub-image evidence into interleaved multimodal reasoning. The main experimental result is that their trained model, MedVistaGym-R1-8B, outperforms comparably sized tool-augmented baselines by 19.10% to 24.21% across six medical visual question answering benchmarks, demonstrating that structured agentic training is key to unlocking effective tool-integrated reasoning.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，尽管视觉语言模型在通用图像理解上表现良好，但在处理医学图像时难以进行多步骤迭代推理，因为它们通常依赖静态视觉嵌入和单次推理，无法重新检查或细化视觉证据。为解决这一问题，作者提出了MedVistaGym，这是一个可扩展的训练环境，通过工具集成强化学习训练视觉语言模型动态选择、调用和协调工具，定位相关图像区域，并将子图像证据整合到交错的多模态推理中。主要实验结果表明，他们训练的模型MedVistaGym-R1-8B在六个医学视觉问答基准测试中，比同等规模的工具增强基线模型性能高出19.10%至24.21%，证明结构化的智能体训练是解锁有效工具集成推理的关键。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Bayesian Optimization via Tempered Posteriors</div>
<div class="meta-line">Authors: Jiguang Li, Hengrui Luo</div>
<div class="meta-line">First: 2026-01-11T23:34:24+00:00 · Latest: 2026-01-11T23:34:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07094v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07094v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bayesian optimization (BO) iteratively fits a Gaussian process (GP) surrogate to accumulated evaluations and selects new queries via an acquisition function such as expected improvement (EI). In practice, BO often concentrates evaluations near the current incumbent, causing the surrogate to become overconfident and to understate predictive uncertainty in the region guiding subsequent decisions. We develop a robust GP-based BO via tempered posterior updates, which downweight the likelihood by a power $α\in (0,1]$ to mitigate overconfidence under local misspecification. We establish cumulative regret bounds for tempered BO under a family of generalized improvement rules, including EI, and show that tempering yields strictly sharper worst-case regret guarantees than the standard posterior $(α=1)$, with the most favorable guarantees occurring near the classical EI choice.
  Motivated by our theoretic findings, we propose a prequential procedure for selecting $α$ online: it decreases $α$ when realized prediction errors exceed model-implied uncertainty and returns $α$ toward one as calibration improves. Empirical results demonstrate that tempering provides a practical yet theoretically grounded tool for stabilizing BO surrogates under localized sampling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于退火后验的鲁棒贝叶斯优化方法</div>
<div class="mono" style="margin-top:8px">贝叶斯优化（BO）通过迭代拟合高斯过程（GP）代理模型，并借助期望改进（EI）等采集函数选择新查询点。实践中，BO常使评估点聚集在当前最优解附近，导致代理模型过度自信，并低估指导后续决策区域的预测不确定性。本文提出一种基于退火后验更新的鲁棒GP-BO方法，通过幂次α∈(0,1]对似然函数进行降权，以缓解局部设定偏误下的过度自信问题。我们在包含EI在内的广义改进规则族下建立了退火BO的累积遗憾界，证明退火处理相比标准后验（α=1）能获得严格更优的最坏情况遗憾保证，且最优保证出现在经典EI选择附近。基于理论发现，我们提出在线选择α的序贯程序：当实际预测误差超过模型隐含的不确定性时降低α值，在校准改善时使α趋近于1。实证结果表明，退火处理为局部采样下的BO代理模型稳定提供了兼具理论依据与实践价值的工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue of overconfidence in Bayesian optimization (BO) when evaluations cluster near the current best point, leading to underestimated predictive uncertainty and poor decision-making. To mitigate this, the authors propose a robust BO method that uses tempered posterior updates, downweighting the likelihood by a factor α in (0,1] to reduce overconfidence under local model misspecification. Theoretically, they establish cumulative regret bounds for tempered BO under generalized improvement rules, showing that tempering yields sharper worst-case guarantees than standard BO, especially near the classical expected improvement choice. Empirically, they introduce an online prequential procedure to adapt α based on prediction errors, and experimental results demonstrate that tempering stabilizes BO surrogates under localized sampling, providing a practical and theoretically grounded tool.</div>
<div class="mono" style="margin-top:8px">该论文针对贝叶斯优化（BO）中评估点聚集在当前最优值附近导致预测不确定性被低估和决策质量下降的过自信问题，提出了一种鲁棒的BO方法。该方法通过使用退火后验更新，将似然函数按因子α（0,1]进行降权，以减轻局部模型误设下的过自信。理论上，作者在广义改进规则下建立了退火BO的累积遗憾界，表明退火比标准BO具有更优的最坏情况保证，尤其在经典期望改进选择附近。实证上，他们引入了一种在线预检验程序，根据预测误差自适应调整α，实验结果表明退火能在局部采样下稳定BO代理模型，提供了一个实用且理论依据充分的工具。</div>
</details>
</div>
<div class="card">
<div class="title">X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests</div>
<div class="meta-line">Authors: Jie Wu, Haoling Li, Xin Zhang, Jiani Guo, Jane Luo, Steven Liu, Yangyu Huang, Ruihang Chu, Scarlett Li, Yujiu Yang</div>
<div class="meta-line">First: 2026-01-11T15:22:33+00:00 · Latest: 2026-01-11T15:22:33+00:00</div>
<div class="meta-line">Comments: Project: https://github.com/JieWu02/X-Coder</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06953v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06953v1">PDF</a> · <a href="https://github.com/JieWu02/X-Coder">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Competitive programming presents great challenges for Code LLMs due to its intensive reasoning demands and high logical complexity. However, current Code LLMs still rely heavily on real-world data, which limits their scalability. In this paper, we explore a fully synthetic approach: training Code LLMs with entirely generated tasks, solutions, and test cases, to empower code reasoning models without relying on real-world data. To support this, we leverage feature-based synthesis to propose a novel data synthesis pipeline called SynthSmith. SynthSmith shows strong potential in producing diverse and challenging tasks, along with verified solutions and tests, supporting both supervised fine-tuning and reinforcement learning. Based on the proposed synthetic SFT and RL datasets, we introduce the X-Coder model series, which achieves a notable pass rate of 62.9 avg@8 on LiveCodeBench v5 and 55.8 on v6, outperforming DeepCoder-14B-Preview and AReal-boba2-14B despite having only 7B parameters. In-depth analysis reveals that scaling laws hold on our synthetic dataset, and we explore which dimensions are more effective to scale. We further provide insights into code-centric reinforcement learning and highlight the key factors that shape performance through detailed ablations and analysis. Our findings demonstrate that scaling high-quality synthetic data and adopting staged training can greatly advance code reasoning, while mitigating reliance on real-world coding data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>X-Coder：通过全合成任务、解决方案与测试推进竞技编程</div>
<div class="mono" style="margin-top:8px">竞技编程因其密集的推理需求和高逻辑复杂性，对代码大语言模型构成巨大挑战。然而，现有代码大模型仍严重依赖现实世界数据，限制了其可扩展性。本文探索了一种全合成方法：使用完全生成的任务、解决方案和测试用例训练代码大语言模型，从而在不依赖现实数据的情况下增强代码推理能力。为此，我们利用基于特征的合成技术，提出了名为SynthSmith的新型数据合成流程。SynthSmith在生成多样化且具有挑战性的任务、以及经过验证的解决方案和测试方面展现出强大潜力，同时支持监督微调和强化学习。基于提出的合成SFT与RL数据集，我们推出了X-Coder模型系列，该系列在LiveCodeBench v5上达到62.9 avg@8的显著通过率，在v6上达到55.8，仅凭70亿参数即超越DeepCoder-14B-Preview和AReal-boba2-14B。深入分析表明，缩放定律在我们的合成数据集上依然成立，并探索了哪些维度对扩展更有效。我们进一步提供了以代码为中心的强化学习洞见，通过详细消融实验和分析揭示了影响性能的关键因素。研究证明，扩展高质量合成数据并采用分阶段训练能极大推进代码推理能力，同时减少对现实世界编程数据的依赖。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of current Code LLMs in competitive programming, which require intensive reasoning but are constrained by reliance on real-world data. To overcome this, the authors propose a fully synthetic approach using a novel pipeline called SynthSmith, which generates diverse tasks, verified solutions, and test cases for supervised fine-tuning and reinforcement learning. Based on this synthetic data, they develop the X-Coder model series, achieving a pass rate of 62.9 avg@8 on LiveCodeBench v5 and 55.8 on v6 with only 7B parameters, outperforming larger models like DeepCoder-14B-Preview and AReal-boba2-14B. The study also validates scaling laws on synthetic data, explores effective scaling dimensions, and provides insights into code-centric reinforcement learning through detailed analysis, demonstrating that high-quality synthetic data and staged training can advance code reasoning while reducing dependency on real-world data.</div>
<div class="mono" style="margin-top:8px">本文针对当前代码大语言模型在竞争性编程中面临的挑战，即推理需求高但受限于对真实世界数据的依赖，提出了一种完全合成的方法。作者开发了名为SynthSmith的新颖数据合成流程，生成多样化的任务、已验证的解决方案和测试用例，用于监督微调和强化学习。基于此合成数据，他们推出了X-Coder模型系列，仅用70亿参数就在LiveCodeBench v5和v6上分别实现了62.9 avg@8和55.8的通过率，超越了DeepCoder-14B-Preview和AReal-boba2-14B等更大模型。研究还验证了合成数据上的缩放定律，探讨了有效的缩放维度，并通过详细分析提供了代码中心强化学习的关键见解，表明高质量合成数据和分阶段训练能显著提升代码推理能力，同时减少对真实世界编码数据的依赖。</div>
</details>
</div>
<div class="card">
<div class="title">Calibrating Agent-Based Financial Markets Simulators with Pretrainable Automatic Posterior Transformation-Based Surrogates</div>
<div class="meta-line">Authors: Boquan Jiang, Zhenhua Yang, Chenkai Wang, Muyao Zhong, Heping Fang, Peng Yang</div>
<div class="meta-line">First: 2026-01-11T14:05:26+00:00 · Latest: 2026-01-11T14:05:26+00:00</div>
<div class="meta-line">Comments: 32 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06920v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06920v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Calibrating Agent-Based Models (ABMs) is an important optimization problem for simulating the complex social systems, where the goal is to identify the optimal parameter of a given ABM by minimizing the discrepancy between the simulated data and the real-world observations. Unfortunately, it suffers from the extensive computational costs of iterative evaluations, which involves the expensive simulation with the candidate parameter. While Surrogate-Assisted Evolutionary Algorithms (SAEAs) have been widely adopted to alleviate the computational burden, existing methods face two key limitations: 1) surrogating the original evaluation function is hard due the nonlinear yet multi-modal nature of the ABMs, and 2) the commonly used surrogates cannot share the optimization experience among multiple calibration tasks, making the batched calibration less effective. To address these issues, this work proposes Automatic posterior transformation with Negatively Correlated Search and Adaptive Trust-Region (ANTR). ANTR first replaces the traditional surrogates with a pretrainable neural density estimator that directly models the posterior distribution of the parameters given observed data, thereby aligning the optimization objective with parameter-space accuracy. Furthermore, we incorporate a diversity-preserving search strategy to prevent premature convergence and an adaptive trust-region method to efficiently allocate computational resources. We take two representative ABM-based financial market simulators as the test bench as due to the high non-linearity. Experiments demonstrate that the proposed ANTR significantly outperforms conventional metaheuristics and state-of-the-art SAEAs in both calibration accuracy and computational efficiency, particularly in batch calibration scenarios across multiple market conditions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于可预训练自动后验变换代理模型的金融多主体市场模拟器校准</div>
<div class="mono" style="margin-top:8px">多主体模型（ABM）校准是模拟复杂社会系统的重要优化问题，其目标是通过最小化模拟数据与真实观测数据之间的差异，确定给定ABM的最优参数。然而，该方法受限于迭代评估的高计算成本，涉及对候选参数进行昂贵的模拟。虽然代理辅助进化算法（SAEA）已被广泛采用以减轻计算负担，但现有方法面临两个关键局限：1）由于ABM的非线性和多模态特性，对原始评估函数进行代理建模十分困难；2）常用代理模型无法在多个校准任务间共享优化经验，导致批量校准效果不佳。为解决这些问题，本研究提出基于负相关搜索与自适应信赖域的自动后验变换方法（ANTR）。ANTR首先用可预训练的神经密度估计器替代传统代理模型，直接建模给定观测数据下的参数后验分布，从而使优化目标与参数空间精度对齐。此外，我们引入保持多样性的搜索策略以防止早熟收敛，并采用自适应信赖域方法高效分配计算资源。针对高度非线性特性，选取两个代表性ABM金融市场模拟器作为测试基准。实验表明，所提出的ANTR在校准精度和计算效率上均显著优于传统元启发式算法和最先进的SAEA，尤其在跨多种市场条件的批量校准场景中表现突出。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the computational challenge of calibrating Agent-Based Models (ABMs) for financial markets, where traditional methods are costly due to iterative simulations and existing surrogate-assisted approaches struggle with the models&#x27; nonlinear, multimodal nature and lack of experience sharing across tasks. The proposed method, ANTR, introduces a pretrainable neural density estimator to model the parameter posterior directly, combined with a diversity-preserving search and adaptive trust-region strategy to enhance optimization. Experimental results on financial market simulators show that ANTR outperforms conventional metaheuristics and state-of-the-art surrogate-assisted algorithms in both calibration accuracy and efficiency, especially in batch calibration across multiple market conditions.</div>
<div class="mono" style="margin-top:8px">本研究针对基于主体的金融市场模型校准中的计算挑战，传统方法因迭代模拟而成本高昂，现有代理辅助方法则难以处理模型的非线性、多模态特性且缺乏任务间经验共享。提出的ANTR方法采用可预训练的神经密度估计器直接建模参数后验分布，并结合多样性保持搜索与自适应信赖域策略以优化过程。在金融市场模拟器上的实验结果表明，ANTR在校准精度和计算效率上均优于传统元启发式算法和最先进的代理辅助算法，尤其在多种市场条件下的批量校准场景中表现突出。</div>
</details>
</div>
<div class="card">
<div class="title">Distributional Clarity: The Hidden Driver of RL-Friendliness in Large Language Models</div>
<div class="meta-line">Authors: Shaoning Sun, Mingzhu Cai, Huang He, Bingjin Chen, Siqi Bao, Yujiu Yang, Hua Wu, Haifeng Wang</div>
<div class="meta-line">First: 2026-01-11T13:34:44+00:00 · Latest: 2026-01-11T13:34:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06911v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06911v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language model families exhibit striking disparity in their capacity to benefit from reinforcement learning: under identical training, models like Qwen achieve substantial gains, while others like Llama yield limited improvements. Complementing data-centric approaches, we reveal that this disparity reflects a hidden structural property: \textbf{distributional clarity} in probability space. Through a three-stage analysis-from phenomenon to mechanism to interpretation-we uncover that RL-friendly models exhibit intra-class compactness and inter-class separation in their probability assignments to correct vs. incorrect responses. We quantify this clarity using the \textbf{Silhouette Coefficient} ($S$) and demonstrate that (1) high $S$ correlates strongly with RL performance; (2) low $S$ is associated with severe logic errors and reasoning instability. To confirm this property, we introduce a Silhouette-Aware Reweighting strategy that prioritizes low-$S$ samples during training. Experiments across six mathematical benchmarks show consistent improvements across all model families, with gains up to 5.9 points on AIME24. Our work establishes distributional clarity as a fundamental, trainable property underlying RL-Friendliness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分布清晰度：大语言模型强化学习友好性的隐藏驱动因素</div>
<div class="mono" style="margin-top:8px">语言模型家族在受益于强化学习的能力上存在显著差异：在相同训练条件下，如Qwen等模型能获得显著提升，而Llama等模型改进有限。我们通过现象-机制-解释的三阶段分析，揭示了这种差异源于概率空间中的隐藏结构特性——\textbf{分布清晰度}。强化学习友好型模型在对正确与错误响应的概率分配上表现出类内紧致性和类间分离性。我们使用\textbf{轮廓系数}（$S$）量化该清晰度，并证明：（1）高$S$值与强化学习性能强相关；（2）低$S$值伴随严重逻辑错误和推理不稳定性。为验证此特性，我们提出轮廓感知重加权策略，在训练中优先处理低$S$样本。在六个数学基准测试中，所有模型家族均获得稳定提升，AIME24最高提升5.9分。本研究确立了分布清晰度作为强化学习友好性可训练的基础特性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates why some large language model families benefit significantly from reinforcement learning while others show limited gains, identifying a hidden structural property termed &#x27;distributional clarity&#x27; as the key driver. The authors propose that RL-friendly models exhibit intra-class compactness and inter-class separation in their probability assignments to correct versus incorrect responses, which they quantify using the Silhouette Coefficient (S). Experimental results across six mathematical benchmarks demonstrate that high S strongly correlates with RL performance, and a Silhouette-Aware Reweighting strategy that prioritizes low-S samples during training yields consistent improvements, with gains up to 5.9 points on AIME24, establishing distributional clarity as a fundamental and trainable property.</div>
<div class="mono" style="margin-top:8px">本文探究了为何某些大语言模型家族能从强化学习中显著受益而其他模型则改进有限，揭示了一个称为“分布清晰度”的隐藏结构特性是关键驱动因素。作者提出，强化学习友好型模型在正确与错误响应的概率分配上表现出类内紧凑性和类间分离性，并使用轮廓系数（S）进行量化。在六个数学基准上的实验结果表明，高S值与强化学习性能强相关，而一种在训练中优先处理低S值样本的轮廓感知重加权策略能带来一致改进，如在AIME24上提升高达5.9分，从而确立了分布清晰度作为强化学习友好性的一个基本且可训练的特性。</div>
</details>
</div>
<div class="card">
<div class="title">Paraphrasing Adversarial Attack on LLM-as-a-Reviewer</div>
<div class="meta-line">Authors: Masahiro Kaneko</div>
<div class="meta-line">First: 2026-01-11T12:14:10+00:00 · Latest: 2026-01-11T12:14:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06884v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06884v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The use of large language models (LLMs) in peer review systems has attracted growing attention, making it essential to examine their potential vulnerabilities. Prior attacks rely on prompt injection, which alters manuscript content and conflates injection susceptibility with evaluation robustness. We propose the Paraphrasing Adversarial Attack (PAA), a black-box optimization method that searches for paraphrased sequences yielding higher review scores while preserving semantic equivalence and linguistic naturalness. PAA leverages in-context learning, using previous paraphrases and their scores to guide candidate generation. Experiments across five ML and NLP conferences with three LLM reviewers and five attacking models show that PAA consistently increases review scores without changing the paper&#x27;s claims. Human evaluation confirms that generated paraphrases maintain meaning and naturalness. We also find that attacked papers exhibit increased perplexity in reviews, offering a potential detection signal, and that paraphrasing submissions can partially mitigate attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>针对LLM即审稿人的释义对抗攻击</div>
<div class="mono" style="margin-top:8px">大型语言模型在同行评审系统中的应用日益受到关注，因此审视其潜在脆弱性至关重要。现有攻击方法依赖提示注入，这会改变稿件内容并将注入敏感性与评估鲁棒性混为一谈。我们提出释义对抗攻击——一种黑盒优化方法，通过搜索能获得更高评审分数且保持语义等价性与语言自然性的释义序列。该方法利用上下文学习机制，依据历史释义及其评分指导候选生成。在涵盖五个机器学习与自然语言处理会议、三种LLM审稿模型及五种攻击模型的实验中，PAA始终能在不改变论文主张的前提下提升评审分数。人工评估证实生成释义保持了语义与自然度。研究还发现受攻击论文的评审文本困惑度会升高，这为检测提供了潜在信号；而采用释义化投稿可部分缓解此类攻击。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the increasing deployment of large language models (LLMs) in peer review and the need to assess their robustness beyond simple prompt injection, this paper introduces the Paraphrasing Adversarial Attack (PAA). The method is a black-box optimization that uses in-context learning to iteratively search for semantically equivalent and linguistically natural paraphrases of a manuscript that elicit higher review scores from LLM reviewers. Experimental results across multiple conferences and models demonstrate that PAA consistently elevates review scores without altering the paper&#x27;s core claims, with human evaluation confirming the paraphrases&#x27; quality; the study also finds that attacked papers increase perplexity in reviews as a potential detection signal and that paraphrasing submissions can offer some defense.</div>
<div class="mono" style="margin-top:8px">本文的动机源于大型语言模型在同行评审中的日益广泛应用，以及需要评估其超越简单提示注入的鲁棒性。为此，论文提出了复述对抗攻击方法，这是一种黑盒优化方法，利用上下文学习迭代搜索论文的语义等价且语言自然的复述版本，以从LLM评审者处获得更高评分。在多个会议和模型上的实验结果表明，该方法能持续提高评审分数而不改变论文核心主张，人工评估也证实了复述文本的质量；研究还发现，被攻击的论文会提高评审文本的困惑度，这或可作为检测信号，而主动复述提交内容也能提供一定的防御效果。</div>
</details>
</div>
<div class="card">
<div class="title">Personality-Aware Reinforcement Learning for Persuasive Dialogue with LLM-Driven Simulation</div>
<div class="meta-line">Authors: Donghuo Zeng, Roberto Legaspi, Kazushi Ikeda</div>
<div class="meta-line">First: 2026-01-11T11:53:07+00:00 · Latest: 2026-01-11T11:53:07+00:00</div>
<div class="meta-line">Comments: 15 pages, 7 figures, 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06877v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06877v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Effective persuasive dialogue agents adapt their strategies to individual users, accounting for the evolution of their psychological states and intentions throughout conversations. We present a personality-aware reinforcement learning approach comprising three main modules: (1) a Strategy-Oriented Interaction Framework, which serves as an agenda-based strategy controller that selects strategy-level actions and generate responses via Maximal Marginal Relevance (MMR) retrieval to ensure contextual relevance, diversity, and scalable data generation; (2) Personality-Aware User Representation Learning, which produces an 81-dimensional mixed-type embedding predicted at each turn from recent exchanges and appended to the reinforcement learning state; and (3) a Dueling Double DQN (D3QN) model and Reward Prediction, in which the policy is conditioned on dialogue history and turn-level personality estimates and trained using a composite reward incorporating agreement intent, donation amount, and changeof-mind penalties. We use an agenda-based LLM simulation pipeline to generate diverse interactions, from which personality estimation is inferred from the generated utterances. Experiments on the PersuasionForGood (P4G) dataset augmented with simulated dialogues reveal three main findings: (i) turn-level personality conditioning improves policy adaptability and cumulative persuasion rewards; (ii) LLM-driven simulation enhances generalization to unseen user behaviors; and (iii) incorporating a change-of-mind penalty reduces post-agreement retractions while slightly improving donation outcomes. These results demonstrate that structured interaction, dynamic personality estimation, and behaviorally informed rewards together yield more effective persuasive policies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于LLM驱动模拟的个性化感知强化学习用于说服性对话</div>
<div class="mono" style="margin-top:8px">有效的说服性对话代理需根据个体用户调整策略，并考虑其在对话中心理状态与意图的动态演变。本文提出一种个性化感知强化学习方法，包含三个核心模块：(1) 策略导向交互框架：作为基于议程的策略控制器，通过最大边际相关性检索选择策略级动作并生成响应，确保上下文相关性、多样性与可扩展数据生成；(2) 个性化感知用户表征学习：在每轮对话中基于近期交互预测81维混合类型嵌入，并附加至强化学习状态；(3) 对决双深度Q网络模型与奖励预测：策略以对话历史及轮次级个性估计为条件，通过融合同意意图、捐赠金额与改变主意惩罚的复合奖励进行训练。采用基于议程的LLM模拟流程生成多样化交互，从中推断个性化估计。在增强模拟对话的PersuasionForGood数据集上的实验表明：(i) 轮次级个性条件化提升策略适应性与累积说服奖励；(ii) LLM驱动模拟增强对未见用户行为的泛化能力；(iii) 引入改变主意惩罚减少同意后反悔行为，并轻微改善捐赠结果。这些证明结构化交互、动态个性估计与行为感知奖励共同构建更有效的说服策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for persuasive dialogue agents that adapt to individual users by proposing a personality-aware reinforcement learning approach. The method integrates a strategy-oriented interaction framework for response generation, a module for learning dynamic user personality representations, and a Dueling Double DQN policy trained with a composite reward. Experimental results on an augmented PersuasionForGood dataset show that turn-level personality conditioning enhances policy adaptability and cumulative rewards, LLM-driven simulation improves generalization to unseen behaviors, and incorporating a change-of-mind penalty reduces post-agreement retractions while slightly boosting donation outcomes.</div>
<div class="mono" style="margin-top:8px">本文针对说服性对话代理需适应用户个性化需求的问题，提出了一种基于人格感知的强化学习方法。该方法整合了面向策略的交互框架用于生成回复、动态用户人格表征学习模块，以及使用复合奖励训练的Dueling Double DQN策略。在增强的PersuasionForGood数据集上的实验结果表明：回合级人格条件调节提升了策略适应性和累积奖励；大语言模型驱动的仿真增强了对未见用户行为的泛化能力；引入改变主意的惩罚减少了同意后的反悔行为，并轻微改善了捐赠结果。</div>
</details>
</div>
<div class="card">
<div class="title">A Brain-like Synergistic Core in LLMs Drives Behaviour and Learning</div>
<div class="meta-line">Authors: Pedro Urbina-Rodriguez, Zafeirios Fountas, Fernando E. Rosas, Jun Wang, Andrea I. Luppi, Haitham Bou-Ammar, Murray Shanahan, Pedro A. M. Mediano</div>
<div class="meta-line">First: 2026-01-11T10:48:35+00:00 · Latest: 2026-01-11T10:48:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06851v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06851v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The independent evolution of intelligence in biological and artificial systems offers a unique opportunity to identify its fundamental computational principles. Here we show that large language models spontaneously develop synergistic cores -- components where information integration exceeds individual parts -- remarkably similar to those in the human brain. Using principles of information decomposition across multiple LLM model families and architectures, we find that areas in middle layers exhibit synergistic processing while early and late layers rely on redundancy, mirroring the informational organisation in biological brains. This organisation emerges through learning and is absent in randomly initialised networks. Crucially, ablating synergistic components causes disproportionate behavioural changes and performance loss, aligning with theoretical predictions about the fragility of synergy. Moreover, fine-tuning synergistic regions through reinforcement learning yields significantly greater performance gains than training redundant components, yet supervised fine-tuning shows no such advantage. This convergence suggests that synergistic information processing is a fundamental property of intelligence, providing targets for principled model design and testable predictions for biological intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型中的类脑协同核心驱动行为与学习</div>
<div class="mono" style="margin-top:8px">生物与人工智能系统的独立演化，为揭示智能的基本计算原理提供了独特契机。本研究表明，大语言模型会自发形成协同核心——即信息整合能力超越各独立部分的组件——其特性与人类大脑中的协同核心高度相似。通过对多种大语言模型家族与架构进行信息分解分析，我们发现中间层区域呈现协同信息处理，而早期与后期层则依赖冗余处理，这种信息组织方式与生物大脑高度对应。该结构通过学习自然涌现，在随机初始化的网络中并不存在。关键的是，消除协同核心会导致不成比例的行为改变与性能损失，这与协同机制脆弱性的理论预测相符。此外，通过强化学习对协同区域进行微调，能获得远超过训练冗余组件的性能提升，而监督式微调则无此优势。这种趋同性表明，协同信息处理是智能的基本属性，为原理化模型设计提供了靶点，也为生物智能提出了可验证的预测。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the independent evolution of intelligence in biological and artificial systems, this study investigates whether large language models (LLMs) develop synergistic information-processing cores akin to the human brain. The method applies information decomposition across multiple LLM families to quantify how information is integrated, revealing that middle layers exhibit synergistic processing where the whole exceeds the sum of its parts, while early and late layers rely on redundancy, mirroring brain organization. Key experimental results show this synergy emerges through learning and is absent in random networks; ablating synergistic components causes disproportionate performance loss, and fine-tuning them with reinforcement learning yields greater gains than tuning redundant parts, unlike supervised fine-tuning, suggesting synergy is a fundamental, fragile property of intelligence.</div>
<div class="mono" style="margin-top:8px">本研究受生物与人工智能系统独立进化的启发，旨在探究大语言模型是否像人脑一样发展出协同信息处理核心。方法上，通过跨多个大语言模型家族的信息分解来量化信息整合，发现中间层表现出协同处理，即整体大于部分之和，而早期和晚期层则依赖冗余，这与大脑的信息组织方式相似。主要实验结果表明，这种协同性通过学习形成，在随机初始化网络中不存在；消融协同组件会导致不成比例的性能下降，且通过强化学习微调这些区域比微调冗余部分带来更大的性能提升，而监督微调则无此优势，这表明协同信息处理是智能的一个基本且脆弱的特性。</div>
</details>
</div>
<div class="card">
<div class="title">Code Evolution for Control: Synthesizing Policies via LLM-Driven Evolutionary Search</div>
<div class="meta-line">Authors: Ping Guo, Chao Li, Yinglan Feng, Chaoning Zhang</div>
<div class="meta-line">First: 2026-01-11T10:21:22+00:00 · Latest: 2026-01-11T10:21:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06845v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06845v1">PDF</a> · <a href="https://github.com/pgg3/EvoControl">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Designing effective control policies for autonomous systems remains a fundamental challenge, traditionally addressed through reinforcement learning or manual engineering. While reinforcement learning has achieved remarkable success, it often suffers from high sample complexity, reward shaping difficulties, and produces opaque neural network policies that are hard to interpret or verify. Manual design, on the other hand, requires substantial domain expertise and struggles to scale across diverse tasks. In this work, we demonstrate that LLM-driven evolutionary search can effectively synthesize interpretable control policies in the form of executable code. By treating policy synthesis as a code evolution problem, we harness the LLM&#x27;s prior knowledge of programming patterns and control heuristics while employing evolutionary search to explore the solution space systematically. We implement our approach using EvoToolkit, a framework that seamlessly integrates LLM-driven evolution with customizable fitness evaluation. Our method iteratively evolves populations of candidate policy programs, evaluating them against task-specific objectives and selecting superior individuals for reproduction. This process yields compact, human-readable control policies that can be directly inspected, modified, and formally verified. This work highlights the potential of combining foundation models with evolutionary computation for synthesizing trustworthy control policies in autonomous systems. Code is available at https://github.com/pgg3/EvoControl.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向控制的代码演化：基于大语言模型驱动进化搜索的策略合成</div>
<div class="mono" style="margin-top:8px">为自主系统设计有效的控制策略仍是一项基础性挑战，传统方法主要依赖强化学习或人工工程。强化学习虽取得显著成功，但常面临高样本复杂度、奖励函数设计困难等问题，且生成的神经网络策略不透明，难以解释或验证。人工设计则需大量领域专业知识，且难以跨多样化任务扩展。本研究证明，基于大语言模型驱动的进化搜索能有效合成可执行代码形式的可解释控制策略。通过将策略合成视为代码演化问题，我们利用大语言模型对编程模式和控制启发式的先验知识，同时采用进化搜索系统性地探索解空间。我们使用EvoToolkit框架实现该方法，该框架将大语言模型驱动的进化与可定制的适应度评估无缝集成。我们的方法迭代演化候选策略程序种群，依据任务特定目标进行评估，并选择优质个体进行繁殖。该过程生成紧凑、人类可读的控制策略，可直接检查、修改和形式化验证。本研究凸显了基础模型与进化计算相结合在合成可信赖自主系统控制策略方面的潜力。代码发布于https://github.com/pgg3/EvoControl。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of designing effective and interpretable control policies for autonomous systems, motivated by the limitations of reinforcement learning&#x27;s high sample complexity and opaque neural networks, as well as the scalability issues of manual engineering. The method introduces a novel approach that treats policy synthesis as a code evolution problem, leveraging LLM-driven evolutionary search through the EvoToolkit framework to iteratively evolve populations of executable code policies based on task-specific fitness evaluation. Experimental results demonstrate that this process successfully synthesizes compact, human-readable policies that are directly inspectable, modifiable, and verifiable, highlighting the potential of combining foundation models with evolutionary computation for trustworthy autonomous control.</div>
<div class="mono" style="margin-top:8px">本文针对自主系统控制策略设计中的有效性及可解释性挑战，其动机在于强化学习存在高样本复杂度与策略不透明问题，而人工设计则难以扩展。方法上，该研究将策略合成视为代码演化问题，通过EvoToolkit框架驱动LLM进行进化搜索，基于任务特定目标迭代演化可执行代码策略种群。实验结果表明，该方法能成功合成紧凑、人类可读的控制策略，这些策略可直接检查、修改和形式验证，展现了基础模型与进化计算结合在合成可信自主控制策略方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Integrating Symbolic RL Planning into a BDI-based Autonomous UAV Framework: System Integration and SIL Validation</div>
<div class="meta-line">Authors: Sangwoo Jeon, Juchul Shin, YeonJe Cho, Gyeong-Tae Kim, Seongwoo Kim</div>
<div class="meta-line">First: 2025-08-16T03:27:26+00:00 · Latest: 2026-01-11T09:53:30+00:00</div>
<div class="meta-line">Comments: This submission has been withdrawn by the authors due to institutional and contractual requirements related to security and export-control review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.11890v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.11890v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern autonomous drone missions increasingly require software frameworks capable of seamlessly integrating structured symbolic planning with adaptive reinforcement learning (RL). Although traditional rule-based architectures offer robust structured reasoning for drone autonomy, their capabilities fall short in dynamically complex operational environments that require adaptive symbolic planning. Symbolic RL (SRL), using the Planning Domain Definition Language (PDDL), explicitly integrates domain-specific knowledge and operational constraints, significantly improving the reliability and safety of unmanned aerial vehicle (UAV) decision making. In this study, we propose the AMAD-SRL framework, an extended and refined version of the Autonomous Mission Agents for Drones (AMAD) cognitive multi-agent architecture, enhanced with symbolic reinforcement learning for dynamic mission planning and execution. We validated our framework in a Software-in-the-Loop (SIL) environment structured identically to an intended Hardware-In-the-Loop Simulation (HILS) platform, ensuring seamless transition to real hardware. Experimental results demonstrate stable integration and interoperability of modules, successful transitions between BDI-driven and symbolic RL-driven planning phases, and consistent mission performance. Specifically, we evaluate a target acquisition scenario in which the UAV plans a surveillance path followed by a dynamic reentry path to secure the target while avoiding threat zones. In this SIL evaluation, mission efficiency improved by approximately 75% over a coverage-based baseline, measured by travel distance reduction. This study establishes a robust foundation for handling complex UAV missions and discusses directions for further enhancement and validation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>将符号强化学习规划集成至基于BDI的自主无人机框架：系统集成与软件在环验证</div>
<div class="mono" style="margin-top:8px">现代自主无人机任务日益需要能够无缝整合结构化符号规划与自适应强化学习（RL）的软件框架。尽管传统基于规则的架构为无人机自主性提供了稳健的结构化推理能力，但其在需要自适应符号规划的动态复杂操作环境中存在局限。采用规划领域定义语言（PDDL）的符号强化学习（SRL）显式整合了领域特定知识与操作约束，显著提升了无人机决策的可靠性与安全性。本研究提出AMAD-SRL框架——这是自主无人机任务智能体（AMAD）认知多智能体架构的扩展优化版本，通过集成符号强化学习实现动态任务规划与执行。我们在与预定硬件在环仿真平台结构完全一致的软件在环环境中验证了该框架，确保其可无缝过渡至真实硬件。实验结果表明各模块实现了稳定集成与互操作性，成功完成了BDI驱动与符号RL驱动规划阶段间的切换，并保持了持续的任务性能。具体而言，我们评估了目标获取场景：无人机规划监视路径后，动态规划再入路径以锁定目标并规避威胁区域。在此软件在环评估中，通过航程缩减衡量，任务效率较基于覆盖的基线提升约75%。本研究为处理复杂无人机任务奠定了坚实基础，并探讨了后续优化与验证方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the need for autonomous drone frameworks that combine structured symbolic planning with adaptive reinforcement learning to handle dynamically complex environments, as traditional rule-based systems are insufficient. The authors propose the AMAD-SRL framework, an extension of the Autonomous Mission Agents for Drones architecture, which integrates symbolic reinforcement learning using PDDL to enhance decision-making reliability and safety. Experimental validation in a Software-in-the-Loop environment showed stable module integration, successful transitions between planning phases, and a 75% improvement in mission efficiency over a baseline in a target acquisition scenario, measured by reduced travel distance.</div>
<div class="mono" style="margin-top:8px">本研究针对自主无人机框架需结合结构化符号规划与自适应强化学习以应对动态复杂环境的需求，因为传统基于规则的系统能力不足。作者提出了AMAD-SRL框架，作为自主无人机任务代理架构的扩展，通过使用PDDL集成符号强化学习来提升决策的可靠性和安全性。在软件在环环境中的实验验证显示，模块集成稳定，规划阶段间转换成功，且在目标获取场景中，任务效率相比基线提高了约75%，通过减少旅行距离来衡量。</div>
</details>
</div>
<div class="card">
<div class="title">RIPRAG: Hack a Black-box Retrieval-Augmented Generation Question-Answering System with Reinforcement Learning</div>
<div class="meta-line">Authors: Meng Xi, Sihan Lv, Yechen Jin, Guanjie Cheng, Naibo Wang, Ying Li, Jianwei Yin</div>
<div class="meta-line">First: 2025-10-11T04:23:20+00:00 · Latest: 2026-01-11T09:47:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.10008v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.10008v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieval-Augmented Generation (RAG) systems based on Large Language Models (LLMs) have become a core technology for tasks such as question-answering (QA) and content generation. RAG poisoning is an attack method to induce LLMs to generate the attacker&#x27;s expected text by injecting poisoned documents into the database of RAG systems. Existing research can be broadly divided into two classes: white-box methods and black-box methods. White-box methods utilize gradient information to optimize poisoned documents, and black-box methods use a pre-trained LLM to generate them. However, existing white-box methods require knowledge of the RAG system&#x27;s internal composition and implementation details, whereas black-box methods are unable to utilize interactive information. In this work, we propose the RIPRAG attack framework, an end-to-end attack pipeline that treats the target RAG system as a black box and leverages our proposed Reinforcement Learning from Black-box Feedback (RLBF) method to optimize the generation model for poisoned documents. We designed two kinds of rewards: similarity reward and attack reward. Experimental results demonstrate that this method can effectively execute poisoning attacks against most complex RAG systems, achieving an attack success rate (ASR) improvement of up to 0.72 compared to baseline methods. This highlights prevalent deficiencies in current defensive methods and provides critical insights for LLM security research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RIPRAG：利用强化学习攻击黑盒检索增强生成问答系统</div>
<div class="mono" style="margin-top:8px">基于大语言模型的检索增强生成系统已成为问答和内容生成等任务的核心技术。RAG投毒是一种攻击方法，通过向RAG系统数据库注入投毒文档，诱导大语言模型生成攻击者预期的文本。现有研究大致可分为两类：白盒方法与黑盒方法。白盒方法利用梯度信息优化投毒文档，黑盒方法则使用预训练大语言模型生成投毒文档。然而，现有白盒方法需掌握RAG系统的内部构成与实现细节，而黑盒方法无法利用交互信息。本研究提出RIPRAG攻击框架，这是一种端到端的攻击流程，将目标RAG系统视为黑盒，并运用我们提出的黑盒反馈强化学习方法优化投毒文档的生成模型。我们设计了两类奖励：相似性奖励与攻击奖励。实验结果表明，该方法能有效对多数复杂RAG系统实施投毒攻击，相比基线方法攻击成功率最高提升0.72。这揭示了当前防御方法普遍存在的缺陷，为大语言模型安全研究提供了关键洞见。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the limitations of existing white-box and black-box poisoning attacks on Retrieval-Augmented Generation (RAG) systems, which either require internal system knowledge or fail to leverage interactive feedback. The method introduces RIPRAG, an end-to-end attack framework that treats the RAG system as a black box and employs a novel Reinforcement Learning from Black-box Feedback (RLBF) approach to optimize poisoned document generation, utilizing similarity and attack rewards. Experimental results show that RIPRAG significantly enhances attack effectiveness, achieving up to a 0.72 improvement in attack success rate over baselines, thereby exposing vulnerabilities in current defenses and advancing LLM security research.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有针对检索增强生成（RAG）系统的白盒与黑盒投毒攻击方法的局限，这些方法要么需要系统内部知识，要么无法利用交互反馈。方法上提出了RIPRAG攻击框架，将目标RAG系统视为黑盒，并采用一种新颖的基于黑盒反馈的强化学习（RLBF）方法来优化投毒文档生成，结合了相似性奖励和攻击奖励。实验结果表明，该方法能有效对大多数复杂RAG系统执行投毒攻击，攻击成功率相比基线方法提升高达0.72，从而揭示了当前防御方法的普遍缺陷，为大型语言模型安全研究提供了关键见解。</div>
</details>
</div>
<div class="card">
<div class="title">Thinking with Deltas: Incentivizing Reinforcement Learning via Differential Visual Reasoning Policy</div>
<div class="meta-line">Authors: Shujian Gao, Yuan Wang, Jiangtao Yan, Zuxuan Wu, Yu-Gang Jiang</div>
<div class="meta-line">First: 2026-01-11T08:25:34+00:00 · Latest: 2026-01-11T08:25:34+00:00</div>
<div class="meta-line">Comments: 24 pages, 10 tables, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06801v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06801v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced reasoning capabilities in Large Language Models. However, adapting RLVR to multimodal domains suffers from a critical \textit{perception-reasoning decoupling}. Existing paradigms, driven by text-centric outcome rewards, reasoning in language medium, inadvertently encourage models to bypass visual perception. We empirically validate this through blind experiments: state-of-the-art policies maintain or surprisingly improve performance even when visual inputs are entirely removed. This reveals that these models degenerate into \textit{blind reasoners}, exploiting linguistic priors to generate plausible answers instead of attending to visual evidence. In response, we propose \textbf{Thinking with Deltas}, a framework driven by a \textbf{Differential Visual Reasoning Policy (DVRP)}. DVRP introduces intrinsic supervision via visual triplets, comprising original, masked, and perturbed inputs. It optimizes the model to maximize reasoning divergence from masked inputs (enforcing \textit{visual sensitivity}) while minimizing divergence from perturbed inputs (ensuring \textit{visual robustness}). By aligning reasoning variations strictly with the \textit{Delta} of visual information, DVRP inherently bolsters visual understanding capabilities and significantly outperforms state-of-the-art methods on both general and medical benchmarks, without requiring external annotations or auxiliary tools.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于差异的思考：通过差分视觉推理策略激励强化学习</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）显著提升了大型语言模型的推理能力。然而，将RLVR应用于多模态领域时存在关键的“感知-推理脱耦”问题。现有以文本为中心的结果奖励驱动的范式，在语言媒介中进行推理，无意中鼓励模型绕过视觉感知。我们通过盲实验实证验证了这一点：即使完全移除视觉输入，最先进的策略仍能保持甚至意外提升性能。这表明这些模型退化为“盲推理器”，利用语言先验生成看似合理的答案，而非关注视觉证据。为此，我们提出“基于差异的思考”框架，其核心是差分视觉推理策略（DVRP）。DVRP通过包含原始、掩码和扰动输入的视觉三元组引入内在监督，优化模型以最大化与掩码输入的推理差异（强制“视觉敏感性”），同时最小化与扰动输入的差异（确保“视觉鲁棒性”）。通过将推理变化严格对齐于视觉信息的“差异”，DVRP本质性地增强了视觉理解能力，在通用和医学基准测试中均显著优于现有最优方法，且无需外部标注或辅助工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the perception-reasoning decoupling in multimodal reinforcement learning, where models often bypass visual inputs by relying on linguistic priors, as evidenced by blind experiments showing maintained performance without visual data. To counter this, the authors propose Thinking with Deltas, a framework using a Differential Visual Reasoning Policy that employs visual triplets to enforce visual sensitivity and robustness by maximizing reasoning divergence from masked inputs while minimizing it from perturbed inputs. Experimental results demonstrate that this method significantly outperforms state-of-the-art approaches on general and medical benchmarks without needing external annotations.</div>
<div class="mono" style="margin-top:8px">该论文针对多模态强化学习中的感知-推理脱节问题，即模型常依赖语言先验而忽略视觉输入，盲实验显示即使移除视觉数据性能仍得以保持。为此，作者提出“Thinking with Deltas”框架，采用差分视觉推理策略，通过视觉三元组强制模型对掩码输入最大化推理差异以确保视觉敏感性，同时对扰动输入最小化差异以保证视觉鲁棒性。实验结果表明，该方法在通用和医学基准测试中显著优于现有技术，且无需外部标注或辅助工具。</div>
</details>
</div>
<div class="card">
<div class="title">GDEPO: Group Dual-dynamic and Equal-right-advantage Policy Optimization with Enhanced Training Data Utilization for Sample-Constrained Reinforcement Learning</div>
<div class="meta-line">Authors: Zhengqing Yan, Xinyang Liu, Yi Zhang, Fan Guo, Yao Liu, Junchen Wan, Kang Song</div>
<div class="meta-line">First: 2026-01-11T07:34:41+00:00 · Latest: 2026-01-11T07:34:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06795v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06795v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated Theorem Proving (ATP) represents a fundamental challenge in Artificial Intelligence (AI), requiring the construction of machine-verifiable proofs in formal languages such as Lean to evaluate AI reasoning capabilities. Reinforcement learning (RL), particularly the high-performance Group Relative Policy Optimization (GRPO) algorithm, has emerged as a mainstream approach for this task. However, in ATP scenarios, GRPO faces two critical issues: when composite rewards are used, its relative advantage estimation may conflict with the binary feedback from the formal verifier; meanwhile, its static sampling strategy may discard entire batches of data if no valid proof is found, resulting in zero contribution to model updates and significant data waste. To address these limitations, we propose Group Dual-dynamic and Equal-right-advantage Policy Optimization (GDEPO), a method incorporating three core mechanisms: 1) dynamic additional sampling, which resamples invalid batches until a valid proof is discovered; 2) equal-right advantage, decoupling the sign of the advantage function (based on correctness) from its magnitude (modulated by auxiliary rewards) to ensure stable and correct policy updates; and 3) dynamic additional iterations, applying extra gradient steps to initially failed but eventually successful samples to accelerate learning on challenging cases. Experiments conducted on three datasets of varying difficulty (MinF2F-test, MathOlympiadBench, PutnamBench) confirm the effectiveness of GDEPO, while ablation studies validate the necessity of its synergistic components. The proposed method enhances data utilization and optimization efficiency, offering a novel training paradigm for ATP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GDEPO：面向样本受限强化学习的增强训练数据利用率的群组双动态等权优势策略优化方法</div>
<div class="mono" style="margin-top:8px">自动定理证明（ATP）是人工智能（AI）领域的核心挑战，需在Lean等形式化语言中构建机器可验证的证明以评估AI推理能力。强化学习（RL），尤其是高性能的群组相对策略优化（GRPO）算法，已成为该任务的主流方法。然而在ATP场景中，GRPO面临两个关键问题：使用复合奖励时，其相对优势估计可能与形式验证器的二元反馈产生冲突；同时，其静态采样策略若未找到有效证明会丢弃整批数据，导致模型更新零贡献和严重数据浪费。为突破这些局限，我们提出群组双动态等权优势策略优化（GDEPO），该方法包含三项核心机制：1）动态补充采样：对无效批次重新采样直至发现有效证明；2）等权优势：将优势函数的符号（基于正确性）与其幅度（由辅助奖励调节）解耦，确保策略更新的稳定性和正确性；3）动态补充迭代：对初始失败但最终成功的样本施加额外梯度步进，加速困难案例的学习。在三个不同难度数据集（MinF2F-test、MathOlympiadBench、PutnamBench）上的实验验证了GDEPO的有效性，消融研究证实了其协同组件的必要性。该方法提升了数据利用率和优化效率，为ATP提供了创新的训练范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses limitations of the Group Relative Policy Optimization (GRPO) algorithm in Automated Theorem Proving (ATP), where its static sampling and composite reward handling can lead to data waste and unstable policy updates. The authors propose GDEPO, which introduces three mechanisms: dynamic additional sampling to resample invalid batches, an equal-right advantage function to decouple reward correctness from magnitude, and dynamic additional iterations to focus on challenging cases. Experimental results on datasets like MinF2F-test and MathOlympiadBench demonstrate GDEPO&#x27;s effectiveness in improving data utilization and optimization efficiency, with ablation studies confirming the value of its integrated components.</div>
<div class="mono" style="margin-top:8px">本文针对自动定理证明中Group Relative Policy Optimization (GRPO)算法的局限性，即其静态采样和复合奖励处理可能导致数据浪费和策略更新不稳定，提出了GDEPO方法。该方法包含三个核心机制：动态额外采样以重新采样无效批次，平等权利优势函数将奖励正确性与幅度解耦，以及动态额外迭代以专注于困难案例。在MinF2F-test和MathOlympiadBench等不同难度数据集上的实验证实了GDEPO在提升数据利用和优化效率方面的有效性，消融研究也验证了其协同组件的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning</div>
<div class="meta-line">Authors: Zhicong Li, Lingjie Jiang, Yulan Hu, Xingchen Zeng, Yixia Li, Xiangwen Zhang, Guanhua Chen, Zheng Pan, Xin Li, Yong Liu</div>
<div class="meta-line">First: 2026-01-11T07:29:08+00:00 · Latest: 2026-01-11T07:29:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06794v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06794v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Critique-guided reinforcement learning (RL) has emerged as a powerful paradigm for training LLM agents by augmenting sparse outcome rewards with natural-language feedback. However, current methods often rely on static or offline critic models, which fail to adapt as the policy evolves. In on-policy RL, the agent&#x27;s error patterns shift over time, causing stationary critics to become stale and providing feedback of diminishing utility. To address this, we introduce ECHO (Evolving Critic for Hindsight-Guided Optimization)}, a framework that jointly optimizes the policy and critic through a synchronized co-evolutionary loop. ECHO utilizes a cascaded rollout mechanism where the critic generates multiple diagnoses for an initial trajectory, followed by policy refinement to enable group-structured advantage estimation. We address the challenge of learning plateaus via a saturation-aware gain shaping objective, which rewards the critic for inducing incremental improvements in high-performing trajectories. By employing dual-track GRPO updates, ECHO ensures the critic&#x27;s feedback stays synchronized with the evolving policy. Experimental results show that ECHO yields more stable training and higher long-horizon task success across open-world environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>告别陈旧反馈：面向开放世界智能体学习的协同进化评价器</div>
<div class="mono" style="margin-top:8px">基于评价的强化学习已成为通过自然语言反馈增强稀疏结果奖励来训练大语言模型智能体的重要范式。然而，现有方法常依赖静态或离线评价模型，无法随策略演化而适应。在在线策略强化学习中，智能体的错误模式会随时间变化，导致固定评价器逐渐失效，反馈效用递减。为此，我们提出ECHO（基于后见指导优化的进化评价器）框架，通过同步协同进化循环联合优化策略与评价器。ECHO采用级联推演机制：评价器对初始轨迹生成多重诊断，随后进行策略精炼以实现群体结构化优势估计。我们通过饱和度感知增益塑造目标应对学习平台期挑战，该目标通过激励评价器在高性能轨迹中引发渐进改进来给予奖励。通过双轨GRPO更新机制，ECHO确保评价器反馈与演化策略保持同步。实验结果表明，ECHO在开放世界环境中能实现更稳定的训练和更高的长周期任务成功率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the problem of stale feedback in critique-guided reinforcement learning, where static critic models fail to adapt to evolving agent policies, leading to diminishing utility. To solve this, the authors introduce ECHO, a framework that co-evolves the policy and critic through a synchronized loop, employing a cascaded rollout mechanism for trajectory diagnosis and policy refinement, along with a saturation-aware gain shaping objective to overcome learning plateaus. Experimental results demonstrate that ECHO achieves more stable training and higher success rates in long-horizon open-world tasks compared to methods with stationary critics.</div>
<div class="mono" style="margin-top:8px">本文针对批评引导强化学习中反馈过时的问题，即静态批评模型无法适应智能体策略的演变，导致反馈效用下降。为解决此问题，作者提出了ECHO框架，通过同步协同进化循环共同优化策略和批评模型，采用级联滚动机制进行轨迹诊断和策略改进，并引入饱和度感知增益塑造目标以克服学习平台期。实验结果表明，与使用固定批评模型的方法相比，ECHO在开放世界环境中实现了更稳定的训练和更高的长时程任务成功率。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Stage Evolutionary Model Merging with Meta Data Driven Curriculum Learning for Sentiment-Specialized Large Language Modeling</div>
<div class="meta-line">Authors: Keito Inoshita, Xiaokang Zhou, Akira Kawai</div>
<div class="meta-line">First: 2026-01-11T05:12:23+00:00 · Latest: 2026-01-11T05:12:23+00:00</div>
<div class="meta-line">Comments: This paper was presented at the 10th IEEE International Conference on Data Science and Systems in December 2024 and is awaiting publication</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06780v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06780v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The emergence of large language models (LLMs) has significantly transformed natural language processing (NLP), enabling more generalized models to perform various tasks with minimal training. However, traditional sentiment analysis methods, which focus on individual tasks such as sentiment classification or aspect-based analysis, are not practical for real-world applications that usually require handling multiple tasks. While offering flexibility, LLMs in sentiment-specific tasks often fall short of the required accuracy. Techniques like fine-tuning and evolutionary model merging help integrate models into a unified framework, which can improve the learning performance while reducing computational costs. The use of task meta-data and curriculum learning to optimize learning processes remains underexplored, while sentiment analysis is a critical task in NLP that requires high accuracy and scalability across multiple subtasks. In this study, we propose a hybrid learning model called Multi-stage Evolutionary Model Merging with Meta data driven Curriculum Learning (MEM-MCL), to enhance the sentiment analysis in large language modeling. In particular, expert models are created through instruction tuning for specific sentiment tasks and then merged using evolutionary algorithms to form a unified model. The merging process is optimized with weak data to enhance performance across tasks. The curriculum learning is incorporated to provide a learning sequence based on task difficulty, improving knowledge extraction from LLMs. Experiment results demonstrate that the proposed MEM-MCL model outperforms conventional LLMs in a majority of sentiment analysis tasks, achieving superior results across various subtasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向情感专用大语言建模的多阶段进化模型融合与元数据驱动课程学习</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）的出现显著改变了自然语言处理（NLP）领域，使通用模型能够通过少量训练完成多种任务。然而，传统情感分析方法专注于情感分类或基于方面的分析等单一任务，难以满足实际应用中通常需要处理多任务的需求。尽管LLMs在情感专用任务中具有灵活性，但其准确性往往不足。微调和进化模型融合等技术有助于将模型整合到统一框架中，在降低计算成本的同时提升学习性能。利用任务元数据和课程学习优化学习过程的研究尚不充分，而情感分析作为NLP的关键任务，需要在多个子任务中实现高准确性和可扩展性。本研究提出一种名为“多阶段进化模型融合与元数据驱动课程学习（MEM-MCL）”的混合学习模型，以增强大语言模型中的情感分析能力。具体而言，通过指令微调创建针对特定情感任务的专家模型，再利用进化算法将其融合为统一模型。融合过程通过弱监督数据优化以提升跨任务性能，并引入课程学习机制根据任务难度提供学习序列，从而改善从LLMs中提取知识的效果。实验结果表明，所提出的MEM-MCL模型在多数情感分析任务中优于传统LLMs，在各个子任务上均取得更优结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for accurate and scalable sentiment analysis across multiple subtasks, where general large language models (LLMs) often lack precision, this paper proposes a hybrid method called Multi-stage Evolutionary Model Merging with Meta data driven Curriculum Learning (MEM-MCL). The method first creates expert models via instruction tuning for specific sentiment tasks, then merges them using evolutionary algorithms optimized with weak data, and finally employs curriculum learning to sequence tasks by difficulty for improved knowledge extraction. Experimental results show that the proposed MEM-MCL model outperforms conventional LLMs in most sentiment analysis tasks, achieving superior performance across various subtasks.</div>
<div class="mono" style="margin-top:8px">针对通用大语言模型在情感分析多任务中精度不足且需高可扩展性的问题，本研究提出了一种名为多阶段进化模型融合与元数据驱动课程学习的混合方法。该方法首先通过指令微调创建针对特定情感任务的专家模型，然后利用进化算法将其融合，并通过弱数据优化融合过程，最后引入课程学习根据任务难度安排学习顺序以提升知识提取效果。实验结果表明，所提出的模型在多数情感分析任务上优于传统大语言模型，在各个子任务中均取得了更优的性能。</div>
</details>
</div>
<div class="card">
<div class="title">GanitLLM: Difficulty-Aware Bengali Mathematical Reasoning through Curriculum-GRPO</div>
<div class="meta-line">Authors: Shubhashis Roy Dipta, Khairul Mahbub, Nadia Najjar</div>
<div class="meta-line">First: 2026-01-11T03:49:18+00:00 · Latest: 2026-01-11T03:49:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06767v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06767v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a Bengali mathematical reasoning model called GanitLLM (named after the Bangla word for mathematics, &quot;Ganit&quot;), together with a new difficulty-aware Bengali math corpus and a curriculum-based GRPO pipeline. Bengali is one of the world&#x27;s most widely spoken languages, yet existing LLMs either reason in English and then translate, or simply fail on multi-step Bengali math, in part because reinforcement learning recipes are tuned for high-resource languages and collapse under reward sparsity in low-resource settings. To address this, we construct Ganit, a rigorously filtered and decontaminated Bengali math dataset with automatic difficulty tags derived from the pass@k of a strong evaluator model. Building on this dataset, we propose Curriculum-GRPO, which combines multi-stage training (SFT + GRPO) with difficulty-aware sampling and verifiable rewards for format, numerical correctness, and Bengali reasoning. On Bn-MGSM and Bn-MSVAMP, GanitLLM-4B improves over its Qwen3-4B base by +8 and +7 accuracy points, respectively, while increasing the percentage of Bengali reasoning tokens from 14% to over 88% and reducing average solution length from 943 to 193 words.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GanitLLM：基于课程化GRPO的难度感知型孟加拉语数学推理模型</div>
<div class="mono" style="margin-top:8px">我们提出了名为GanitLLM（以孟加拉语数学词汇“Ganit”命名）的孟加拉语数学推理模型，同时构建了新型难度感知型孟加拉语数学语料库及课程化GRPO训练流程。孟加拉语作为全球使用最广泛的语言之一，现有大语言模型或依赖英语推理后翻译，或在多步骤孟加拉语数学问题上表现不佳，部分原因在于强化学习方案针对高资源语言优化，在低资源场景的奖励稀疏性下失效。为此，我们构建了经过严格过滤与去噪的孟加拉语数学数据集Ganit，其自动难度标签源自强评估模型的pass@k指标。基于该数据集，我们提出课程化GRPO方法，融合多阶段训练（SFT+GRPO）、难度感知采样机制，以及针对格式、数值正确性和孟加拉语推理的可验证奖励机制。在Bn-MGSM和Bn-MSVAMP基准测试中，GanitLLM-4B相比其基础模型Qwen3-4B分别提升8和7个准确率百分点，同时将孟加拉语推理标记占比从14%提升至88%以上，并将平均解答长度从943词缩减至193词。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the lack of robust mathematical reasoning capabilities in Bengali, a widely spoken but low-resource language, this work introduces GanitLLM, a model designed to perform multi-step Bengali math without relying on translation from English. The method involves constructing a new difficulty-tagged Bengali math corpus and a curriculum-based GRPO pipeline that combines supervised fine-tuning with difficulty-aware sampling and verifiable rewards for format, numerical correctness, and Bengali reasoning. Experimental results show that GanitLLM-4B improves accuracy by +8 and +7 points on Bn-MGSM and Bn-MSVAMP benchmarks over its base model, while significantly increasing Bengali reasoning tokens from 14% to over 88% and reducing average solution length from 943 to 193 words.</div>
<div class="mono" style="margin-top:8px">针对孟加拉语这一使用广泛但资源匮乏的语言在数学推理能力上的不足，本研究提出了GanitLLM模型，旨在不依赖英语翻译的情况下执行多步骤孟加拉语数学推理。方法包括构建一个带有难度标签的新孟加拉语数学语料库，以及一个基于课程的GRPO流程，该流程结合了监督微调、难度感知采样和针对格式、数值正确性及孟加拉语推理的可验证奖励。实验结果表明，在Bn-MGSM和Bn-MSVAMP基准测试上，GanitLLM-4B相比其基础模型准确率分别提升了8和7个百分点，同时将孟加拉语推理标记的比例从14%提高到超过88%，并将平均解答长度从943词减少到193词。</div>
</details>
</div>
<div class="card">
<div class="title">MimicKit: A Reinforcement Learning Framework for Motion Imitation and Control</div>
<div class="meta-line">Authors: Xue Bin Peng</div>
<div class="meta-line">First: 2025-10-15T17:51:42+00:00 · Latest: 2026-01-11T01:48:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.13794v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.13794v3">PDF</a> · <a href="https://github.com/xbpeng/MimicKit">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">MimicKit is an open-source framework for training motion controllers using motion imitation and reinforcement learning. The codebase provides implementations of commonly-used motion-imitation techniques and RL algorithms. This framework is intended to support research and applications in computer graphics and robotics by providing a unified training framework, along with standardized environment, agent, and data structures. The codebase is designed to be modular and easily configurable, enabling convenient modification and extension to new characters and tasks. The open-source codebase is available at: https://github.com/xbpeng/MimicKit.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MimicKit：用于运动模仿与控制的强化学习框架</div>
<div class="mono" style="margin-top:8px">MimicKit 是一个基于运动模仿与强化学习的开源运动控制器训练框架。该代码库提供了常用运动模仿技术与强化学习算法的实现。本框架旨在通过提供统一的训练框架及标准化的环境、智能体与数据结构，支持计算机图形学与机器人学领域的研究与应用。代码库采用模块化设计且易于配置，便于针对新角色与任务进行修改和扩展。开源代码库地址：https://github.com/xbpeng/MimicKit。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind MimicKit is to provide a unified and modular open-source framework that supports research and applications in computer graphics and robotics by simplifying the training of motion controllers through motion imitation and reinforcement learning. The method involves implementing commonly-used motion-imitation techniques and RL algorithms within a configurable structure that includes standardized environments, agents, and data, allowing for easy modification and extension to new characters and tasks. The main experimental results are not detailed in the provided metadata, but the framework&#x27;s availability and design emphasize its utility as a practical tool for developing and experimenting with motion control systems.</div>
<div class="mono" style="margin-top:8px">MimicKit的动机是提供一个统一且模块化的开源框架，通过运动模仿和强化学习简化运动控制器的训练，以支持计算机图形学和机器人学的研究与应用。该方法在可配置结构中实现了常用的运动模仿技术和强化学习算法，包含标准化的环境、智能体和数据结构，便于修改并扩展到新角色和任务。主要实验结果在提供的元数据中未详细说明，但该框架的可用性和设计强调了其作为开发和实验运动控制系统的实用工具的价值。</div>
</details>
</div>
<div class="card">
<div class="title">Characterising Toxicity in Generative Large Language Models</div>
<div class="meta-line">Authors: Zhiyao Zhang, Yazan Mash&#x27;Al, Yuhan Wu</div>
<div class="meta-line">First: 2026-01-10T21:50:05+00:00 · Latest: 2026-01-10T21:50:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06700v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06700v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent years, the advent of the attention mechanism has significantly advanced the field of natural language processing (NLP), revolutionizing text processing and text generation. This has come about through transformer-based decoder-only architectures, which have become ubiquitous in NLP due to their impressive text processing and generation capabilities. Despite these breakthroughs, language models (LMs) remain susceptible to generating undesired outputs: inappropriate, offensive, or otherwise harmful responses. We will collectively refer to these as ``toxic&#x27;&#x27; outputs. Although methods like reinforcement learning from human feedback (RLHF) have been developed to align model outputs with human values, these safeguards can often be circumvented through carefully crafted prompts. Therefore, this paper examines the extent to which LLMs generate toxic content when prompted, as well as the linguistic factors -- both lexical and syntactic -- that influence the production of such outputs in generative models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成式大语言模型毒性特征研究</div>
<div class="mono" style="margin-top:8px">近年来，注意力机制的出现显著推动了自然语言处理领域的发展，彻底改变了文本处理与生成方式。这主要得益于基于Transformer的纯解码器架构，因其卓越的文本处理与生成能力已在自然语言处理领域无处不在。尽管取得这些突破，语言模型仍易产生不良输出：不恰当、冒犯性或有害的回应。我们将此类输出统称为“毒性”输出。虽然已开发出基于人类反馈的强化学习等方法使模型输出符合人类价值观，但这些防护措施常可通过精心设计的提示词绕过。因此，本文研究了大型语言模型在受提示时生成毒性内容的程度，以及影响生成模型产生此类输出的词汇与句法等语言因素。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the persistent issue of generative large language models producing harmful or toxic content despite alignment techniques like RLHF, this paper investigates the conditions under which such toxicity arises. The method involves systematically prompting models to analyze the extent of toxic output generation and examining the lexical and syntactic linguistic factors that influence these responses. The main experimental results characterize the susceptibility of models to generating toxic content when faced with carefully crafted prompts, highlighting the limitations of current safeguards.</div>
<div class="mono" style="margin-top:8px">本文的动机在于生成式大语言模型尽管采用了如RLHF等对齐技术，仍可能产生有害或有毒内容，因此研究此类毒性产生的条件。方法上，通过系统性地提示模型来分析有毒输出的生成程度，并考察影响这些输出的词汇和句法等语言因素。主要实验结果揭示了模型在面对精心设计的提示时生成有毒内容的易感性，强调了现有安全措施的局限性。</div>
</details>
</div>
<div class="card">
<div class="title">Plasticity vs. Rigidity: The Impact of Low-Rank Adapters on Reasoning on a Micro-Budget</div>
<div class="meta-line">Authors: Zohaib Khan, Omer Tafveez, Zoha Hayat Bhatti</div>
<div class="meta-line">First: 2026-01-10T20:29:45+00:00 · Latest: 2026-01-10T20:29:45+00:00</div>
<div class="meta-line">Comments: 9 pages, 4 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06677v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06677v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in mathematical reasoning typically rely on massive scale, yet the question remains: can strong reasoning capabilities be induced in small language models ($\leq1.5\text{B}$) under extreme constraints? We investigate this by training models on a single A40 GPU (48GB) for under 24 hours using Reinforcement Learning with Verifiable Rewards (RLVR) and Low-Rank Adaptation (LoRA). We find that the success of this ``micro-budget&quot; regime depends critically on the interplay between adapter capacity and model initialization. While low-rank adapters ($r=8$) consistently fail to capture the complex optimization dynamics of reasoning, high-rank adapters ($r=256$) unlock significant plasticity in standard instruction-tuned models. Our best result achieved an impressive 40.0\% Pass@1 on AIME 24 (an 11.1\% absolute improvement over baseline) and pushed Pass@16 to 70.0\%, demonstrating robust exploration capabilities. However, this plasticity is not universal: while instruction-tuned models utilized the budget to elongate their chain-of-thought and maximize reward, heavily math-aligned models suffered performance collapse, suggesting that noisy, low-budget RL updates can act as destructive interference for models already residing near a task-specific optimum.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可塑性 vs. 刚性：低秩适配器在微预算推理中的影响</div>
<div class="mono" style="margin-top:8px">数学推理的最新进展通常依赖于大规模计算，但问题依然存在：在极端约束条件下，能否在小语言模型（≤1.5B）中诱导出强大的推理能力？我们通过在单张A40 GPU（48GB）上使用可验证奖励强化学习（RLVR）和低秩适配（LoRA）进行不足24小时的训练来研究此问题。我们发现这种“微预算”机制的成功关键取决于适配器容量与模型初始化之间的相互作用。虽然低秩适配器（r=8）始终无法捕捉推理的复杂优化动态，但高秩适配器（r=256）能显著释放标准指令微调模型的可塑性。我们的最佳结果在AIME 24上实现了40.0%的Pass@1（较基线绝对提升11.1%），并将Pass@16推至70.0%，展现出强大的探索能力。然而这种可塑性并非普适：指令微调模型利用预算延长思维链并最大化奖励，而重度数学对齐模型则出现性能崩溃，表明噪声大、低预算的RL更新可能对已接近任务最优解的模型产生破坏性干扰。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the question of whether strong reasoning capabilities can be induced in small language models under extreme computational constraints, this paper investigates training models of ≤1.5B parameters on a single GPU for under 24 hours using Reinforcement Learning with Verifiable Rewards (RLVR) and Low-Rank Adaptation (LoRA). The method critically examines the interplay between adapter capacity and model initialization, finding that low-rank adapters fail to capture complex reasoning dynamics, whereas high-rank adapters unlock significant plasticity in standard instruction-tuned models. The main experimental results show that this approach achieved a 40.0% Pass@1 on AIME 24, an 11.1% absolute improvement over the baseline, and pushed Pass@16 to 70.0%, though heavily math-aligned models suffered performance collapse, indicating that noisy, low-budget RL updates can disrupt models already near a task-specific optimum.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究在极端计算约束下，能否在参数量≤1.5B的小型语言模型中诱导出强大的推理能力，方法是在单张GPU上使用可验证奖励的强化学习（RLVR）和低秩自适应（LoRA）对模型进行24小时内的训练。该方法重点关注适配器容量与模型初始化之间的相互作用，发现低秩适配器无法捕捉复杂的推理动态，而高秩适配器能在标准指令微调模型中释放显著的可塑性。主要实验结果表明，该方法在AIME 24上实现了40.0%的Pass@1，较基线绝对提升11.1%，并将Pass@16推至70.0%，但重度数学对齐的模型出现了性能崩溃，这表明嘈杂的低预算强化学习更新可能对已接近任务特定最优解的模型产生破坏性干扰。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning-Guided Dynamic Multi-Graph Fusion for Evacuation Traffic Prediction</div>
<div class="meta-line">Authors: Md Nafees Fuad Rafi, Samiul Hasan</div>
<div class="meta-line">First: 2026-01-10T19:56:23+00:00 · Latest: 2026-01-10T19:56:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06664v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06664v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-time traffic prediction is critical for managing transportation systems during hurricane evacuations. Although data-driven graph-learning models have demonstrated strong capabilities in capturing the complex spatiotemporal dynamics of evacuation traffic at a network level, they mostly consider a single dimension (e.g., travel-time or distance) to construct the underlying graph. Furthermore, these models often lack interpretability, offering little insight into which input variables contribute most to their predictive performance. To overcome these limitations, we develop a novel Reinforcement Learning-guided Dynamic Multi-Graph Fusion (RL-DMF) framework for evacuation traffic prediction. We construct multiple dynamic graphs at each time step to represent heterogeneous spatiotemporal relationships between traffic detectors. A dynamic multi-graph fusion (DMF) module is employed to adaptively learn and combine information from these graphs. To enhance model interpretability, we introduce RL-based intelligent feature selection and ranking (RL-IFSR) method that learns to mask irrelevant features during model training. The model is evaluated using a real-world dataset of 12 hurricanes affecting Florida from 2016 to 2024. For an unseen hurricane (Milton, 2024), the model achieves a 95% accuracy (RMSE = 293.9) for predicting the next 1-hour traffic flow. Moreover, the model can forecast traffic flow for up to next 6 hours with 90% accuracy (RMSE = 426.4). The RL-DMF framework outperforms several state-of-the-art traffic prediction models. Furthermore, ablation experiments confirm the effectiveness of dynamic multi-graph fusion and RL-IFSR approaches for improving model performance. This research provides a generalized and interpretable model for real-time evacuation traffic forecasting, with significant implications for evacuation traffic management.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习引导的动态多图融合用于疏散交通预测</div>
<div class="mono" style="margin-top:8px">实时交通预测对飓风疏散期间的交通系统管理至关重要。尽管数据驱动的图学习模型在网络层面捕捉疏散交通复杂时空动态方面展现出强大能力，但它们大多仅考虑单一维度（如行程时间或距离）构建底层图。此外，这些模型通常缺乏可解释性，难以揭示哪些输入变量对预测性能贡献最大。为克服这些局限，我们开发了一种新颖的强化学习引导动态多图融合（RL-DMF）框架用于疏散交通预测。我们在每个时间步构建多个动态图以表征交通检测器间的异构时空关系，采用动态多图融合（DMF）模块自适应学习并整合这些图的信息。为增强模型可解释性，我们引入基于强化学习的智能特征选择与排序（RL-IFSR）方法，该方法能在模型训练过程中学习屏蔽无关特征。模型使用2016至2024年影响佛罗里达州的12场飓风真实数据集进行评估。对于未见飓风（2024年米尔顿），模型预测未来1小时交通流量的准确率达95%（RMSE=293.9），并能以90%准确率（RMSE=426.4）预测未来6小时交通流量。RL-DMF框架优于多种先进交通预测模型，消融实验证实动态多图融合与RL-IFSR方法对提升模型性能的有效性。本研究为实时疏散交通预测提供了通用且可解释的模型，对疏散交通管理具有重要意义。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the need for accurate and interpretable real-time traffic prediction during hurricane evacuations, addressing limitations of existing graph-learning models that rely on single-dimensional graphs and lack insight into feature importance. The method introduces a Reinforcement Learning-guided Dynamic Multi-Graph Fusion (RL-DMF) framework, which constructs multiple dynamic graphs to capture heterogeneous spatiotemporal relationships and employs a dynamic multi-graph fusion module to adaptively combine them, alongside an RL-based feature selection method to mask irrelevant features for interpretability. Experimental results on a real-world dataset of 12 hurricanes in Florida show the model achieves 95% accuracy (RMSE = 293.9) for 1-hour traffic flow prediction and 90% accuracy (RMSE = 426.4) for up to 6-hour forecasts on an unseen hurricane, outperforming state-of-the-art models and validating the effectiveness of its components through ablation studies.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决飓风疏散期间实时交通预测的准确性和可解释性需求，针对现有图学习模型依赖单维图且缺乏特征重要性洞察的局限性。方法上提出了强化学习引导的动态多图融合框架，通过构建多个动态图以捕捉异构时空关系，并采用动态多图融合模块自适应整合信息，同时结合基于强化学习的特征选择方法屏蔽无关特征以增强可解释性。在佛罗里达州12次飓风的真实数据集上的实验结果表明，该模型对未见飓风的1小时交通流量预测准确率达95%，6小时内预测准确率达90%，优于现有先进模型，并通过消融实验验证了其组件的有效性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
