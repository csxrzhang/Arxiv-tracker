<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-21 03:47</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260221_0347</div>
    <div class="row"><div class="card">
<div class="title">SMAC: Score-Matched Actor-Critics for Robust Offline-to-Online Transfer</div>
<div class="meta-line">Authors: Nathan S. de Lara, Florian Shkurti</div>
<div class="meta-line">First: 2026-02-19T18:47:31+00:00 · Latest: 2026-02-19T18:47:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17632v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17632v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern offline Reinforcement Learning (RL) methods find performant actor-critics, however, fine-tuning these actor-critics online with value-based RL algorithms typically causes immediate drops in performance. We provide evidence consistent with the hypothesis that, in the loss landscape, offline maxima for prior algorithms and online maxima are separated by low-performance valleys that gradient-based fine-tuning traverses. Following this, we present Score Matched Actor-Critic (SMAC), an offline RL method designed to learn actor-critics that transition to online value-based RL algorithms with no drop in performance. SMAC avoids valleys between offline and online maxima by regularizing the Q-function during the offline phase to respect a first-order derivative equality between the score of the policy and action-gradient of the Q-function. We experimentally demonstrate that SMAC converges to offline maxima that are connected to better online maxima via paths with monotonically increasing reward found by first-order optimization. SMAC achieves smooth transfer to Soft Actor-Critic and TD3 in 6/6 D4RL tasks. In 4/6 environments, it reduces regret by 34-58% over the best baseline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SMAC：基于分数匹配的演员-评论家算法实现稳健的离线至在线迁移</div>
<div class="mono" style="margin-top:8px">现代离线强化学习方法虽能训练出高性能的演员-评论家模型，但使用基于值的在线强化学习算法对其进行微调通常会导致性能骤降。我们提供的证据支持以下假设：在损失函数景观中，现有算法的离线最优解与在线最优解之间存在低性能谷地，而基于梯度的微调过程会穿越这些谷地。基于此，我们提出分数匹配演员-评论家算法（SMAC），这是一种离线强化学习方法，旨在训练出能够无缝迁移至在线值基强化学习算法且保持性能不降的演员-评论家模型。SMAC通过在离线阶段对Q函数施加正则化约束，使其满足策略分数与Q函数动作梯度之间的一阶导数等式，从而规避离线与在线最优解之间的性能谷地。实验表明，SMAC收敛至的离线最优解可通过一阶优化找到的单调递增奖励路径与更优的在线最优解相连。在全部6项D4RL任务中，SMAC均能平滑迁移至Soft Actor-Critic和TD3算法；在4/6的环境中，其遗憾值较最佳基线降低34-58%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the observation that fine-tuning offline RL actor-critics online with value-based methods often leads to immediate performance drops, which the authors hypothesize is due to low-performance valleys separating offline and online maxima in the loss landscape. To address this, they propose Score Matched Actor-Critic (SMAC), a method that regularizes the Q-function during offline training to enforce a first-order derivative equality between the policy score and the action-gradient of the Q-function, thereby learning actor-critics that can transition smoothly to online fine-tuning. Experimental results show that SMAC enables monotonic performance improvement during online transfer, achieving smooth adaptation to Soft Actor-Critic and TD3 across all six D4RL tasks tested, with regret reductions of 34-58% in four of the environments compared to the best baseline.</div>
<div class="mono" style="margin-top:8px">本文的动机是观察到离线强化学习中的行动者-评论家模型在使用基于值的方法进行在线微调时通常会出现性能立即下降的问题，作者假设这是由于损失景观中离线最大值和在线最大值之间存在低性能谷地所致。为解决此问题，他们提出了评分匹配行动者-评论家（SMAC）方法，该方法在离线训练期间通过正则化Q函数，强制策略评分与Q函数的动作梯度之间满足一阶导数相等，从而学习能够平滑过渡到在线微调的模型。实验结果表明，SMAC在在线迁移过程中实现了性能的单调提升，在测试的六个D4RL任务中均能顺利适应Soft Actor-Critic和TD3算法，并在其中四个环境中相比最佳基线将遗憾降低了34-58%。</div>
</details>
</div>
<div class="card">
<div class="title">Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs</div>
<div class="meta-line">Authors: Luke Huang, Zhuoyang Zhang, Qinghao Hu, Shang Yang, Song Han</div>
<div class="meta-line">First: 2026-02-19T18:40:51+00:00 · Latest: 2026-02-19T18:40:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17616v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17616v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is widely used to improve large language models on reasoning tasks, and asynchronous RL training is attractive because it increases end-to-end throughput. However, for widely adopted critic-free policy-gradient methods such as REINFORCE and GRPO, high asynchrony makes the policy-gradient estimator markedly $\textbf{higher variance}$: training on stale rollouts creates heavy-tailed importance ratios, causing a small fraction of samples to dominate updates. This amplification makes gradients noisy and learning unstable relative to matched on-policy training. Across math and general reasoning benchmarks, we find collapse is reliably predicted by effective sample size (ESS) and unstable gradient norms. Motivated by this diagnosis, we propose $\textbf{V}$ariance $\textbf{C}$ontrolled $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{VCPO}$), a general stabilization method for REINFORCE/GRPO-style algorithms that (i) scales learning rate based on effective sample size to dampen unreliable updates, and (ii) applies a closed-form minimum-variance baseline for the off-policy setting, avoiding an auxiliary value model and adding minimal overhead. Empirically, VCPO substantially improves robustness for asynchronous training across math, general reasoning, and tool-use tasks, outperforming a broad suite of baselines spanning masking/clipping stabilizers and algorithmic variants. This reduces long-context, multi-turn training time by 2.5$\times$ while matching synchronous performance, demonstrating that explicit control of policy-gradient variance is key for reliable asynchronous RL at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>稳定异步：面向大语言模型的方差控制离策略强化学习</div>
<div class="mono" style="margin-top:8px">强化学习被广泛用于提升大语言模型在推理任务上的表现，异步强化学习训练因其能提高端到端吞吐量而备受关注。然而，对于广泛采用的无评论家策略梯度方法（如REINFORCE和GRPO），高度异步性会使策略梯度估计器产生显著的$\textbf{高方差}$：基于陈旧轨迹样本的训练会产生重尾重要性权重，导致少量样本主导参数更新。这种放大效应使梯度噪声增大，学习过程相对于匹配的同策略训练更不稳定。在数学和通用推理基准测试中，我们发现训练崩溃可通过有效样本量和不稳定梯度范数可靠预测。基于此诊断，我们提出$\textbf{方差控制策略优化}$方法（$\textbf{VCPO}$），这是一种针对REINFORCE/GRPO类算法的通用稳定方法，其（i）根据有效样本量动态缩放学习率以抑制不可靠更新，（ii）为离策略场景应用闭式最小方差基线，无需辅助价值模型且计算开销极小。实验表明，VCPO在数学推理、通用推理和工具使用任务的异步训练中显著提升鲁棒性，优于包括掩码/截断稳定器和算法变体在内的广泛基线方法。该方法在保持同步训练性能的同时，将长上下文多轮训练时间缩短2.5$\times$，证明显式控制策略梯度方差是实现大规模可靠异步强化学习的关键。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the instability in asynchronous reinforcement learning (RL) for large language models, where high asynchrony leads to high variance in policy-gradient estimators like REINFORCE and GRPO due to stale rollouts and heavy-tailed importance ratios, causing noisy gradients and learning collapse. To mitigate this, the authors propose Variance Controlled Policy Optimization (VCPO), a stabilization method that scales the learning rate based on effective sample size to dampen unreliable updates and employs a closed-form minimum-variance baseline for off-policy settings without needing an auxiliary value model. Experimental results on math, general reasoning, and tool-use benchmarks show that VCPO substantially improves robustness and reduces training time by 2.5× while matching synchronous performance, demonstrating its effectiveness in controlling policy-gradient variance for scalable asynchronous RL.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型异步强化学习中的不稳定性问题展开研究，指出高异步性会导致REINFORCE和GRPO等策略梯度估计器方差显著增大，原因是陈旧的样本轨迹和重尾重要性权重使得少量样本主导更新，引发梯度噪声和学习崩溃。为解决此问题，作者提出了方差控制策略优化（VCPO）方法，该方法基于有效样本量调整学习率以抑制不可靠更新，并采用闭式最小方差基线处理离策略场景，无需额外价值模型且开销极小。在数学、通用推理和工具使用等任务上的实验表明，VCPO大幅提升了异步训练的鲁棒性，将长上下文多轮训练时间减少2.5倍的同时达到同步训练性能，验证了显式控制策略梯度方差对大规模异步强化学习可靠性的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery</div>
<div class="meta-line">Authors: Jowaria Khan, Anindya Sarkar, Yevgeniy Vorobeychik, Elizabeth Bondi-Kelly</div>
<div class="meta-line">First: 2026-02-19T18:30:18+00:00 · Latest: 2026-02-19T18:30:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17605v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17605v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In many real-world settings, such as environmental monitoring, disaster response, or public health, with costly and difficult data collection and dynamic environments, strategically sampling from unobserved regions is essential for efficiently uncovering hidden targets under tight resource constraints. Yet, sparse and biased geospatial ground truth limits the applicability of existing learning-based methods, such as reinforcement learning. To address this, we propose a unified geospatial discovery framework that integrates active learning, online meta-learning, and concept-guided reasoning. Our approach introduces two key innovations built on a shared notion of *concept relevance*, which captures how domain-specific factors influence target presence: a *concept-weighted uncertainty sampling strategy*, where uncertainty is modulated by learned relevance based on readily-available domain-specific concepts (e.g., land cover, source proximity); and a *relevance-aware meta-batch formation strategy* that promotes semantic diversity during online-meta updates, improving generalization in dynamic environments. Our experiments include testing on a real-world dataset of cancer-causing PFAS (Per- and polyfluoroalkyl substances) contamination, showcasing our method&#x27;s reliability at uncovering targets with limited data and a varying environment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>动态主动适应：面向地理空间发现的概念引导在线元学习框架</div>
<div class="mono" style="margin-top:8px">在环境监测、灾害响应或公共卫生等现实场景中，数据采集成本高昂且环境动态变化，如何在资源受限条件下通过战略性地对未观测区域进行采样以高效发现隐藏目标至关重要。然而，稀疏且存在偏差的地理空间真实数据限制了强化学习等现有基于学习的方法的适用性。为此，我们提出一个统一的地理空间发现框架，整合了主动学习、在线元学习和概念引导推理。该框架基于*概念相关性*这一共享概念（用于捕捉领域特定因素如何影响目标存在性）构建了两项关键创新：一是*概念加权不确定性采样策略*，通过基于易获取的领域特定概念（如土地覆盖、污染源邻近度）学习到的相关性来调节不确定性；二是*相关性感知元批次构建策略*，在在线元更新过程中促进语义多样性，从而提升动态环境下的泛化能力。我们在真实世界致癌性PFAS（全氟及多氟烷基化合物）污染数据集上进行了实验验证，结果表明本方法在数据有限且环境多变的条件下仍能可靠地发现目标。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient geospatial discovery in dynamic, data-scarce real-world settings like environmental monitoring, this paper proposes a unified framework integrating active learning, online meta-learning, and concept-guided reasoning. The method introduces two innovations based on a shared notion of concept relevance: a concept-weighted uncertainty sampling strategy that modulates uncertainty using domain-specific concepts, and a relevance-aware meta-batch formation strategy to enhance semantic diversity during online updates. Experimental results on a real-world PFAS contamination dataset demonstrate the method&#x27;s reliability in uncovering targets with limited data under varying environmental conditions.</div>
<div class="mono" style="margin-top:8px">本文针对环境监测等动态、数据稀缺的真实场景中高效地理空间发现的需求，提出了一个融合主动学习、在线元学习和概念引导推理的统一框架。该方法基于共享的概念相关性概念引入了两项创新：一种利用领域特定概念调节不确定性的概念加权不确定性采样策略，以及一种在在线更新中增强语义多样性的相关性感知元批次形成策略。在真实世界PFAS污染数据集上的实验结果表明，该方法能在数据有限和环境变化的条件下可靠地发现目标。</div>
</details>
</div>
<div class="card">
<div class="title">MASPO: Unifying Gradient Utilization, Probability Mass, and Signal Reliability for Robust and Sample-Efficient LLM Reasoning</div>
<div class="meta-line">Authors: Xiaoliang Fu, Jiaye Lin, Yangyi Fang, Binbin Zheng, Chaowen Hu, Zekai Shao, Cong Qin, Lu Pan, Ke Zeng, Xunliang Cai</div>
<div class="meta-line">First: 2026-02-19T17:05:20+00:00 · Latest: 2026-02-19T17:05:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17550v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17550v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing Reinforcement Learning with Verifiable Rewards (RLVR) algorithms, such as GRPO, rely on rigid, uniform, and symmetric trust region mechanisms that are fundamentally misaligned with the complex optimization dynamics of Large Language Models (LLMs). In this paper, we identify three critical challenges in these methods: (1) inefficient gradient utilization caused by the binary cutoff of hard clipping, (2) insensitive probability mass arising from uniform ratio constraints that ignore the token distribution, and (3) asymmetric signal reliability stemming from the disparate credit assignment ambiguity between positive and negative samples. To bridge these gaps, we propose Mass-Adaptive Soft Policy Optimization (MASPO), a unified framework designed to harmonize these three dimensions. MASPO integrates a differentiable soft Gaussian gating to maximize gradient utility, a mass-adaptive limiter to balance exploration across the probability spectrum, and an asymmetric risk controller to align update magnitudes with signal confidence. Extensive evaluations demonstrate that MASPO serves as a robust, all-in-one RLVR solution, significantly outperforming strong baselines. Our code is available at: https://anonymous.4open.science/r/ma1/README.md.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MASPO：统一梯度利用、概率质量与信号可靠性以实现鲁棒且样本高效的大语言模型推理</div>
<div class="mono" style="margin-top:8px">现有基于可验证奖励的强化学习算法（如GRPO）依赖僵化、均匀且对称的信任域机制，这与大语言模型复杂的优化动态存在根本性错配。本文指出此类方法的三大关键挑战：（1）硬截断的二元截断导致梯度利用效率低下；（2）均匀比例约束忽视词元分布，引发概率质量不敏感问题；（3）正负样本间信用分配模糊度差异导致信号可靠性不对称。为弥合这些缺陷，我们提出质量自适应软策略优化框架MASPO，通过可微分高斯软门控最大化梯度效用，采用质量自适应限幅器平衡概率空间探索，并引入非对称风险控制器使更新幅度与信号置信度对齐。大量实验表明，MASPO作为一体化RLVR解决方案，显著优于现有基线方法。代码已开源：https://anonymous.4open.science/r/ma1/README.md。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the limitations of existing Reinforcement Learning with Verifiable Rewards (RLVR) methods for Large Language Models, which use rigid trust regions that lead to inefficient gradient use, insensitive handling of token probabilities, and unreliable credit assignment for positive versus negative rewards. To address these issues, the method introduces Mass-Adaptive Soft Policy Optimization (MASPO), a unified framework incorporating soft Gaussian gating for better gradient utilization, a mass-adaptive limiter to adjust probability mass constraints, and an asymmetric risk controller to align updates with reward signal reliability. Experimental results show that MASPO robustly outperforms strong baselines, offering a sample-efficient and effective RLVR solution for LLM reasoning.</div>
<div class="mono" style="margin-top:8px">本文的动机源于现有基于可验证奖励的强化学习（RLVR）方法在大型语言模型中的局限性，这些方法使用僵化的信任区域机制，导致梯度利用效率低、对令牌概率分布不敏感，以及正负奖励样本的信度分配不可靠。为解决这些问题，该方法提出了质量自适应软策略优化（MASPO），这是一个统一框架，集成了软高斯门控以优化梯度利用，质量自适应限制器以平衡概率谱上的探索，以及非对称风险控制器以使更新幅度与奖励信号可信度对齐。大量实验评估表明，MASPO作为一种稳健的一体化RLVR解决方案，显著优于现有基线方法，为LLM推理提供了样本高效且强大的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Capturing Individual Human Preferences with Reward Features</div>
<div class="meta-line">Authors: André Barreto, Vincent Dumoulin, Yiran Mao, Mark Rowland, Nicolas Perez-Nieves, Bobak Shahriari, Yann Dauphin, Doina Precup, Hugo Larochelle</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-03-21T17:39:33+00:00 · Latest: 2026-02-19T16:23:22+00:00</div>
<div class="meta-line">Comments: Published at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.17338v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.17338v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning from human feedback usually models preferences using a reward function that does not distinguish between people. We argue that this is unlikely to be a good design choice in contexts with high potential for disagreement, like in the training of large language models. We formalise and analyse the problem of learning a reward model that can be specialised to a user. Using the principle of empirical risk minimisation, we derive a probably approximately correct (PAC) bound showing the dependency of the approximation error on the number of training examples, as usual, and also on the number of human raters who provided feedback on them. Based on our theoretical findings, we discuss how to best collect pairwise preference data and argue that adaptive reward models should be beneficial when there is considerable disagreement among users. We also propose a concrete architecture for an adaptive reward model. Our approach leverages the observation that individual preferences can be captured as a linear combination of a set of general reward features. We show how to learn such features and subsequently use them to quickly adapt the reward model to a specific individual, even if their preferences are not reflected in the training data. We present experiments with large language models illustrating our theoretical results and comparing the proposed architecture with a non-adaptive baseline. Consistent with our analysis, the benefits provided by our model increase with the number of raters and the heterogeneity of their preferences. We also show that our model compares favourably to adaptive counterparts, including those performing in-context personalisation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用奖励特征捕捉个体人类偏好</div>
<div class="mono" style="margin-top:8px">基于人类反馈的强化学习通常使用不区分个体的奖励函数来建模偏好。我们认为，在存在高度分歧可能性的场景（如大语言模型训练）中，这并非理想的设计选择。我们形式化并分析了学习可适配特定用户的奖励模型的问题。基于经验风险最小化原则，我们推导出近似正确概率（PAC）边界，揭示了近似误差对训练样本数量的依赖（如常规情况），以及对提供反馈的人类评分者数量的依赖。基于理论发现，我们探讨了如何最优收集成对偏好数据，并论证当用户间存在显著分歧时，自适应奖励模型应具有优势。我们还提出了一种自适应奖励模型的具体架构。该方法利用了个体偏好可表示为通用奖励特征集的线性组合这一观察。我们展示了如何学习此类特征，并随后利用它们快速将奖励模型适配到特定个体，即使其偏好未体现在训练数据中。我们通过大语言模型实验展示了理论结果，并将所提架构与非自适应基线进行比较。与我们的分析一致，模型带来的优势随评分者数量及其偏好异质性的增加而提升。我们还证明，该模型优于包括执行上下文个性化在内的其他自适应方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to model diverse human preferences in contexts like large language model training, this paper proposes learning adaptive reward models that capture individual differences. The method formalizes the problem using empirical risk minimization, deriving a PAC bound linking approximation error to both the number of training examples and human raters, and introduces an architecture where individual preferences are represented as linear combinations of general reward features, enabling quick personalization. Experimental results with large language models show that the adaptive model outperforms non-adaptive baselines, with benefits increasing with the number of raters and preference heterogeneity, and it compares favorably to other adaptive approaches, including in-context personalization methods.</div>
<div class="mono" style="margin-top:8px">本文的动机是在大型语言模型训练等场景中，需要建模多样化的人类偏好，因此提出了学习能够捕捉个体差异的自适应奖励模型。方法上，通过经验风险最小化形式化该问题，推导了将近似误差与训练样本数量及人类评分者数量相关联的PAC界，并提出一种架构，将个体偏好表示为通用奖励特征的线性组合，从而实现快速个性化。在大型语言模型上的实验结果表明，该自适应模型优于非自适应基线，其优势随评分者数量和偏好异质性的增加而提升，并且与其他自适应方法（包括上下文个性化方法）相比表现更佳。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Reinforcement Learning for Large Language Models with Intrinsic Exploration</div>
<div class="meta-line">Authors: Yan Sun, Jia Guo, Stanley Kok, Zihao Wang, Zujie Wen, Zhiqiang Zhang</div>
<div class="meta-line">First: 2025-11-02T04:16:47+00:00 · Latest: 2026-02-19T16:18:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.00794v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.00794v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) has improved the reasoning ability of large language models, yet training remains costly because many rollouts contribute little to optimization, considering the amount of computation required. This study investigates how simply leveraging intrinsic data properties, almost free benefit during training, can improve data efficiency for RLVR. We propose PREPO with two complementary components. First, we adopt prompt perplexity as an indicator of model adaptability in learning, enabling the model to progress from well-understood contexts to more challenging ones. Second, we amplify the discrepancy among the rollouts by differentiating their relative entropy, and prioritize sequences that exhibit a higher degree of exploration. Together, these mechanisms reduce rollout demand while preserving competitive performance. On the Qwen and Llama models, PREPO achieves effective results on mathematical reasoning benchmarks with up to 3 times fewer rollouts than the baselines. Beyond empirical gains, we provide theoretical and in-depth analyses explaining the underlying rationale of our method to improve the data efficiency of RLVR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于内在探索的大语言模型高效强化学习</div>
<div class="mono" style="margin-top:8px">可验证奖励的强化学习（RLVR）提升了大语言模型的推理能力，但由于大量计算需求，许多训练轨迹对优化贡献有限，导致训练成本高昂。本研究探讨如何利用训练中几乎零成本的内在数据特性，提升RLVR的数据效率。我们提出PREPO方法，包含两个互补组件：首先，采用提示困惑度作为模型学习适应性的指标，使模型从易理解语境逐步过渡至更具挑战性的语境；其次，通过区分轨迹间的相对熵来放大其差异，并优先选择探索程度更高的序列。这些机制共同作用，在保持竞争力的同时减少了轨迹需求。在Qwen和Llama模型上，PREPO在数学推理基准测试中仅需基线方法1/3的轨迹即可取得显著效果。除实证优势外，我们还通过理论与深度分析阐释了该方法提升RLVR数据效率的内在原理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the high computational cost of reinforcement learning with verifiable rewards (RLVR) for large language models, where many rollouts contribute little to optimization. The authors propose PREPO, a method that improves data efficiency by leveraging intrinsic data properties through two components: using prompt perplexity to guide learning from easier to harder contexts, and amplifying rollout discrepancy via relative entropy to prioritize exploratory sequences. Experiments on Qwen and Llama models show that PREPO achieves competitive performance on mathematical reasoning benchmarks while reducing rollout demand by up to three times compared to baselines, supported by theoretical analysis of its efficiency gains.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型的可验证奖励强化学习训练成本高、许多计算贡献有限的问题，提出通过利用内在数据特性提升数据效率。方法PREPO包含两个互补部分：使用提示困惑度作为模型适应性的指标，引导学习从易到难；通过区分相对熵放大探索差异，优先选择探索性序列。在Qwen和Llama模型上的实验表明，PREPO在数学推理基准上取得了有竞争力的性能，同时将训练所需的计算量减少至基线方法的三分之一，并通过理论分析解释了其提升数据效率的原理。</div>
</details>
</div>
<div class="card">
<div class="title">LexiSafe: Offline Safe Reinforcement Learning with Lexicographic Safety-Reward Hierarchy</div>
<div class="meta-line">Authors: Hsin-Jung Yang, Zhanhong Jiang, Prajwal Koirala, Qisai Liu, Cody Fleming, Soumik Sarkar</div>
<div class="meta-line">First: 2026-02-19T12:22:50+00:00 · Latest: 2026-02-19T12:22:50+00:00</div>
<div class="meta-line">Comments: 17th ACM/IEEE International Conference on Cyber-Physical Systems</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17312v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17312v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline safe reinforcement learning (RL) is increasingly important for cyber-physical systems (CPS), where safety violations during training are unacceptable and only pre-collected data are available. Existing offline safe RL methods typically balance reward-safety tradeoffs through constraint relaxation or joint optimization, but they often lack structural mechanisms to prevent safety drift. We propose LexiSafe, a lexicographic offline RL framework designed to preserve safety-aligned behavior. We first develop LexiSafe-SC, a single-cost formulation for standard offline safe RL, and derive safety-violation and performance-suboptimality bounds that together yield sample-complexity guarantees. We then extend the framework to hierarchical safety requirements with LexiSafe-MC, which supports multiple safety costs and admits its own sample-complexity analysis. Empirically, LexiSafe demonstrates reduced safety violations and improved task performance compared to constrained offline baselines. By unifying lexicographic prioritization with structural bias, LexiSafe offers a practical and theoretically grounded approach for safety-critical CPS decision-making.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LexiSafe：基于词典序安全-奖励层级的离线安全强化学习</div>
<div class="mono" style="margin-top:8px">离线安全强化学习对信息物理系统日益重要，因其训练阶段的安全违规不可接受且仅能使用预收集数据。现有方法通常通过约束松弛或联合优化平衡奖励与安全，但缺乏防止安全漂移的结构机制。本文提出词典序离线强化学习框架LexiSafe以保持安全对齐行为：首先构建单成本形式LexiSafe-SC，推导出安全违规与性能次优性边界，共同形成样本复杂度保证；进而扩展为多安全成本框架LexiSafe-MC，支持层级安全需求并具备独立样本复杂度分析。实验表明，相比约束离线基线方法，LexiSafe能减少安全违规并提升任务性能。该框架通过词典序优先级与结构偏置的统一，为安全关键信息物理系统决策提供兼具理论依据与实践价值的新途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for LexiSafe arises from the need for safe offline reinforcement learning in cyber-physical systems, where training must avoid safety violations using only pre-collected data, and existing methods often lack mechanisms to prevent safety drift. The method introduces a lexicographic framework that prioritizes safety over reward, with LexiSafe-SC for single safety costs and LexiSafe-MC for multiple hierarchical safety requirements, both supported by theoretical sample-complexity guarantees. Experimental results show that LexiSafe reduces safety violations and improves task performance compared to constrained offline baselines, offering a practical, theoretically grounded approach for safety-critical decision-making.</div>
<div class="mono" style="margin-top:8px">LexiSafe的提出动机源于信息物理系统对安全离线强化学习的需求，即训练必须仅使用预先收集的数据且避免安全违规，而现有方法常缺乏防止安全漂移的机制。该方法引入了一种词典序框架，将安全性置于奖励之上，其中LexiSafe-SC处理单一安全成本，LexiSafe-MC处理多层级安全需求，两者均得到理论样本复杂度保证的支持。实验结果表明，与受限离线基线相比，LexiSafe减少了安全违规并提升了任务性能，为安全关键决策提供了一种实用且理论扎实的方法。</div>
</details>
</div>
<div class="card">
<div class="title">RLGT: A reinforcement learning framework for extremal graph theory</div>
<div class="meta-line">Authors: Ivan Damnjanović, Uroš Milivojević, Irena Đorđević, Dragan Stevanović</div>
<div class="meta-line">First: 2026-02-19T11:25:22+00:00 · Latest: 2026-02-19T11:25:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17276v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17276v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is a subfield of machine learning that focuses on developing models that can autonomously learn optimal decision-making strategies over time. In a recent pioneering paper, Wagner demonstrated how the Deep Cross-Entropy RL method can be applied to tackle various problems from extremal graph theory by reformulating them as combinatorial optimization problems. Subsequently, many researchers became interested in refining and extending the framework introduced by Wagner, thereby creating various RL environments specialized for graph theory. Moreover, a number of problems from extremal graph theory were solved through the use of RL. In particular, several inequalities concerning the Laplacian spectral radius of graphs were refuted, new lower bounds were obtained for certain Ramsey numbers, and contributions were made to the Turán-type extremal problem in which the forbidden structures are cycles of length three and four. Here, we present Reinforcement Learning for Graph Theory (RLGT), a novel RL framework that systematizes the previous work and provides support for both undirected and directed graphs, with or without loops, and with an arbitrary number of edge colors. The framework efficiently represents graphs and aims to facilitate future RL-based research in extremal graph theory through optimized computational performance and a clean and modular design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RLGT：极值图论中的强化学习框架</div>
<div class="mono" style="margin-top:8px">强化学习是机器学习的一个子领域，专注于开发能够随时间自主习得最优决策策略的模型。在近期一篇开创性论文中，Wagner展示了如何通过将极值图论中的各类问题重构为组合优化问题，应用深度交叉熵强化学习方法加以解决。随后，众多研究者致力于改进和扩展Wagner提出的框架，由此创建了多种专用于图论的强化学习环境。此外，已有若干极值图论问题借助强化学习得以解决：具体包括证伪了若干关于图谱拉普拉斯半径的不等式、获得了某些拉姆齐数的新下界，并对禁止结构为三阶与四阶环的图兰型极值问题作出了贡献。本文提出“图论强化学习”框架，该系统化整合了先前工作，支持含或不含自环的无向图与有向图，并可处理任意边着色数量。该框架通过高效图表示、优化计算性能及清晰模块化设计，旨在为极值图论领域基于强化学习的未来研究提供支持。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to systematize and extend prior research that applied reinforcement learning (RL) to problems in extremal graph theory, which often involve combinatorial optimization. The method introduces RLGT, a novel RL framework designed to support undirected and directed graphs, loops, and multiple edge colors, featuring efficient graph representation and a modular design for optimized performance. The main experimental results, building on earlier applications, include refuting several inequalities about Laplacian spectral radii, obtaining new lower bounds for certain Ramsey numbers, and contributing to Turán-type extremal problems involving forbidden cycles.</div>
<div class="mono" style="margin-top:8px">这项工作的动机是系统化和扩展先前将强化学习应用于极值图论问题的研究，这些问题通常涉及组合优化。方法上提出了RLGT，这是一个新颖的强化学习框架，支持无向图、有向图、自环和多边色，具有高效的图表示和模块化设计以优化性能。主要实验结果基于早期应用，包括反驳了关于拉普拉斯谱半径的几个不等式，获得了某些拉姆齐数的新下界，并对涉及禁止循环的Turán型极值问题做出了贡献。</div>
</details>
</div>
<div class="card">
<div class="title">LRT-Diffusion: Calibrated Risk-Aware Guidance for Diffusion Policies</div>
<div class="meta-line">Authors: Ximan Sun, Xiang Cheng</div>
<div class="meta-line">First: 2025-10-28T21:26:18+00:00 · Latest: 2026-02-19T11:08:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.24983v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.24983v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion policies are competitive for offline reinforcement learning (RL) but are typically guided at sampling time by heuristics that lack a statistical notion of risk. We introduce LRT-Diffusion, a risk-aware sampling rule that treats each denoising step as a sequential hypothesis test between the unconditional prior and the state-conditional policy head. Concretely, we accumulate a log-likelihood ratio and gate the conditional mean with a logistic controller whose threshold tau is calibrated once under H0 to meet a user-specified Type-I level alpha. This turns guidance from a fixed push into an evidence-driven adjustment with a user-interpretable risk budget. Importantly, we deliberately leave training vanilla (two heads with standard epsilon-prediction) under the structure of DDPM. LRT guidance composes naturally with Q-gradients: critic-gradient updates can be taken at the unconditional mean, at the LRT-gated mean, or a blend, exposing a continuum from exploitation to conservatism. We standardize states and actions consistently at train and test time and report a state-conditional out-of-distribution (OOD) metric alongside return. On D4RL MuJoCo tasks, LRT-Diffusion improves the return-OOD trade-off over strong Q-guided baselines in our implementation while honoring the desired alpha. Theoretically, we establish level-alpha calibration, concise stability bounds, and a return comparison showing when LRT surpasses Q-guidance-especially when off-support errors dominate. Overall, LRT-Diffusion is a drop-in, inference-time method that adds principled, calibrated risk control to diffusion policies for offline RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LRT-Diffusion：扩散策略的校准风险感知引导</div>
<div class="mono" style="margin-top:8px">扩散策略在离线强化学习中具有竞争力，但采样时通常依赖缺乏统计风险概念的启发式引导。本文提出LRT-Diffusion，这是一种风险感知采样规则，将每个去噪步骤视为无条件先验与状态条件策略头之间的序贯假设检验。具体而言，我们累积对数似然比，并通过逻辑控制器对条件均值进行门控——该控制器的阈值τ在H0假设下一次校准，以满足用户指定的I类错误水平α。这使得引导从固定推动转变为证据驱动的调整，并具有用户可解释的风险预算。重要的是，我们在DDPM结构下刻意保持训练过程原始性（采用标准ε预测的双头结构）。LRT引导可与Q梯度自然组合：评论家梯度更新可在无条件均值、LRT门控均值或二者混合处进行，从而形成从利用到保守的连续谱。我们在训练和测试时对状态与动作进行标准化处理，并同时报告状态条件分布外指标与回报。在D4RL MuJoCo任务中，LRT-Diffusion在实现中优于强Q引导基线，改善了回报-分布外权衡关系，同时严格遵守目标α水平。理论上，我们建立了α水平校准、简洁的稳定性边界，以及揭示LRT何时超越Q引导的回报比较分析——尤其当分布外误差占主导时。总体而言，LRT-Diffusion是一种即插即用的推理时方法，为离线RL的扩散策略增添了原则性、可校准的风险控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces LRT-Diffusion, a method to add calibrated risk-aware guidance to diffusion policies in offline reinforcement learning, motivated by the lack of a statistical risk notion in existing heuristic sampling-time guidance. The method treats each denoising step as a sequential hypothesis test, accumulating a log-likelihood ratio and using a logistic controller with a calibrated threshold to gate the conditional mean, thereby transforming guidance into an evidence-driven adjustment with a user-specified risk budget while keeping training unchanged. Experimental results on D4RL MuJoCo tasks show that LRT-Diffusion improves the trade-off between return and out-of-distribution detection over strong Q-guided baselines while maintaining the desired statistical risk level, with theoretical support for calibration and performance bounds.</div>
<div class="mono" style="margin-top:8px">本文提出了LRT-Diffusion方法，旨在为离线强化学习中的扩散策略添加经过校准的风险感知引导，其动机是现有启发式采样时引导缺乏统计风险概念。该方法将每个去噪步骤视为序贯假设检验，累积对数似然比并使用具有校准阈值的逻辑控制器来门控条件均值，从而将引导转变为基于证据的调整，并允许用户指定风险预算，同时保持训练过程不变。在D4RL MuJoCo任务上的实验结果表明，LRT-Diffusion在保持期望统计风险水平的同时，相比强大的Q引导基线方法，改善了回报与分布外检测之间的权衡，并得到了校准和性能界限的理论支持。</div>
</details>
</div>
<div class="card">
<div class="title">Continual uncertainty learning</div>
<div class="meta-line">Authors: Heisei Yonezawa, Ansei Yonezawa, Itsuro Kajiwara</div>
<div class="meta-line">First: 2026-02-19T08:39:42+00:00 · Latest: 2026-02-19T08:39:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17174v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17174v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robust control of mechanical systems with multiple uncertainties remains a fundamental challenge, particularly when nonlinear dynamics and operating-condition variations are intricately intertwined. While deep reinforcement learning (DRL) combined with domain randomization has shown promise in mitigating the sim-to-real gap, simultaneously handling all sources of uncertainty often leads to sub-optimal policies and poor learning efficiency. This study formulates a new curriculum-based continual learning framework for robust control problems involving nonlinear dynamical systems in which multiple sources of uncertainty are simultaneously superimposed. The key idea is to decompose a complex control problem with multiple uncertainties into a sequence of continual learning tasks, in which strategies for handling each uncertainty are acquired sequentially. The original system is extended into a finite set of plants whose dynamic uncertainties are gradually expanded and diversified as learning progresses. The policy is stably updated across the entire plant sets associated with tasks defined by different uncertainty configurations without catastrophic forgetting. To ensure learning efficiency, we jointly incorporate a model-based controller (MBC), which guarantees a shared baseline performance across the plant sets, into the learning process to accelerate the convergence. This residual learning scheme facilitates task-specific optimization of the DRL agent for each uncertainty, thereby enhancing sample efficiency. As a practical industrial application, this study applies the proposed method to designing an active vibration controller for automotive powertrains. We verified that the resulting controller is robust against structural nonlinearities and dynamic variations, realizing successful sim-to-real transfer.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>持续不确定性学习</div>
<div class="mono" style="margin-top:8px">具有多重不确定性的机械系统鲁棒控制仍是一个根本性挑战，尤其在非线性动力学与工况变化深度交织时。虽然深度强化学习结合领域随机化在缩小仿真-现实差距方面展现出潜力，但同步处理所有不确定性源常导致策略次优与学习效率低下。本研究针对涉及多重不确定性同时叠加的非线性动力系统鲁棒控制问题，提出一种基于课程学习的持续学习框架。其核心思想是将含多重不确定性的复杂控制问题分解为一系列持续学习任务，按序习得处理各类不确定性的策略。通过将原系统扩展为有限对象集，其动态不确定性随学习进程逐步扩展与多样化。策略可在不同不确定性配置定义的任务所关联的完整对象集上稳定更新，避免灾难性遗忘。为提升学习效率，我们将基于模型的控制器（保障对象集间共享基线性能）联合融入学习过程以加速收敛。该残差学习机制促进深度强化学习智能体针对各不确定性的任务专属优化，从而提升样本效率。作为实际工业应用，本研究将所提方法用于汽车动力总成主动振动控制器设计，验证了所得控制器对结构非线性和动态变化的鲁棒性，成功实现了仿真到现实的迁移。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of robust control for nonlinear mechanical systems with multiple, intertwined uncertainties, where standard deep reinforcement learning (DRL) with domain randomization often yields inefficient learning and sub-optimal policies. The authors propose a continual learning framework that decomposes the complex control problem into a sequence of tasks, gradually expanding and diversifying the dynamic uncertainties across a set of simulated plants to learn strategies for each uncertainty sequentially without catastrophic forgetting. To improve efficiency, a model-based controller provides a shared performance baseline, enabling residual DRL optimization for each uncertainty. Experimental application to an automotive powertrain vibration controller demonstrates robustness against nonlinearities and dynamic variations, achieving successful sim-to-real transfer.</div>
<div class="mono" style="margin-top:8px">本文针对具有多种交织不确定性的非线性机械系统的鲁棒控制挑战，其中标准的深度强化学习与领域随机化方法常导致学习效率低下和策略次优。作者提出一种持续学习框架，将复杂的控制问题分解为一系列任务，在学习过程中逐步扩展和多样化一组仿真模型中的动态不确定性，从而顺序学习处理每种不确定性的策略且避免灾难性遗忘。为提高效率，一个基于模型的控制器提供了共享的性能基线，使深度强化学习能对每种不确定性进行残差优化。在汽车动力总成振动控制的实际工业应用中，实验验证了该方法对结构非线性和动态变化的鲁棒性，实现了成功的仿真到现实迁移。</div>
</details>
</div>
<div class="card">
<div class="title">CaveAgent: Transforming LLMs into Stateful Runtime Operators</div>
<div class="meta-line">Authors: Maohao Ran, Zhenglin Wan, Cooper Lin, Yanting Zhang, Hongyu Xin, Hongwei Fan, Yibo Xu, Beier Luo, Yaxin Zhou, Wangbo Zhao, Lijie Yang, Lang Feng, Fuchao Yang, Jingxuan Wu, Yiqiao Huang, Chendong Ma, Dailing Jiang, Jianbo Deng, Sirui Han, Yang You, Bo An, Yike Guo, Jun Song</div>
<div class="meta-line">First: 2026-01-04T15:32:47+00:00 · Latest: 2026-02-19T07:07:15+00:00</div>
<div class="meta-line">Comments: ver.2</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01569v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.01569v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-based agents are increasingly capable of complex task execution, yet current agentic systems remain constrained by text-centric paradigms that struggle with long-horizon tasks due to fragile multi-turn dependencies and context drift. We present CaveAgent, a framework that shifts tool use from ``LLM-as-Text-Generator&#x27;&#x27; to ``LLM-as-Runtime-Operator.&#x27;&#x27; CaveAgent introduces a dual-stream architecture that inverts the conventional paradigm: rather than treating the LLM&#x27;s text context as the primary workspace with tools as auxiliary, CaveAgent elevates the persistent Python runtime as the central locus of state, with a lightweight semantic stream serving as its orchestrator. Beyond leveraging code generation to resolve interdependent sub-tasks (e.g., loops, conditionals) in a single step, CaveAgent introduces \textit{Stateful Runtime Management}: it injects, manipulates, and retrieves complex Python objects (e.g., DataFrames, database connections) that persist across turns, unlike existing code-based approaches that remain text-bound. CaveAgent further provides a runtime-integrated skill management system that extends the Agent Skills open standard, enabling ecosystem interoperability through executable skill injections. This persistence mechanism serves as a high-fidelity external memory that reduces context drift in multi-turn interactions and preserves processed data for downstream applications without information loss. Evaluations show consistent improvement across challenging benchmarks, enabling CaveAgent to handle data scales that cause context overflow in both JSON-based and code-based agents. The accessible runtime state further provides programmatically verifiable feedback, enabling automated evaluation and reward signal generation without human annotation and establishing a structural foundation for future research in Reinforcement Learning with Verifiable Rewards (RLVR).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CaveAgent：将大语言模型转化为有状态运行时算子</div>
<div class="mono" style="margin-top:8px">基于大语言模型的智能体执行复杂任务的能力日益增强，但现有智能体系统仍受限于以文本为中心的范式，因其脆弱的多轮依赖关系和语境漂移问题，难以处理长周期任务。本文提出CaveAgent框架，将工具使用范式从&#x27;LLM作为文本生成器&#x27;转变为&#x27;LLM作为运行时算子&#x27;。该框架采用双流架构颠覆传统范式：不再将大语言模型的文本语境作为主要工作空间、工具作为辅助，而是将持久化Python运行时提升为核心状态载体，由轻量级语义流充当协调器。除通过代码生成单步解决相互依赖的子任务（如循环、条件判断）外，CaveAgent引入&#x27;有状态运行时管理&#x27;机制：注入、操作和检索跨轮次持久化的复杂Python对象（如DataFrame、数据库连接），突破现有基于代码方法仍受文本束缚的限制。框架进一步提供运行时集成的技能管理系统，扩展了Agent Skills开放标准，通过可执行技能注入实现生态互操作性。这种持久化机制作为高保真外部记忆，减少多轮交互中的语境漂移，完整保存已处理数据供下游应用使用。实验表明，在多项挑战性基准测试中均取得稳定提升，使CaveAgent能够处理导致JSON基和代码基智能体语境溢出的数据规模。可访问的运行时状态进一步提供可编程验证的反馈，无需人工标注即可实现自动评估与奖励信号生成，为&#x27;可验证奖励强化学习&#x27;的未来研究奠定结构基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind CaveAgent is to overcome the limitations of text-centric LLM agents, which struggle with long-horizon tasks due to fragile multi-turn dependencies and context drift. The method introduces a dual-stream architecture that shifts from treating the LLM as a text generator to using it as a runtime operator, elevating a persistent Python runtime as the central state locus with a lightweight semantic stream as orchestrator, enabling stateful runtime management of complex Python objects across turns and a runtime-integrated skill system. Main experimental results show consistent improvements on challenging benchmarks, allowing CaveAgent to handle data scales that cause context overflow in other agents, while its accessible runtime state supports programmatically verifiable feedback for automated evaluation and reward signal generation.</div>
<div class="mono" style="margin-top:8px">CaveAgent的动机是克服以文本为中心的大型语言模型代理在长时程任务中因脆弱的多轮依赖和上下文漂移而受限的问题。该方法采用双流架构，将LLM从文本生成器转变为运行时操作符，将持久的Python运行时提升为核心状态载体，并以轻量级语义流作为协调器，实现了跨轮次的复杂Python对象的状态化运行时管理和运行时集成的技能系统。主要实验结果表明，在具有挑战性的基准测试中性能持续提升，CaveAgent能够处理导致其他代理上下文溢出的数据规模，其可访问的运行时状态还为自动化评估和奖励信号生成提供了可编程验证的反馈。</div>
</details>
</div>
<div class="card">
<div class="title">Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical Reinforcement Learning</div>
<div class="meta-line">Authors: Jaebak Hwang, Sanghyeon Lee, Jeongmo Kim, Seungyul Han</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-06-26T06:35:42+00:00 · Latest: 2026-02-19T06:46:42+00:00</div>
<div class="meta-line">Comments: 10 pages for main, 26 pages for total, Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.21039v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.21039v2">PDF</a> · <a href="https://github.com/Jaebak1996/SSE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long-horizon goal-conditioned tasks pose fundamental challenges for reinforcement learning (RL), particularly when goals are distant and rewards are sparse. While hierarchical and graph-based methods offer partial solutions, their reliance on conventional hindsight relabeling often fails to correct subgoal infeasibility, leading to inefficient high-level planning. To address this, we propose Strict Subgoal Execution (SSE), a graph-based hierarchical RL framework that integrates Frontier Experience Replay (FER) to separate unreachable from admissible subgoals and streamline high-level decision making. FER delineates the reachability frontier using failure and partial-success transitions, which identifies unreliable subgoals, increases subgoal reliability, and reduces unnecessary high-level decisions. Additionally, SSE employs a decoupled exploration policy to cover underexplored regions of the goal space and a path refinement that adjusts edge costs using observed low-level failures. Experimental results across diverse long-horizon benchmarks show that SSE consistently outperforms existing goal-conditioned and hierarchical RL methods in both efficiency and success rate. Our code is available at https://github.com/Jaebak1996/SSE</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>严格子目标执行：分层强化学习中可靠的长时程规划</div>
<div class="mono" style="margin-top:8px">长时程目标条件任务对强化学习（RL）提出了根本性挑战，尤其当目标遥远且奖励稀疏时。分层和图基方法虽提供部分解决方案，但其对传统后见之明重标记的依赖常无法修正子目标不可行性，导致高层规划效率低下。为此，我们提出严格子目标执行（SSE），一种基于图的分层RL框架，集成前沿经验回放（FER）以分离不可达与可采纳子目标，并优化高层决策。FER利用失败与部分成功转移划定可达性前沿，识别不可靠子目标，提升子目标可靠性，并减少不必要的高层决策。此外，SSE采用解耦探索策略覆盖目标空间未充分探索区域，并通过路径细化利用观测到的底层失败调整边成本。在多样长时程基准测试中，SSE在效率与成功率上均持续优于现有目标条件与分层RL方法。代码发布于 https://github.com/Jaebak1996/SSE</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of long-horizon goal-conditioned reinforcement learning, where sparse rewards and distant goals hinder performance. The proposed method, Strict Subgoal Execution (SSE), is a hierarchical framework that integrates Frontier Experience Replay to filter out unreachable subgoals, a decoupled exploration policy for broader goal-space coverage, and path refinement based on low-level failures to improve planning. Experiments on various benchmarks demonstrate that SSE achieves higher success rates and greater efficiency compared to existing hierarchical and goal-conditioned RL approaches.</div>
<div class="mono" style="margin-top:8px">本文针对长视野目标条件强化学习中因奖励稀疏和目标遥远导致的性能挑战，提出了一种名为严格子目标执行的分层框架。该方法整合了边界经验回放以筛除不可达子目标，采用解耦探索策略扩大目标空间覆盖，并基于底层失败进行路径优化以改进规划。在多个基准测试上的实验结果表明，该方法在成功率和效率上均优于现有的分层与目标条件强化学习方法。</div>
</details>
</div>
<div class="card">
<div class="title">Deep Reinforcement Learning for Optimal Portfolio Allocation: A Comparative Study with Mean-Variance Optimization</div>
<div class="meta-line">Authors: Srijan Sood, Kassiani Papasotiriou, Marius Vaiciulis, Tucker Balch</div>
<div class="meta-line">First: 2026-02-19T05:47:23+00:00 · Latest: 2026-02-19T05:47:23+00:00</div>
<div class="meta-line">Comments: 9 pages, 6 figures. Published at the FinPlan&#x27;23 Workshop, the 33rd International Conference on Automated Planning and Scheduling (ICAPS 2023)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17098v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17098v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Portfolio Management is the process of overseeing a group of investments, referred to as a portfolio, with the objective of achieving predetermined investment goals. Portfolio optimization is a key component that involves allocating the portfolio assets so as to maximize returns while minimizing risk taken. It is typically carried out by financial professionals who use a combination of quantitative techniques and investment expertise to make decisions about the portfolio allocation.
  Recent applications of Deep Reinforcement Learning (DRL) have shown promising results when used to optimize portfolio allocation by training model-free agents on historical market data. Many of these methods compare their results against basic benchmarks or other state-of-the-art DRL agents but often fail to compare their performance against traditional methods used by financial professionals in practical settings. One of the most commonly used methods for this task is Mean-Variance Portfolio Optimization (MVO), which uses historical time series information to estimate expected asset returns and covariances, which are then used to optimize for an investment objective.
  Our work is a thorough comparison between model-free DRL and MVO for optimal portfolio allocation. We detail the specifics of how to make DRL for portfolio optimization work in practice, also noting the adjustments needed for MVO. Backtest results demonstrate strong performance of the DRL agent across many metrics, including Sharpe ratio, maximum drawdowns, and absolute returns.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度强化学习在最优投资组合配置中的应用：与均值-方差优化方法的比较研究</div>
<div class="mono" style="margin-top:8px">投资组合管理是监督一组投资（称为投资组合）以实现预定投资目标的过程。投资组合优化是其关键组成部分，涉及配置组合资产以最大化收益并最小化风险。通常由金融专业人士运用量化技术与投资专业知识相结合的方式进行决策。
深度强化学习（DRL）的最新应用通过基于历史市场数据训练无模型智能体来优化投资组合配置，已显示出良好效果。许多方法将其结果与基础基准或其他先进DRL智能体比较，但往往未与金融实务中使用的传统方法进行对比。均值-方差投资组合优化（MVO）是该领域最常用的方法之一，它利用历史时间序列信息估计资产预期收益与协方差，进而优化投资目标。
本研究对无模型DRL与MVO在最优投资组合配置方面进行了全面比较。详细阐述了DRL应用于投资组合优化的实践方法，同时说明了MVO所需的调整。回测结果表明，DRL智能体在夏普比率、最大回撤和绝对收益等多个指标上均表现优异。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to compare modern deep reinforcement learning (DRL) approaches for portfolio optimization against the traditional and widely-used mean-variance optimization (MVO) method, as prior DRL studies often lack such practical benchmarking. The method involves a detailed practical implementation of a model-free DRL agent trained on historical market data, alongside necessary adjustments for the MVO baseline. The main experimental results from backtesting show that the DRL agent achieves strong performance, outperforming MVO across key financial metrics including Sharpe ratio, maximum drawdown, and absolute returns.</div>
<div class="mono" style="margin-top:8px">本文的研究动机源于需要将现代深度强化学习（DRL）投资组合优化方法与传统的、广泛使用的均值-方差优化（MVO）方法进行对比，因为先前的DRL研究常常缺乏此类实际基准测试。研究方法包括基于历史市场数据训练一个免模型DRL智能体的详细实践实现，以及对MVO基线方法进行必要的调整。主要的回溯测试实验结果表明，该DRL智能体表现强劲，在夏普比率、最大回撤和绝对回报等多个关键金融指标上均优于MVO方法。</div>
</details>
</div>
<div class="card">
<div class="title">Puzzle it Out: Local-to-Global World Model for Offline Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Sijia li, Xinran Li, Shibo Chen, Jun Zhang</div>
<div class="meta-line">First: 2026-01-12T12:17:11+00:00 · Latest: 2026-02-19T05:27:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07463v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.07463v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline multi-agent reinforcement learning (MARL) aims to solve cooperative decision-making problems in multi-agent systems using pre-collected datasets. Existing offline MARL methods primarily constrain training within the dataset distribution, resulting in overly conservative policies that struggle to generalize beyond the support of the data. While model-based approaches offer a promising solution by expanding the original dataset with synthetic data generated from a learned world model, the high dimensionality, non-stationarity, and complexity of multi-agent systems make it challenging to accurately estimate the transitions and reward functions in offline MARL. Given the difficulty of directly modeling joint dynamics, we propose a local-to-global (LOGO) world model, a novel framework that leverages local predictions-which are easier to estimate-to infer global state dynamics, thus improving prediction accuracy while implicitly capturing agent-wise dependencies. Using the trained world model, we generate synthetic data to augment the original dataset, expanding the effective state-action space. To ensure reliable policy learning, we further introduce an uncertainty-aware sampling mechanism that adaptively weights synthetic data by prediction uncertainty, reducing approximation error propagation to policies. In contrast to conventional ensemble-based methods, our approach requires only an additional encoder for uncertainty estimation, significantly reducing computational overhead while maintaining accuracy. Extensive experiments across 8 scenarios against 8 baselines demonstrate that our method surpasses state-of-the-art baselines on standard offline MARL benchmarks, establishing a new model-based baseline for generalizable offline multi-agent learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>拼图求解：面向离线多智能体强化学习的局部到全局世界模型</div>
<div class="mono" style="margin-top:8px">离线多智能体强化学习（MARL）旨在利用预收集数据集解决多智能体系统中的协作决策问题。现有离线MARL方法主要将训练约束在数据集分布内，导致策略过于保守，难以泛化至数据支持范围之外。基于模型的方法通过学习的世界模型生成合成数据以扩展原始数据集，为问题提供了可行方案，但多智能体系统的高维性、非平稳性和复杂性使得准确估计离线MARL中的状态转移和奖励函数极具挑战。鉴于直接建模联合动力学的困难，我们提出局部到全局（LOGO）世界模型，该创新框架利用更易估计的局部预测来推断全局状态动力学，在隐式捕获智能体间依赖关系的同时提升预测精度。借助训练好的世界模型，我们生成合成数据以增强原始数据集，扩展有效状态-动作空间。为确保策略学习的可靠性，我们进一步引入不确定性感知采样机制，通过预测不确定性自适应加权合成数据，减少近似误差向策略的传播。与传统的基于集成的方法相比，本方法仅需额外编码器进行不确定性估计，在保持精度的同时显著降低计算开销。在8种场景下与8个基线方法的广泛实验表明，本方法在标准离线MARL基准测试中超越了现有最优基线，为可泛化的离线多智能体学习建立了新的基于模型基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of overly conservative policies in offline multi-agent reinforcement learning (MARL) by proposing a model-based framework to generate synthetic data for dataset augmentation. The method introduces a local-to-global (LOGO) world model that first predicts easier local dynamics to infer global state transitions, improving accuracy while capturing agent dependencies, and incorporates an uncertainty-aware sampling mechanism to weight synthetic data by prediction uncertainty, reducing error propagation. Experimental results across eight scenarios show that this approach outperforms state-of-the-art baselines on standard offline MARL benchmarks, establishing a new model-based baseline for generalizable multi-agent learning.</div>
<div class="mono" style="margin-top:8px">本文针对离线多智能体强化学习中策略过于保守的问题，提出了一种基于模型的框架，通过生成合成数据来扩增数据集。该方法采用局部到全局的世界模型，先预测较易建模的局部动态以推断全局状态转移，从而提高准确性并捕捉智能体间依赖关系，同时引入不确定性感知的采样机制，根据预测不确定性加权合成数据以减少误差传播。在八个场景上的实验结果表明，该方法在标准离线多智能体强化学习基准测试中超越了现有先进基线，为可泛化的多智能体学习建立了新的基于模型的基准。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamic Decision-Making under Model Misspecification: A Stochastic Stability Approach</div>
<div class="meta-line">Authors: Xinyu Dai, Daniel Chen, Yian Qian</div>
<div class="meta-line">First: 2026-02-19T05:14:09+00:00 · Latest: 2026-02-19T05:14:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17086v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17086v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dynamic decision-making under model uncertainty is central to many economic environments, yet existing bandit and reinforcement learning algorithms rely on the assumption of correct model specification. This paper studies the behavior and performance of one of the most commonly used Bayesian reinforcement learning algorithms, Thompson Sampling (TS), when the model class is misspecified. We first provide a complete dynamic classification of posterior evolution in a misspecified two-armed Gaussian bandit, identifying distinct regimes: correct model concentration, incorrect model concentration, and persistent belief mixing, characterized by the direction of statistical evidence and the model-action mapping. These regimes yield sharp predictions for limiting beliefs, action frequencies, and asymptotic regret. We then extend the analysis to a general finite model class and develop a unified stochastic stability framework that represents posterior evolution as a Markov process on the belief simplex. This approach characterizes two sufficient conditions to classify the ergodic and transient behaviors and provides inductive dimensional reductions of the posterior dynamics. Our results offer the first qualitative and geometric classification of TS under misspecification, bridging Bayesian learning with evolutionary dynamics, and also build the foundations of robust decision-making in structured bandits.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>模型误设下的动态决策：一种随机稳定性方法</div>
<div class="mono" style="margin-top:8px">模型不确定性下的动态决策是许多经济环境的核心问题，但现有的多臂赌博机与强化学习算法均依赖于模型正确设定的假设。本文研究了最常用的贝叶斯强化学习算法之一——汤普森采样在模型类别误设时的行为与性能表现。我们首先对误设双臂高斯赌博机中的后验演化进行了完整的动态分类，识别出由统计证据方向与模型-行动映射关系所界定的三种不同机制：正确模型集中、错误模型集中以及持续信念混合。这些机制对极限信念、行动频率与渐近遗憾给出了精确预测。随后我们将分析扩展至一般有限模型类别，并建立了一个统一的随机稳定性框架，将后验演化表示为信念单纯形上的马尔可夫过程。该方法通过两个充分条件对遍历性与暂态行为进行分类，并提供了后验动力学的归纳降维分析。我们的研究首次对误设条件下的汤普森采样进行了定性与几何分类，搭建了贝叶斯学习与演化动力学之间的桥梁，同时为结构化赌博机中的鲁棒决策奠定了理论基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the performance of Thompson Sampling, a widely used Bayesian reinforcement learning algorithm, when the underlying model is misspecified, a common but often overlooked scenario in economic decision-making. The authors first analyze a two-armed Gaussian bandit, identifying three distinct dynamic regimes of posterior belief evolution—correct concentration, incorrect concentration, and persistent mixing—which precisely predict asymptotic behavior and regret. They then generalize this analysis to finite model classes using a novel stochastic stability framework, modeling posterior dynamics as a Markov process to characterize ergodic and transient behaviors through sufficient conditions and dimensional reductions. The main experimental results provide the first geometric classification of Thompson Sampling under misspecification, linking Bayesian learning with evolutionary dynamics and establishing foundations for robust decision-making in structured bandits.</div>
<div class="mono" style="margin-top:8px">本文研究了在模型设定错误的情况下，广泛使用的贝叶斯强化学习算法汤普森采样的性能，这是经济决策中常见但常被忽视的场景。作者首先分析了一个双臂高斯赌博机，识别了后验信念演化的三种动态机制——正确集中、错误集中和持续混合——这些机制精确预测了渐近行为和遗憾。随后，他们使用一种新颖的随机稳定性框架将分析推广到有限模型类，将后验动态建模为马尔可夫过程，通过充分条件和维度约简来刻画遍历性和瞬态行为。主要实验结果首次提供了汤普森采样在模型设定错误下的几何分类，将贝叶斯学习与演化动力学联系起来，并为结构化赌博机中的稳健决策奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">EnterpriseBench Corecraft: Training Generalizable Agents on High-Fidelity RL Environments</div>
<div class="meta-line">Authors: Sushant Mehta, Logan Ritchie, Suhaas Garre, Nick Heiner, Edwin Chen</div>
<div class="meta-line">First: 2026-02-18T04:35:46+00:00 · Latest: 2026-02-19T05:10:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16179v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.16179v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We show that training AI agents on high-fidelity reinforcement learning environments produces capabilities that generalize beyond the training distribution. We introduce CoreCraft, the first environment in EnterpriseBench, Surge AI&#x27;s suite of agentic RL environments. CoreCraft is a fully operational enterprise simulation of a customer support organization, comprising over 2,500 entities across 14 entity types with 23 unique tools, designed to measure whether AI agents can perform the multi-step, domain-specific work that real jobs demand. Frontier models such as GPT-5.2 and Claude Opus 4.6 solve fewer than 30% of tasks when all expert-authored rubric criteria must be satisfied. Using this environment, we train GLM 4.6 with Group Relative Policy Optimization (GRPO) and adaptive clipping. After a single epoch of training, the model improves from 25.37% to 36.76% task pass rate on held-out evaluation tasks. More importantly, these gains transfer to out-of-distribution benchmarks: +4.5% on BFCL Parallel, +7.4% on Tau2-Bench Retail, and +6.8% on Tool Decathlon (Pass@1). We believe three environment properties are consistent with the observed transfer: task-centric world building that optimizes for diverse, challenging tasks; expert-authored rubrics enabling reliable reward computation; and enterprise workflows that reflect realistic professional patterns. Our results suggest that environment quality, diversity, and realism are key factors enabling generalizable agent capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EnterpriseBench Corecraft：在高保真强化学习环境中训练可泛化智能体</div>
<div class="mono" style="margin-top:8px">研究表明，在高保真强化学习环境中训练AI智能体可产生超越训练分布的泛化能力。我们推出CoreCraft——Surge AI智能体强化学习环境套件EnterpriseBench的首个环境。CoreCraft是一个完全可运行的客户支持组织企业仿真系统，包含14种实体类型超过2500个实体及23种独特工具，旨在评估AI智能体能否执行现实岗位所需的多步骤、领域特定工作。当需满足所有专家制定的评估标准时，GPT-5.2和Claude Opus 4.6等前沿模型的任务通过率不足30%。在此环境中，我们采用组相对策略优化（GRPO）与自适应剪裁技术训练GLM 4.6模型。经单轮训练后，模型在保留评估任务上的通过率从25.37%提升至36.76%。更重要的是，这些增益可迁移至分布外基准测试：BFCL Parallel提升4.5%、Tau2-Bench Retail提升7.4%、Tool Decathlon（Pass@1）提升6.8%。我们认为三个环境特性与观察到的迁移效应相符：以任务为中心的世界构建优化了多样性与挑战性；专家制定的评估准则实现了可靠奖励计算；企业工作流反映了真实专业模式。结果表明，环境质量、多样性和真实性是形成可泛化智能体能力的关键因素。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study is motivated by the need to develop AI agents that can generalize beyond their training distribution, particularly for complex enterprise tasks. The method involves training agents on CoreCraft, a high-fidelity reinforcement learning environment that simulates a customer support organization with over 2,500 entities and 23 tools, using GLM 4.6 with Group Relative Policy Optimization and adaptive clipping. Experimental results show that after one training epoch, task pass rates improved from 25.37% to 36.76% on held-out tasks, with significant transfer gains on out-of-distribution benchmarks, such as +4.5% on BFCL Parallel and +6.8% on Tool Decathlon, indicating that environment quality and realism are crucial for generalizable agent capabilities.</div>
<div class="mono" style="margin-top:8px">该研究的动机是开发能够超越训练分布进行泛化的AI智能体，特别是针对复杂的企业任务。方法包括在CoreCraft上训练智能体，这是一个高保真强化学习环境，模拟了包含超过2,500个实体和23种工具的客户支持组织，使用GLM 4.6模型结合组相对策略优化和自适应剪裁技术。实验结果表明，经过一个训练周期后，智能体在保留任务上的通过率从25.37%提升至36.76%，并在分布外基准测试中展现出显著的迁移增益，例如在BFCL Parallel上提升4.5%，在Tool Decathlon上提升6.8%，这表明环境的质量和真实性是培养智能体泛化能力的关键因素。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Improving Skill Learning for Robust Skill-based Meta-Reinforcement Learning</div>
<div class="meta-line">Authors: Sanghyeon Lee, Sangjun Bae, Yisak Park, Seungyul Han</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-02-06T03:28:45+00:00 · Latest: 2026-02-19T05:01:11+00:00</div>
<div class="meta-line">Comments: 10 pages main, 27 pages appendix with reference. Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.03752v4">Abs</a> · <a href="https://arxiv.org/pdf/2502.03752v4">PDF</a> · <a href="https://github.com/epsilog/SISL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Meta-reinforcement learning (Meta-RL) facilitates rapid adaptation to unseen tasks but faces challenges in long-horizon environments. Skill-based approaches tackle this by decomposing state-action sequences into reusable skills and employing hierarchical decision-making. However, these methods are highly susceptible to noisy offline demonstrations, leading to unstable skill learning and degraded performance. To address this, we propose Self-Improving Skill Learning (SISL), which performs self-guided skill refinement using decoupled high-level and skill improvement policies, while applying skill prioritization via maximum return relabeling to focus updates on task-relevant trajectories, resulting in robust and stable adaptation even under noisy and suboptimal data. By mitigating the effect of noise, SISL achieves reliable skill learning and consistently outperforms other skill-based meta-RL methods on diverse long-horizon tasks. Our code is available at https://github.com/epsilog/SISL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向鲁棒技能元强化学习的自改进技能学习方法</div>
<div class="mono" style="margin-top:8px">元强化学习（Meta-RL）虽能快速适应未知任务，但在长时程环境中面临挑战。基于技能的方法通过将状态-动作序列分解为可复用技能并采用分层决策来解决此问题，但这些方法极易受噪声离线演示影响，导致技能学习不稳定与性能下降。为此，我们提出自改进技能学习（SISL），该方法通过解耦的高层策略与技能改进策略进行自引导式技能优化，同时利用基于最大回报重标注的技能优先级机制，将更新聚焦于任务相关轨迹，从而即使在噪声和次优数据下也能实现鲁棒稳定的适应。通过抑制噪声影响，SISL实现了可靠的技能学习，并在多种长时程任务中持续优于其他基于技能的元强化学习方法。代码发布于 https://github.com/epsilog/SISL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of skill-based meta-reinforcement learning (Meta-RL) being highly sensitive to noisy offline demonstrations, which leads to unstable skill learning and poor adaptation in long-horizon tasks. The proposed method, Self-Improving Skill Learning (SISL), introduces a self-guided skill refinement mechanism using decoupled high-level and skill improvement policies, along with skill prioritization via maximum return relabeling to focus updates on the most task-relevant trajectories. Experimental results demonstrate that SISL effectively mitigates the impact of noise, enabling robust skill learning and consistently outperforming other skill-based Meta-RL methods across various long-horizon environments.</div>
<div class="mono" style="margin-top:8px">本文针对基于技能的元强化学习对噪声离线演示高度敏感、导致技能学习不稳定和在长时程任务中适应性能差的问题展开研究。提出的方法称为自改进技能学习（SISL），它通过解耦的高层策略和技能改进策略进行自引导的技能优化，并利用基于最大回报重标记的技能优先级机制，将更新集中在与任务最相关的轨迹上。实验结果表明，SISL有效减轻了噪声的影响，实现了鲁棒的技能学习，并在多种长时程任务上持续优于其他基于技能的元强化学习方法。</div>
</details>
</div>
<div class="card">
<div class="title">Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Xuefeng Wang, Lei Zhang, Henglin Pu, Ahmed H. Qureshi, Husheng Li</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-09-11T04:12:50+00:00 · Latest: 2026-02-19T04:56:13+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026. 21 pages, 13 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.09135v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.09135v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing reinforcement learning (RL) methods struggle with complex dynamical systems that demand interactions at high frequencies or irregular time intervals. Continuous-time RL (CTRL) has emerged as a promising alternative by replacing discrete-time Bellman recursion with differential value functions defined as viscosity solutions of the Hamilton--Jacobi--Bellman (HJB) equation. While CTRL has shown promise, its applications have been largely limited to the single-agent domain. This limitation stems from two key challenges: (i) conventional solution methods for HJB equations suffer from the curse of dimensionality (CoD), making them intractable in high-dimensional systems; and (ii) even with HJB-based learning approaches, accurately approximating centralized value functions in multi-agent settings remains difficult, which in turn destabilizes policy training. In this paper, we propose a CT-MARL framework that uses physics-informed neural networks (PINNs) to approximate HJB-based value functions at scale. To ensure the value is consistent with its differential structure, we align value learning with value-gradient learning by introducing a Value Gradient Iteration (VGI) module that iteratively refines value gradients along trajectories. This improves gradient fidelity, in turn yielding more accurate values and stronger policy learning. We evaluate our method using continuous-time variants of standard benchmarks, including multi-agent particle environment (MPE) and multi-agent MuJoCo. Our results demonstrate that our approach consistently outperforms existing continuous-time RL baselines and scales to complex multi-agent dynamics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多智能体强化学习的连续时间价值迭代方法</div>
<div class="mono" style="margin-top:8px">现有强化学习方法在处理需要高频或不规则时间间隔交互的复杂动态系统时面临困难。连续时间强化学习通过用汉密尔顿-雅可比-贝尔曼方程粘性解定义的微分价值函数替代离散时间贝尔曼递归，成为有前景的替代方案。尽管连续时间强化学习展现出潜力，但其应用主要局限于单智能体领域。这源于两个关键挑战：(i)传统HJB方程求解方法受维度灾难影响，在高维系统中难以求解；(ii)即使采用基于HJB的学习方法，在多智能体设置中精确逼近集中式价值函数仍很困难，进而导致策略训练不稳定。本文提出CT-MARL框架，采用物理信息神经网络大规模逼近基于HJB的价值函数。为确保价值函数与其微分结构的一致性，我们通过引入价值梯度迭代模块，将价值学习与价值梯度学习对齐，该模块沿轨迹迭代优化价值梯度。这提升了梯度保真度，进而产生更精确的价值估计和更强的策略学习能力。我们在连续时间版本的标准基准测试中评估方法，包括多智能体粒子环境和多智能体MuJoCo。结果表明，我们的方法持续优于现有连续时间强化学习基线，并能扩展到复杂的多智能体动态系统。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of existing reinforcement learning methods in handling complex dynamical systems requiring high-frequency or irregular interactions, particularly in multi-agent settings where continuous-time RL has been underexplored due to the curse of dimensionality and difficulties in approximating centralized value functions. The proposed CT-MARL framework employs physics-informed neural networks to approximate Hamilton-Jacobi-Bellman-based value functions at scale and introduces a Value Gradient Iteration module to align value learning with value-gradient learning, thereby refining gradients along trajectories for improved accuracy and policy stability. Experimental results on continuous-time variants of multi-agent particle environment and multi-agent MuJoCo benchmarks show that the method consistently outperforms existing continuous-time RL baselines and scales effectively to complex multi-agent dynamics.</div>
<div class="mono" style="margin-top:8px">本文针对现有强化学习方法在处理需要高频或不规则交互的复杂动态系统时的局限性，特别是在多智能体场景中，由于维度灾难和集中式价值函数近似困难，连续时间强化学习的研究不足。提出的CT-MARL框架采用物理信息神经网络来大规模近似基于Hamilton-Jacobi-Bellman方程的价值函数，并引入价值梯度迭代模块，将价值学习与价值梯度学习对齐，从而沿轨迹细化梯度以提高准确性和策略稳定性。在多智能体粒子环境和多智能体MuJoCo基准的连续时间变体上的实验结果表明，该方法持续优于现有连续时间强化学习基线，并能有效扩展到复杂的多智能体动态系统中。</div>
</details>
</div>
<div class="card">
<div class="title">Spatio-temporal dual-stage hypergraph MARL for human-centric multimodal corridor traffic signal control</div>
<div class="meta-line">Authors: Xiaocai Zhang, Neema Nassir, Milad Haghani</div>
<div class="meta-line">First: 2026-02-19T04:18:50+00:00 · Latest: 2026-02-19T04:18:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17068v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17068v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human-centric traffic signal control in corridor networks must increasingly account for multimodal travelers, particularly high-occupancy public transportation, rather than focusing solely on vehicle-centric performance. This paper proposes STDSH-MARL (Spatio-Temporal Dual-Stage Hypergraph based Multi-Agent Reinforcement Learning), a scalable multi-agent deep reinforcement learning framework that follows a centralized training and decentralized execution paradigm. The proposed method captures spatio-temporal dependencies through a novel dual-stage hypergraph attention mechanism that models interactions across both spatial and temporal hyperedges. In addition, a hybrid discrete action space is introduced to jointly determine the next signal phase configuration and its corresponding green duration, enabling more adaptive signal timing decisions. Experiments conducted on a corridor network under five traffic scenarios demonstrate that STDSH-MARL consistently improves multimodal performance and provides clear benefits for public transportation priority. Compared with state-of-the-art baseline methods, the proposed approach achieves superior overall performance. Further ablation studies confirm the contribution of each component of STDSH-MARL, with temporal hyperedges identified as the most influential factor driving the observed performance gains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向人本化多模式走廊交通信号控制的时空双阶段超图多智能体强化学习方法</div>
<div class="mono" style="margin-top:8px">走廊网络中的人本化交通信号控制需日益关注多模式出行者（尤其是高载客量的公共交通），而非仅聚焦车辆导向的通行效率。本文提出STDSH-MARL（基于时空双阶段超图的多智能体强化学习框架），该可扩展的深度强化学习框架遵循集中训练与分散执行的范式。该方法通过创新的双阶段超图注意力机制捕获时空依赖关系，该机制可同时建模空间超边与时间超边的交互作用。此外，引入混合离散动作空间以联合确定下一信号相位配置及其对应绿灯时长，从而实现更具适应性的信号配时决策。在五种交通场景下对走廊网络的实验表明，STDSH-MARL能持续提升多模式交通性能，并为公共交通优先提供显著效益。与前沿基线方法相比，本方法实现了更优的综合性能。进一步的消融研究证实了STDSH-MARL各组成部分的贡献，其中时间超边被证实是驱动性能提升的最关键因素。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to prioritize multimodal travelers, especially public transportation, over vehicle-centric metrics in corridor traffic signal control, this paper introduces STDSH-MARL, a multi-agent reinforcement learning framework. The method employs a dual-stage hypergraph attention mechanism to model spatio-temporal dependencies and a hybrid action space to jointly select signal phases and durations. Experimental results across five traffic scenarios show that STDSH-MARL enhances multimodal performance, particularly benefiting public transport priority, and outperforms state-of-the-art baselines, with ablation studies confirming the key role of temporal hyperedges.</div>
<div class="mono" style="margin-top:8px">本文的动机是在走廊交通信号控制中优先考虑多模式出行者（特别是公共交通），而非仅关注车辆性能。方法上提出了STDSH-MARL框架，采用双阶段超图注意力机制捕捉时空依赖，并引入混合动作空间联合决定信号相位和时长。实验结果表明，在五种交通场景下，该方法持续提升了多模式性能，明显有利于公共交通优先，且优于现有基线方法，消融研究证实了时序超边是性能提升的关键因素。</div>
</details>
</div>
<div class="card">
<div class="title">Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Yonghyeon Jo, Sunwoo Lee, Seungyul Han</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-19T04:07:55+00:00 · Latest: 2026-02-19T04:07:55+00:00</div>
<div class="meta-line">Comments: 10 technical page followed by references and appendix. Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17062v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17062v1">PDF</a> · <a href="https://github.com/hyeon1996/S2Q">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance. Our code is available at https://github.com/hyeon1996/S2Q.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多智能体强化学习中保留次优动作以追踪动态最优解</div>
<div class="mono" style="margin-top:8px">值分解是协作式多智能体强化学习的核心方法。然而，现有方法仍依赖单一最优动作，当训练中基础值函数发生偏移时难以适应，常收敛至次优策略。为突破此局限，我们提出连续次值Q学习，通过习得多个次值函数来保留替代性高价值动作。将这些次值函数整合至基于Softmax的行为策略后，该方法能促进持续探索，使总Q值快速适应动态最优解。在挑战性多智能体强化学习基准测试中的实验表明，该方法持续优于各类多智能体强化学习算法，展现出更强的适应性与综合性能。代码发布于https://github.com/hyeon1996/S2Q。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation in cooperative multi-agent reinforcement learning where existing value decomposition methods rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often leading to suboptimal convergence. The proposed method, Successive Sub-value Q-learning (S2Q), learns multiple sub-value functions to retain alternative high-value actions and incorporates them into a Softmax-based behavior policy to encourage persistent exploration and enable rapid adjustment to changing optima. Experimental results on challenging MARL benchmarks confirm that S2Q consistently outperforms various baseline algorithms, demonstrating improved adaptability and overall performance.</div>
<div class="mono" style="margin-top:8px">本文针对协作多智能体强化学习中现有价值分解方法依赖单一最优动作、在训练期间价值函数发生偏移时难以适应并常收敛于次优策略的局限性展开研究。提出的方法称为连续次价值Q学习（S2Q），它通过学习多个次价值函数来保留替代的高价值动作，并将其整合到一个基于Softmax的行为策略中，以鼓励持续探索并使总Q值能够快速适应变化的最优解。在具有挑战性的多智能体强化学习基准测试上的实验结果表明，S2Q consistently outperforms various baseline algorithms，证明了其改进的适应性和整体性能。</div>
</details>
</div>
<div class="card">
<div class="title">Phase-Aware Mixture of Experts for Agentic Reinforcement Learning</div>
<div class="meta-line">Authors: Shengtian Yang, Yu Li, Shuo He, Yewen Li, Qingpeng Cai, Peng Jiang, Lei Feng</div>
<div class="meta-line">First: 2026-02-19T03:18:30+00:00 · Latest: 2026-02-19T03:18:30+00:00</div>
<div class="meta-line">Comments: 16 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17038v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17038v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has equipped LLM agents with a strong ability to solve complex tasks. However, existing RL methods normally use a \emph{single} policy network, causing \emph{simplicity bias} where simple tasks occupy most parameters and dominate gradient updates, leaving insufficient capacity for complex tasks. A plausible remedy could be employing the Mixture-of-Experts (MoE) architecture in the policy network, as MoE allows different parameters (experts) to specialize in different tasks, preventing simple tasks from dominating all parameters. However, a key limitation of traditional MoE is its token-level routing, where the router assigns each token to specialized experts, which fragments phase-consistent patterns into scattered expert assignments and thus undermines expert specialization. In this paper, we propose \textbf{Phase-Aware Mixture of Experts (PA-MoE)}. It first features a lightweight \emph{phase router} that learns latent phase boundaries directly from the RL objective without pre-defining phase categories. Then, the phase router allocates temporally consistent assignments to the same expert, allowing experts to preserve phase-specific expertise. Experimental results demonstrate the effectiveness of our proposed PA-MoE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向智能体强化学习的相位感知专家混合模型</div>
<div class="mono" style="margin-top:8px">强化学习（RL）为LLM智能体赋予了解决复杂任务的强大能力。然而，现有RL方法通常采用单一策略网络，导致简单任务占据大部分参数并主导梯度更新的简单性偏差，使复杂任务缺乏足够建模容量。在策略网络中采用专家混合（MoE）架构是一种可行的解决方案，因为MoE允许不同参数（专家）专精于不同任务，防止简单任务垄断所有参数。但传统MoE的关键局限在于其基于令牌的路由机制——路由器将每个令牌分配给特定专家，这会将相位一致的模式碎片化为分散的专家分配，从而削弱专家专精性。本文提出相位感知专家混合模型（PA-MoE），其核心是轻量级相位路由器，该组件直接从RL目标学习潜在相位边界而无需预定义相位类别。相位路由器为时序一致的任务分配相同专家，使专家能保持相位特定的专长。实验结果验证了PA-MoE的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the simplicity bias in reinforcement learning (RL) for LLM agents, where a single policy network leads to simple tasks dominating parameter updates and hindering performance on complex tasks. To mitigate this, the authors propose Phase-Aware Mixture of Experts (PA-MoE), which replaces token-level routing with a lightweight phase router that learns latent phase boundaries from the RL objective, enabling temporally consistent expert assignments to preserve phase-specific expertise. Experimental results validate the effectiveness of PA-MoE in improving agent performance by allowing experts to specialize in different task phases without fragmentation.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中LLM智能体因使用单一策略网络而导致的简单任务主导参数更新的“简单性偏差”问题，提出了一种相位感知专家混合模型（PA-MoE）。该方法通过轻量级相位路由器从强化学习目标中学习潜在相位边界，替代传统的令牌级路由，从而为同一专家分配时间上一致的输入，以保持相位特定专业知识。实验结果表明，PA-MoE能有效提升智能体性能，避免专家分配碎片化，使专家专注于不同任务阶段。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning to Discover a North-East Monsoon Index for Rainfall Prediction in Thailand</div>
<div class="meta-line">Authors: Kiattikun Chobtham</div>
<div class="meta-line">First: 2026-01-15T08:40:01+00:00 · Latest: 2026-02-19T03:18:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10181v5">Abs</a> · <a href="https://arxiv.org/pdf/2601.10181v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurately predicting long-term rainfall is challenging. Global climate indices, such as the El Niño-Southern Oscillation, are standard input features for machine learning. However, a significant gap persists regarding local-scale indices capable of improving predictive accuracy in specific regions of Thailand. This paper introduces a novel North-East monsoon climate index calculated from sea surface temperature to reflect the climatology of the boreal winter monsoon. To optimise the calculated areas used for this index, a Deep Q-Network reinforcement learning agent explores and selects the most effective rectangles based on their correlation with seasonal rainfall. Rainfall stations were classified into 12 distinct clusters to distinguish rainfall patterns between southern and upper Thailand. Experimental results show that incorporating the optimised index into Long Short-Term Memory models significantly improves long-term monthly rainfall prediction skill in most cluster areas. This approach effectively reduces the Root Mean Square Error for 12-month-ahead forecasts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用强化学习发现东北季风指数以提升泰国降雨预测精度</div>
<div class="mono" style="margin-top:8px">长期降雨预测具有挑战性。全球气候指数（如厄尔尼诺-南方涛动）是机器学习常用的输入特征，但针对泰国特定区域、能提升预测精度的局地尺度指数仍存在明显空白。本文提出一种基于海表温度计算的新型东北季风气候指数，以反映北半球冬季季风的气候特征。为优化该指数的计算区域，采用深度Q网络强化学习智能体探索并筛选与季节性降雨相关性最高的矩形海域。研究将泰国降雨站点划分为12个独立聚类，以区分南部与北部地区的降雨模式。实验结果表明，将优化后的指数融入长短期记忆模型，能显著提升多数聚类区域的长期月降雨预测能力，有效降低12个月前瞻预测的均方根误差。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of accurate long-term rainfall prediction and the lack of effective local-scale climate indices for Thailand, this paper proposes a novel method to discover a North-East monsoon index. The method uses a Deep Q-Network reinforcement learning agent to optimize the sea surface temperature areas used for calculating the index by selecting rectangles that maximize correlation with seasonal rainfall, which is then incorporated into Long Short-Term Memory models for prediction across 12 regional clusters. The main experimental results demonstrate that this optimized index significantly improves the skill of 12-month-ahead monthly rainfall forecasts in most clusters, effectively reducing the Root Mean Square Error compared to models without it.</div>
<div class="mono" style="margin-top:8px">针对长期降雨预测的准确性挑战以及泰国地区缺乏有效局地气候指数的问题，本文提出了一种发现东北季风指数的新方法。该方法利用深度Q网络强化学习智能体，通过选择与季节性降雨相关性最高的海表温度区域矩形来优化指数计算，并将该指数输入长短期记忆模型，在12个区域集群中进行预测。主要实验结果表明，这一优化指数显著提高了大多数集群中提前12个月的月降雨预测能力，有效降低了均方根误差。</div>
</details>
</div>
<div class="card">
<div class="title">Action-Graph Policies: Learning Action Co-dependencies in Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Nikunj Gupta, James Zachary Hare, Jesse Milzman, Rajgopal Kannan, Viktor Prasanna</div>
<div class="meta-line">First: 2026-02-19T02:13:29+00:00 · Latest: 2026-02-19T02:13:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17009v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17009v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Coordinating actions is the most fundamental form of cooperation in multi-agent reinforcement learning (MARL). Successful decentralized decision-making often depends not only on good individual actions, but on selecting compatible actions across agents to synchronize behavior, avoid conflicts, and satisfy global constraints. In this paper, we propose Action Graph Policies (AGP), that model dependencies among agents&#x27; available action choices. It constructs, what we call, \textit{coordination contexts}, that enable agents to condition their decisions on global action dependencies. Theoretically, we show that AGPs induce a strictly more expressive joint policy compared to fully independent policies and can realize coordinated joint actions that are provably more optimal than greedy execution even from centralized value-decomposition methods. Empirically, we show that AGP achieves 80-95\% success on canonical coordination tasks with partial observability and anti-coordination penalties, where other MARL methods reach only 10-25\%. We further demonstrate that AGP consistently outperforms these baselines in diverse multi-agent environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>动作图策略：多智能体强化学习中动作协同依赖关系的学习</div>
<div class="mono" style="margin-top:8px">在多智能体强化学习（MARL）中，动作协调是最基础的合作形式。成功的分散式决策不仅依赖于良好的个体动作，更需要跨智能体选择兼容动作以实现行为同步、避免冲突并满足全局约束。本文提出动作图策略（AGP），该模型能刻画智能体可用动作选择间的依赖关系。它构建了被称为“协调情境”的框架，使智能体能够根据全局动作依赖关系调整决策。理论上，我们证明相较于完全独立策略，AGP能生成表达能力严格更强的联合策略，并能实现经证明比集中式价值分解方法的贪婪执行更优的协调联合动作。实证研究表明，在具有部分可观测性和反协调惩罚的典型协调任务中，AGP达到80-95%的成功率，而其他MARL方法仅达到10-25%。我们进一步证明，AGP在多样化多智能体环境中持续优于这些基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need for effective coordination in multi-agent reinforcement learning (MARL), where successful decentralized decision-making depends not only on individual actions but also on selecting compatible actions across agents to synchronize behavior and avoid conflicts. The method proposed, Action Graph Policies (AGP), models dependencies among agents&#x27; available action choices by constructing coordination contexts, allowing agents to condition their decisions on global action dependencies. Theoretically, AGPs are shown to be more expressive than fully independent policies and can achieve more optimal coordinated actions than greedy execution from centralized value-decomposition methods. Experimentally, AGP achieves 80-95% success on canonical coordination tasks with partial observability and anti-coordination penalties, significantly outperforming other MARL methods that reach only 10-25%, and it consistently excels in diverse multi-agent environments.</div>
<div class="mono" style="margin-top:8px">本文的动机源于多智能体强化学习中对有效协调的需求，其中成功的分散决策不仅依赖于个体行动，还取决于跨智能体选择兼容行动以同步行为并避免冲突。所提出的方法称为行动图策略（AGP），它通过构建协调上下文来建模智能体可用行动选择之间的依赖关系，使智能体能够基于全局行动依赖来调整决策。理论上，AGP被证明比完全独立策略更具表达力，并能实现比集中式价值分解方法的贪婪执行更优的协调行动。实验结果表明，在具有部分可观测性和反协调惩罚的典型协调任务中，AGP取得了80-95%的成功率，显著优于其他仅达到10-25%的多智能体强化学习方法，并在多样化的多智能体环境中持续表现出色。</div>
</details>
</div>
<div class="card">
<div class="title">The Chicken and Egg Dilemma: Co-optimizing Data and Model Configurations for LLMs</div>
<div class="meta-line">Authors: Zhiliang Chen, Alfred Wei Lun Leong, Shao Yong Ong, Apivich Hemachandra, Gregory Kang Ruey Lau, Chuan-Sheng Foo, Zhengyuan Liu, Nancy F. Chen, Bryan Kian Hsiang Low</div>
<div class="meta-line">First: 2026-02-09T07:33:40+00:00 · Latest: 2026-02-19T01:32:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08351v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08351v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Co-optimizing data and model configurations for training LLMs presents a classic chicken-and-egg dilemma: The best training data configuration (e.g., data mixture) for a downstream task depends on the chosen model configuration (e.g., model architecture), and vice versa. However, jointly optimizing both data and model configurations is often deemed intractable, and existing methods focus on either data or model optimization without considering their interaction. We introduce JoBS, an approach that uses a scaling-law-inspired performance predictor to aid Bayesian optimization (BO) in jointly optimizing LLM training data and model configurations efficiently. JoBS allocates a portion of the optimization budget to learn an LLM performance predictor that predicts how promising a training configuration is from a small number of training steps. The remaining budget is used to perform BO entirely with the predictor, effectively amortizing the cost of running full-training runs. We study JoBS&#x27;s average regret and devise the optimal budget allocation to minimize regret. JoBS outperforms existing multi-fidelity BO baselines, as well as data and model optimization approaches across diverse LLM tasks under the same optimization budget.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>鸡与蛋的困境：大语言模型数据与模型配置的协同优化</div>
<div class="mono" style="margin-top:8px">为大语言模型训练协同优化数据与模型配置呈现出一个经典的鸡与蛋困境：下游任务的最佳训练数据配置（如数据混合比例）取决于所选模型配置（如模型架构），反之亦然。然而，联合优化数据与模型配置常被视为难以处理，现有方法多聚焦于单一维度的优化而忽略其相互作用。本文提出JoBS方法，利用基于缩放定律的性能预测器辅助贝叶斯优化，高效实现大语言模型训练数据与模型配置的联合优化。JoBS将部分优化预算用于学习大语言模型性能预测器，该预测器可通过少量训练步骤评估训练配置的潜力；剩余预算则完全通过预测器执行贝叶斯优化，从而有效分摊完整训练的高昂成本。我们分析了JoBS的平均遗憾值并设计了最小化遗憾的最优预算分配方案。在相同优化预算下，JoBS在多种大语言模型任务中均优于现有多保真度贝叶斯优化基线以及单维度优化方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the interdependent challenge of co-optimizing data and model configurations for LLMs, a chicken-and-egg problem where each optimal configuration depends on the other. The authors propose JoBS, a method that uses a scaling-law-inspired performance predictor to guide Bayesian optimization, efficiently allocating a budget to first learn the predictor from early training steps and then using it to explore configurations without costly full training runs. Experimental results show that JoBS achieves lower average regret and outperforms existing multi-fidelity Bayesian optimization baselines, as well as isolated data or model optimization methods, across various LLM tasks under a fixed budget.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型中数据和模型配置相互依赖的协同优化难题，即经典的“鸡与蛋”困境，其中任一配置的最优选择都依赖于另一者。研究者提出了JoBS方法，该方法利用基于缩放定律的性能预测器来指导贝叶斯优化，通过分配部分预算从早期训练步骤学习预测器，然后使用该预测器高效探索配置，避免了昂贵的完整训练成本。实验结果表明，在相同优化预算下，JoBS在多种大语言模型任务中实现了更低的平均遗憾，并优于现有的多保真度贝叶斯优化基线以及孤立的数据或模型优化方法。</div>
</details>
</div>
<div class="card">
<div class="title">A Unified Framework for Locality in Scalable MARL</div>
<div class="meta-line">Authors: Sourav Chakraborty, Amit Kiran Rege, Claire Monteleoni, Lijun Chen</div>
<div class="meta-line">First: 2026-02-19T00:02:02+00:00 · Latest: 2026-02-19T00:02:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16966v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16966v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scalable Multi-Agent Reinforcement Learning (MARL) is fundamentally challenged by the curse of dimensionality. A common solution is to exploit locality, which hinges on an Exponential Decay Property (EDP) of the value function. However, existing conditions that guarantee the EDP are often conservative, as they are based on worst-case, environment-only bounds (e.g., supremums over actions) and fail to capture the regularizing effect of the policy itself. In this work, we establish that locality can also be a \emph{policy-dependent} phenomenon. Our central contribution is a novel decomposition of the policy-induced interdependence matrix, $H^π$, which decouples the environment&#x27;s sensitivity to state ($E^{\mathrm{s}}$) and action ($E^{\mathrm{a}}$) from the policy&#x27;s sensitivity to state ($Π(π)$). This decomposition reveals that locality can be induced by a smooth policy (small $Π(π)$) even when the environment is strongly action-coupled, exposing a fundamental locality-optimality tradeoff. We use this framework to derive a general spectral condition $ρ(E^{\mathrm{s}}+E^{\mathrm{a}}Π(π)) &lt; 1$ for exponential decay, which is strictly tighter than prior norm-based conditions. Finally, we leverage this theory to analyze a provably-sound localized block-coordinate policy improvement framework with guarantees tied directly to this spectral radius.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可扩展多智能体强化学习中局部性的统一框架</div>
<div class="mono" style="margin-top:8px">可扩展多智能体强化学习（MARL）面临维度灾难的根本性挑战。利用局部性是常见解决方案，其关键在于价值函数具备指数衰减特性（EDP）。然而，现有保证EDP的条件通常较为保守，因为它们基于环境的最坏情况界限（如动作上的上确界），未能捕捉策略本身的正则化效应。本文论证了局部性同样可以是一种策略依赖现象。核心贡献在于提出一种新颖的策略诱导互依矩阵分解方法$H^π$，将环境对状态（$E^{\mathrm{s}}$）和动作（$E^{\mathrm{a}}$）的敏感度与策略对状态（$Π(π)$）的敏感度解耦。该分解表明：即使环境存在强动作耦合，平滑策略（较小的$Π(π)$）仍可诱导局部性，这揭示了局部性与最优性之间的根本权衡。基于此框架，我们推导出指数衰减的普适谱条件$ρ(E^{\mathrm{s}}+E^{\mathrm{a}}Π(π)) &lt; 1$，该条件严格优于先前的范数型条件。最后，我们运用该理论分析了一个可证明的局部块坐标策略改进框架，其保证直接与该谱半径相关联。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of scalability in Multi-Agent Reinforcement Learning (MARL) by refining the understanding of locality, which is often assumed via an Exponential Decay Property (EDP) of the value function. The motivation stems from the limitations of existing conservative, environment-only bounds that ignore policy effects. The method introduces a novel decomposition of the policy-induced interdependence matrix, separating environment sensitivity from policy sensitivity, to show that locality can be policy-dependent and emerges from smooth policies even in action-coupled environments. Experimental results include a tighter spectral condition for exponential decay and a provably-sound localized policy improvement framework with guarantees linked to this condition, demonstrating improved scalability over prior norm-based approaches.</div>
<div class="mono" style="margin-top:8px">本文针对可扩展多智能体强化学习（MARL）中的维度诅咒问题，通过细化对局部性的理解来提升可扩展性，其动机是现有基于环境最坏情况边界的保守条件忽略了策略本身的调节作用。方法上，提出了一种新的策略诱导相互依赖矩阵分解，将环境对状态和动作的敏感性与策略对状态的敏感性解耦，揭示了即使环境动作耦合强，平滑策略也能诱导局部性，并暴露了局部性与最优性的权衡。主要实验结果包括推导出更严格的指数衰减谱条件，并基于此理论分析了一个可证明的局部块坐标策略改进框架，其保证直接与该谱半径相关，从而提升了可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">Automating Agent Hijacking via Structural Template Injection</div>
<div class="meta-line">Authors: Xinhao Deng, Jiaqing Wu, Miao Chen, Yue Xiao, Ke Xu, Qi Li</div>
<div class="meta-line">First: 2026-02-18T23:52:14+00:00 · Latest: 2026-02-18T23:52:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16958v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16958v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agent hijacking, highlighted by OWASP as a critical threat to the Large Language Model (LLM) ecosystem, enables adversaries to manipulate execution by injecting malicious instructions into retrieved content. Most existing attacks rely on manually crafted, semantics-driven prompt manipulation, which often yields low attack success rates and limited transferability to closed-source commercial models. In this paper, we propose Phantom, an automated agent hijacking framework built upon Structured Template Injection that targets the fundamental architectural mechanisms of LLM agents. Our key insight is that agents rely on specific chat template tokens to separate system, user, assistant, and tool instructions. By injecting optimized structured templates into the retrieved context, we induce role confusion and cause the agent to misinterpret the injected content as legitimate user instructions or prior tool outputs. To enhance attack transferability against black-box agents, Phantom introduces a novel attack template search framework. We first perform multi-level template augmentation to increase structural diversity and then train a Template Autoencoder (TAE) to embed discrete templates into a continuous, searchable latent space. Subsequently, we apply Bayesian optimization to efficiently identify optimal adversarial vectors that are decoded into high-potency structured templates. Extensive experiments on Qwen, GPT, and Gemini demonstrate that our framework significantly outperforms existing baselines in both Attack Success Rate (ASR) and query efficiency. Moreover, we identified over 70 vulnerabilities in real-world commercial products that have been confirmed by vendors, underscoring the practical severity of structured template-based hijacking and providing an empirical foundation for securing next-generation agentic systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于结构化模板注入的智能体劫持自动化攻击</div>
<div class="mono" style="margin-top:8px">智能体劫持被OWASP列为大语言模型生态系统的关键威胁，攻击者可通过在检索内容中注入恶意指令操控执行流程。现有攻击多依赖人工构建的语义驱动提示词操纵，成功率低且对闭源商业模型的迁移性有限。本文提出Phantom框架，基于结构化模板注入实现自动化智能体劫持，针对LLM智能体的底层架构机制。核心洞见在于：智能体依赖特定对话模板标记来区分系统、用户、助手及工具指令。通过向检索上下文注入优化的结构化模板，可诱发角色混淆，使智能体将注入内容误判为合法用户指令或先前的工具输出。为提升对黑盒智能体的攻击迁移性，Phantom设计了新型攻击模板搜索框架：首先通过多级模板增强提升结构多样性，随后训练模板自编码器将离散模板嵌入连续可搜索的隐空间，进而运用贝叶斯优化高效解码出高效对抗性结构化模板。在Qwen、GPT和Gemini上的实验表明，该框架在攻击成功率和查询效率上均显著超越现有基线。此外，我们在实际商业产品中发现70余个已获厂商确认的漏洞，印证了结构化模板劫持的现实危害性，为下一代智能体系统的安全防护提供了实证基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of manual, semantics-based prompt attacks which suffer from low success rates and poor transferability to closed-source models, this paper introduces Phantom, an automated agent hijacking framework based on Structural Template Injection. The method exploits LLM agents&#x27; reliance on specific chat template tokens by injecting optimized structured templates into retrieved context to induce role confusion, and it enhances transferability via a novel attack template search framework that uses template augmentation, a Template Autoencoder for latent space embedding, and Bayesian optimization to find high-potency templates. Experimental results on models like Qwen, GPT, and Gemini show that Phantom significantly outperforms existing baselines in Attack Success Rate and query efficiency, and the discovery of over 70 vulnerabilities in real-world commercial products confirms the practical severity of this approach.</div>
<div class="mono" style="margin-top:8px">针对现有基于语义的手动提示攻击成功率低、对闭源模型可迁移性差的问题，本文提出了Phantom，一种基于结构化模板注入的自动化智能体劫持框架。该方法利用大语言模型智能体依赖特定聊天模板令牌的机制，通过向检索内容中注入优化的结构化模板来引发角色混淆；为提升对黑盒智能体的攻击可迁移性，框架引入了新颖的攻击模板搜索方法，结合多级模板增强、模板自编码器进行潜在空间嵌入，并利用贝叶斯优化高效搜索解码出高效对抗模板。在Qwen、GPT和Gemini等模型上的大量实验表明，该框架在攻击成功率和查询效率上显著优于现有基线，且在现实商业产品中发现了70多个已获厂商确认的漏洞，证实了结构化模板劫持的实际严重性。</div>
</details>
</div>
<div class="card">
<div class="title">LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation</div>
<div class="meta-line">Authors: Hejia Zhang, Zhongming Yu, Chia-Tung Ho, Haoxing Ren, Brucek Khailany, Jishen Zhao</div>
<div class="meta-line">First: 2026-02-18T23:36:46+00:00 · Latest: 2026-02-18T23:36:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16953v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16953v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Execution-aware LLM agents offer a promising paradigm for learning from tool feedback, but such feedback is often expensive and slow to obtain, making online reinforcement learning (RL) impractical. High-coverage hardware verification exemplifies this challenge due to its reliance on industrial simulators and non-differentiable execution signals. We propose LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions guided by deterministic evaluators. Building on this formulation, we introduce execution-validated data curation, policy-aware agentic data synthesis, and worst-state-prioritized sampling to enable scalable learning under execution constraints. We further curate a reality-aligned benchmark adapted from an existing verification suite through a revised evaluation protocol. Using the proposed pipeline, a compact 4B-parameter model achieves 69.2% coverage pass rate under agentic evaluation, outperforming its teacher by 5.3% and demonstrating competitive performance against models an order of magnitude larger.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM4Cov：面向高覆盖率测试平台生成的可执行感知智能体学习</div>
<div class="mono" style="margin-top:8px">可执行感知的LLM智能体为从工具反馈中学习提供了有前景的范式，但此类反馈通常获取成本高昂且速度缓慢，使得在线强化学习（RL）难以实施。高覆盖率硬件验证正是这一挑战的典型体现，因其依赖工业级模拟器和不可微分的执行信号。我们提出LLM4ov——一种离线智能体学习框架，将验证建模为由确定性评估器引导的无记忆状态转移。基于此形式化框架，我们引入执行验证数据筛选、策略感知智能体数据合成及最差状态优先采样机制，以在执行约束下实现可扩展学习。我们进一步通过修订的评估协议，从现有验证套件中构建了贴合实际场景的基准测试。采用该流程后，一个紧凑的40亿参数模型在智能体评估中达到69.2%的覆盖率通过率，较其教师模型提升5.3%，并与规模大一个数量级的模型展现出可比性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of using execution feedback from slow and expensive tools, such as industrial hardware simulators, for online reinforcement learning in high-coverage testbench generation. It proposes LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions and employs techniques like execution-validated data curation and worst-state-prioritized sampling to enable scalable learning. Experimental results show that a compact 4B-parameter model trained with this pipeline achieves a 69.2% coverage pass rate, outperforming its teacher by 5.3% and competing with much larger models.</div>
<div class="mono" style="margin-top:8px">本文针对在高覆盖率测试平台生成中，使用来自工业模拟器等缓慢且昂贵工具的执行反馈进行在线强化学习所面临的挑战，提出了LLM4Cov离线智能体学习框架。该框架将验证建模为无记忆状态转移，并采用执行验证数据筛选和最差状态优先采样等技术，以实现受限执行条件下的可扩展学习。实验结果表明，通过该流程训练的紧凑型40亿参数模型达到了69.2%的覆盖率通过率，比其教师模型高出5.3%，并与规模大一个数量级的模型性能相当。</div>
</details>
</div>
<div class="card">
<div class="title">Discovering Multiagent Learning Algorithms with Large Language Models</div>
<div class="meta-line">Authors: Zun Li, John Schultz, Daniel Hennes, Marc Lanctot</div>
<div class="meta-line">First: 2026-02-18T22:41:00+00:00 · Latest: 2026-02-18T22:41:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16928v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16928v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Much of the advancement of Multi-Agent Reinforcement Learning (MARL) in imperfect-information games has historically depended on manual iterative refinement of baselines. While foundational families like Counterfactual Regret Minimization (CFR) and Policy Space Response Oracles (PSRO) rest on solid theoretical ground, the design of their most effective variants often relies on human intuition to navigate a vast algorithmic design space. In this work, we propose the use of AlphaEvolve, an evolutionary coding agent powered by large language models, to automatically discover new multiagent learning algorithms. We demonstrate the generality of this framework by evolving novel variants for two distinct paradigms of game-theoretic learning. First, in the domain of iterative regret minimization, we evolve the logic governing regret accumulation and policy derivation, discovering a new algorithm, Volatility-Adaptive Discounted (VAD-)CFR. VAD-CFR employs novel, non-intuitive mechanisms-including volatility-sensitive discounting, consistency-enforced optimism, and a hard warm-start policy accumulation schedule-to outperform state-of-the-art baselines like Discounted Predictive CFR+. Second, in the regime of population based training algorithms, we evolve training-time and evaluation-time meta strategy solvers for PSRO, discovering a new variant, Smoothed Hybrid Optimistic Regret (SHOR-)PSRO. SHOR-PSRO introduces a hybrid meta-solver that linearly blends Optimistic Regret Matching with a smoothed, temperature-controlled distribution over best pure strategies. By dynamically annealing this blending factor and diversity bonuses during training, the algorithm automates the transition from population diversity to rigorous equilibrium finding, yielding superior empirical convergence compared to standard static meta-solvers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用大语言模型发现多智能体学习算法</div>
<div class="mono" style="margin-top:8px">在不完全信息博弈中，多智能体强化学习（MARL）的进展历来依赖于对基线的迭代人工优化。尽管反事实遗憾最小化（CFR）和策略空间响应预言（PSRO）等基础算法族建立在坚实的理论基础上，但其最有效变体的设计往往需依赖人类直觉在广阔的算法设计空间中探索。本研究提出使用由大语言模型驱动的进化编码智能体AlphaEvolve，以自动发现新的多智能体学习算法。我们通过为两种不同的博弈论学习范式演化新变体，证明了该框架的通用性。首先，在迭代遗憾最小化领域，我们演化出控制遗憾累积与策略推导的逻辑，发现了一种新算法——波动自适应折扣（VAD-）CFR。该算法采用包括波动敏感折扣、一致性增强乐观策略及硬性热启动策略累积机制等反直觉设计，其性能超越了如折扣预测CFR+等前沿基线。其次，在基于种群的训练算法领域，我们为PSRO演化出训练时与评估时的元策略求解器，发现了新变体——平滑混合乐观遗憾（SHOR-）PSRO。该算法引入了一种混合元求解器，将乐观遗憾匹配与基于温度控制平滑处理的最优纯策略分布线性融合，并通过在训练中动态调整混合因子与多样性奖励，实现了从种群多样性到精确均衡求解的自动化过渡，其经验收敛性优于传统静态元求解器。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the reliance on manual refinement in multi-agent reinforcement learning (MARL) for imperfect-information games, this work introduces AlphaEvolve, an evolutionary coding agent powered by large language models, to automatically discover novel learning algorithms. The method applies this framework to two paradigms: for iterative regret minimization, it evolves regret accumulation and policy derivation logic, yielding Volatility-Adaptive Discounted CFR (VAD-CFR), which incorporates mechanisms like volatility-sensitive discounting and consistency-enforced optimism; for population-based training, it evolves meta strategy solvers for Policy Space Response Oracles, producing Smoothed Hybrid Optimistic Regret PSRO (SHOR-PSRO), featuring a hybrid meta-solver with dynamic annealing. Experimental results show that VAD-CFR outperforms state-of-the-art baselines like Discounted Predictive CFR+, while SHOR-PSRO achieves superior empirical convergence compared to standard static meta-solvers.</div>
<div class="mono" style="margin-top:8px">本研究针对不完全信息博弈中多智能体强化学习依赖人工迭代改进的问题，提出利用大语言模型驱动的进化编码智能体AlphaEvolve来自动发现新的学习算法。该方法将该框架应用于两个范式：在迭代后悔最小化领域，进化出控制后悔累积与策略推导的逻辑，发现了新算法VAD-CFR，其采用波动敏感折扣、一致性强制乐观等机制；在基于种群的训练算法中，进化出PSRO的元策略求解器，得到新变体SHOR-PSRO，引入了混合元求解器，动态融合乐观后悔匹配与平滑分布。实验结果表明，VAD-CFR在性能上超越了如Discounted Predictive CFR+等先进基线，而SHOR-PSRO相比静态元求解器实现了更优的经验收敛性。</div>
</details>
</div>
<div class="card">
<div class="title">Online Robust Reinforcement Learning with General Function Approximation</div>
<div class="meta-line">Authors: Debamita Ghosh, George K. Atia, Yue Wang</div>
<div class="meta-line">Venue: ICML 2026</div>
<div class="meta-line">First: 2025-12-22T02:12:04+00:00 · Latest: 2026-02-18T21:18:07+00:00</div>
<div class="meta-line">Comments: This version corresponds to the ICML 2026 submission. Major updates include: extension from TV to general phi-divergence uncertainty sets such as TV, chi-square, and KL; introduction of the robust Bellman-Eluder dimension; refined regret and sample complexity bounds; improved proofs; and new empirical evaluations</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.18957v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.18957v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In many real-world settings, reinforcement learning systems suffer performance degradation when the environment encountered at deployment differs from that observed during training. Distributionally robust reinforcement learning (DR-RL) mitigates this issue by seeking policies that maximize performance under the most adverse transition dynamics within a prescribed uncertainty set. Most existing DR-RL approaches, however, rely on strong data availability assumptions, such as access to a generative model or large offline datasets, and are largely restricted to tabular settings.
  In this work, we propose a fully online DR-RL algorithm with general function approximation that learns robust policies solely through interaction, without requiring prior knowledge or pre-collected data. Our approach is based on a dual-driven fitted robust Bellman procedure that simultaneously estimates the value function and the corresponding worst-case backup operator. We establish regret guarantees for online DR-RL characterized by an intrinsic complexity notion, the robust Bellman-Eluder dimension, covering a broad class of phi-divergence uncertainty sets. The resulting regret bounds are sublinear, do not scale with the size of the state or action spaces, and specialize to tight rates in structured problem classes, demonstrating the practicality and scalability of our framework.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于通用函数逼近的在线鲁棒强化学习</div>
<div class="mono" style="margin-top:8px">在现实场景中，当部署环境与训练环境存在差异时，强化学习系统常面临性能下降问题。分布鲁棒强化学习通过寻找在预设不确定性集合内最差转移动态下仍能保持最优性能的策略来缓解此问题。然而现有方法大多依赖强数据假设（如生成模型或大规模离线数据集），且主要局限于表格化设置。本研究提出一种完全在线的通用函数逼近DR-RL算法，仅通过交互学习鲁棒策略，无需先验知识或预收集数据。该方法基于对偶驱动的拟合鲁棒贝尔曼过程，同步估计值函数及对应最坏情况回溯算子。我们建立了以鲁棒贝尔曼-埃尔uder维度这一内在复杂度概念表征的在线DR-RL遗憾保证，覆盖广泛的phi散度不确定性集合。所得遗憾界具有次线性特征，不随状态/动作空间规模增长，在结构化问题类中可收敛至紧致速率，证明了框架的实用性与可扩展性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the performance degradation of reinforcement learning systems when deployment environments differ from training, this paper addresses distributionally robust reinforcement learning (DR-RL) to maximize performance under worst-case transition dynamics within uncertainty sets. The method introduces a fully online algorithm with general function approximation, based on a dual-driven fitted robust Bellman procedure that estimates value functions and worst-case backup operators through interaction alone, without prior data. Experimental results, supported by theoretical analysis, show sublinear regret guarantees characterized by the robust Bellman-Eluder dimension, covering phi-divergence uncertainty sets, with bounds that do not scale with state or action space sizes and achieve tight rates in structured classes, demonstrating practicality and scalability.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习系统在部署环境与训练环境不同时性能下降的问题，研究分布鲁棒强化学习（DR-RL），以在不确定性集合内的最坏情况转移动态下最大化性能。方法提出了一种具有通用函数逼近的完全在线算法，基于对偶驱动的拟合鲁棒贝尔曼过程，仅通过交互估计价值函数和对应最坏情况备份算子，无需先验数据。实验结果表明，该算法在理论分析支持下实现了以鲁棒贝尔曼-埃尔德维度为特征的次线性遗憾保证，覆盖了phi-散度不确定性集合，其遗憾界限不随状态或动作空间大小缩放，并在结构化问题类中达到紧致速率，证明了其实用性和可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">$\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts</div>
<div class="meta-line">Authors: Mert Cemri, Nived Rajaraman, Rishabh Tiwari, Xiaoxuan Liu, Kurt Keutzer, Ion Stoica, Kannan Ramchandran, Ahmad Beirami, Ziteng Sun</div>
<div class="meta-line">First: 2025-06-15T05:50:05+00:00 · Latest: 2026-02-18T21:13:11+00:00</div>
<div class="meta-line">Comments: 28 pages, 6 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.15733v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.15733v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scaling test-time compute has driven the recent advances in the reasoning capabilities of large language models (LLMs), typically by allocating additional computation for more thorough exploration. However, increased compute often comes at the expense of higher user-facing latency, directly impacting user experience. Current test-time scaling methods primarily optimize for accuracy based on total compute resources (FLOPS), often overlooking latency constraints. To address this gap, we propose $\texttt{SPECS}$, a latency-aware test-time scaling method inspired by speculative decoding. $\texttt{SPECS}$~uses a smaller, faster model to generate candidate sequences efficiently, and evaluates these candidates using signals from both a larger target model and a dedicated reward model. We introduce new integration strategies, including reward-guided soft verification and a reward-based deferral mechanism. Empirical results on MATH500, AMC23 and OlympiadBench datasets show that $\texttt{SPECS}$~matches or surpasses beam search accuracy while reducing latency by up to $\sim$19.1\%. Our theoretical analysis shows that our algorithm converges to the solution of a KL-regularized reinforcement learning objective with increasing beam width.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPECS：通过推测性草稿实现更快的测试时扩展</div>
<div class="mono" style="margin-top:8px">测试时计算扩展推动了大型语言模型（LLMs）推理能力的近期进展，通常通过分配额外计算资源进行更彻底的探索。然而，增加计算往往以更高的用户端延迟为代价，直接影响用户体验。当前测试时扩展方法主要基于总计算资源（FLOPS）优化准确率，常忽略延迟约束。为填补这一空白，我们提出SPECS——一种受推测解码启发的延迟感知测试时扩展方法。SPECS使用更小、更快的模型高效生成候选序列，并利用大型目标模型和专用奖励模型的双重信号评估这些候选序列。我们引入了新的集成策略，包括奖励引导的软验证和基于奖励的延迟机制。在MATH500、AMC23和OlympiadBench数据集上的实验结果表明，SPECS在保持或超越束搜索准确率的同时，将延迟降低高达约19.1%。理论分析表明，随着束宽增加，我们的算法会收敛到KL正则化强化学习目标的解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the trade-off between increased test-time compute for improved reasoning and the resulting higher latency that harms user experience, this paper introduces SPECS, a latency-aware test-time scaling method inspired by speculative decoding. The method employs a smaller, faster model to efficiently generate candidate sequences, which are then evaluated using signals from both a larger target model and a dedicated reward model, incorporating novel strategies like reward-guided soft verification and a reward-based deferral mechanism. Experimental results on datasets including MATH500, AMC23, and OlympiadBench demonstrate that SPECS achieves accuracy comparable to or better than beam search while reducing latency by up to approximately 19.1%, with theoretical analysis showing convergence to a KL-regularized reinforcement learning objective as beam width increases.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决大语言模型在测试时增加计算以提升推理能力时，往往导致延迟升高而损害用户体验的问题，为此提出了SPECS，一种受推测解码启发的延迟感知测试时扩展方法。该方法利用更小、更快的模型高效生成候选序列，然后通过更大目标模型和专用奖励模型的信号进行评估，并引入了奖励引导的软验证和基于奖励的延迟机制等新策略。在MATH500、AMC23和OlympiadBench数据集上的实验结果表明，SPECS在达到或超越波束搜索精度的同时，将延迟降低了最高约19.1%，理论分析显示该算法随波束宽度增加会收敛到KL正则化的强化学习目标。</div>
</details>
</div>
<div class="card">
<div class="title">SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation</div>
<div class="meta-line">Authors: Kushal Kedia, Tyler Ga Wei Lum, Jeannette Bohg, C. Karen Liu</div>
<div class="meta-line">First: 2026-02-18T20:42:39+00:00 · Latest: 2026-02-18T20:42:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16863v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16863v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ability to manipulate tools significantly expands the set of tasks a robot can perform. Yet, tool manipulation represents a challenging class of dexterity, requiring grasping thin objects, in-hand object rotations, and forceful interactions. Since collecting teleoperation data for these behaviors is challenging, sim-to-real reinforcement learning (RL) is a promising alternative. However, prior approaches typically require substantial engineering effort to model objects and tune reward functions for each task. In this work, we propose SimToolReal, taking a step towards generalizing sim-to-real RL policies for tool manipulation. Instead of focusing on a single object and task, we procedurally generate a large variety of tool-like object primitives in simulation and train a single RL policy with the universal goal of manipulating each object to random goal poses. This approach enables SimToolReal to perform general dexterous tool manipulation at test-time without any object or task-specific training. We demonstrate that SimToolReal outperforms prior retargeting and fixed-grasp methods by 37% while matching the performance of specialist RL policies trained on specific target objects and tasks. Finally, we show that SimToolReal generalizes across a diverse set of everyday tools, achieving strong zero-shot performance over 120 real-world rollouts spanning 24 tasks, 12 object instances, and 6 tool categories.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SimToolReal：一种面向零样本灵巧工具操作的物体中心策略</div>
<div class="mono" style="margin-top:8px">操作工具的能力显著扩展了机器人可执行的任务范围。然而，工具操作代表了一类具有挑战性的灵巧性任务，需要抓握细薄物体、进行手内物体旋转以及强力的交互。由于收集这些行为的遥操作数据较为困难，仿真到现实的强化学习成为一种有前景的替代方案。但以往方法通常需要大量工程努力来建模物体并为每个任务调整奖励函数。本研究提出SimToolReal，旨在推动仿真到现实强化学习策略在工具操作中的泛化。该方法不再专注于单一物体和任务，而是在仿真中程序化生成大量多样化的工具状物体基元，并训练一个统一的强化学习策略，其通用目标是操纵每个物体至随机目标姿态。这一方法使SimToolReal在测试时无需任何物体或任务特定训练即可执行通用的灵巧工具操作。实验表明，SimToolReal在性能上超越先前的重定向和固定抓取方法37%，同时达到针对特定目标物体和任务训练的专用强化学习策略的水平。最后，我们证明SimToolReal能够泛化至多种日常工具，在涵盖24个任务、12个物体实例和6个工具类别的120次现实世界测试中均表现出强大的零样本性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the difficulty of collecting real-world teleoperation data for dexterous tool manipulation, this work introduces SimToolReal, a sim-to-real reinforcement learning approach that aims for generalization. The method procedurally generates a diverse set of tool-like object primitives in simulation and trains a single policy to manipulate any of these objects to random goal poses, avoiding task-specific engineering. Experimental results show the policy outperforms prior retargeting and fixed-grasp methods by 37%, matches specialist policies trained on specific objects, and demonstrates strong zero-shot generalization across 24 real-world tasks involving 12 tools from 6 categories.</div>
<div class="mono" style="margin-top:8px">本研究针对灵巧工具操作中真实遥操作数据收集困难的问题，提出了SimToolReal这一旨在实现泛化的仿真到现实强化学习方法。该方法在仿真中程序化生成多样化的类工具物体基元，并训练一个单一策略来将任何此类物体操控至随机目标姿态，避免了针对特定任务的工程调优。实验结果表明，该策略的性能优于先前的重定向和固定抓取方法37%，与针对特定物体训练的专用策略表现相当，并在涉及6个类别、12个工具实例的24个真实世界任务中展现了强大的零样本泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Training Large Reasoning Models Efficiently via Progressive Thought Encoding</div>
<div class="meta-line">Authors: Zeliang Zhang, Xiaodong Liu, Hao Cheng, Hao Sun, Chenliang Xu, Jianfeng Gao</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-18T20:03:38+00:00 · Latest: 2026-02-18T20:03:38+00:00</div>
<div class="meta-line">Comments: ICLR 2026, 15 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16839v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16839v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models (LRMs) excel on complex problems but face a critical barrier to efficiency: reinforcement learning (RL) training requires long rollouts for outcome-based rewards, where autoregressive decoding dominates time and memory usage. While sliding-window cache strategies can bound memory, they disrupt long-context reasoning and degrade performance. We introduce Progressive Thought Encoding, a parameter-efficient fine-tuning method that enables LRMs to reason effectively under fixed-size caches. By progressively encoding intermediate reasoning into fixed-size vector representations, our approach eliminates the need to backpropagate through full-cache rollouts, thereby reducing memory usage, while maintaining constant memory during inference. Experiments on three models, including Qwen2.5-3B-Instruct, Qwen2.5-7B-Instruct, and DeepSeek-R1-Distill-Llama-8B, on six widely used challenging mathematical benchmarks show consistent gains: our method achieves +19.3% improvement over LoRA-based fine-tuning and +29.9% over LRMs without fine-tuning on average, with up to +23.4 accuracy improvement on AIME2024/2025 under the same tight cache budgets. These results demonstrate that Progressive Thought Encoding not only improves reasoning accuracy but also makes RL training of LRMs substantially more efficient and scalable under real-world memory constraints.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过渐进式思维编码高效训练大型推理模型</div>
<div class="mono" style="margin-top:8px">大型推理模型（LRMs）在复杂问题上表现出色，但其效率面临关键瓶颈：基于结果的强化学习训练需要长序列展开，其中自回归解码占据大量时间和内存。滑动窗口缓存策略虽能限制内存，但会破坏长上下文推理并降低性能。本文提出渐进式思维编码，一种参数高效的微调方法，使LRMs能在固定大小缓存下有效推理。通过将中间推理逐步编码为固定大小的向量表示，该方法无需在完整缓存展开中进行反向传播，从而降低内存使用，并在推理时保持恒定内存。在Qwen2.5-3B-Instruct、Qwen2.5-7B-Instruct和DeepSeek-R1-Distill-Llama-8B三个模型上，对六个广泛使用的挑战性数学基准的实验显示一致提升：在相同严格缓存限制下，本方法相比基于LoRA的微调平均提升19.3%，相比未微调的LRMs平均提升29.9%，在AIME2024/2025上最高提升23.4%准确率。结果表明，渐进式思维编码不仅提高推理准确性，还使LRMs的强化学习训练在实际内存限制下显著更高效、可扩展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to overcome the efficiency bottleneck in training large reasoning models (LRMs), where reinforcement learning with long autoregressive rollouts consumes excessive time and memory, and sliding-window caches degrade reasoning performance. The method introduced, Progressive Thought Encoding, is a parameter-efficient fine-tuning technique that progressively compresses intermediate reasoning steps into fixed-size vector representations, eliminating the need for backpropagation through full rollouts and maintaining constant memory during inference. Experimental results on three models across six mathematical benchmarks show that this approach consistently outperforms alternatives, achieving an average improvement of +19.3% over LoRA-based fine-tuning and up to +23.4 accuracy points on AIME2024/2025 under tight cache constraints, thereby enhancing both reasoning accuracy and training efficiency under memory limits.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大型推理模型训练中的效率瓶颈，即基于强化学习的训练需要长序列自回归解码，导致时间和内存消耗巨大，而滑动窗口缓存策略又会破坏长上下文推理并降低性能。所提出的方法称为渐进式思维编码，这是一种参数高效的微调技术，通过将中间推理步骤逐步编码为固定大小的向量表示，避免了在完整序列上进行反向传播，并在推理时保持恒定内存。在三个模型和六个数学基准上的实验结果表明，该方法在相同严格缓存预算下，平均比基于LoRA的微调提升19.3%，在AIME2024/2025上准确率最高提升23.4个百分点，显著提高了推理准确性，并使强化学习训练在现实内存约束下更加高效和可扩展。</div>
</details>
</div>
<div class="card">
<div class="title">VAM: Verbalized Action Masking for Controllable Exploration in RL Post-Training -- A Chess Case Study</div>
<div class="meta-line">Authors: Zhicheng Zhang, Ziyan Wang, Yali Du, Fei Fang</div>
<div class="meta-line">First: 2026-02-18T19:56:43+00:00 · Latest: 2026-02-18T19:56:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16833v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16833v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Exploration remains a key bottleneck for reinforcement learning (RL) post-training of large language models (LLMs), where sparse feedback and large action spaces can lead to premature collapse into repetitive behaviors. We propose Verbalized Action Masking (VAM), which verbalizes an action mask in the prompt and enforces that the model outputs an action from the masked set. Building on this interface, we introduce iterative action-space pruning: if the target action is not sampled, we remove valid sampled actions from the mask and resample under the reduced candidate set, repeating until the target is sampled or a fixed budget is exhausted. We study VAM in chess and evaluate it under two training regimes: an engine-play regime that generates states via play against an engine opponent and a fixed-dataset regime that trains from a fixed dataset of positions with verifier scores. Across held-out chess puzzles and full-game play measured by average centipawn loss (ACPL), VAM improves learning efficiency and final performance over strong baselines, highlighting verbalized masking as a practical mechanism for controllable exploration in LLM RL post-training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VAM：强化学习后训练中的言语化动作掩码可控探索——以国际象棋为例</div>
<div class="mono" style="margin-top:8px">探索仍是大型语言模型（LLM）强化学习（RL）后训练的关键瓶颈，稀疏反馈与庞大动作空间易导致模型过早陷入重复行为。本文提出言语化动作掩码（VAM），通过在提示中言语化动作掩码，强制模型从掩码集合输出动作。基于此接口，我们引入迭代动作空间剪枝：若未采样到目标动作，则从掩码中移除已采样的有效动作，在缩减候选集中重新采样，直至采样到目标动作或耗尽固定预算。我们在国际象棋中验证VAM，并在两种训练机制下评估：一是通过与引擎对弈生成状态的引擎对弈机制，二是基于带验证器评分的固定棋局数据集训练机制。在保留的象棋谜题和以平均厘兵损失（ACPL）衡量的完整对弈中，VAM较基线方法显著提升了学习效率与最终性能，表明言语化掩码是LLM强化学习后训练中实现可控探索的有效机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the exploration bottleneck in reinforcement learning (RL) post-training of large language models (LLMs), where sparse rewards and large action spaces often lead to repetitive behaviors. The method proposed, Verbalized Action Masking (VAM), verbalizes an action mask within the prompt to restrict the model&#x27;s output to a masked set of actions, and incorporates iterative action-space pruning that removes sampled non-target actions from the mask to resample from a reduced candidate set. Experimental results in chess, under both engine-play and fixed-dataset training regimes, show that VAM improves learning efficiency and final performance on held-out puzzles and full-game play as measured by average centipawn loss, demonstrating its effectiveness for controllable exploration.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大型语言模型（LLM）在强化学习（RL）后训练中的探索瓶颈问题，即稀疏奖励和巨大动作空间常导致模型行为陷入重复。所提出的方法称为言语化动作掩码（VAM），通过在提示中言语化动作掩码来限制模型输出为掩码集中的动作，并结合迭代动作空间剪枝，从掩码中移除采样的非目标动作以在缩减的候选集中重新采样。在国际象棋中的实验结果表明，在引擎对弈和固定数据集两种训练机制下，VAM在保留谜题测试和完整对局（以平均百分兵损失衡量）中提高了学习效率和最终性能，证明了其作为可控探索机制的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">References Improve LLM Alignment in Non-Verifiable Domains</div>
<div class="meta-line">Authors: Kejian Shi, Yixin Liu, Peifeng Wang, Alexander R. Fabbri, Shafiq Joty, Arman Cohan</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-18T19:03:34+00:00 · Latest: 2026-02-18T19:03:34+00:00</div>
<div class="meta-line">Comments: ICLR 2026 Camera Ready</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16802v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16802v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft &quot;verifiers&quot;. First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>参考文献提升大语言模型在不可验证领域的对齐能力</div>
<div class="mono" style="margin-top:8px">尽管基于可验证奖励的强化学习（RLVR）在推理任务中表现出色，但其无法直接应用于缺乏真实验证器的不可验证领域，如大语言模型对齐。本研究探讨了基于参考文献的大语言模型评估器能否作为软性“验证器”来弥合这一差距。首先，我们设计了利用参考输出增强基于大语言模型的评估器用于模型对齐的评估方案。通过全面实验表明：采用前沿模型生成的参考文献能显著提升较弱大语言模型评判者的准确性；高质量（如人工撰写）参考文献也能增强较强评判者的能力。基于这些改进的评判者，我们证明了高质量参考文献在对齐调优中的实用性——通过参考文献引导的大语言模型作为评判者进行自我改进。实验显示，基于参考文献的自我改进方法明显优于直接在参考输出上进行监督微调的方法，也优于无参考文献评判者的自我改进方法，其性能可与使用强大微调奖励模型ArmoRM的训练结果相媲美。具体而言，我们的方法在Llama-3-8B-Instruct模型上于AlpacaEval和Arena-Hard分别达到73.1%和58.7%，在Qwen2.5-7B模型上分别达到70.0%和74.1%，相较于监督微调蒸馏方法在AlpacaEval/Arena-Hard平均绝对提升+20.2/+17.1个百分点，相较于无参考文献自我改进方法平均绝对提升+5.3/+3.6个百分点。这些结果凸显了利用参考文献引导的大语言模型评估器在不可验证领域实现有效模型后训练的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of aligning large language models (LLMs) in non-verifiable domains, where ground-truth rewards are unavailable, by proposing to use reference-guided LLM-evaluators as soft verifiers. The method enhances LLM-based evaluators by incorporating reference outputs, showing that less capable judges improve with references from frontier models, while stronger judges benefit from high-quality human-written references. Experimental results demonstrate that reference-guided self-improvement outperforms both supervised fine-tuning on references and reference-free self-judgment, achieving performance comparable to training with a strong finetuned reward model, with significant gains on benchmarks like AlpacaEval and Arena-Hard using models such as Llama-3-8B-Instruct and Qwen2.5-7B.</div>
<div class="mono" style="margin-top:8px">该论文针对不可验证领域（如大语言模型对齐）中缺乏真实奖励信号的问题，提出使用参考输出引导的LLM评估器作为软验证器。方法通过引入参考输出来增强基于LLM的评估器，实验表明，能力较弱的评估器借助前沿模型的参考输出可大幅提升准确性，而更强的评估器则能通过高质量人工撰写的参考得到改进。在此基础上，参考引导的自改进方法在模型对齐调优中显示出优势，其性能优于直接在参考输出上进行监督微调以及无参考的自评估，达到了与使用强奖励模型训练相当的效果，在AlpacaEval和Arena-Hard等基准测试中，使用Llama-3-8B-Instruct和Qwen2.5-7B模型取得了显著提升。</div>
</details>
</div>
<div class="card">
<div class="title">Factorization Machine with Quadratic-Optimization Annealing for RNA Inverse Folding and Evaluation of Binary-Integer Encoding and Nucleotide Assignment</div>
<div class="meta-line">Authors: Shuta Kikuchi, Shu Tanaka</div>
<div class="meta-line">First: 2026-02-18T17:32:55+00:00 · Latest: 2026-02-18T17:32:55+00:00</div>
<div class="meta-line">Comments: 17 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16643v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16643v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The RNA inverse folding problem aims to identify nucleotide sequences that preferentially adopt a given target secondary structure. While various heuristic and machine learning-based approaches have been proposed, many require a large number of sequence evaluations, which limits their applicability when experimental validation is costly. We propose a method to solve the problem using a factorization machine with quadratic-optimization annealing (FMQA). FMQA is a discrete black-box optimization method reported to obtain high-quality solutions with a limited number of evaluations. Applying FMQA to the problem requires converting nucleotides into binary variables. However, the influence of integer-to-nucleotide assignments and binary-integer encoding on the performance of FMQA has not been thoroughly investigated, even though such choices determine the structure of the surrogate model and the search landscape, and thus can directly affect solution quality. Therefore, this study aims both to establish a novel FMQA framework for RNA inverse folding and to analyze the effects of these assignments and encoding methods. We evaluated all 24 possible assignments of the four nucleotides to the ordered integers (0-3), in combination with four binary-integer encoding methods. Our results demonstrated that one-hot and domain-wall encodings outperform binary and unary encodings in terms of the normalized ensemble defect value. In domain-wall encoding, nucleotides assigned to the boundary integers (0 and 3) appeared with higher frequency. In the RNA inverse folding problem, assigning guanine and cytosine to these boundary integers promoted their enrichment in stem regions, which led to more thermodynamically stable secondary structures than those obtained with one-hot encoding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于二次优化退火的因子分解机用于RNA逆折叠及二进制整数编码与核苷酸分配评估</div>
<div class="mono" style="margin-top:8px">RNA逆折叠问题旨在识别优先采用给定目标二级结构的核苷酸序列。尽管已有多种启发式和基于机器学习的方法被提出，但许多方法需要大量序列评估，这在实验验证成本高昂时限制了其适用性。本研究提出一种使用二次优化退火因子分解机（FMQA）解决该问题的方法。FMQA是一种离散黑盒优化方法，据报道能以有限评估次数获得高质量解。将FMQA应用于此问题需将核苷酸转换为二进制变量，但整数-核苷酸分配和二进制整数编码对FMQA性能的影响尚未得到深入研究——这些选择决定了代理模型的结构和搜索空间，从而直接影响解的质量。因此，本研究旨在建立新的RNA逆折叠FMQA框架，并分析这些分配与编码方法的影响。我们评估了四种核苷酸与有序整数（0-3）之间所有24种可能的分配方式，并结合四种二进制整数编码方法。结果表明，在归一化集合缺陷值方面，独热编码和畴壁编码优于二进制编码和一元编码。在畴壁编码中，分配给边界整数（0和3）的核苷酸出现频率更高。在RNA逆折叠问题中，将鸟嘌呤和胞嘧啶分配至这些边界整数可促进其在茎区的富集，从而获得比独热编码热力学更稳定的二级结构。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the RNA inverse folding problem, which seeks nucleotide sequences that fold into a target secondary structure, motivated by the need for efficient methods that minimize costly experimental evaluations. The authors propose a novel approach using a factorization machine with quadratic-optimization annealing (FMQA), a discrete black-box optimization technique designed to yield high-quality solutions with limited evaluations, and systematically investigate the impact of binary-integer encoding schemes and nucleotide-to-integer assignments on performance. Experimental results show that one-hot and domain-wall encodings outperform binary and unary encodings in reducing normalized ensemble defect, with domain-wall encoding particularly favoring assignments of guanine and cytosine to boundary integers, thereby enriching stem regions and enhancing thermodynamic stability compared to one-hot encoding.</div>
<div class="mono" style="margin-top:8px">本研究针对RNA逆折叠问题，即寻找能折叠成目标二级结构的核苷酸序列，其动机在于需要开发高效方法以减少昂贵的实验验证成本。作者提出了一种使用因子分解机与二次优化退火（FMQA）的新方法，这是一种离散黑盒优化技术，旨在通过有限评估获得高质量解，并系统研究了二进制整数编码方案和核苷酸到整数分配对性能的影响。实验结果表明，在降低归一化整体缺陷值方面，独热编码和畴壁编码优于二进制和一元编码，其中畴壁编码特别倾向于将鸟嘌呤和胞嘧啶分配到边界整数，从而富集茎区结构，相比独热编码能产生热力学更稳定的二级结构。</div>
</details>
</div>
<div class="card">
<div class="title">Almost Sure Convergence of Differential Temporal Difference Learning for Average Reward Markov Decision Processes</div>
<div class="meta-line">Authors: Ethan Blaser, Jiuqi Wang, Shangtong Zhang</div>
<div class="meta-line">First: 2026-02-18T17:24:27+00:00 · Latest: 2026-02-18T17:24:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16629v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16629v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The average reward is a fundamental performance metric in reinforcement learning (RL) focusing on the long-run performance of an agent. Differential temporal difference (TD) learning algorithms are a major advance for average reward RL as they provide an efficient online method to learn the value functions associated with the average reward in both on-policy and off-policy settings. However, existing convergence guarantees require a local clock in learning rates tied to state visit counts, which practitioners do not use and does not extend beyond tabular settings. We address this limitation by proving the almost sure convergence of on-policy $n$-step differential TD for any $n$ using standard diminishing learning rates without a local clock. We then derive three sufficient conditions under which off-policy $n$-step differential TD also converges without a local clock. These results strengthen the theoretical foundations of differential TD and bring its convergence analysis closer to practical implementations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>平均奖励马尔可夫决策过程中差分时序差分学习的几乎必然收敛性</div>
<div class="mono" style="margin-top:8px">平均奖励是强化学习中关注智能体长期性能的基本指标。差分时序差分学习算法是平均奖励强化学习的重要进展，为同策略和异策略场景下学习与平均奖励相关的价值函数提供了高效的在线方法。然而，现有收敛性证明要求学习率需绑定状态访问计数的局部时钟机制，这在实际应用中未被采用且无法推广至表格化设定之外。我们通过证明同策略n步差分时序差分算法在使用标准递减学习率（无需局部时钟）时对任意n值均具有几乎必然收敛性，解决了这一局限。随后推导出三个充分条件，证明异策略n步差分时序差分算法在无局部时钟时同样收敛。这些成果强化了差分时序差分的理论基础，使其收敛性分析更贴近实际应用场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need to strengthen the theoretical convergence guarantees for differential temporal difference (TD) learning algorithms in average reward Markov decision processes, as existing proofs rely on impractical local clock learning rates that are not used in practice and do not generalize beyond tabular settings. The method involves proving the almost sure convergence of on-policy n-step differential TD using standard diminishing learning rates without a local clock, and then deriving three sufficient conditions for the convergence of off-policy n-step differential TD under the same practical learning rate scheme. The main experimental results are theoretical, demonstrating that both on-policy and off-policy differential TD algorithms converge almost surely under these more realistic and widely adopted conditions, thereby bridging the gap between theory and practical implementation.</div>
<div class="mono" style="margin-top:8px">本文的动机是强化差分时序差分学习算法在平均奖励马尔可夫决策过程中的理论收敛性保证，因为现有证明依赖于不切实际的局部时钟学习率，这些学习率在实践中未被采用且无法推广到表格设置之外。方法包括证明在无需局部时钟的标准递减学习率下，同策略n步差分时序差分算法几乎必然收敛，并推导出三个充分条件以确保异策略n步差分时序差分算法在相同实用学习率方案下的收敛性。主要实验结果基于理论分析，表明在同策略和异策略设置下，差分时序差分算法在这些更现实且广泛采用的条件下几乎必然收敛，从而缩小了理论与实际应用之间的差距。</div>
</details>
</div>
<div class="card">
<div class="title">ReaCritic: Reasoning Transformer-based DRL Critic-model Scaling For Wireless Networks</div>
<div class="meta-line">Authors: Feiran You, Hongyang Du</div>
<div class="meta-line">First: 2025-05-16T08:42:08+00:00 · Latest: 2026-02-18T16:45:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.10992v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.10992v2">PDF</a> · <a href="https://github.com/NICE-HKU/ReaCritic">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Heterogeneous Networks (HetNets) pose critical challenges for intelligent management due to the diverse user requirements and time-varying wireless conditions. These factors introduce significant decision complexity, which limits the adaptability of existing Deep Reinforcement Learning (DRL) methods. In many DRL algorithms, especially those involving value-based or actor-critic structures, the critic component plays a key role in guiding policy learning by estimating value functions. However, conventional critic models often use shallow architectures that map observations directly to scalar estimates, limiting their ability to handle multi-task complexity. In contrast, recent progress in inference-time scaling of Large Language Models (LLMs) has shown that generating intermediate reasoning steps can significantly improve decision quality. Motivated by this, we propose ReaCritic, a reasoning transformer-based critic-model scaling scheme that brings reasoning-like ability into DRL. ReaCritic performs horizontal reasoning over parallel state-action inputs and vertical reasoning through deep transformer stacks. It is compatible with a broad range of value-based and actor-critic DRL algorithms and enhances generalization in dynamic wireless environments. Extensive experiments demonstrate that ReaCritic improves convergence speed and final performance across various HetNet settings and standard OpenAI Gym control tasks. The code of ReaCritic is available at https://github.com/NICE-HKU/ReaCritic.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReaCritic：基于推理Transformer的无线网络DRL评论家模型扩展方法</div>
<div class="mono" style="margin-top:8px">异构网络因用户需求多样性与无线环境时变性，对智能管理提出严峻挑战，这些因素导致决策复杂度显著增加，限制了现有深度强化学习方法的自适应性。在众多DRL算法（尤其是基于价值或演员-评论家架构的算法）中，评论家组件通过估计价值函数在策略学习中起关键指导作用。然而传统评论家模型常采用浅层架构，直接将观测映射为标量估计，限制了其处理多任务复杂性的能力。相比之下，近期大语言模型在推理时扩展方面的进展表明，生成中间推理步骤能显著提升决策质量。受此启发，我们提出ReaCritic——一种基于推理Transformer的评论家模型扩展方案，将类推理能力引入DRL。ReaCritic通过对并行状态-动作输入进行横向推理，并借助深度Transformer堆栈实现纵向推理。该方案兼容广泛的基于价值及演员-评论家DRL算法，能增强动态无线环境中的泛化能力。大量实验表明，ReaCritic在多种异构网络场景和标准OpenAI Gym控制任务中均能提升收敛速度与最终性能。代码已开源：https://github.com/NICE-HKU/ReaCritic。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of conventional Deep Reinforcement Learning (DRL) critic models, which use shallow architectures that struggle with the decision complexity in dynamic Heterogeneous Networks (HetNets). Motivated by the success of reasoning in Large Language Models, the authors propose ReaCritic, a scaling scheme that integrates a reasoning transformer-based critic model into DRL algorithms; it performs horizontal reasoning over parallel state-action inputs and vertical reasoning through deep transformer stacks to enhance value estimation. Experimental results across HetNet scenarios and standard control tasks show that ReaCritic improves convergence speed and final performance compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本文针对传统深度强化学习评论家模型架构较浅、难以处理动态异构网络决策复杂性的问题，受大语言模型中推理能力成功的启发，提出了ReaCritic方案，将基于推理的Transformer评论家模型引入DRL算法，通过并行状态-动作输入的横向推理和深层Transformer堆栈的纵向推理来提升价值估计。在多种异构网络设置和标准控制任务上的实验表明，ReaCritic相比现有方法提高了收敛速度和最终性能。</div>
</details>
</div>
<div class="card">
<div class="title">A Scalable Approach to Solving Simulation-Based Network Security Games</div>
<div class="meta-line">Authors: Michael Lanier, Yevgeniy Vorobeychik</div>
<div class="meta-line">First: 2026-02-18T16:07:01+00:00 · Latest: 2026-02-18T16:07:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16564v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16564v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce MetaDOAR, a lightweight meta-controller that augments the Double Oracle / PSRO paradigm with a learned, partition-aware filtering layer and Q-value caching to enable scalable multi-agent reinforcement learning on very large cyber-network environments. MetaDOAR learns a compact state projection from per node structural embeddings to rapidly score and select a small subset of devices (a top-k partition) on which a conventional low-level actor performs focused beam search utilizing a critic agent. Selected candidate actions are evaluated with batched critic forwards and stored in an LRU cache keyed by a quantized state projection and local action identifiers, dramatically reducing redundant critic computation while preserving decision quality via conservative k-hop cache invalidation. Empirically, MetaDOAR attains higher player payoffs than SOTA baselines on large network topologies, without significant scaling issues in terms of memory usage or training time. This contribution provide a practical, theoretically motivated path to efficient hierarchical policy learning for large-scale networked decision problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种可扩展的基于仿真的网络安全博弈求解方法</div>
<div class="mono" style="margin-top:8px">本文提出MetaDOAR——一种轻量级元控制器，通过引入基于学习的感知分区过滤层和Q值缓存机制，增强双Oracle/PSRO范式，从而在超大规模网络环境中实现可扩展的多智能体强化学习。MetaDOAR通过学习从节点结构嵌入到紧凑状态投影的映射，快速评估并选择少量设备（top-k分区），使传统底层执行器能在其上利用评论家智能体进行聚焦束搜索。所选候选动作通过批量评论家前向传播进行评估，并存储在以量化状态投影和局部动作标识符为键的LRU缓存中，通过保守的k跳缓存失效策略，在保持决策质量的同时显著减少冗余的评论家计算。实验表明，在大型网络拓扑中，MetaDOAR在内存使用和训练时间未出现显著扩展问题的前提下，获得了优于现有基线方法的玩家收益。该研究为大规模网络化决策问题的高效分层策略学习提供了兼具理论依据与实践可行性的路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for scalable solutions in large-scale network security games, this paper introduces MetaDOAR, a meta-controller that enhances the Double Oracle/PSRO framework with a learned filtering layer and Q-value caching to enable efficient multi-agent reinforcement learning. The method learns a compact state projection to select a critical subset of network devices, where a low-level actor performs focused beam search guided by a critic, and employs caching with invalidation to minimize redundant computations. Experimental results demonstrate that MetaDOAR achieves higher player payoffs than state-of-the-art baselines on large network topologies, with efficient scaling in memory and training time.</div>
<div class="mono" style="margin-top:8px">本文旨在解决大规模网络安全博弈中的可扩展性问题，提出了MetaDOAR这一元控制器，它通过引入学习型分区感知过滤层和Q值缓存来增强双Oracle/PSRO范式，以实现高效的多智能体强化学习。该方法学习紧凑的状态投影以快速评分并选择关键网络设备子集，由底层执行器在批评者智能体的引导下进行聚焦波束搜索，并利用缓存及失效机制大幅减少冗余计算。实验结果表明，在大型网络拓扑上，MetaDOAR相比现有先进基线获得了更高的玩家收益，且在内存使用和训练时间上未出现显著的可扩展性问题。</div>
</details>
</div>
<div class="card">
<div class="title">RIDER: 3D RNA Inverse Design with Reinforcement Learning-Guided Diffusion</div>
<div class="meta-line">Authors: Tianmeng Hu, Yongzheng Cui, Biao Luo, Ke Li</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-18T15:52:26+00:00 · Latest: 2026-02-18T15:52:26+00:00</div>
<div class="meta-line">Comments: Accepted as a conference paper at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16548v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16548v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The inverse design of RNA three-dimensional (3D) structures is crucial for engineering functional RNAs in synthetic biology and therapeutics. While recent deep learning approaches have advanced this field, they are typically optimized and evaluated using native sequence recovery, which is a limited surrogate for structural fidelity, since different sequences can fold into similar 3D structures and high recovery does not necessarily indicate correct folding. To address this limitation, we propose RIDER, an RNA Inverse DEsign framework with Reinforcement learning that directly optimizes for 3D structural similarity. First, we develop and pre-train a GNN-based generative diffusion model conditioned on the target 3D structure, achieving a 9% improvement in native sequence recovery over state-of-the-art methods. Then, we fine-tune the model with an improved policy gradient algorithm using four task-specific reward functions based on 3D self-consistency metrics. Experimental results show that RIDER improves structural similarity by over 100% across all metrics and discovers designs that are distinct from native sequences.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RIDER：基于强化学习引导扩散的RNA三维结构逆向设计</div>
<div class="mono" style="margin-top:8px">RNA三维结构的逆向设计对于合成生物学和治疗学中的功能RNA工程至关重要。尽管近期的深度学习方法推动了该领域发展，但这些方法通常以天然序列恢复率为优化和评估指标，这作为结构保真度的替代指标存在局限——不同序列可能折叠为相似的三维结构，且高恢复率未必意味着正确折叠。为解决此问题，我们提出RIDER框架，通过强化学习直接优化三维结构相似性。首先，我们开发并预训练了基于图神经网络的生成扩散模型，该模型以目标三维结构为条件，在天然序列恢复率上较现有最优方法提升9%。随后，我们采用改进的策略梯度算法，基于四项三维自洽度量指标构建任务特定奖励函数，对模型进行微调。实验结果表明，RIDER在所有指标上均将结构相似性提升超100%，并能发现与天然序列不同的设计方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces RIDER, a framework for the inverse design of RNA 3D structures, motivated by the limitation of existing deep learning methods that rely on native sequence recovery as a proxy for structural fidelity, which is inadequate because different sequences can adopt similar folds. The method involves pre-training a GNN-based conditional diffusion model on target structures and then fine-tuning it with a reinforcement learning policy gradient algorithm using four reward functions based on 3D self-consistency metrics. Experimental results demonstrate that RIDER achieves a 9% improvement in native sequence recovery over state-of-the-art methods and enhances structural similarity by over 100% across all metrics, while generating designs distinct from native sequences.</div>
<div class="mono" style="margin-top:8px">本文提出了RIDER框架，用于RNA三维结构的逆向设计，其动机在于现有深度学习方法依赖天然序列恢复作为结构保真度的代理指标存在局限，因为不同序列可能折叠成相似结构。该方法首先预训练一个基于图神经网络的条件扩散模型，然后使用基于三维自洽度量的四种奖励函数，通过强化学习策略梯度算法进行微调。实验结果表明，RIDER在天然序列恢复上比现有最优方法提升了9%，在所有结构相似性指标上均提高了超过100%，并能生成与天然序列不同的设计。</div>
</details>
</div>
<div class="card">
<div class="title">Safe But Not Sorry: Reducing Over-Conservatism in Safety Critics via Uncertainty-Aware Modulation</div>
<div class="meta-line">Authors: Daniel Bethell, Simos Gerasimou, Radu Calinescu, Calum Imrie</div>
<div class="meta-line">First: 2025-10-21T09:57:44+00:00 · Latest: 2026-02-18T15:47:54+00:00</div>
<div class="meta-line">Comments: Accepted into AAMAS &#x27;26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.18478v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.18478v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ensuring the safe exploration of reinforcement learning (RL) agents is critical for deployment in real-world systems. Yet existing approaches struggle to strike the right balance: methods that tightly enforce safety often cripple task performance, while those that prioritize reward leave safety constraints frequently violated, producing diffuse cost landscapes that flatten gradients and stall policy improvement. We introduce the Uncertain Safety Critic (USC), a novel approach that integrates uncertainty-aware modulation and refinement into critic training. By concentrating conservatism in uncertain and costly regions while preserving sharp gradients in safe areas, USC enables policies to achieve effective reward-safety trade-offs. Extensive experiments show that USC reduces safety violations by approximately 40% while maintaining competitive or higher rewards, and reduces the error between predicted and true cost gradients by approximately 83%, breaking the prevailing trade-off between safety and performance and paving the way for scalable safe RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>安全而不遗憾：通过不确定性感知调制降低安全评判器的过度保守性</div>
<div class="mono" style="margin-top:8px">确保强化学习（RL）智能体的安全探索对于实际系统部署至关重要。然而现有方法难以取得恰当平衡：严格保障安全的方法常严重损害任务性能，而优先追求奖励的方法则频繁违反安全约束，产生使梯度平坦化并阻碍策略改进的弥散代价景观。我们提出不确定性安全评判器（USC），这是一种将不确定性感知调制与精细化训练相结合的新方法。通过将保守性集中在不确定且高代价区域，同时在安全区域保持锐利梯度，USC使策略能实现有效的奖励-安全权衡。大量实验表明，USC在保持竞争力或更高奖励的同时，将安全违规减少约40%，并将预测与真实代价梯度间的误差降低约83%，打破了安全与性能间的固有权衡，为可扩展的安全RL开辟了新路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge in safe reinforcement learning where existing methods either overly constrain task performance or inadequately enforce safety, leading to diffuse cost landscapes that hinder policy improvement. The authors propose the Uncertain Safety Critic (USC), a method that modulates conservatism based on uncertainty, focusing it on risky regions while maintaining sharp gradients in safe areas to enable better reward-safety trade-offs. Experimental results demonstrate that USC reduces safety violations by about 40% while preserving or improving rewards, and cuts the error between predicted and true cost gradients by approximately 83%, effectively breaking the traditional safety-performance trade-off.</div>
<div class="mono" style="margin-top:8px">本文针对安全强化学习中现有方法要么过度限制任务性能、要么安全约束不足导致成本景观弥散从而阻碍策略改进的挑战，提出了一种不确定安全评论家方法。该方法基于不确定性调制保守性，将其集中在高风险区域，同时在安全区域保持锐利梯度以实现更好的奖励与安全权衡。实验结果表明，该方法将安全违规减少了约40%，同时保持或提高了奖励，并将预测与真实成本梯度之间的误差降低了约83%，有效打破了安全与性能之间的传统权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Vulnerability Analysis of Safe Reinforcement Learning via Inverse Constrained Reinforcement Learning</div>
<div class="meta-line">Authors: Jialiang Fan, Shixiong Jiang, Mengyu Liu, Fanxin Kong</div>
<div class="meta-line">First: 2026-02-18T15:43:36+00:00 · Latest: 2026-02-18T15:43:36+00:00</div>
<div class="meta-line">Comments: 12 pages, 6 figures, supplementary material included</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16543v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16543v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safe reinforcement learning (Safe RL) aims to ensure policy performance while satisfying safety constraints. However, most existing Safe RL methods assume benign environments, making them vulnerable to adversarial perturbations commonly encountered in real-world settings. In addition, existing gradient-based adversarial attacks typically require access to the policy&#x27;s gradient information, which is often impractical in real-world scenarios. To address these challenges, we propose an adversarial attack framework to reveal vulnerabilities of Safe RL policies. Using expert demonstrations and black-box environment interaction, our framework learns a constraint model and a surrogate (learner) policy, enabling gradient-based attack optimization without requiring the victim policy&#x27;s internal gradients or the ground-truth safety constraints. We further provide theoretical analysis establishing feasibility and deriving perturbation bounds. Experiments on multiple Safe RL benchmarks demonstrate the effectiveness of our approach under limited privileged access.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于逆向约束强化学习的强化学习安全性漏洞分析</div>
<div class="mono" style="margin-top:8px">安全强化学习旨在确保策略性能的同时满足安全约束。然而，现有方法多假设环境友好，使其易受现实场景中常见对抗性扰动的影响。现有基于梯度的对抗攻击通常需获取策略梯度信息，这在现实中往往不切实际。为此，我们提出一种对抗攻击框架以揭示安全强化学习策略的脆弱性。通过专家示范和黑盒环境交互，本框架学习约束模型与代理策略，可在无需受害者策略内部梯度或真实安全约束的情况下实现梯度攻击优化。我们进一步通过理论分析论证可行性并推导扰动边界。在多个安全强化学习基准上的实验表明，本方法在有限权限访问下具有显著有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the vulnerability of Safe Reinforcement Learning (Safe RL) policies, which are typically designed for benign environments, to adversarial perturbations in real-world settings. The authors propose an adversarial attack framework that uses expert demonstrations and black-box environment interactions to learn a constraint model and a surrogate policy, enabling gradient-based attack optimization without needing the victim policy&#x27;s internal gradients or ground-truth safety constraints. Experimental results on multiple Safe RL benchmarks demonstrate the effectiveness of this approach in revealing policy vulnerabilities under limited access, supported by theoretical analysis on feasibility and perturbation bounds.</div>
<div class="mono" style="margin-top:8px">本文针对安全强化学习策略在现实对抗环境中易受攻击的脆弱性问题展开研究，这些策略通常假设环境是良性的。作者提出了一种对抗攻击框架，利用专家演示和黑盒环境交互学习约束模型和代理策略，从而在不依赖受害者策略内部梯度或真实安全约束的情况下实现基于梯度的攻击优化。在多个安全强化学习基准测试上的实验结果表明，该方法在有限访问权限下能有效揭示策略漏洞，并辅以关于可行性和扰动界限的理论分析支持。</div>
</details>
</div>
<div class="card">
<div class="title">Capacity-constrained demand response in smart grids using deep reinforcement learning</div>
<div class="meta-line">Authors: Shafagh Abband Pashaki, Sepehr Maleki, Amir Badiee</div>
<div class="meta-line">First: 2026-02-18T15:13:07+00:00 · Latest: 2026-02-18T15:13:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16525v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16525v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a capacity-constrained incentive-based demand response approach for residential smart grids. It aims to maintain electricity grid capacity limits and prevent congestion by financially incentivising end users to reduce or shift their energy consumption. The proposed framework adopts a hierarchical architecture in which a service provider adjusts hourly incentive rates based on wholesale electricity prices and aggregated residential load. The financial interests of both the service provider and end users are explicitly considered. A deep reinforcement learning approach is employed to learn optimal real-time incentive rates under explicit capacity constraints. Heterogeneous user preferences are modelled through appliance-level home energy management systems and dissatisfaction costs. Using real-world residential electricity consumption and price data from three households, simulation results show that the proposed approach effectively reduces peak demand and smooths the aggregated load profile. This leads to an approximately 22.82% reduction in the peak-to-average ratio compared to the no-demand-response case.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度强化学习的智能电网容量约束需求响应</div>
<div class="mono" style="margin-top:8px">本文提出一种面向居民智能电网的容量约束激励型需求响应方法，旨在通过经济激励引导终端用户削减或转移用电负荷，以维持电网容量限制并预防拥堵。该框架采用分层架构：服务商根据批发电价和居民总负荷动态调整小时级激励费率，并同时兼顾服务商与终端用户的经济利益。研究采用深度强化学习方法，在明确容量约束下学习最优实时激励费率，并通过设备级家庭能源管理系统与用户不满成本建模异质性用户偏好。基于三户家庭的实际用电与电价数据进行仿真，结果表明该方法能有效降低峰值需求、平滑总负荷曲线，与无需求响应情况相比，峰均比降低约22.82%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to maintain grid capacity and prevent congestion in residential smart grids, this paper proposes a capacity-constrained, incentive-based demand response framework. The method employs a hierarchical architecture where a service provider sets hourly incentives, using deep reinforcement learning to optimize these rates under explicit capacity constraints while modeling user preferences via appliance-level management and dissatisfaction costs. Experimental simulations with real-world data from three households demonstrate that the approach effectively reduces peak demand and smooths the aggregated load, achieving an approximately 22.82% reduction in the peak-to-average ratio compared to scenarios without demand response.</div>
<div class="mono" style="margin-top:8px">本文旨在维持住宅智能电网的容量限制并防止拥塞，提出了一种基于容量约束的激励型需求响应方法。该方法采用分层架构，服务提供商根据批发电价和总住宅负荷调整每小时激励费率，并利用深度强化学习在明确容量约束下学习最优实时激励率，同时通过设备级家庭能源管理系统和用户不满意成本来建模异构用户偏好。基于三个家庭的实际用电和价格数据进行仿真，结果表明该方法有效降低了峰值需求并平滑了总负荷曲线，与无需求响应的情况相比，峰均比降低了约22.82%。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning for Parameterized Quantum State Preparation: A Comparative Study</div>
<div class="meta-line">Authors: Gerhard Stenzel, Isabella Debelic, Michael Kölle, Tobias Rohe, Leo Sünkel, Julian Hager, Claudia Linnhoff-Popien</div>
<div class="meta-line">First: 2026-02-18T15:10:43+00:00 · Latest: 2026-02-18T15:10:43+00:00</div>
<div class="meta-line">Comments: Extended version of a short paper to be published at ICAART 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16523v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16523v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We extend directed quantum circuit synthesis (DQCS) with reinforcement learning from purely discrete gate selection to parameterized quantum state preparation with continuous single-qubit rotations \(R_x\), \(R_y\), and \(R_z\). We compare two training regimes: a one-stage agent that jointly selects the gate type, the affected qubit(s), and the rotation angle; and a two-stage variant that first proposes a discrete circuit and subsequently optimizes the rotation angles with Adam using parameter-shift gradients. Using Gymnasium and PennyLane, we evaluate Proximal Policy Optimization (PPO) and Advantage Actor--Critic (A2C) on systems comprising two to ten qubits and on targets of increasing complexity with \(λ\) ranging from one to five. Whereas A2C does not learn effective policies in this setting, PPO succeeds under stable hyperparameters (one-stage: learning rate approximately \(5\times10^{-4}\) with a self-fidelity-error threshold of 0.01; two-stage: learning rate approximately \(10^{-4}\)). Both approaches reliably reconstruct computational basis states (between 83\% and 99\% success) and Bell states (between 61\% and 77\% success). However, scalability saturates for \(λ\) of approximately three to four and does not extend to ten-qubit targets even at \(λ=2\). The two-stage method offers only marginal accuracy gains while requiring around three times the runtime. For practicality under a fixed compute budget, we therefore recommend the one-stage PPO policy, provide explicit synthesized circuits, and contrast with a classical variational baseline to outline avenues for improved scalability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>参数化量子态制备的强化学习比较研究</div>
<div class="mono" style="margin-top:8px">我们将基于强化学习的定向量子电路合成方法从纯离散门选择扩展到包含连续单量子比特旋转（Rx、Ry、Rz）的参数化量子态制备。比较两种训练模式：单阶段智能体联合选择门类型、作用量子比特及旋转角度；双阶段变体首先生成离散电路，再利用参数平移梯度通过Adam优化旋转角度。基于Gymnasium和PennyLane平台，我们在2至10量子比特系统上评估近端策略优化与优势演员-评论家算法，目标复杂度λ取值1至5。A2C算法在此场景中未能习得有效策略，而PPO在稳定超参数下取得成功（单阶段：学习率约5×10⁻⁴，自保真误差阈值0.01；双阶段：学习率约10⁻⁴）。两种方法均能可靠重构计算基态（成功率83%-99%）和贝尔态（成功率61%-77%），但可扩展性在λ≈3-4时达到饱和，即使λ=2也无法扩展至10量子比特目标。双阶段方法仅带来边际精度提升，却需约三倍运行时间。基于固定计算资源的实用性考量，我们推荐单阶段PPO策略，提供显式合成电路，并通过与经典变分基线的对比提出改进可扩展性的路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study extends directed quantum circuit synthesis with reinforcement learning to handle continuous rotation parameters for quantum state preparation, motivated by the need to synthesize parameterized quantum circuits efficiently. The method compares a one-stage agent that jointly selects gate types, qubits, and angles with a two-stage variant that first proposes a discrete circuit and then optimizes angles using Adam, employing Proximal Policy Optimization and Advantage Actor-Critic algorithms via Gymnasium and PennyLane. Experimental results on systems of two to ten qubits show that PPO succeeds with stable hyperparameters, achieving high success rates for computational basis states (83-99%) and Bell states (61-77%), but scalability saturates at complexity levels around λ=3-4 and fails on ten-qubit targets, with the two-stage method offering only marginal accuracy gains at triple the runtime, leading to a recommendation for the one-stage PPO policy for practical use.</div>
<div class="mono" style="margin-top:8px">本研究将强化学习应用于定向量子电路合成，扩展至包含连续旋转参数的量子态制备，旨在高效合成参数化量子电路。方法比较了两种训练策略：一阶段智能体联合选择门类型、作用量子位和旋转角度，以及两阶段变体先提出离散电路再用Adam优化角度，使用Proximal Policy Optimization和Advantage Actor-Critic算法在Gymnasium和PennyLane平台上实现。实验在二至十量子位系统上进行，结果显示PPO在稳定超参数下成功，对计算基态（83-99%）和贝尔态（61-77%）实现了较高成功率，但可扩展性在复杂度λ约3-4时饱和，且无法处理十量子位目标，两阶段方法仅带来边际精度提升而运行时增加三倍，因此推荐一阶段PPO策略以提升实用性。</div>
</details>
</div>
<div class="card">
<div class="title">Shrinking the Variance: Shrinkage Baselines for Reinforcement Learning with Verifiable Rewards</div>
<div class="meta-line">Authors: Guanning Zeng, Zhaoyi Zhou, Daman Arora, Andrea Zanette</div>
<div class="meta-line">First: 2025-11-05T18:43:15+00:00 · Latest: 2026-02-18T15:08:01+00:00</div>
<div class="meta-line">Comments: Preprint. Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.03710v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.03710v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful paradigm for post-training large reasoning models (LRMs) using policy-gradient methods such as GRPO. To stabilize training, these methods typically center trajectory rewards by subtracting the empirical mean reward for each prompt. Statistically, this centering acts as a control variate (baseline), reducing the variance of the policy-gradient estimator. In practice, the mean reward is estimated using per-prompt empirical averages computed from the generations for each prompt in a batch. Motivated by Stein&#x27;s paradox, we propose shrinkage estimators that combine per-prompt and across-prompt means to improve per-prompt mean estimation accuracy, especially in the low-generation regime typical of RLVR. Theoretically, we construct a shrinkage-based baseline that provably yields lower-variance policy-gradient estimators across algorithms. Our baseline is a drop-in replacement for standard per-prompt mean baselines and requires no additional hyperparameters or computation. Empirically, shrinkage baselines consistently outperform empirical-mean baselines, producing lower-variance gradient updates and improved training stability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>缩小方差：基于可验证奖励的强化学习收缩基线方法</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）已成为使用GRPO等策略梯度方法对大型推理模型进行后训练的强大范式。为稳定训练，这些方法通常通过减去每个提示的实证平均奖励来对轨迹奖励进行中心化处理。从统计学角度看，这种中心化相当于控制变量（基线），能降低策略梯度估计量的方差。实践中，平均奖励通过批量中每个提示生成结果的逐提示经验平均值进行估计。受斯坦悖论启发，我们提出收缩估计量，结合逐提示与跨提示均值以提高逐提示均值估计精度，尤其适用于RLVR典型的低生成量场景。理论上，我们构建了一种基于收缩的基线，可证明在不同算法中产生方差更低的策略梯度估计量。该基线可直接替代标准的逐提示均值基线，无需额外超参数或计算开销。实证表明，收缩基线始终优于经验均值基线，能产生方差更低的梯度更新并提升训练稳定性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the high variance in policy-gradient estimators used in Reinforcement Learning with Verifiable Rewards (RLVR) for training large reasoning models. Motivated by Stein&#x27;s paradox, the authors propose a novel shrinkage baseline method that combines per-prompt and across-prompt mean reward estimates to improve the accuracy of baseline estimation, particularly when few generations are available per prompt. Theoretically, this shrinkage-based baseline provably reduces the variance of the policy-gradient estimator. Empirically, it outperforms standard empirical-mean baselines, leading to lower-variance gradient updates and enhanced training stability, all without requiring extra hyperparameters or computational overhead.</div>
<div class="mono" style="margin-top:8px">本文针对使用可验证奖励的强化学习训练大型推理模型时，策略梯度估计器方差高的问题展开研究。受斯坦悖论启发，作者提出了一种新颖的收缩基线方法，该方法结合了每个提示内和跨提示的平均奖励估计，以提高基线估计的准确性，尤其在每个提示生成样本较少的典型场景中。理论上，这种基于收缩的基线被证明能降低策略梯度估计器的方差。实验结果表明，该基线 consistently 优于标准的经验均值基线，实现了更低方差的梯度更新和更好的训练稳定性，且无需额外的超参数或计算成本。</div>
</details>
</div>
<div class="card">
<div class="title">STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens</div>
<div class="meta-line">Authors: Shiqi Liu, Zeyu He, Guojian Zhan, Letian Tao, Zhilong Zheng, Jiang Wu, Yinuo Wang, Yang Guan, Kehua Sheng, Bo Zhang, Keqiang Li, Jingliang Duan, Shengbo Eben Li</div>
<div class="meta-line">First: 2026-02-17T14:46:48+00:00 · Latest: 2026-02-18T14:13:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15620v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.15620v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often suffer from late-stage performance collapse, leading to degraded reasoning quality and unstable training. Our analysis shows that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. We find that training instability can be caused by a tiny fraction of tokens, approximately 0.01\%, which we term \emph{spurious tokens}. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. To mitigate this instability, we design S2T (silencing spurious tokens) mechanism to efficiently identify spurious tokens through characteristic signals with low probability, low entropy, and positive advantage, and then to suppress their gradient perturbations during optimization. Incorporating this mechanism into a group-based objective, we propose Spurious-Token-Aware Policy Optimization (STAPO), which promotes stable and effective large-scale model refinement. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\% ($ρ_{\mathrm{T}}$=1.0, top-p=1.0) and 3.69\% ($ρ_{\mathrm{T}}$=0.7, top-p=0.9) over GRPO, 20-Entropy and JustRL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>STAPO：通过抑制罕见伪标记实现大语言模型强化学习的稳定化</div>
<div class="mono" style="margin-top:8px">强化学习显著提升了大语言模型的推理能力，但现有RL微调方法严重依赖熵正则化和权重调整等启发式技术以维持稳定性。实践中常出现后期性能崩溃，导致推理质量下降和训练不稳定。分析表明，RL中逐标记策略梯度的大小与标记概率及局部策略熵呈负相关。训练不稳定性可能由约0.01%的极少数标记引发，我们称之为伪标记。当此类标记出现在正确响应中时，对推理结果贡献甚微却继承完整的序列级奖励，导致梯度更新异常放大。为缓解此问题，我们设计S2T机制，通过低概率、低熵与正优势值的特征信号高效识别伪标记，并在优化过程中抑制其梯度扰动。将该机制融入分组目标函数，提出伪标记感知策略优化方法STAPO，促进大规模模型的稳定高效精调。在基于Qwen 1.7B、8B和14B基础模型的六个数学推理基准测试中，STAPO始终展现更优的熵稳定性，相比GRPO、20-Entropy和JustRL平均性能提升7.13%（$ρ_{\mathrm{T}}$=1.0, top-p=1.0）和3.69%（$ρ_{\mathrm{T}}$=0.7, top-p=0.9）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the instability in reinforcement learning fine-tuning for large language models, which often suffers from late-stage performance collapse due to rare spurious tokens that cause abnormally amplified gradient updates. The authors propose STAPO, a method that identifies spurious tokens based on low probability, low entropy, and positive advantage signals, and suppresses their gradient perturbations through a silencing mechanism integrated into a group-based objective. Experiments on six mathematical reasoning benchmarks with Qwen models show STAPO improves training stability and achieves average performance gains of 7.13% and 3.69% over baseline methods.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型强化学习微调中的不稳定性问题展开研究，该问题常因罕见伪标记引发梯度异常放大，导致后期性能崩溃。作者提出STAPO方法，通过低概率、低熵和正优势信号识别伪标记，并采用抑制机制在分组优化目标中消除其梯度扰动。在六个数学推理基准上使用Qwen模型的实验表明，STAPO能显著提升训练稳定性，相比基线方法平均性能提高7.13%和3.69%。</div>
</details>
</div>
<div class="card">
<div class="title">Causally-Guided Automated Feature Engineering with Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Arun Vignesh Malarkkan, Wangyang Ying, Yanjie Fu</div>
<div class="meta-line">First: 2026-02-18T13:12:11+00:00 · Latest: 2026-02-18T13:12:11+00:00</div>
<div class="meta-line">Comments: 11 Pages, References and Appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16435v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16435v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated feature engineering (AFE) enables AI systems to autonomously construct high-utility representations from raw tabular data. However, existing AFE methods rely on statistical heuristics, yielding brittle features that fail under distribution shift. We introduce CAFE, a framework that reformulates AFE as a causally-guided sequential decision process, bridging causal discovery with reinforcement learning-driven feature construction. Phase I learns a sparse directed acyclic graph over features and the target to obtain soft causal priors, grouping features as direct, indirect, or other based on their causal influence with respect to the target. Phase II uses a cascading multi-agent deep Q-learning architecture to select causal groups and transformation operators, with hierarchical reward shaping and causal group-level exploration strategies that favor causally plausible transformations while controlling feature complexity. Across 15 public benchmarks (classification with macro-F1; regression with inverse relative absolute error), CAFE achieves up to 7% improvement over strong AFE baselines, reduces episodes-to-convergence, and delivers competitive time-to-target. Under controlled covariate shifts, CAFE reduces performance drop by ~4x relative to a non-causal multi-agent baseline, and produces more compact feature sets with more stable post-hoc attributions. These findings underscore that causal structure, used as a soft inductive prior rather than a rigid constraint, can substantially improve the robustness and efficiency of automated feature engineering.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于因果引导与多智能体强化学习的自动化特征工程</div>
<div class="mono" style="margin-top:8px">自动化特征工程（AFE）使AI系统能够从原始表格数据中自主构建高效用表征。然而，现有AFE方法依赖统计启发式，产生的特征在分布偏移下表现脆弱。我们提出CAFE框架，将AFE重构为因果引导的序列决策过程，融合因果发现与强化学习驱动的特征构建。第一阶段通过特征与目标变量学习稀疏有向无环图，获取软因果先验，根据特征对目标的因果影响将其分组为直接、间接或其他类型。第二阶段采用级联多智能体深度Q学习架构选择因果组和变换算子，通过分层奖励塑造与因果组级探索策略，优先选择因果合理的变换并控制特征复杂度。在15个公开基准测试（分类任务使用宏F1；回归任务使用逆相对绝对误差）中，CAFE较主流AFE基线提升达7%，减少收敛所需训练轮次，并实现具有竞争力的目标达成时间。在受控协变量偏移下，CAFE相比非因果多智能体基线将性能下降降低约4倍，并生成更紧凑的特征集与更稳定的后归因结果。这些发现表明，将因果结构作为软归纳先验而非刚性约束，可显著提升自动化特征工程的鲁棒性与效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the brittleness of existing automated feature engineering (AFE) methods under distribution shift, this paper introduces CAFE, a framework that reformulates AFE as a causally-guided sequential decision process. The method first learns a sparse directed acyclic graph to obtain soft causal priors, grouping features by their causal influence, and then employs a cascading multi-agent deep Q-learning architecture to select causal groups and transformation operators with hierarchical reward shaping and group-level exploration. Experimental results on 15 benchmarks show CAFE achieves up to 7% improvement over strong AFE baselines, reduces convergence episodes, and under covariate shift cuts performance drop by about 4x compared to a non-causal baseline while producing more compact and stable feature sets.</div>
<div class="mono" style="margin-top:8px">针对现有自动化特征工程方法在分布偏移下表现脆弱的问题，本文提出了CAFE框架，将自动化特征工程重新定义为因果引导的序列决策过程。该方法首先学习稀疏有向无环图以获得软因果先验，根据特征对目标的因果影响进行分组，然后采用级联多智能体深度Q学习架构，通过分层奖励塑造和组级探索策略选择因果组和变换算子。在15个公开基准测试上的实验结果表明，CAFE相比强基线方法性能提升最高达7%，收敛所需回合数减少，且在协变量偏移下性能下降比非因果多智能体基线降低约4倍，同时生成更紧凑、特征归因更稳定的特征集。</div>
</details>
</div>
<div class="card">
<div class="title">Model-Agnostic Dynamic Feature Selection with Uncertainty Quantification</div>
<div class="meta-line">Authors: Javier Fumanal-Idocin, Raquel Fernandez-Peralta, Javier Andreu-Perez</div>
<div class="meta-line">First: 2025-08-04T16:21:43+00:00 · Latest: 2026-02-18T13:04:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.02566v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.02566v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dynamic feature selection (DFS) addresses budget constraints in decision-making by sequentially acquiring features for each instance, making it appealing for resource-limited scenarios. However, existing DFS methods require models specifically designed for the sequential acquisition setting, limiting compatibility with models already deployed in practice. Furthermore, they provide limited uncertainty quantification, undermining trust in high-stakes decisions. In this work, we show that DFS introduces new uncertainty sources compared to the static setting. We formalise how model adaptation to feature subsets induces epistemic uncertainty, how standard imputation strategies bias aleatoric uncertainty estimation, and why predictive confidence fails to discriminate between good and bad selection policies. We also propose a model-agnostic DFS framework compatible with pre-trained classifiers, including interpretable-by-design models, through efficient subset reparametrization strategies. Empirical evaluation on tabular and image datasets demonstrates competitive accuracy against state-of-the-art greedy and reinforcement learning-based DFS methods with both neural and rule-based classifiers. We further show that the identified uncertainty sources persist across most existing approaches, highlighting the need for uncertainty-aware DFS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>模型无关的动态特征选择与不确定性量化</div>
<div class="mono" style="margin-top:8px">动态特征选择（DFS）通过为每个实例顺序获取特征来解决决策中的预算约束，适用于资源受限场景。然而，现有DFS方法需要专门为顺序获取设置设计的模型，限制了与实践中已部署模型的兼容性。此外，它们提供的不确定性量化有限，削弱了对高风险决策的信任。本研究表明，与静态设置相比，DFS引入了新的不确定性来源：我们形式化分析了模型适应特征子集如何引发认知不确定性，标准插补策略如何偏置偶然不确定性估计，以及预测置信度为何无法区分优劣选择策略。同时，我们提出一种模型无关的DFS框架，通过高效子集重参数化策略兼容预训练分类器（包括可解释设计模型）。在表格和图像数据集上的实证评估表明，该框架在使用神经网络和基于规则的分类器时，相较于最先进的贪婪方法和基于强化学习的DFS方法具有竞争力。我们进一步证明，所识别的不确定性来源在多数现有方法中持续存在，凸显了不确定性感知DFS的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of existing dynamic feature selection (DFS) methods, which are often model-specific and lack robust uncertainty quantification, hindering their trustworthiness in high-stakes applications. The authors propose a model-agnostic DFS framework that is compatible with pre-trained classifiers, including interpretable models, using efficient subset reparametrization strategies. Experimental results on tabular and image datasets show that the method achieves competitive accuracy compared to state-of-the-art greedy and reinforcement learning-based DFS approaches, while also demonstrating that uncertainty issues persist across most existing methods, underscoring the need for uncertainty-aware DFS.</div>
<div class="mono" style="margin-top:8px">本文针对现有动态特征选择（DFS）方法的局限性展开研究，这些方法通常依赖于特定模型且缺乏可靠的不确定性量化，影响了其在关键决策中的可信度。作者提出了一种模型无关的DFS框架，通过高效的子集重参数化策略，使其能够兼容预训练的分类器，包括可解释模型。在表格和图像数据集上的实验结果表明，该方法相比基于贪心或强化学习的最先进DFS方法具有竞争力的准确率，同时揭示了不确定性问题在多数现有方法中普遍存在，强调了发展不确定性感知DFS的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Reinforcement Learning: Fast and Scalable Quantum Circuit Synthesis</div>
<div class="meta-line">Authors: Lukas Theißinger, Thore Gerlach, David Berghaus, Christian Bauckhage</div>
<div class="meta-line">First: 2026-02-16T19:43:43+00:00 · Latest: 2026-02-18T09:41:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15146v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.15146v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quantum unitary synthesis addresses the problem of translating abstract quantum algorithms into sequences of hardware-executable quantum gates. Solving this task exactly is infeasible in general due to the exponential growth of the underlying combinatorial search space. Existing approaches suffer from misaligned optimization objectives, substantial training costs and limited generalization across different qubit counts. We mitigate these limitations by using supervised learning to approximate the minimum description length of residual unitaries and combining this estimate with stochastic beam search to identify near optimal gate sequences. Our method relies on a lightweight model with zero-shot generalization, substantially reducing training overhead compared to prior baselines. Across multiple benchmarks, we achieve faster wall-clock synthesis times while exceeding state-of-the-art methods in terms of success rate for complex circuits.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越强化学习：快速可扩展的量子电路合成方法</div>
<div class="mono" style="margin-top:8px">量子幺正合成旨在将抽象量子算法转化为硬件可执行的量子门序列。由于底层组合搜索空间呈指数级增长，精确求解该任务通常不可行。现有方法存在优化目标失准、训练成本高昂及对不同量子比特数泛化能力有限等问题。我们通过监督学习逼近剩余幺正算符的最小描述长度，并将该估计与随机束搜索相结合以识别近似最优的门序列，从而缓解这些局限。该方法依赖具备零样本泛化能力的轻量模型，相比现有基线显著降低训练开销。在多项基准测试中，我们实现了更快的实际合成速度，同时在复杂电路成功率方面超越现有最优方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of quantum unitary synthesis, where the exponential search space and misaligned objectives of existing methods lead to high training costs and poor generalization. The authors propose a method that uses supervised learning to approximate the minimum description length of residual unitaries and combines this with stochastic beam search to find near-optimal gate sequences efficiently. Experimental results show that their lightweight model achieves faster synthesis times and higher success rates for complex circuits compared to state-of-the-art approaches, with zero-shot generalization across different qubit counts.</div>
<div class="mono" style="margin-top:8px">本文针对量子幺正合成中的挑战展开研究，现有方法因指数级搜索空间和目标不匹配导致训练成本高且泛化能力差。作者提出一种方法，利用监督学习近似残差幺正的最小描述长度，并结合随机束搜索高效寻找近似最优的门序列。实验结果表明，其轻量级模型相比现有先进方法，在复杂电路上实现了更快的合成时间和更高的成功率，并能零样本泛化到不同量子比特数。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-agent cooperation through in-context co-player inference</div>
<div class="meta-line">Authors: Marissa A. Weis, Maciej Wołczyk, Rajai Nasser, Rif A. Saurous, Blaise Agüera y Arcas, João Sacramento, Alexander Meulemans</div>
<div class="meta-line">First: 2026-02-18T09:31:43+00:00 · Latest: 2026-02-18T09:31:43+00:00</div>
<div class="meta-line">Comments: 26 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16301v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16301v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between &quot;learning-aware&quot; agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between &quot;naive learners&quot; updating on fast timescales and &quot;meta-learners&quot; observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent&#x27;s in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behaviors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于上下文共玩家推断的多智能体协作</div>
<div class="mono" style="margin-top:8px">在自利智能体间实现协作仍是多智能体强化学习的核心挑战。近期研究表明，通过考虑并塑造共玩家的学习动态，可在‘学习感知型’智能体间诱导相互协作。然而现有方法通常依赖对共玩家学习规则的硬编码假设（常存在矛盾），或强制划分‘在快速时间尺度更新的朴素学习者’与‘观察这些更新的元学习者’。本文证明，序列模型的上下文学习能力可实现共玩家学习感知，无需硬编码假设或显式时间尺度分离。通过让序列模型智能体与多样化共玩家分布进行训练，可自然诱导出上下文最优响应策略，在快速情节内时间尺度上有效发挥学习算法功能。研究发现，先前工作中识别的协作机制——易受勒索的脆弱性驱动相互塑造——在此设定中自然涌现：上下文适应使智能体易受勒索，而由此产生的相互施压塑造对手的上下文学习动态，最终演化为协作行为的学习。结果表明，序列模型的标准分散式强化学习结合共玩家多样性，为学习协作行为提供了可扩展路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of fostering cooperation among self-interested agents in multi-agent reinforcement learning, moving beyond prior methods that rely on hardcoded assumptions about opponent learning rules or strict timescale separation. The authors propose leveraging the in-context learning capabilities of sequence models, training agents against a diverse distribution of co-players to naturally induce in-context best-response strategies that function as fast-timescale learning algorithms. Experimental results show that this approach naturally leads to the emergence of cooperative behavior, as in-context adaptation makes agents vulnerable to extortion, and mutual pressure to shape opponents&#x27; learning dynamics resolves into cooperation, suggesting a scalable path to learning cooperative behaviors through decentralized training and co-player diversity.</div>
<div class="mono" style="margin-top:8px">本文针对多智能体强化学习中自利智能体间难以合作的问题，提出了一种新方法，避免了以往依赖对对手学习规则的硬编码假设或严格时间尺度分离的局限。该方法利用序列模型的上下文学习能力，通过让智能体与多样化的对手分布进行训练，自然诱导出作为快速时间尺度学习算法的上下文最佳响应策略。实验结果表明，这种设置下会自然涌现合作行为：上下文适应使智能体易受勒索，而相互塑造对手学习动态的压力最终促成了合作行为，这表明通过分散式训练和对手多样性，为学习合作行为提供了一条可扩展的路径。</div>
</details>
</div>
<div class="card">
<div class="title">VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models</div>
<div class="meta-line">Authors: Yuchen Yan, Jin Jiang, Zhenbang Ren, Yijun Li, Xudong Cai, Yang Liu, Xin Xu, Mengdi Zhang, Jian Shao, Yongliang Shen, Jun Xiao, Yueting Zhuang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-05-21T17:54:43+00:00 · Latest: 2026-02-18T08:45:15+00:00</div>
<div class="meta-line">Comments: ICLR 2026: https://openreview.net/forum?id=JfsjGmuFxz Project Page: https://zju-real.github.io/VerifyBench Dataset: https://huggingface.co/datasets/ZJU-REAL/VerifyBench Code: https://github.com/ZJU-REAL/VerifyBench</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.15801v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.15801v4">PDF</a> · <a href="https://huggingface.co/datasets/ZJU-REAL/VerifyBench">Code1</a> · <a href="https://github.com/ZJU-REAL/VerifyBench">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a> · <a href="https://zju-real.github.io/VerifyBench">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models such as OpenAI o1 and DeepSeek-R1 have demonstrated remarkable performance in complex reasoning tasks. A critical component of their training is the incorporation of reference-based reward systems within reinforcement learning (RL), where model outputs are evaluated against ground truth references. However, existing reward benchmarks focus on preference comparisons between responses rather than evaluating verification against ground truth references, leaving a critical gap in our ability to evaluate verification systems used in reasoning model training. In this paper, we introduce VerifyBench and its challenging variant VerifyBench-Hard, two benchmarks specifically designed to assess reference-based reward systems. These benchmarks are constructed through meticulous data collection and curation, followed by careful human annotation to ensure high quality. Our comprehensive evaluation reveals that while larger model-based verifiers show promise on standard cases, all current systems demonstrate substantial room for improvement on challenging instances. Through systematic analysis of performance patterns across reasoning tasks and error categories, we provide insights for advancing reference-based reward systems. These benchmarks establish a standardized framework for improving verification accuracy, ultimately enhancing reasoning capabilities in models trained via RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VerifyBench：面向大语言模型的基于参考的奖励系统基准测试</div>
<div class="mono" style="margin-top:8px">OpenAI o1 和 DeepSeek-R1 等大型推理模型在复杂推理任务中展现出卓越性能。其训练的关键环节是在强化学习（RL）中引入基于参考的奖励系统，即依据真实参考答案评估模型输出。然而，现有奖励基准多关注回答间的偏好比较，而非基于真实参考的验证评估，导致推理模型训练中验证系统的评估存在关键空白。本文提出 VerifyBench 及其挑战性变体 VerifyBench-Hard，这两个基准专门用于评估基于参考的奖励系统。通过精细的数据收集、整理及人工标注确保数据高质量。综合评估表明：基于更大模型的验证器在标准案例中表现良好，但所有现有系统在挑战性实例上仍有显著改进空间。通过对推理任务和错误类别的系统性绩效模式分析，我们为推进基于参考的奖励系统提供洞见。这些基准建立了提升验证准确度的标准化框架，最终增强基于 RL 训练的模型推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces VerifyBench and VerifyBench-Hard to address the lack of benchmarks for evaluating reference-based reward systems in large language models, which are crucial for training advanced reasoning models like OpenAI o1 and DeepSeek-R1. The method involves meticulous data collection, curation, and human annotation to create high-quality benchmarks that assess how well verifiers compare model outputs against ground truth references. Experimental results show that while larger model-based verifiers perform adequately on standard tasks, all existing systems struggle significantly with challenging instances, highlighting substantial room for improvement and providing insights for advancing verification accuracy in reinforcement learning training.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型中基于参考的奖励系统缺乏评估基准的问题，引入了VerifyBench及其挑战性变体VerifyBench-Hard，这些系统对于训练如OpenAI o1和DeepSeek-R1等高级推理模型至关重要。方法通过细致的数据收集、整理和人工标注，构建了高质量基准，用于评估验证器将模型输出与真实参考答案对比的能力。实验结果表明，尽管基于更大模型的验证器在标准任务上表现尚可，但所有现有系统在挑战性实例上均存在显著不足，揭示了巨大的改进空间，并为提升强化学习训练中的验证准确性提供了见解。</div>
</details>
</div>
<div class="card">
<div class="title">MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision</div>
<div class="meta-line">Authors: Zhonghao Yan, Muxi Diao, Yuxuan Yang, Ruoyan Jing, Jiayuan Xu, Kaizhou Zhang, Lele Yang, Yanxi Liu, Kongming Liang, Zhanyu Ma</div>
<div class="meta-line">First: 2025-08-11T16:59:06+00:00 · Latest: 2026-02-18T08:36:10+00:00</div>
<div class="meta-line">Comments: AAAI2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.08177v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.08177v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurately grounding regions of interest (ROIs) is critical for diagnosis and treatment planning in medical imaging. While multimodal large language models (MLLMs) combine visual perception with natural language, current medical-grounding pipelines still rely on supervised fine-tuning with explicit spatial hints, making them ill-equipped to handle the implicit queries common in clinical practice. This work makes three core contributions. We first define Unified Medical Reasoning Grounding (UMRG), a novel vision-language task that demands clinical reasoning and pixel-level grounding. Second, we release U-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside implicit clinical queries and reasoning traces, spanning 10 modalities, 15 super-categories, and 108 specific categories. Finally, we introduce MedReasoner, a modular framework that distinctly separates reasoning from segmentation: an MLLM reasoner is optimized with reinforcement learning, while a frozen segmentation expert converts spatial prompts into masks, with alignment achieved through format and accuracy rewards. MedReasoner achieves state-of-the-art performance on U-MRG-14K and demonstrates strong generalization to unseen clinical queries, underscoring the significant promise of reinforcement learning for interpretable medical grounding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MedReasoner：强化学习驱动从临床思维到像素级精度的推理定位</div>
<div class="mono" style="margin-top:8px">在医学影像中，准确定位感兴趣区域（ROI）对诊断和治疗规划至关重要。尽管多模态大语言模型（MLLM）融合了视觉感知与自然语言，现有医学定位流程仍依赖带有显式空间提示的监督微调，难以处理临床实践中常见的隐式查询。本研究作出三项核心贡献：首先定义统一医学推理定位（UMRG）这一需要临床推理与像素级定位的新颖视觉-语言任务；其次发布U-MRG-14K数据集，包含1.4万个样本，涵盖像素级掩码、隐式临床查询与推理轨迹，涉及10种模态、15个超类别和108个具体类别；最后提出MedReasoner模块化框架，明确分离推理与分割：通过强化学习优化MLLM推理器，同时利用冻结的分割专家将空间提示转换为掩码，并通过格式与精度奖励实现对齐。MedReasoner在U-MRG-14K上达到最先进性能，并对未见临床查询展现出强大泛化能力，彰显强化学习在可解释医学定位中的巨大潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of accurately grounding regions of interest in medical imaging based on implicit clinical queries, which current supervised methods struggle with. The authors propose a novel Unified Medical Reasoning Grounding (UMRG) task, introduce the U-MRG-14K dataset with 14K samples of implicit queries and pixel masks, and present MedReasoner, a modular framework that separates reasoning from segmentation. In MedReasoner, a multimodal large language model reasoner is optimized via reinforcement learning with format and accuracy rewards, while a frozen segmentation expert generates masks from spatial prompts. Experiments show that MedReasoner achieves state-of-the-art performance on U-MRG-14K and generalizes well to unseen clinical queries, demonstrating the promise of reinforcement learning for interpretable medical grounding.</div>
<div class="mono" style="margin-top:8px">本文针对医学影像中基于隐含临床查询精确标注感兴趣区域的挑战，现有监督方法对此处理不佳。作者提出了一种新颖的统一医学推理标注（UMRG）任务，发布了包含1.4万个隐含查询和像素掩码样本的U-MRG-14K数据集，并介绍了MedReasoner这一模块化框架，该框架将推理与分割分离。在MedReasoner中，通过强化学习优化多模态大语言模型推理器，并配合固定的分割专家从空间提示生成掩码，使用格式和准确性奖励进行对齐。实验表明，MedReasoner在U-MRG-14K上取得了最先进的性能，并能很好地泛化到未见过的临床查询，证明了强化学习在可解释医学标注中的巨大潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Features as Rewards: Scalable Supervision for Open-Ended Tasks via Interpretability</div>
<div class="meta-line">Authors: Aaditya Vikram Prasad, Connor Watts, Jack Merullo, Dhruvil Gala, Owen Lewis, Thomas McGrath, Ekdeep Singh Lubana</div>
<div class="meta-line">First: 2026-02-10T18:33:45+00:00 · Latest: 2026-02-18T07:19:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10067v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.10067v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language models trained on large-scale datasets have been shown to learn features that encode abstract concepts such as factuality or intent. Such features are traditionally used for test-time monitoring or steering. We present an alternative affordance: features as scalable supervision for open-ended tasks. We consider the case of hallucination-reduction as a desirable, yet open-ended behavior and design a reinforcement learning (RL) pipeline, titled RLFR (Reinforcement Learning from Feature Rewards), that uses features as reward functions. Grounded in a novel probing framework that identifies candidate hallucinated claims, our pipeline teaches a model to intervene and correct its completions when it is uncertain of their factuality. Furthermore, the pipeline enables scalable test-time compute, guided once more by our reward features. This end-to-end process operationalized on Gemma-3-12B-IT results in a policy that is 58% less likely to hallucinate compared to the original model (when run in tandem with our probing harness), while preserving performance on standard benchmarks. Taken together, by grounding supervision in the language of features, this paper introduces a novel paradigm in the use of interpretability for learning open-ended tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>特征作为奖励：通过可解释性为开放式任务提供可扩展的监督</div>
<div class="mono" style="margin-top:8px">在大规模数据集上训练的语言模型已被证明能学习编码抽象概念（如事实性或意图）的特征。这些特征传统上用于测试时监控或引导。本文提出一种替代性功能：将特征作为开放式任务的可扩展监督。我们以减少幻觉这一理想但开放的行为为例，设计了一个强化学习（RL）流程——RLFR（基于特征奖励的强化学习），将特征用作奖励函数。基于一个识别候选幻觉主张的新型探测框架，该流程教导模型在不确定生成内容的真实性时进行干预并修正其补全结果。此外，该流程还能通过奖励特征引导实现可扩展的测试时计算。在Gemma-3-12B-IT模型上实施的端到端流程产生了一种策略，与原始模型相比（在与我们的探测框架并行运行时），幻觉概率降低了58%，同时保持了标准基准测试的性能。综上所述，通过以特征语言为基础构建监督机制，本文为利用可解释性学习开放式任务引入了一种新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need for scalable supervision in open-ended tasks, particularly to reduce hallucinations in language models. The method introduces Reinforcement Learning from Feature Rewards (RLFR), which uses interpretable features—such as those encoding factuality—as reward functions within a reinforcement learning pipeline. This approach is grounded in a probing framework that identifies potential hallucinations, enabling the model to intervene and correct uncertain completions. Experimental results on Gemma-3-12B-IT show that the resulting policy reduces hallucination likelihood by 58% compared to the original model, while maintaining performance on standard benchmarks, demonstrating the efficacy of using features as scalable supervision.</div>
<div class="mono" style="margin-top:8px">本文的动机在于为开放式任务提供可扩展的监督方法，特别是减少语言模型中的幻觉问题。方法上提出了基于特征奖励的强化学习（RLFR），利用可解释的特征（如事实性编码）作为强化学习中的奖励函数，并通过一个探测框架识别可能的幻觉主张，使模型能够干预并纠正其不确定的生成内容。实验结果表明，在Gemma-3-12B-IT模型上应用该方法后，策略的幻觉可能性比原始模型降低了58%，同时在标准基准测试上保持了性能，这验证了使用特征作为可扩展监督的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Graphon Mean-Field Subsampling for Cooperative Heterogeneous Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Emile Anand, Richard Hoffmann, Sarah Liaw, Adam Wierman</div>
<div class="meta-line">First: 2026-02-18T05:34:07+00:00 · Latest: 2026-02-18T05:34:07+00:00</div>
<div class="meta-line">Comments: 43 pages, 5 figures, 1 table</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16196v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16196v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Coordinating large populations of interacting agents is a central challenge in multi-agent reinforcement learning (MARL), where the size of the joint state-action space scales exponentially with the number of agents. Mean-field methods alleviate this burden by aggregating agent interactions, but these approaches assume homogeneous interactions. Recent graphon-based frameworks capture heterogeneity, but are computationally expensive as the number of agents grows. Therefore, we introduce $\texttt{GMFS}$, a $\textbf{G}$raphon $\textbf{M}$ean-$\textbf{F}$ield $\textbf{S}$ubsampling framework for scalable cooperative MARL with heterogeneous agent interactions. By subsampling $κ$ agents according to interaction strength, we approximate the graphon-weighted mean-field and learn a policy with sample complexity $\mathrm{poly}(κ)$ and optimality gap $O(1/\sqrtκ)$. We verify our theory with numerical simulations in robotic coordination, showing that $\texttt{GMFS}$ achieves near-optimal performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>图子均值场子采样用于协作异构多智能体强化学习</div>
<div class="mono" style="margin-top:8px">协调大规模交互智能体群体是多智能体强化学习（MARL）的核心挑战，其联合状态-动作空间规模随智能体数量呈指数级增长。均值场方法通过聚合智能体交互来缓解这一负担，但这些方法假设交互是同质的。基于图子的最新框架虽能捕捉异质性，但随着智能体数量增加，计算成本高昂。为此，我们提出$\texttt{GMFS}$——一种面向可扩展协作MARL的$\textbf{图子均值场子采样}$框架，用于处理异构智能体交互。通过根据交互强度对$κ$个智能体进行子采样，我们近似图子加权均值场，并以$\mathrm{poly}(κ)$的样本复杂度和$O(1/\sqrtκ)$的最优性差距学习策略。我们通过机器人协同的数值模拟验证理论，表明$\texttt{GMFS}$能实现接近最优的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of scaling cooperative multi-agent reinforcement learning (MARL) to large, heterogeneous populations, where traditional mean-field methods assume homogeneous interactions and graphon-based approaches become computationally prohibitive. The proposed method, Graphon Mean-Field Subsampling (GMFS), introduces a scalable framework that subsamples a subset of agents based on interaction strength to approximate the graphon-weighted mean-field interactions, thereby reducing complexity. Experimental results in robotic coordination simulations demonstrate that GMFS achieves near-optimal performance with a sample complexity polynomial in the subsample size and an optimality gap that diminishes as the subsample size increases.</div>
<div class="mono" style="margin-top:8px">本文针对大规模异构智能体群体中协同多智能体强化学习（MARL）的可扩展性挑战，传统均值场方法假设同质交互，而基于图论的方法在计算上代价高昂。提出的方法——图论均值场子采样（GMFS），通过根据交互强度对智能体进行子采样，以近似图论加权的均值场交互，从而降低计算复杂度。在机器人协同模拟中的实验结果表明，GMFS能够实现接近最优的性能，其样本复杂度与子采样规模呈多项式关系，且最优性差距随子采样规模增大而减小。</div>
</details>
</div>
<div class="card">
<div class="title">Edge Learning via Federated Split Decision Transformers for Metaverse Resource Allocation</div>
<div class="meta-line">Authors: Fatih Temiz, Shavbo Salehi, Melike Erol-Kantarci</div>
<div class="meta-line">First: 2026-02-18T04:19:57+00:00 · Latest: 2026-02-18T04:19:57+00:00</div>
<div class="meta-line">Comments: 6 pages, 4 figures, Accepted paper at IEEE International Conference on Communications (ICC) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16174v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16174v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mobile edge computing (MEC) based wireless metaverse services offer an untethered, immersive experience to users, where the superior quality of experience (QoE) needs to be achieved under stringent latency constraints and visual quality demands. To achieve this, MEC-based intelligent resource allocation for virtual reality users needs to be supported by coordination across MEC servers to harness distributed data. Federated learning (FL) is a promising solution, and can be combined with reinforcement learning (RL) to develop generalized policies across MEC-servers. However, conventional FL incurs transmitting the full model parameters across the MEC-servers and the cloud, and suffer performance degradation due to naive global aggregation, especially in heterogeneous multi-radio access technology environments. To address these challenges, this paper proposes Federated Split Decision Transformer (FSDT), an offline RL framework where the transformer model is partitioned between MEC servers and the cloud. Agent-specific components (e.g., MEC-based embedding and prediction layers) enable local adaptability, while shared global layers in the cloud facilitate cooperative training across MEC servers. Experimental results demonstrate that FSDT enhances QoE for up to 10% in heterogeneous environments compared to baselines, while offloadingnearly 98% of the transformer model parameters to the cloud, thereby reducing the computational burden on MEC servers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向元宇宙资源分配的联邦分割决策Transformer边缘学习</div>
<div class="mono" style="margin-top:8px">基于移动边缘计算（MEC）的无线元宇宙服务为用户提供无束缚的沉浸式体验，需在严格延迟约束与视觉质量要求下实现卓越体验质量（QoE）。为实现这一目标，面向虚拟现实用户的MEC智能资源分配需通过跨MEC服务器协同来利用分布式数据。联邦学习（FL）是一种前景广阔的解决方案，可与强化学习（RL）结合以制定跨MEC服务器的通用策略。然而，传统FL需要在MEC服务器与云端间传输完整模型参数，且因简单的全局聚合导致性能下降，在异构多无线接入技术环境中尤为明显。为应对这些挑战，本文提出联邦分割决策Transformer（FSDT），这是一种将Transformer模型分割部署于MEC服务器与云端的离线RL框架。智能体专用组件（如基于MEC的嵌入层和预测层）实现本地自适应，而云端共享的全局层促进跨MEC服务器的协同训练。实验结果表明，在异构环境中FSDT相比基线方法将QoE提升达10%，同时将近98%的Transformer模型参数卸载至云端，显著减轻MEC服务器的计算负担。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need for efficient resource allocation in mobile edge computing (MEC)-based wireless metaverse services, which require high quality of experience under strict latency and visual quality constraints. The proposed method, Federated Split Decision Transformer (FSDT), is an offline reinforcement learning framework that partitions a transformer model between MEC servers and the cloud, keeping agent-specific layers locally for adaptability while training shared global layers cooperatively in the cloud. Experimental results show that FSDT improves quality of experience by up to 10% in heterogeneous environments compared to baselines and offloads nearly 98% of model parameters to the cloud, significantly reducing computational burden on MEC servers.</div>
<div class="mono" style="margin-top:8px">本文的动机源于无线元宇宙中移动边缘计算服务需要满足严格延迟和视觉质量要求下的高效资源分配需求。所提出的方法名为联邦拆分决策变换器，是一种离线强化学习框架，它将变换器模型分割部署在移动边缘服务器和云端，其中代理特定层保留在本地以适应异构环境，而共享的全局层在云端进行协同训练。实验结果表明，与基线方法相比，该框架在异构环境中将用户体验质量提升了高达10%，同时将近98%的模型参数卸载到云端，显著减轻了移动边缘服务器的计算负担。</div>
</details>
</div>
<div class="card">
<div class="title">HiPER: Hierarchical Reinforcement Learning with Explicit Credit Assignment for Large Language Model Agents</div>
<div class="meta-line">Authors: Jiangweizhi Peng, Yuanxin Liu, Ruida Zhou, Charles Fleming, Zhaoran Wang, Alfredo Garcia, Mingyi Hong</div>
<div class="meta-line">First: 2026-02-18T03:31:34+00:00 · Latest: 2026-02-18T03:31:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16165v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16165v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training LLMs as interactive agents for multi-turn decision-making remains challenging, particularly in long-horizon tasks with sparse and delayed rewards, where agents must execute extended sequences of actions before receiving meaningful feedback. Most existing reinforcement learning (RL) approaches model LLM agents as flat policies operating at a single time scale, selecting one action at each turn. In sparse-reward settings, such flat policies must propagate credit across the entire trajectory without explicit temporal abstraction, which often leads to unstable optimization and inefficient credit assignment.
  We propose HiPER, a novel Hierarchical Plan-Execute RL framework that explicitly separates high-level planning from low-level execution. HiPER factorizes the policy into a high-level planner that proposes subgoals and a low-level executor that carries them out over multiple action steps. To align optimization with this structure, we introduce a key technique called hierarchical advantage estimation (HAE), which carefully assigns credit at both the planning and execution levels. By aggregating returns over the execution of each subgoal and coordinating updates across the two levels, HAE provides an unbiased gradient estimator and provably reduces variance compared to flat generalized advantage estimation.
  Empirically, HiPER achieves state-of-the-art performance on challenging interactive benchmarks, reaching 97.4\% success on ALFWorld and 83.3\% on WebShop with Qwen2.5-7B-Instruct (+6.6\% and +8.3\% over the best prior method), with especially large gains on long-horizon tasks requiring multiple dependent subtasks. These results highlight the importance of explicit hierarchical decomposition for scalable RL training of multi-turn LLM agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HiPER：面向大语言模型智能体的显式信用分配分层强化学习框架</div>
<div class="mono" style="margin-top:8px">将大语言模型训练为多轮决策交互智能体仍具挑战性，尤其在奖励稀疏延迟的长周期任务中，智能体需执行长序列动作才能获得有效反馈。现有强化学习方法多将大语言模型智能体建模为单时间尺度的平面策略，每轮选择单一动作。在稀疏奖励场景下，此类平面策略需在全轨迹中传播信用而缺乏显式时间抽象，常导致优化不稳定与信用分配低效。
我们提出HiPER——一种显式分离高层规划与底层执行的分层规划-执行强化学习框架。该框架将策略分解为提出子目标的高层规划器与多步执行子目标的底层执行器。为实现结构对齐优化，我们引入分层优势估计关键技术，在规划与执行层面精准分配信用。通过聚合各子目标执行期间的回报并协调双层级更新，该方法提供无偏梯度估计器，较平面广义优势估计可证明降低方差。
实验表明，HiPER在交互基准测试中取得最优性能：基于Qwen2.5-7B-Instruct模型在ALFWorld达到97.4%成功率，在WebShop达到83.3%（较先前最佳方法分别提升6.6%与8.3%），在需多依赖子任务的长周期任务中提升尤为显著。这些结果凸显了显式分层分解对多轮大语言模型智能体可扩展强化学习训练的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of training large language models as interactive agents for long-horizon tasks with sparse rewards, where flat reinforcement learning policies struggle with inefficient credit assignment and unstable optimization. The proposed method, HiPER, introduces a hierarchical reinforcement learning framework that separates high-level planning for subgoal generation from low-level execution over multiple steps, employing hierarchical advantage estimation to assign credit explicitly across both levels and reduce variance. Experimental results demonstrate state-of-the-art performance, achieving 97.4% success on ALFWorld and 83.3% on WebShop with a Qwen2.5-7B-Instruct model, showing significant improvements over prior methods, particularly in complex tasks requiring sequential subtasks.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在稀疏奖励的长时程交互任务中训练困难的问题，提出了一种分层强化学习框架HiPER，以解决传统扁平策略在信用分配和优化稳定性上的不足。该方法通过将策略分解为高层规划子目标和底层执行动作的分层结构，并采用分层优势估计技术，在规划和执行两个层面进行显式信用分配，从而降低方差。实验结果表明，HiPER在ALFWorld和WebShop基准测试中分别达到97.4%和83.3%的成功率，基于Qwen2.5-7B-Instruct模型相比先前最佳方法有显著提升，尤其在需要多步依赖子任务的复杂场景中优势明显。</div>
</details>
</div>
<div class="card">
<div class="title">Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution</div>
<div class="meta-line">Authors: Nithin Sivakumaran, Shoubin Yu, Hyunji Lee, Yue Zhang, Ali Payani, Mohit Bansal, Elias Stengel-Eskin</div>
<div class="meta-line">First: 2026-02-18T02:55:55+00:00 · Latest: 2026-02-18T02:55:55+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/nsivaku/remul</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16154v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16154v1">PDF</a> · <a href="https://github.com/nsivaku/remul">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Chain-of-thought (CoT) reasoning sometimes fails to faithfully reflect the true computation of a large language model (LLM), hampering its utility in explaining how LLMs arrive at their answers. Moreover, optimizing for faithfulness and interpretability in reasoning often degrades task performance. To address this tradeoff and improve CoT faithfulness, we propose Reasoning Execution by Multiple Listeners (REMUL), a multi-party reinforcement learning approach. REMUL builds on the hypothesis that reasoning traces which other parties can follow will be more faithful. A speaker model generates a reasoning trace, which is truncated and passed to a pool of listener models who &quot;execute&quot; the trace, continuing the trace to an answer. Speakers are rewarded for producing reasoning that is clear to listeners, with additional correctness regularization via masked supervised finetuning to counter the tradeoff between faithfulness and performance. On multiple reasoning benchmarks (BIG-Bench Extra Hard, MuSR, ZebraLogicBench, and FOLIO), REMUL consistently and substantially improves three measures of faithfulness -- hint attribution, early answering area over the curve (AOC), and mistake injection AOC -- while also improving accuracy. Our analysis finds that these gains are robust across training domains, translate to legibility gains, and are associated with shorter and more direct CoTs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过多监听器软执行平衡推理的忠实性与性能</div>
<div class="mono" style="margin-top:8px">思维链推理有时无法忠实反映大语言模型的真实计算过程，这削弱了其在解释模型如何得出答案方面的效用。此外，为提升推理的忠实性和可解释性进行优化往往会降低任务性能。为解决这一权衡问题并改进思维链的忠实性，我们提出了一种多方强化学习方法——多监听器推理执行。该方法基于一个假设：能被其他方遵循的推理轨迹将更具忠实性。发言者模型生成推理轨迹，该轨迹被截断后传递给一组监听器模型，监听器“执行”该轨迹并继续推导出答案。发言者因生成对监听器清晰的推理而获得奖励，同时通过掩码监督微调进行额外的正确性正则化，以抵消忠实性与性能之间的权衡。在多个推理基准测试中，该方法持续显著提升了三项忠实性指标——提示归因、早期回答曲线下面积和错误注入曲线下面积——同时提高了准确率。我们的分析发现，这些增益在不同训练领域均表现稳健，可转化为可读性提升，并与更简短直接的思维链相关。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the trade-off between faithfulness and task performance in chain-of-thought (CoT) reasoning for large language models, where faithful reasoning traces that accurately reflect the model&#x27;s computation often come at the cost of reduced accuracy. The authors propose Reasoning Execution by Multiple Listeners (REMUL), a multi-party reinforcement learning method that encourages faithfulness by having a speaker model generate a reasoning trace, which is then truncated and executed by a pool of listener models to produce an answer; speakers are rewarded for clarity to listeners, with masked supervised fine-tuning used to maintain correctness. Experimental results on reasoning benchmarks (BIG-Bench Extra Hard, MuSR, ZebraLogicBench, FOLIO) show that REMUL consistently improves three faithfulness metrics—hint attribution, early answering AOC, and mistake injection AOC—while also enhancing accuracy, with analysis indicating these gains are robust across domains and associated with shorter, more direct CoTs.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型中思维链推理的忠实性与任务性能之间的权衡问题展开研究，即忠实反映模型计算过程的推理轨迹常以降低准确性为代价。作者提出了多听众软执行方法，这是一种多参与者强化学习方法，通过让说话者模型生成推理轨迹，截断后由一组听众模型执行以得出答案，说话者因听众可理解而获得奖励，并结合掩码监督微调来保持正确性。在多个推理基准上的实验结果表明，该方法在提升三个忠实性指标的同时也提高了准确性，且分析发现这些收益在不同训练领域具有鲁棒性，并与更简短、直接的思维链相关。</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning Up the Instruction Ladder for Controllable Language Models</div>
<div class="meta-line">Authors: Zishuo Zheng, Vidhisha Balachandran, Chan Young Park, Faeze Brahman, Sachin Kumar</div>
<div class="meta-line">First: 2025-10-30T22:13:31+00:00 · Latest: 2026-02-18T02:51:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.04694v4">Abs</a> · <a href="https://arxiv.org/pdf/2511.04694v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language model (LLM) based systems take on high-stakes roles in real-world decision-making, they must reconcile competing instructions from multiple sources (e.g., model developers, users, and tools) within a single prompt context. Thus, enforcing an instruction hierarchy (IH) in LLMs, where higher-level directives override lower-priority requests, is critical for the reliability and controllability of LLMs. In this work, we reframe instruction hierarchy resolution as a reasoning task. Specifically, the model must first &quot;think&quot; about the relationship between a given user prompt and higher-priority (system) instructions before generating a response. To enable this capability via training, we construct VerIH, an instruction hierarchy dataset of constraint-following tasks with verifiable answers. This dataset comprises ~7K aligned and conflicting system-user instructions. We show that lightweight reinforcement learning with VerIH effectively transfers general reasoning capabilities of models to instruction prioritization. Our finetuned models achieve consistent improvements on instruction following and instruction hierarchy benchmarks, achieving roughly a 20% improvement on the IHEval conflict setup. This reasoning ability also generalizes to safety-critical settings beyond the training distribution. By treating safety issues as resolving conflicts between adversarial user inputs and predefined higher-priority policies, our trained model enhances robustness against jailbreak and prompt injection attacks, providing up to a 20% reduction in attack success rate (ASR). These results demonstrate that reasoning over instruction hierarchies provides a practical path to reliable LLMs, where updates to system prompts yield controllable and robust changes in model behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>沿指令阶梯推理实现可控语言模型</div>
<div class="mono" style="margin-top:8px">随着基于大语言模型（LLM）的系统在现实世界决策中承担高风险角色，它们必须在单一提示上下文中协调来自多个来源（如模型开发者、用户和工具）的竞争性指令。因此，在LLM中实施指令层级（IH）——即高级指令覆盖低优先级请求——对LLM的可靠性和可控性至关重要。本研究将指令层级解析重构为推理任务：模型在生成响应前需先“思考”给定用户提示与高优先级（系统）指令间的关系。为通过训练实现此能力，我们构建了VerIH——一个包含可验证答案的约束遵循任务的指令层级数据集，涵盖约7K条对齐及冲突的系统-用户指令。研究表明，基于VerIH的轻量级强化学习能有效将模型的通用推理能力迁移至指令优先级处理。经微调的模型在指令遵循和指令层级基准测试中取得持续改进，在IHEval冲突设置上提升约20%。该推理能力还可泛化至训练分布外的安全关键场景：通过将安全问题视为对抗性用户输入与预定义高优先级策略间的冲突解析，训练后的模型增强了对越狱和提示注入攻击的鲁棒性，攻击成功率（ASR）降低达20%。这些结果表明，基于指令层级的推理为构建可靠LLM提供了可行路径，系统提示的更新能实现模型行为的可控且稳健的转变。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to enhance the reliability and controllability of large language models (LLMs) in high-stakes decision-making by enabling them to reconcile conflicting instructions from different sources through a clear hierarchy. The method reframes instruction hierarchy resolution as a reasoning task, where models must first reason about the relationship between user prompts and higher-priority system instructions before responding; to train this capability, the authors construct VerIH, a dataset of about 7,000 aligned and conflicting system-user instructions with verifiable answers, and apply lightweight reinforcement learning. The main experimental results show that finetuned models achieve consistent improvements on instruction following and hierarchy benchmarks, with roughly a 20% improvement on conflict setups, and this reasoning ability generalizes to safety-critical settings, reducing attack success rates by up to 20% against jailbreak and prompt injection attacks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过建立清晰的指令层级，使大语言模型（LLM）能够在高风险决策中协调来自不同来源的冲突指令，从而提升其可靠性和可控性。方法将指令层级解析重构为推理任务，要求模型在生成响应前先推理用户提示与高优先级系统指令之间的关系；为此，作者构建了VerIH数据集，包含约7,000条可验证的对齐和冲突系统-用户指令，并采用轻量级强化学习进行训练。主要实验结果表明，微调后的模型在指令遵循和层级基准测试中取得了一致性改进，冲突设置下的性能提升约20%，且这种推理能力可泛化至安全关键场景，将越狱和提示注入攻击的成功率降低高达20%。</div>
</details>
</div>
<div class="card">
<div class="title">DeepVision-103K: A Visually Diverse, Broad-Coverage, and Verifiable Mathematical Dataset for Multimodal Reasoning</div>
<div class="meta-line">Authors: Haoxiang Sun, Lizhen Xu, Bing Zhao, Wotao Yin, Wei Wang, Boyu Yang, Rui Wang, Hu Wei</div>
<div class="meta-line">First: 2026-02-18T01:51:21+00:00 · Latest: 2026-02-18T01:51:21+00:00</div>
<div class="meta-line">Comments: Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16742v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16742v1">PDF</a> · <a href="https://huggingface.co/datasets/skylenage/DeepVision-103K}{this">Code1</a> · <a href="https://huggingface.co/datasets/skylenage/DeepVision-103K">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has been shown effective in enhancing the visual reflection and reasoning capabilities of Large Multimodal Models (LMMs). However, existing datasets are predominantly derived from either small-scale manual construction or recombination of prior resources, which limits data diversity and coverage, thereby constraining further gains in model performance. To this end, we introduce \textbf{DeepVision-103K}, a comprehensive dataset for RLVR training that covers diverse K12 mathematical topics, extensive knowledge points, and rich visual elements. Models trained on DeepVision achieve strong performance on multimodal mathematical benchmarks, and generalize effectively to general multimodal reasoning tasks. Further analysis reveals enhanced visual perception, reflection and reasoning capabilities in trained models, validating DeepVision&#x27;s effectiveness for advancing multimodal reasoning. Data: \href{https://huggingface.co/datasets/skylenage/DeepVision-103K}{this url}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeepVision-103K：一个视觉多样、覆盖广泛且可验证的多模态推理数学数据集</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）已被证明能有效增强大型多模态模型的视觉反思与推理能力。然而，现有数据集主要源自小规模人工构建或既有资源的重组，这限制了数据的多样性与覆盖范围，从而制约了模型性能的进一步提升。为此，我们提出了\textbf{DeepVision-103K}——一个用于RLVR训练的综合数据集，涵盖多样化的K12数学主题、广泛的知识点及丰富的视觉元素。基于DeepVision训练的模型在多模态数学基准测试中表现优异，并能有效泛化至通用多模态推理任务。进一步分析表明，训练后的模型在视觉感知、反思与推理能力方面均得到增强，验证了DeepVision对推进多模态推理的有效性。数据地址：\href{https://huggingface.co/datasets/skylenage/DeepVision-103K}{此链接}。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces DeepVision-103K, a dataset created to address limitations in existing resources for Reinforcement Learning with Verifiable Rewards (RLVR), which are often small-scale or recombined, restricting data diversity and coverage and thus model performance gains. The method involves constructing a comprehensive dataset covering diverse K12 math topics, extensive knowledge points, and rich visual elements to train Large Multimodal Models. Experimental results show that models trained on DeepVision achieve strong performance on multimodal mathematical benchmarks and generalize well to general multimodal reasoning tasks, with analysis confirming enhanced visual perception, reflection, and reasoning capabilities.</div>
<div class="mono" style="margin-top:8px">本文提出了DeepVision-103K数据集，旨在解决现有可验证奖励强化学习（RLVR）数据资源多为小规模人工构建或重组、导致数据多样性和覆盖范围有限、从而制约模型性能提升的问题。该方法通过构建一个涵盖多样化K12数学主题、广泛知识点和丰富视觉元素的综合数据集来训练大型多模态模型。实验结果表明，基于DeepVision训练的模型在多模态数学基准测试中表现强劲，并能有效泛化至一般多模态推理任务，分析进一步验证了模型在视觉感知、反思和推理能力上的提升。</div>
</details>
</div>
<div class="card">
<div class="title">Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs</div>
<div class="meta-line">Authors: Xiaoke Huang, Ningsen Wang, Hui Liu, Xianfeng Tang, Yuyin Zhou</div>
<div class="meta-line">Venue: ICLR</div>
<div class="meta-line">First: 2025-10-29T18:10:44+00:00 · Latest: 2026-02-18T01:32:31+00:00</div>
<div class="meta-line">Comments: Project page, code, data, and models: https://ucsc-vlaa.github.io/MedVLSynther/ ; Accepted by ICLR&#x27;26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.25867v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.25867v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ucsc-vlaa.github.io/MedVLSynther/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Multimodal Models (LMMs) are increasingly capable of answering medical questions that require joint reasoning over images and text, yet training general medical VQA systems is impeded by the lack of large, openly usable, high-quality corpora. We present MedVLSynther, a rubric-guided generator-verifier framework that synthesizes high-quality multiple-choice VQA items directly from open biomedical literature by conditioning on figures, captions, and in-text references. The generator produces self-contained stems and parallel, mutually exclusive options under a machine-checkable JSON schema; a multi-stage verifier enforces essential gates (self-containment, single correct answer, clinical validity, image-text consistency), awards fine-grained positive points, and penalizes common failure modes before acceptance. Applying this pipeline to PubMed Central yields MedSynVQA: 13,087 audited questions over 14,803 images spanning 13 imaging modalities and 28 anatomical regions. Training open-weight LMMs with reinforcement learning using verifiable rewards improves accuracy across six medical VQA benchmarks, achieving averages of 55.85 (3B) and 58.15 (7B), with up to 77.57 on VQA-RAD and 67.76 on PathVQA, outperforming strong medical LMMs. A Ablations verify that both generation and verification are necessary and that more verified data consistently helps, and a targeted contamination analysis detects no leakage from evaluation suites. By operating entirely on open literature and open-weight models, MedVLSynther offers an auditable, reproducible, and privacy-preserving path to scalable medical VQA training data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于生成器-验证器大语言模型从医学文献中合成高质量视觉问答数据</div>
<div class="mono" style="margin-top:8px">大规模多模态模型在需要结合图像与文本进行推理的医学问题解答方面能力日益增强，但高质量、可公开使用的大规模语料库的缺乏阻碍了通用医学视觉问答系统的训练。本研究提出MedVLSynther——一个基于规则指导的生成器-验证器框架，通过结合图表、标题及文内引用，直接从开放生物医学文献中合成高质量多选题形式的视觉问答条目。生成器按照机器可校验的JSON格式生成自包含的题干及并行互斥的选项；多阶段验证器在接收前执行核心校验（自包含性、单一正确答案、临床有效性、图文一致性），授予细粒度正向评分，并对常见错误模式进行扣分。将该流程应用于PubMed Central数据库得到MedSynVQA数据集：包含13,087道已审核问题，覆盖14,803张图像，涉及13种影像模态和28个解剖区域。使用可验证奖励通过强化学习训练开放权重的大规模多模态模型，在六项医学视觉问答基准测试中准确率均获提升，3B和7B模型平均准确率分别达55.85和58.15，其中VQA-RAD最高达77.57，PathVQA达67.76，优于现有主流医学大规模多模态模型。消融实验证实生成与验证环节均不可或缺，更多验证数据持续带来性能增益；定向污染分析未检测到评估集泄露。通过完全基于开放文献与开放权重模型，MedVLSynther为可扩展的医学视觉问答训练数据提供了可审计、可复现且保护隐私的技术路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is the scarcity of large, high-quality, open datasets for training general medical visual question answering (VQA) systems. The method, MedVLSynther, is a rubric-guided generator-verifier framework that synthesizes multiple-choice VQA items from open biomedical literature by conditioning on figures, captions, and text; a generator produces questions and options, while a multi-stage verifier enforces quality gates like clinical validity and image-text consistency. The main experimental results show that applying this pipeline to PubMed Central created MedSynVQA, a dataset of over 13,000 audited questions, and training open-weight LMMs with reinforcement learning using verifiable rewards improved accuracy across six medical VQA benchmarks, outperforming strong medical LMMs with averages of 55.85 (3B) and 58.15 (7B) and up to 77.57 on VQA-RAD.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于缺乏用于训练通用医学视觉问答系统的大规模、高质量、开放数据集。该方法名为MedVLSynther，是一个基于规则的生成器-验证器框架，通过结合开放生物医学文献中的图像、标题和文本参考，合成多项选择题形式的视觉问答项目；生成器负责生成问题和选项，而多阶段验证器则强制执行临床有效性、图文一致性等质量关卡。主要实验结果表明，将该流程应用于PubMed Central生成了包含超过13,000个审核问题的MedSynVQA数据集，并且使用可验证奖励通过强化学习训练开放权重大型多模态模型，在六个医学视觉问答基准测试中提升了准确率，以3B和7B模型分别达到55.85和58.15的平均分，在VQA-RAD上最高达77.57，超越了现有强医学多模态模型。</div>
</details>
</div>
<div class="card">
<div class="title">Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation</div>
<div class="meta-line">Authors: Yujun Zhou, Zhenwen Liang, Haolin Liu, Wenhao Yu, Kishan Panaganti, Linfeng Song, Dian Yu, Xiangliang Zhang, Haitao Mi, Dong Yu</div>
<div class="meta-line">First: 2025-09-18T17:50:04+00:00 · Latest: 2026-02-18T00:51:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.15194v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.15194v3">PDF</a> · <a href="https://github.com/YujunZhou/EVOL-RL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly trained with reinforcement learning from verifiable rewards (RLVR), yet real-world deployment demands models that can self-improve without labels or external judges. Existing self-improvement approaches primarily rely on self-confirmation signals (e.g., confidence, entropy, or consistency) to generate rewards. This reliance drives models toward over-confident, majority-favored solutions, causing an entropy collapse that degrades pass@n and reasoning complexity. To address this, we propose EVOL-RL, a label-free framework that mirrors the evolutionary principle of balancing selection with variation. Concretely, EVOL-RL retains the majority-voted answer as an anchor for stability, but adds a novelty-aware reward that scores each sampled solution by how different its reasoning is from other concurrently generated responses. This majority-for-stability + novelty-for-exploration rule mirrors the variation-selection principle: selection prevents drift, while novelty prevents collapse. Evaluation results show that EVOL-RL consistently outperforms the majority-only baseline; e.g., training on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from baseline&#x27;s 4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents in-domain diversity collapse but also improves out-of-domain generalization (from math reasoning to broader tasks, e.g., MMLU-Pro and BBEH). The code is available at: https://github.com/YujunZhou/EVOL-RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无需标注的语言模型进化：多数驱动选择，新颖性促进变异</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）越来越多地采用可验证奖励的强化学习（RLVR）进行训练，但实际部署需要模型能在无标注或外部评判的情况下自我改进。现有的自我改进方法主要依赖自我确认信号（如置信度、熵或一致性）生成奖励。这种依赖导致模型趋向过度自信、多数偏好的解，引发熵崩溃，从而降低pass@n指标和推理复杂度。为解决此问题，我们提出EVOL-RL——一个模拟“选择与变异平衡”进化原理的无标注框架。具体而言，EVOL-RL保留多数投票答案作为稳定性锚点，同时引入新颖性感知奖励，根据每个采样解的推理过程与同期生成的其他响应的差异程度进行评分。这种“多数求稳定+新颖促探索”的规则体现了变异-选择原理：选择防止漂移，新颖性防止崩溃。评估结果显示，EVOL-RL始终优于纯多数基线；例如，在无标注的AIME24数据上训练，使Qwen3-4B-Base模型在AIME25的pass@1从基线的4.6%提升至16.4%，pass@16从18.5%提升至37.9%。EVOL-RL不仅防止了领域内多样性崩溃，还提升了跨领域泛化能力（从数学推理扩展到更广泛任务，如MMLU-Pro和BBEH）。代码已开源：https://github.com/YujunZhou/EVOL-RL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the problem of large language models (LLMs) degrading in diversity and reasoning complexity when self-improving via reinforcement learning without external labels, as existing methods rely on self-confirmation signals that lead to entropy collapse. To overcome this, the authors propose EVOL-RL, a label-free framework inspired by evolutionary principles, which combines a majority-voted answer for stability with a novelty-aware reward that encourages variation by scoring solutions based on reasoning differences from other generated responses. Experimental results demonstrate that EVOL-RL significantly outperforms majority-only baselines, improving pass@1 and pass@16 scores on AIME25 and enhancing out-of-domain generalization to tasks like MMLU-Pro and BBEH, thereby preventing diversity collapse while boosting performance.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在无外部标签的强化学习自改进过程中，因依赖自我确认信号而导致熵崩溃、多样性和推理复杂性下降的问题。为解决此问题，作者提出了EVOL-RL，一个受进化原理启发的无标签框架，它结合多数投票答案以保持稳定性，并引入新颖性奖励来促进变异，通过评估解决方案与其他生成响应的推理差异来评分。实验结果表明，EVOL-RL显著优于仅基于多数的基线方法，在AIME25上提升了pass@1和pass@16分数，并增强了在MMLU-Pro和BBEH等跨域任务上的泛化能力，从而在防止多样性崩溃的同时提高了性能。</div>
</details>
</div>
<div class="card">
<div class="title">Refined Bayesian Optimization for Efficient Beam Alignment in Intelligent Indoor Wireless Environments</div>
<div class="meta-line">Authors: Parth Ashokbhai Shiroya, Amod Ashtekar, Swarnagowri Shashidhar, Mohammed E. Eltayeb</div>
<div class="meta-line">First: 2025-11-12T22:46:39+00:00 · Latest: 2026-02-17T23:23:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.00036v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.00036v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Future intelligent indoor wireless environments require fast and reliable beam alignment to sustain high-throughput links under mobility and blockage. Exhaustive beam training achieves optimal performance but is prohibitively costly. In indoor settings, dense scatterers and transceiver hardware imperfections introduce multipath and sidelobe leakage, producing measurable power across multiple angles and reducing the effectiveness of outdoor-oriented alignment algorithms. This paper presents a Refined Bayesian Optimization (R-BO) framework that exploits the inherent structure of mmWave transceiver patterns, where received power gradually increases as the transmit and receive beams converge toward the optimum. R-BO integrates a Gaussian Process (GP) surrogate with a Matern kernel and an Expected Improvement (EI) acquisition function, followed by a localized refinement around the predicted optimum. The GP hyperparameters are re-optimized online to adapt to irregular variations in the measured angular power field caused by reflections and sidelobe leakage. Experiments across 43 receiver positions in an indoor laboratory demonstrate 97.7% beam-alignment accuracy within 10 degrees, less than 0.3 dB average loss, and an 88% reduction in probing overhead compared to exhaustive search. These results establish R-BO as an efficient and adaptive beam-alignment solution for real-time intelligent indoor wireless environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向智能室内无线环境高效波束对准的精细化贝叶斯优化方法</div>
<div class="mono" style="margin-top:8px">未来智能室内无线环境需通过快速可靠的波束对准，在移动与遮挡条件下维持高吞吐量链路。穷举式波束训练虽能实现最优性能，但成本过高。室内环境中，密集散射体与收发硬件缺陷会引发多径效应和旁瓣泄漏，导致多角度可测功率分布，降低面向室外场景的对准算法效能。本文提出一种精细化贝叶斯优化框架，该框架利用毫米波收发波束图的内在特性——当收发波束向最优方向收敛时接收功率逐渐增强。该方法集成采用马特恩核的高斯过程代理模型与期望提升采集函数，并在预测最优值附近进行局部精细化。高斯过程超参数通过在线重优化以适应由反射和旁瓣泄漏引起的角度功率场不规则变化。在室内实验室43个接收位置的实验表明：该方法在10度误差范围内实现97.7%的波束对准精度，平均损耗低于0.3 dB，且探测开销较穷举搜索降低88%。这些结果证实了精细化贝叶斯优化可作为实时智能室内无线环境的高效自适应波束对准解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for fast and reliable beam alignment in intelligent indoor wireless environments, where exhaustive beam training is too costly and outdoor-oriented algorithms are ineffective due to multipath and hardware imperfections, this paper proposes a Refined Bayesian Optimization (R-BO) framework. The method leverages the gradual power increase near optimal beams by using a Gaussian Process surrogate with a Matern kernel and Expected Improvement acquisition, followed by localized refinement, with online hyperparameter re-optimization to adapt to irregular power variations. Experimental results from 43 indoor receiver positions show 97.7% alignment accuracy within 10 degrees, less than 0.3 dB average loss, and an 88% reduction in probing overhead compared to exhaustive search, establishing R-BO as an efficient adaptive solution.</div>
<div class="mono" style="margin-top:8px">本文针对智能室内无线环境中快速可靠的波束对准需求，旨在解决穷举波束训练成本过高以及室外算法因多径和硬件缺陷而失效的问题，提出了一种精细化贝叶斯优化（R-BO）框架。该方法利用波束接近最优时功率逐渐增加的特性，采用带有Matern核的高斯过程代理和期望提升采集函数，并进行局部细化，同时在线重新优化超参数以适应不规则功率变化。在室内实验室43个接收位置的实验结果表明，该方法在10度内实现了97.7%的对准精度，平均损耗低于0.3 dB，且探测开销相比穷举搜索降低了88%，证明了R-BO是一种高效自适应的波束对准解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design</div>
<div class="meta-line">Authors: Sha Li, Stefano Petrangeli, Yu Shen, Xiang Chen</div>
<div class="meta-line">First: 2026-02-14T22:31:49+00:00 · Latest: 2026-02-17T22:24:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13912v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.13912v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs&#x27; limited spatial reasoning and the lack of opacity in design decision making. Instead of operating at the pixel level, we reformulate layout design as a policy learning problem over a structured textual spatial environment that explicitly encodes canvas geometry, element attributes, and inter-element relationships. LaySPA produces dual-level outputs comprising interpretable reasoning traces and structured layout specifications, enabling transparent and controllable design decision making. Layout design policy is optimized via a multi-objective spatial critique that decomposes layout quality into geometric validity, relational coherence, and aesthetic consistency, and is trained using relative group optimization to stabilize learning in open-ended design spaces. Experiments demonstrate that LaySPA improves structural validity and visual quality, outperforming larger proprietary LLMs and achieving performance comparable to specialized SOTA layout generators while requiring fewer annotated samples and reduced latency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从像素到策略：强化语言模型的空间推理能力以实现内容感知的版面设计</div>
<div class="mono" style="margin-top:8px">我们提出LaySPA——一种强化学习框架，通过显式且可解释的空间推理能力赋能大语言模型，实现内容感知的图形版面设计。LaySPA解决两大核心挑战：大语言模型有限的空间推理能力，以及设计决策过程缺乏透明度。我们摒弃像素级操作，将版面设计重构为结构化文本空间环境中的策略学习问题，该环境显式编码画布几何、元素属性及元素间关系。LaySPA生成包含可解释推理轨迹与结构化版面规范的双层输出，实现透明可控的设计决策。版面设计策略通过多目标空间评估机制进行优化，该机制将版面质量解构为几何有效性、关系连贯性与美学一致性，并采用相对群体优化方法进行训练，以稳定开放设计空间中的学习过程。实验表明，LaySPA在提升结构有效性与视觉质量方面，不仅超越规模更大的专有大语言模型，其性能更可与专业SOTA版面生成器相媲美，同时所需标注样本更少、延迟更低。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by large language models&#x27; limited spatial reasoning and opaque design decisions in layout tasks, this paper introduces LaySPA, a reinforcement learning framework that reformulates graphic layout design as policy learning over a structured textual spatial environment encoding canvas geometry, element attributes, and relationships. The method produces interpretable reasoning traces and structured layout specifications, optimized via a multi-objective spatial critique for geometric validity, relational coherence, and aesthetic consistency, using relative group optimization to stabilize training. Experimental results show LaySPA improves structural validity and visual quality, outperforming larger proprietary LLMs and matching specialized state-of-the-art layout generators with fewer annotated samples and lower latency.</div>
<div class="mono" style="margin-top:8px">针对大语言模型在布局任务中空间推理能力有限且设计决策不透明的问题，本文提出了LaySPA强化学习框架，将图形布局设计重新定义为在结构化文本空间环境上的策略学习，该环境显式编码画布几何、元素属性和相互关系。该方法生成可解释的推理轨迹和结构化布局规范，通过多目标空间评判器优化几何有效性、关系一致性和美学一致性，并采用相对群体优化稳定开放设计空间的训练。实验结果表明，LaySPA提升了结构有效性和视觉质量，优于更大的专有大语言模型，并与专业的最先进布局生成器性能相当，同时需要更少的标注样本和更低的延迟。</div>
</details>
</div>
<div class="card">
<div class="title">MARLEM: A Multi-Agent Reinforcement Learning Simulation Framework for Implicit Cooperation in Decentralized Local Energy Markets</div>
<div class="meta-line">Authors: Nelson Salazar-Pena, Alejandra Tabares, Andres Gonzalez-Mancera</div>
<div class="meta-line">First: 2026-02-17T22:22:45+00:00 · Latest: 2026-02-17T22:22:45+00:00</div>
<div class="meta-line">Comments: 32 pages, 7 figures, 1 table, 1 algorithm</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16063v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16063v1">PDF</a> · <a href="https://github.com/salazarna/marlem">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces a novel, open-source MARL simulation framework for studying implicit cooperation in LEMs, modeled as a decentralized partially observable Markov decision process and implemented as a Gymnasium environment for MARL. Our framework features a modular market platform with plug-and-play clearing mechanisms, physically constrained agent models (including battery storage), a realistic grid network, and a comprehensive analytics suite to evaluate emergent coordination. The main contribution is a novel method to foster implicit cooperation, where agents&#x27; observations and rewards are enhanced with system-level key performance indicators to enable them to independently learn strategies that benefit the entire system and aim for collectively beneficial outcomes without explicit communication. Through representative case studies (available in a dedicated GitHub repository in https://github.com/salazarna/marlem, we show the framework&#x27;s ability to analyze how different market configurations (such as varying storage deployment) impact system performance. This illustrates its potential to facilitate emergent coordination, improve market efficiency, and strengthen grid stability. The proposed simulation framework is a flexible, extensible, and reproducible tool for researchers and practitioners to design, test, and validate strategies for future intelligent, decentralized energy systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MARLEM：用于去中心化本地能源市场中隐性合作的多智能体强化学习仿真框架</div>
<div class="mono" style="margin-top:8px">本文提出了一种新颖的开源多智能体强化学习仿真框架，用于研究本地能源市场中的隐性合作。该框架建模为去中心化部分可观测马尔可夫决策过程，并实现为适用于多智能体强化学习的Gymnasium环境。其特点包括：采用即插即用出清机制的模块化市场平台、物理约束的智能体模型（含电池储能）、真实的电网网络，以及用于评估涌现协调的综合分析套件。主要贡献在于提出了一种促进隐性合作的新方法——通过系统级关键性能指标增强智能体的观测与奖励，使其能独立学习有益于整体系统的策略，在无需显式通信的情况下实现集体效益。通过代表性案例研究（详见GitHub仓库https://github.com/salazarna/marlem），我们展示了该框架分析不同市场配置（如储能部署方案）如何影响系统性能的能力，证明了其在促进涌现协调、提升市场效率、增强电网稳定性方面的潜力。该仿真框架为研究人员和从业者设计、测试和验证未来智能去中心化能源系统策略提供了灵活、可扩展且可复现的工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to study implicit cooperation in decentralized local energy markets (LEMs) without explicit communication, this paper introduces MARLEM, a novel open-source multi-agent reinforcement learning (MARL) simulation framework. The method models the LEM as a decentralized partially observable Markov decision process, implemented as a Gymnasium environment, and features a modular platform with plug-and-play market clearing, physically constrained agent models, a realistic grid network, and an analytics suite. A key methodological contribution is enhancing agents&#x27; observations and rewards with system-level key performance indicators to foster implicit cooperation. Experimental results from case studies demonstrate the framework&#x27;s ability to analyze how factors like storage deployment impact system performance, showing its potential to facilitate emergent coordination, improve market efficiency, and strengthen grid stability.</div>
<div class="mono" style="margin-top:8px">本文旨在研究无需显式通信的去中心化本地能源市场中的隐性合作，为此提出了MARLEM，一个新颖的开源多智能体强化学习仿真框架。该方法将市场建模为去中心化部分可观测马尔可夫决策过程，并实现为Gymnasium环境；其核心是一个模块化平台，支持即插即用的市场出清机制、包含物理约束的智能体模型、真实的电网网络以及分析套件。方法上的主要创新是通过系统级关键绩效指标增强智能体的观察和奖励，以促进隐性合作。代表性案例研究表明，该框架能够分析不同市场配置（如储能部署）对系统性能的影响，验证了其在促进涌现协调、提升市场效率和增强电网稳定性方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Harnessing Implicit Cooperation: A Multi-Agent Reinforcement Learning Approach Towards Decentralized Local Energy Markets</div>
<div class="meta-line">Authors: Nelson Salazar-Pena, Alejandra Tabares, Andres Gonzalez-Mancera</div>
<div class="meta-line">First: 2026-02-17T22:22:32+00:00 · Latest: 2026-02-17T22:22:32+00:00</div>
<div class="meta-line">Comments: 42 pages, 7 figures, 10 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16062v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16062v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper proposes implicit cooperation, a framework enabling decentralized agents to approximate optimal coordination in local energy markets without explicit peer-to-peer communication. We formulate the problem as a decentralized partially observable Markov decision problem that is solved through a multi-agent reinforcement learning task in which agents use stigmergic signals (key performance indicators at the system level) to infer and react to global states. Through a 3x3 factorial design on an IEEE 34-node topology, we evaluated three training paradigms (CTCE, CTDE, DTDE) and three algorithms (PPO, APPO, SAC). Results identify APPO-DTDE as the optimal configuration, achieving a coordination score of 91.7% relative to the theoretical centralized benchmark (CTCE). However, a critical trade-off emerges between efficiency and stability: while the centralized benchmark maximizes allocative efficiency with a peer-to-peer trade ratio of 0.6, the fully decentralized approach (DTDE) demonstrates superior physical stability. Specifically, DTDE reduces the variance of grid balance by 31% compared to hybrid architectures, establishing a highly predictable, import-biased load profile that simplifies grid regulation. Furthermore, topological analysis reveals emergent spatial clustering, where decentralized agents self-organize into stable trading communities to minimize congestion penalties. While SAC excelled in hybrid settings, it failed in decentralized environments due to entropy-driven instability. This research proves that stigmergic signaling provides sufficient context for complex grid coordination, offering a robust, privacy-preserving alternative to expensive centralized communication infrastructure.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用隐式合作：面向去中心化本地能源市场的多智能体强化学习方法</div>
<div class="mono" style="margin-top:8px">本文提出隐式合作框架，使去中心化智能体无需显式点对点通信即可在本地能源市场中实现近似最优协调。我们将问题建模为去中心化部分可观测马尔可夫决策问题，通过多智能体强化学习任务求解，其中智能体利用共识信号（系统级关键性能指标）推断并响应全局状态。基于IEEE 34节点拓扑的3×3因子设计，我们评估了三种训练范式（CTCE、CTDE、DTDE）和三种算法（PPO、APPO、SAC）。结果表明APPO-DTDE为最优配置，相对于理论集中式基准（CTCE）达到91.7%的协调得分。但效率与稳定性存在关键权衡：集中式基准以0.6的点对点交易比率实现分配效率最大化，而完全去中心化方法（DTDE）展现出更优的物理稳定性。具体而言，DTDE较混合架构将电网平衡方差降低31%，形成高度可预测的偏进口负荷特性，简化了电网调控。拓扑分析揭示了涌现的空间聚类现象，去中心化智能体自组织形成稳定交易社区以最小化拥堵惩罚。SAC在混合场景表现优异，但因熵驱动的不稳定性在去中心化环境中失效。本研究证明共识信号为复杂电网协调提供充分上下文，为昂贵的集中式通信基础设施提供了稳健且保护隐私的替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient coordination in local energy markets without relying on explicit communication, this paper introduces an implicit cooperation framework using multi-agent reinforcement learning, where agents infer global states via stigmergic signals like system-level performance indicators. The method involves solving a decentralized partially observable Markov decision problem, evaluated through a factorial design on an IEEE 34-node topology comparing training paradigms (CTCE, CTDE, DTDE) and algorithms (PPO, APPO, SAC). Experimental results identify APPO-DTDE as optimal, achieving 91.7% coordination relative to a centralized benchmark, but reveal a trade-off: while centralized approaches maximize allocative efficiency, fully decentralized DTDE enhances grid stability by reducing balance variance by 31% and fostering emergent spatial clustering into stable trading communities, though SAC underperformed in decentralized settings due to instability.</div>
<div class="mono" style="margin-top:8px">本文旨在解决本地能源市场中无需显式通信的高效协调问题，提出了一种基于多智能体强化学习的隐式合作框架，其中智能体通过系统级性能指标等间接信号推断全局状态。方法涉及求解去中心化部分可观测马尔可夫决策问题，通过在IEEE 34节点拓扑上进行因子设计实验，比较了三种训练范式（CTCE、CTDE、DTDE）和三种算法（PPO、APPO、SAC）。实验结果表明，APPO-DTDE配置最优，相对于集中式基准实现了91.7%的协调度，但揭示了效率与稳定性之间的权衡：集中式方法虽最大化分配效率，而完全去中心化的DTDE通过降低电网平衡方差31%并促进形成稳定的交易社区集群，显著提升了物理稳定性，不过SAC算法在去中心化环境中因熵驱动的不稳定性而表现不佳。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260220_0349.html">20260220_0349</a>
<a href="archive/20260219_0406.html">20260219_0406</a>
<a href="archive/20260218_0358.html">20260218_0358</a>
<a href="archive/20260217_0329.html">20260217_0329</a>
<a href="archive/20260216_0321.html">20260216_0321</a>
<a href="archive/20260215_0335.html">20260215_0335</a>
<a href="archive/20260213_0416.html">20260213_0416</a>
<a href="archive/20260212_0417.html">20260212_0417</a>
<a href="archive/20260211_0421.html">20260211_0421</a>
<a href="archive/20260210_0359.html">20260210_0359</a>
<a href="archive/20260209_0321.html">20260209_0321</a>
<a href="archive/20260208_0336.html">20260208_0336</a>
<a href="archive/20260207_0400.html">20260207_0400</a>
<a href="archive/20260206_0358.html">20260206_0358</a>
<a href="archive/20260205_0400.html">20260205_0400</a>
<a href="archive/20260204_0411.html">20260204_0411</a>
<a href="archive/20260202_0320.html">20260202_0320</a>
<a href="archive/20260201_0343.html">20260201_0343</a>
<a href="archive/20260131_0357.html">20260131_0357</a>
<a href="archive/20260130_0355.html">20260130_0355</a>
<a href="archive/20260129_0353.html">20260129_0353</a>
<a href="archive/20260128_0342.html">20260128_0342</a>
<a href="archive/20260127_0320.html">20260127_0320</a>
<a href="archive/20260126_0313.html">20260126_0313</a>
<a href="archive/20260125_0321.html">20260125_0321</a>
<a href="archive/20260124_0344.html">20260124_0344</a>
<a href="archive/20260123_0345.html">20260123_0345</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0408.html">20260121_0408</a>
<a href="archive/20260120_0316.html">20260120_0316</a>
<a href="archive/20260119_0312.html">20260119_0312</a>
<a href="archive/20260118_0323.html">20260118_0323</a>
<a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
