<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-09 03:43</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260109_0343</div>
    <div class="row"><div class="card">
<div class="title">Agentic Rubrics as Contextual Verifiers for SWE Agents</div>
<div class="meta-line">Authors: Mohit Raghavendra, Anisha Gunjal, Bing Liu, Yunzhong He</div>
<div class="meta-line">First: 2026-01-07T18:38:23+00:00 · Latest: 2026-01-07T18:38:23+00:00</div>
<div class="meta-line">Comments: 31 pages, 11 Figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04171v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04171v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Verification is critical for improving agents: it provides the reward signal for Reinforcement Learning and enables inference-time gains through Test-Time Scaling (TTS). Despite its importance, verification in software engineering (SWE) agent settings often relies on code execution, which can be difficult to scale due to environment setup overhead. Scalable alternatives such as patch classifiers and heuristic methods exist, but they are less grounded in codebase context and harder to interpret. To this end, we explore Agentic Rubrics: an expert agent interacts with the repository to create a context-grounded rubric checklist, and candidate patches are then scored against it without requiring test execution. On SWE-Bench Verified under parallel TTS evaluation, Agentic Rubrics achieve a score of 54.2% on Qwen3-Coder-30B-A3B and 40.6% on Qwen3-32B, with at least a +3.5 percentage-point gain over the strongest baseline in our comparison set. We further analyze rubric behavior, showing that rubric scores are consistent with ground-truth tests while also flagging issues that tests do not capture. Our ablations show that agentic context gathering is essential for producing codebase-specific, unambiguous criteria. Together, these results suggest that Agentic Rubrics provide an efficient, scalable, and granular verification signal for SWE agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>作为软件工程智能体上下文验证器的代理式评估准则</div>
<div class="mono" style="margin-top:8px">验证对改进智能体至关重要：它为强化学习提供奖励信号，并通过测试时扩展（TTS）实现推理阶段增益。尽管验证在软件工程（SWE）智能体场景中极为重要，但当前主要依赖代码执行，而环境配置开销使其难以扩展。虽然存在补丁分类器和启发式方法等可扩展替代方案，但这些方法较少基于代码库上下文且可解释性较差。为此，我们探索了代理式评估准则：专家智能体通过交互式分析代码库，创建基于上下文的准则检查表，随后无需执行测试即可依据该准则对候选补丁进行评分。在并行TTS评估的SWE-Bench Verified基准测试中，代理式评估准则在Qwen3-Coder-30B-A3B模型上获得54.2%的得分，在Qwen3-32B模型上获得40.6%的得分，较对比集中最强基线至少提升3.5个百分点。我们进一步分析准则行为，表明准则评分与真实测试结果一致，同时能标记出测试未覆盖的问题。消融实验证明，代理式上下文收集对生成代码库特异性、无歧义的评判标准至关重要。综合结果表明，代理式评估准则为SWE智能体提供了高效、可扩展且细粒度的验证信号。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of scalable verification for software engineering (SWE) agents, where traditional reliance on code execution is hindered by environment setup overhead, and existing scalable alternatives lack contextual grounding and interpretability. To overcome this, the authors propose Agentic Rubrics, a method where an expert agent interacts with a repository to create a context-grounded rubric checklist, which is then used to score candidate patches without requiring test execution. Experimental results on SWE-Bench Verified show that Agentic Rubrics achieve scores of 54.2% on Qwen3-Coder-30B-A3B and 40.6% on Qwen3-32B, outperforming the strongest baseline by at least +3.5 percentage points, while analysis confirms that rubric scores align with ground-truth tests and can identify additional issues, with ablations highlighting the necessity of agentic context gathering for producing specific, unambiguous criteria.</div>
<div class="mono" style="margin-top:8px">本文针对软件工程（SWE）智能体的可扩展验证挑战，传统方法依赖代码执行但受限于环境设置开销，而现有可扩展替代方案如补丁分类器和启发式方法则缺乏代码库上下文基础和可解释性。为此，作者提出Agentic Rubrics方法，通过专家智能体与代码库交互生成基于上下文的评分清单，用于在不执行测试的情况下评估候选补丁。在SWE-Bench Verified上的实验结果表明，该方法在Qwen3-Coder-30B-A3B上获得54.2%的分数，在Qwen3-32B上获得40.6%的分数，比最强基线至少提升3.5个百分点；分析显示评分清单与真实测试结果一致并能识别额外问题，消融实验验证了智能体上下文收集对于生成具体、明确标准的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Reward Is Enough: LLMs Are In-Context Reinforcement Learners</div>
<div class="meta-line">Authors: Kefan Song, Amir Moeini, Peng Wang, Lei Gong, Rohan Chandra, Shangtong Zhang, Yanjun Qi</div>
<div class="meta-line">First: 2025-05-21T16:15:01+00:00 · Latest: 2026-01-07T17:58:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.06303v4">Abs</a> · <a href="https://arxiv.org/pdf/2506.06303v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is a framework for solving sequential decision-making problems. In this work, we demonstrate that, surprisingly, RL emerges during the inference time of large language models (LLMs), a phenomenon we term in-context RL (ICRL). To reveal this capability, we introduce a simple multi-round prompting framework, we call ICRL prompting, for inference-time self-improvement. The goal of ICRL prompting is to guide LLMs to perform reinforcement learning during inference for self-improvement on a given task. After each response, the model receives numerical scalar feedback, denoted as a reward. In the next round, we prompt the LLM again together with a context that concatenates all prior responses and their associated rewards. We consistently observe that response quality improves as the context grows. In other words, the LLM can optimize scalar reward signals during inference, exhibiting behavior analogous to reinforcement learning. We evaluate ICRL prompting on Game of 24, creative writing, ScienceWorld, and Olympiad-level math competitions (AIME and HMMT), demonstrating significant improvements over baselines such as Self-Refine and Reflexion. Notably, even when the reward signals are generated by the same LLM, ICRL prompting still improves performance, highlighting a promising new paradigm for test-time scaling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>奖励即足够：大语言模型是上下文强化学习者</div>
<div class="mono" style="margin-top:8px">强化学习（RL）是解决序列决策问题的框架。本研究发现，在大语言模型（LLM）的推理阶段会自发涌现强化学习行为，我们称之为上下文强化学习（ICRL）。为揭示此能力，我们提出了一个简单的多轮提示框架——ICRL提示法，用于实现推理时的自我改进。该框架旨在引导LLM在推理过程中通过强化学习优化特定任务表现：模型每轮生成响应后获得数值标量反馈（即奖励），下一轮提示时将历史响应及其对应奖励拼接为上下文输入。实验表明，随着上下文扩展，响应质量持续提升——LLM能在推理过程中优化标量奖励信号，表现出类强化学习行为。我们在24点游戏、创意写作、ScienceWorld及奥林匹克数学竞赛（AIME/HMMT）上评估ICRL提示法，其性能显著优于Self-Refine、Reflexion等基线方法。值得注意的是，即使奖励信号由同一LLM生成，ICRL仍能提升表现，这为测试时扩展提供了新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the hypothesis that reinforcement learning (RL) can emerge during the inference of large language models (LLMs) without explicit training, termed in-context RL (ICRL). The method introduces ICRL prompting, a multi-round framework where the LLM receives numerical reward feedback after each response and is reprompted with a context of prior responses and rewards to iteratively self-improve. Experimental results across tasks like Game of 24, creative writing, ScienceWorld, and math competitions show that response quality consistently improves with more rounds, outperforming baselines such as Self-Refine and Reflexion, even when rewards are generated by the same LLM, indicating a promising paradigm for test-time scaling.</div>
<div class="mono" style="margin-top:8px">本文的动机是假设强化学习（RL）可以在大型语言模型（LLM）的推理过程中自发涌现，而无需显式训练，这被称为上下文强化学习（ICRL）。方法上提出了ICRL提示框架，通过多轮交互，在每轮响应后给予数值奖励反馈，并将历史响应与奖励作为上下文重新提示模型，以实现迭代自我改进。实验结果表明，在24点游戏、创意写作、ScienceWorld和数学竞赛等任务中，响应质量随轮次增加持续提升，优于Self-Refine和Reflexion等基线方法，即使奖励由同一LLM生成也能改善性能，这为测试时扩展提供了新范式。</div>
</details>
</div>
<div class="card">
<div class="title">InfiniteWeb: Scalable Web Environment Synthesis for GUI Agent Training</div>
<div class="meta-line">Authors: Ziyun Zhang, Zezhou Wang, Xiaoyi Zhang, Zongyu Guo, Jiahao Li, Bin Li, Yan Lu</div>
<div class="meta-line">First: 2026-01-07T17:40:08+00:00 · Latest: 2026-01-07T17:40:08+00:00</div>
<div class="meta-line">Comments: Work In Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04126v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04126v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">GUI agents that interact with graphical interfaces on behalf of users represent a promising direction for practical AI assistants. However, training such agents is hindered by the scarcity of suitable environments. We present InfiniteWeb, a system that automatically generates functional web environments at scale for GUI agent training. While LLMs perform well on generating a single webpage, building a realistic and functional website with many interconnected pages faces challenges. We address these challenges through unified specification, task-centric test-driven development, and a combination of website seed with reference design image to ensure diversity. Our system also generates verifiable task evaluators enabling dense reward signals for reinforcement learning. Experiments show that InfiniteWeb surpasses commercial coding agents at realistic website construction, and GUI agents trained on our generated environments achieve significant performance improvements on OSWorld and Online-Mind2Web, demonstrating the effectiveness of proposed system.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InfiniteWeb：面向GUI智能体训练的可扩展网页环境合成系统</div>
<div class="mono" style="margin-top:8px">代表用户与图形界面交互的GUI智能体是实用AI助手的重要发展方向，但当前缺乏合适的训练环境制约了其发展。本文提出InfiniteWeb系统，能够自动大规模生成用于GUI智能体训练的功能性网页环境。虽然大语言模型在生成单个网页方面表现良好，但构建具有多页面互连的真实功能性网站仍面临挑战。我们通过统一规范、以任务为中心的测试驱动开发、结合网站种子与参考设计图像确保多样性等方法应对这些挑战。本系统还能生成可验证的任务评估器，为强化学习提供密集奖励信号。实验表明，InfiniteWeb在真实网站构建方面优于商业编程智能体，基于生成环境训练的GUI智能体在OSWorld和Online-Mind2Web基准上取得显著性能提升，证明了该系统的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for InfiniteWeb stems from the scarcity of suitable training environments for GUI agents that interact with graphical interfaces. The method introduces a system that automatically synthesizes functional web environments at scale by addressing challenges in generating multi-page websites through unified specification, task-centric test-driven development, and combining website seeds with reference design images to ensure diversity, while also generating verifiable task evaluators for dense reward signals. Main experimental results show that InfiniteWeb outperforms commercial coding agents in realistic website construction, and GUI agents trained on its generated environments achieve significant performance improvements on benchmarks like OSWorld and Online-Mind2Web, demonstrating the system&#x27;s effectiveness.</div>
<div class="mono" style="margin-top:8px">InfiniteWeb的动机源于缺乏适合图形用户界面（GUI）代理交互的训练环境。该方法提出了一个系统，通过统一规范、以任务为中心的测试驱动开发、结合网站种子与参考设计图像以确保多样性，来自动大规模合成功能性网络环境，并生成可验证的任务评估器以提供密集奖励信号。主要实验结果表明，InfiniteWeb在构建真实网站方面超越了商业编码代理，且在其生成环境中训练的GUI代理在OSWorld和Online-Mind2Web等基准测试上取得了显著性能提升，证明了该系统的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Quantifying the Impact of Modules and Their Interactions in the PSO-X Framework</div>
<div class="meta-line">Authors: Christian L. Camacho-Villalón, Ana Nikolikj, Katharina Dost, Eva Tuba, Sašo Džeroski, Tome Eftimov</div>
<div class="meta-line">First: 2026-01-07T17:06:05+00:00 · Latest: 2026-01-07T17:06:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04100v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04100v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The PSO-X framework incorporates dozens of modules that have been proposed for solving single-objective continuous optimization problems using particle swarm optimization. While modular frameworks enable users to automatically generate and configure algorithms tailored to specific optimization problems, the complexity of this process increases with the number of modules in the framework and the degrees of freedom defined for their interaction. Understanding how modules affect the performance of algorithms for different problems is critical to making the process of finding effective implementations more efficient and identifying promising areas for further investigation. Despite their practical applications and scientific relevance, there is a lack of empirical studies investigating which modules matter most in modular optimization frameworks and how they interact. In this paper, we analyze the performance of 1424 particle swarm optimization algorithms instantiated from the PSO-X framework on the 25 functions in the CEC&#x27;05 benchmark suite with 10 and 30 dimensions. We use functional ANOVA to quantify the impact of modules and their combinations on performance in different problem classes. In practice, this allows us to identify which modules have greater influence on PSO-X performance depending on problem features such as multimodality, mathematical transformations and varying dimensionality. We then perform a cluster analysis to identify groups of problem classes that share similar module effect patterns. Our results show low variability in the importance of modules in all problem classes, suggesting that particle swarm optimization performance is driven by a few influential modules.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>量化PSO-X框架中模块及其交互的影响</div>
<div class="mono" style="margin-top:8px">PSO-X框架整合了数十个用于解决单目标连续优化问题的粒子群优化模块。模块化框架虽能自动生成并配置针对特定优化问题的算法，但其复杂性随模块数量及交互自由度增加而提升。理解模块如何影响不同问题的算法性能，对于高效寻找有效实现方案及确定后续研究方向至关重要。尽管具有实际应用与科学意义，目前仍缺乏关于模块化优化框架中关键模块及其交互作用的实证研究。本文基于CEC&#x27;05基准测试集的25个函数（10维与30维），分析了从PSO-X框架实例化的1424种粒子群优化算法性能。通过函数方差分析量化模块及其组合在不同问题类别中对性能的影响，从而依据多模态性、数学变换及维度变化等特征识别对PSO-X性能影响显著的模块。进一步通过聚类分析发现具有相似模块效应模式的问题类别组。结果表明，所有问题类别中模块重要性变异度较低，表明粒子群优化性能主要由少数关键模块驱动。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study is motivated by the need to understand how individual modules and their interactions affect algorithm performance within the modular PSO-X framework for particle swarm optimization, as the growing complexity of such frameworks makes manual configuration inefficient. The method involves instantiating 1424 PSO algorithms from PSO-X and evaluating them on the CEC&#x27;05 benchmark suite across 10 and 30 dimensions, using functional ANOVA to quantify module impacts and cluster analysis to group problems by similar effect patterns. The main experimental results reveal that module importance exhibits low variability across different problem classes, indicating that PSO performance is primarily driven by only a few influential modules, which helps streamline algorithm configuration and guide future research.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决模块化粒子群优化框架PSO-X中，由于模块数量及交互自由度增加导致手动配置低效的问题，需要量化各模块及其交互对算法性能的影响。方法上，研究从PSO-X框架实例化了1424种算法，在CEC&#x27;05基准测试集的10维和30维问题上进行评估，采用功能方差分析量化模块影响，并通过聚类分析按模块效应模式对问题分类。主要实验结果表明，所有问题类别中模块的重要性变异度较低，这意味着粒子群优化性能主要由少数关键模块驱动，这有助于优化算法配置过程并指明未来研究方向。</div>
</details>
</div>
<div class="card">
<div class="title">Cells on Autopilot: Adaptive Cell (Re)Selection via Reinforcement Learning</div>
<div class="meta-line">Authors: Marvin Illian, Ramin Khalili, Antonio A. de A. Rocha, Lin Wang</div>
<div class="meta-line">First: 2026-01-07T16:51:33+00:00 · Latest: 2026-01-07T16:51:33+00:00</div>
<div class="meta-line">Comments: 11 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04083v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04083v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The widespread deployment of 5G networks, together with the coexistence of 4G/LTE networks, provides mobile devices a diverse set of candidate cells to connect to. However, associating mobile devices to cells to maximize overall network performance, a.k.a. cell (re)selection, remains a key challenge for mobile operators. Today, cell (re)selection parameters are typically configured manually based on operator experience and rarely adapted to dynamic network conditions. In this work, we ask: Can an agent automatically learn and adapt cell (re)selection parameters to consistently improve network performance? We present a reinforcement learning (RL)-based framework called CellPilot that adaptively tunes cell (re)selection parameters by learning spatiotemporal patterns of mobile network dynamics. Our study with real-world data demonstrates that even a lightweight RL agent can outperform conventional heuristic reconfigurations by up to 167%, while generalizing effectively across different network scenarios. These results indicate that data-driven approaches can significantly improve cell (re)selection configurations and enhance mobile network performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自动驾驶的蜂窝：基于强化学习的自适应蜂窝（重）选择</div>
<div class="mono" style="margin-top:8px">5G网络的广泛部署与4G/LTE网络的共存，为移动设备提供了多样化的候选蜂窝连接选择。然而，如何将移动设备关联至蜂窝以最大化整体网络性能——即蜂窝（重）选择——仍是移动运营商面临的关键挑战。当前，蜂窝（重）选择参数通常基于运营商经验手动配置，极少适应动态网络条件。本研究提出：能否通过智能体自动学习并调整蜂窝（重）选择参数，持续提升网络性能？我们提出名为CellPilot的强化学习框架，通过学习移动网络动态的时空模式自适应调整蜂窝（重）选择参数。基于真实数据的实验表明，即使轻量级强化学习智能体也能超越传统启发式重配置方法达167%，并在不同网络场景中展现有效泛化能力。这些结果证明数据驱动方法可显著优化蜂窝（重）选择配置，提升移动网络性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of manually configuring cell (re)selection parameters in dynamic 4G/5G networks, this paper introduces CellPilot, a reinforcement learning framework that autonomously adapts these parameters by learning spatiotemporal network patterns. The method employs a lightweight RL agent to tune configurations in response to real network conditions. Experimental results using real-world data show that this approach outperforms conventional heuristic reconfigurations by up to 167% in performance improvement and generalizes effectively across diverse network scenarios, demonstrating the potential of data-driven methods to enhance mobile network performance.</div>
<div class="mono" style="margin-top:8px">本文针对4G/5G动态网络中手动配置小区（重）选择参数的挑战，提出了CellPilot这一强化学习框架，通过学习时空网络模式来自主调整参数。该方法采用轻量级强化学习智能体，根据实际网络条件自适应配置。基于真实数据的实验结果表明，该方法相比传统启发式重配置性能提升最高达167%，并能有效泛化到不同网络场景，证明了数据驱动方法在提升移动网络性能方面的显著潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Uncertainty-Aware Robotic World Model Makes Offline Model-Based Reinforcement Learning Work on Real Robots</div>
<div class="meta-line">Authors: Chenhao Li, Andreas Krause, Marco Hutter</div>
<div class="meta-line">First: 2025-04-23T12:58:15+00:00 · Latest: 2026-01-07T15:37:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.16680v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.16680v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has achieved impressive results in robotics, yet high-performing pipelines remain highly task-specific, with little reuse of prior data. Offline Model-based RL (MBRL) offers greater data efficiency by training policies entirely from existing datasets, but suffers from compounding errors and distribution shift in long-horizon rollouts. Although existing methods have shown success in controlled simulation benchmarks, robustly applying them to the noisy, biased, and partially observed datasets typical of real-world robotics remains challenging. We present a principled pipeline for making offline MBRL effective on physical robots. Our RWM-U extends autoregressive world models with epistemic uncertainty estimation, enabling temporally consistent multi-step rollouts with uncertainty effectively propagated over long horizons. We combine RWM-U with MOPO-PPO, which adapts uncertainty-penalized policy optimization to the stable, on-policy PPO framework for real-world control. We evaluate our approach on diverse manipulation and locomotion tasks in simulation and on real quadruped and humanoid, training policies entirely from offline datasets. The resulting policies consistently outperform model-free and uncertainty-unaware model-based baselines, and fusing real-world data in model learning further yields robust policies that surpass online model-free baselines trained solely in simulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>不确定性感知机器人世界模型使离线模型强化学习在真实机器人上生效</div>
<div class="mono" style="margin-top:8px">强化学习（RL）在机器人领域已取得显著成果，但高性能流程仍高度任务特定，对先前数据的复用极少。离线模型强化学习（MBRL）通过完全基于现有数据集训练策略，提供了更高的数据效率，但在长时程推演中易受复合误差和分布偏移影响。尽管现有方法在受控仿真基准测试中已展现成功，但将其稳健应用于真实机器人领域典型的噪声、偏差及部分可观测数据集仍具挑战。我们提出了一种原则性流程，使离线MBRL在实体机器人上有效。我们的RWM-U通过认知不确定性估计扩展了自回归世界模型，实现了时间一致的多步推演，并将不确定性在长时程中有效传播。我们将RWM-U与MOPO-PPO结合，后者将不确定性惩罚策略优化适配至稳定的同策略PPO框架，以用于现实世界控制。我们在仿真及真实四足与人形机器人上，通过多样化的操作与移动任务评估本方法，完全基于离线数据集训练策略。所得策略持续优于无模型及无不确定性感知的模型基线，且在模型学习中融合真实世界数据进一步产生了超越纯仿真训练的在线无模型基线的稳健策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of applying offline model-based reinforcement learning (MBRL) to real robots, where noisy and biased datasets often lead to compounding errors and distribution shift. The authors propose RWM-U, an autoregressive world model enhanced with epistemic uncertainty estimation to enable temporally consistent long-horizon rollouts, combined with MOPO-PPO for uncertainty-penalized policy optimization adapted to the stable PPO framework. Experimental results on manipulation and locomotion tasks in simulation and on real quadruped and humanoid robots show that policies trained entirely from offline data outperform model-free and uncertainty-unaware model-based baselines, and incorporating real-world data further yields robust policies surpassing online model-free baselines trained in simulation.</div>
<div class="mono" style="margin-top:8px">本文针对离线模型强化学习在真实机器人应用中因数据噪声和偏差导致累积误差与分布偏移的挑战，提出了一种改进方法。作者开发了RWM-U，这是一种结合认知不确定性估计的自回归世界模型，能够实现时间一致的长时程推演，并与适应稳定PPO框架的不确定性惩罚策略优化方法MOPO-PPO相结合。在仿真和真实四足及人形机器人的多种操作与运动任务实验中，完全基于离线数据训练的策略一致优于无模型及无不确定性感知的模型基线，且融合真实世界数据进一步产生了超越仅仿真训练的在线无模型基线的鲁棒策略。</div>
</details>
</div>
<div class="card">
<div class="title">Graph Reinforcement Learning for Power Grids: A Comprehensive Survey</div>
<div class="meta-line">Authors: Mohamed Hassouna, Clara Holzhüter, Pawel Lytaev, Josephine Thomas, Bernhard Sick, Christoph Scholz</div>
<div class="meta-line">First: 2024-07-05T14:07:15+00:00 · Latest: 2026-01-07T15:09:22+00:00</div>
<div class="meta-line">Comments: Accepted in Energy &amp; AI, in-press</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2407.04522v4">Abs</a> · <a href="https://arxiv.org/pdf/2407.04522v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The increasing share of renewable energy and distributed electricity generation requires the development of deep learning approaches to address the lack of flexibility inherent in traditional power grid methods. In this context, Graph Neural Networks are a promising solution due to their ability to learn from graph-structured data. Combined with Reinforcement Learning, they can be used as control approaches to determine remedial actions. This review analyses how Graph Reinforcement Learning can improve representation learning and decision-making in power grid applications, particularly transmission and distribution grids. We analyze the reviewed approaches in terms of the graph structure, the Graph Neural Network architecture, and the Reinforcement Learning approach. Although Graph Reinforcement Learning has demonstrated adaptability to unpredictable events and noisy data, its current stage is primarily proof-of-concept, and it is not yet deployable to real-world applications. We highlight the open challenges and limitations for real-world applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向电网的图强化学习：全面综述</div>
<div class="mono" style="margin-top:8px">可再生能源与分布式发电占比的持续提升，亟需发展深度学习方法来应对传统电网方法固有的灵活性不足问题。在此背景下，图神经网络因其能从图结构数据中学习的能力而成为前景广阔的解决方案。结合强化学习技术，可将其作为控制方法来确定补救措施。本文综述分析了图强化学习如何提升电网应用（特别是输配电网络）中的表征学习与决策能力，并从图结构、图神经网络架构及强化学习方法三个维度对现有研究进行了系统剖析。尽管图强化学习已展现出对不可预测事件与噪声数据的适应能力，但其现阶段仍主要处于概念验证期，尚未能部署至实际应用。本文进一步指出了实际应用面临的关键挑战与局限。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This survey is motivated by the need for more flexible control methods in power grids due to the rise of renewable energy, which traditional approaches lack. The method reviewed is Graph Reinforcement Learning (GRL), which combines Graph Neural Networks to learn from grid topology with Reinforcement Learning for decision-making, analyzing specific implementations based on graph structure, network architecture, and RL approach. The main experimental insight from the reviewed studies is that GRL has shown promise in handling unpredictable events and noisy data in applications like transmission and distribution grid control, but the field remains largely at a proof-of-concept stage, not yet ready for real-world deployment, with significant open challenges identified.</div>
<div class="mono" style="margin-top:8px">本综述的动机是，随着可再生能源占比提高，传统电网方法缺乏灵活性，因此需要开发更灵活的控制方法。所综述的方法是图强化学习，它结合了从电网拓扑中学习的图神经网络和用于决策的强化学习，并基于图结构、网络架构和强化学习方法对具体实现进行了分析。从所综述的研究中得到的主要实验结论是，图强化学习在输配电网格控制等应用中，已显示出处理不可预测事件和噪声数据的潜力，但该领域目前主要处于概念验证阶段，尚未准备好投入实际应用，且存在明显的开放性挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Anti-Length Shift: Dynamic Outlier Truncation for Training Efficient Reasoning Models</div>
<div class="meta-line">Authors: Wei Wu, Liyi Chen, Congxi Xiao, Tianfu Wang, Qimeng Wang, Chengqiang Lu, Yan Gao, Yi Wu, Yao Hu, Hui Xiong</div>
<div class="meta-line">First: 2026-01-07T14:31:07+00:00 · Latest: 2026-01-07T14:31:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03969v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03969v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models enhanced by reinforcement learning with verifiable rewards have achieved significant performance gains by extending their chain-of-thought. However, this paradigm incurs substantial deployment costs as models often exhibit excessive verbosity on simple queries. Existing efficient reasoning methods relying on explicit length penalties often introduce optimization conflicts and leave the generative mechanisms driving overthinking largely unexamined. In this paper, we identify a phenomenon termed length shift where models increasingly generate unnecessary reasoning on trivial inputs during training. To address this, we introduce Dynamic Outlier Truncation (DOT), a training-time intervention that selectively suppresses redundant tokens. This method targets only the extreme tail of response lengths within fully correct rollout groups while preserving long-horizon reasoning capabilities for complex problems. To complement this intervention and ensure stable convergence, we further incorporate auxiliary KL regularization and predictive dynamic sampling. Experimental results across multiple model scales demonstrate that our approach significantly pushes the efficiency-performance Pareto frontier outward. Notably, on the AIME-24, our method reduces inference token usage by 78% while simultaneously increasing accuracy compared to the initial policy and surpassing state-of-the-art efficient reasoning methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>抗长度偏移：面向高效推理模型训练的动态异常截断方法</div>
<div class="mono" style="margin-top:8px">通过可验证奖励的强化学习增强的大型推理模型，通过扩展其思维链已取得显著性能提升。然而，该范式在简单查询上常表现出过度冗余，导致部署成本大幅增加。现有依赖显式长度惩罚的高效推理方法常引发优化冲突，且对驱动过度思考的生成机制缺乏深入探究。本文发现一种称为&#x27;长度偏移&#x27;的现象：模型在训练过程中对简单输入逐渐产生不必要的推理。为此，我们提出动态异常截断（DOT），这是一种训练时干预方法，能选择性抑制冗余标记。该方法仅针对完全正确的推演组中响应长度的极端尾部进行处理，同时保留对复杂问题的长程推理能力。为配合此干预并确保稳定收敛，我们进一步引入辅助KL正则化与预测性动态采样。多模型规模的实验结果表明，我们的方法显著拓展了效率-性能的帕累托前沿。值得注意的是，在AIME-24基准上，本方法在比初始策略提升准确率的同时减少78%的推理标记使用量，且优于当前最先进的高效推理方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the excessive verbosity and high deployment costs of large reasoning models trained with reinforcement learning, which often generate unnecessarily long chains of thought on simple queries. To address this, the authors identify a &#x27;length shift&#x27; phenomenon and propose Dynamic Outlier Truncation (DOT), a training-time method that selectively truncates redundant tokens from the extreme tail of response lengths in correct rollouts, while preserving long reasoning for complex problems, aided by auxiliary KL regularization and predictive dynamic sampling for stability. Experimental results across multiple model scales show the method significantly improves the efficiency-performance trade-off, notably reducing inference token usage by 78% on AIME-24 while increasing accuracy compared to the initial policy and outperforming state-of-the-art efficient reasoning methods.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，通过强化学习训练的大型推理模型在简单查询上常产生过度冗长的思维链，导致部署成本高昂。为解决此问题，作者识别出一种“长度偏移”现象，并提出了动态异常截断方法，这是一种训练时干预技术，选择性地截断完全正确响应中长度极端尾部的冗余标记，同时保留复杂问题的长程推理能力，辅以辅助KL正则化和预测性动态采样以确保稳定收敛。实验结果表明，该方法在多模型规模上显著提升了效率与性能的帕累托前沿，特别是在AIME-24数据集上，相比初始策略，推理令牌使用量减少78%的同时准确率提高，并超越了现有高效推理方法。</div>
</details>
</div>
<div class="card">
<div class="title">Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification</div>
<div class="meta-line">Authors: Rui Sun, Yifan Sun, Sheng Xu, Li Zhao, Jing Li, Daxin Jiang, Chen Hua, Zuo Bai</div>
<div class="meta-line">First: 2026-01-07T14:03:22+00:00 · Latest: 2026-01-07T14:03:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03948v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03948v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to achieve remarkable reasoning in domains like mathematics and coding, where verifiable rewards provide clear signals. However, extending this paradigm to financial decision is challenged by the market&#x27;s stochastic nature: rewards are verifiable but inherently noisy, causing standard RL to degenerate into reward hacking. To address this, we propose Trade-R1, a model training framework that bridges verifiable rewards to stochastic environments via process-level reasoning verification. Our key innovation is a verification method that transforms the problem of evaluating reasoning over lengthy financial documents into a structured Retrieval-Augmented Generation (RAG) task. We construct a triangular consistency metric, assessing pairwise alignment between retrieved evidence, reasoning chains, and decisions to serve as a validity filter for noisy market returns. We explore two reward integration strategies: Fixed-effect Semantic Reward (FSR) for stable alignment signals, and Dynamic-effect Semantic Reward (DSR) for coupled magnitude optimization. Experiments on different country asset selection demonstrate that our paradigm reduces reward hacking, with DSR achieving superior cross-market generalization while maintaining the highest reasoning consistency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Trade-R1：通过过程级推理验证将可验证奖励桥接至随机环境</div>
<div class="mono" style="margin-top:8px">强化学习（RL）使大型语言模型（LLM）在数学和编程等可验证奖励提供明确信号的领域实现了卓越的推理能力。然而，将这一范式扩展到金融决策面临市场随机性的挑战：奖励可验证但本质上是噪声的，导致标准RL退化为奖励黑客行为。为此，我们提出Trade-R1，一种通过过程级推理验证将可验证奖励桥接至随机环境的模型训练框架。我们的核心创新是一种验证方法，将评估长篇金融文档推理的问题转化为结构化检索增强生成（RAG）任务。我们构建了一个三角一致性度量，评估检索证据、推理链和决策之间的两两对齐，作为噪声市场回报的有效性过滤器。我们探索了两种奖励整合策略：用于稳定对齐信号的固定效应语义奖励（FSR），以及用于耦合幅度优化的动态效应语义奖励（DSR）。在不同国家资产选择上的实验表明，我们的范式减少了奖励黑客行为，其中DSR在保持最高推理一致性的同时实现了优越的跨市场泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of applying reinforcement learning (RL) to financial decision-making, where market rewards are verifiable but inherently noisy, leading standard RL to degenerate into reward hacking. The authors propose Trade-R1, a framework that bridges verifiable rewards to stochastic environments through process-level reasoning verification, transforming the evaluation of reasoning over financial documents into a structured Retrieval-Augmented Generation (RAG) task. They introduce a triangular consistency metric to assess alignment between retrieved evidence, reasoning chains, and decisions, serving as a validity filter for noisy returns, and explore two reward integration strategies: Fixed-effect Semantic Reward (FSR) for stable signals and Dynamic-effect Semantic Reward (DSR) for coupled optimization. Experimental results on cross-country asset selection show that this paradigm reduces reward hacking, with DSR achieving superior generalization and the highest reasoning consistency.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习在金融决策中的应用挑战，即市场回报可验证但本质具有噪声，导致标准强化学习退化为奖励黑客行为。作者提出了Trade-R1框架，通过过程级推理验证将可验证奖励与随机环境连接起来，将评估长篇金融文档推理的问题转化为结构化检索增强生成任务。他们引入三角一致性度量来评估检索证据、推理链和决策之间的对齐性，作为噪声回报的有效性过滤器，并探索了两种奖励整合策略：用于稳定对齐信号的固定效应语义奖励和用于耦合幅度优化的动态效应语义奖励。在不同国家资产选择上的实验结果表明，该范式减少了奖励黑客行为，其中动态效应语义奖励实现了更优的跨市场泛化能力并保持了最高的推理一致性。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive-Boundary-Clipping GRPO: Ensuring Bounded Ratios for Stable and Generalizable Training</div>
<div class="meta-line">Authors: Chi Liu, Xin Chen</div>
<div class="meta-line">First: 2026-01-07T13:04:52+00:00 · Latest: 2026-01-07T13:04:52+00:00</div>
<div class="meta-line">Comments: 10 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03895v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03895v1">PDF</a> · <a href="https://github.com/chi2liu/ABC-GRPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Group Relative Policy Optimization (GRPO) has emerged as a popular algorithm for reinforcement learning with large language models (LLMs). However, upon analyzing its clipping mechanism, we argue that it is suboptimal in certain scenarios. With appropriate modifications, GRPO can be significantly enhanced to improve both flexibility and generalization. To this end, we propose Adaptive-Boundary-Clipping GRPO (ABC-GRPO), an asymmetric and adaptive refinement of the original GRPO framework. We demonstrate that ABC-GRPO achieves superior performance over standard GRPO on mathematical reasoning tasks using the Qwen3 LLMs. Moreover, ABC-GRPO maintains substantially higher entropy throughout training, thereby preserving the model&#x27;s exploration capacity and mitigating premature convergence. The implementation code is available online to ease reproducibility https://github.com/chi2liu/ABC-GRPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自适应边界裁剪GRPO：通过有界比率确保稳定且可泛化的训练</div>
<div class="mono" style="margin-top:8px">群体相对策略优化（GRPO）已成为大语言模型（LLM）强化学习的主流算法。然而，通过分析其裁剪机制，我们认为其在某些场景下存在不足。经过适当改进，GRPO的灵活性与泛化能力均可显著提升。为此，我们提出自适应边界裁剪GRPO（ABC-GRPO），这是对原始GRPO框架的非对称自适应改进。实验表明，在基于Qwen3大语言模型的数学推理任务中，ABC-GRPO较标准GRPO取得更优性能。此外，ABC-GRPO在训练全程保持显著更高的熵值，从而维持模型探索能力并缓解早熟收敛。实现代码已开源以促进可复现性：https://github.com/chi2liu/ABC-GRPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from identified limitations in the clipping mechanism of Group Relative Policy Optimization (GRPO), a popular reinforcement learning algorithm for large language models, which is argued to be suboptimal in certain scenarios. The method introduces Adaptive-Boundary-Clipping GRPO (ABC-GRPO), an asymmetric and adaptive refinement designed to enhance flexibility and generalization by ensuring bounded ratios for more stable training. Main experimental results demonstrate that ABC-GRPO achieves superior performance over standard GRPO on mathematical reasoning tasks using Qwen3 LLMs, while maintaining substantially higher entropy throughout training to preserve exploration capacity and mitigate premature convergence.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于对群组相对策略优化（GRPO）裁剪机制局限性的分析，该机制是大型语言模型强化学习中常用算法，但在某些场景下被认为不够理想。方法上提出了自适应边界裁剪GRPO（ABC-GRPO），这是一种非对称且自适应的改进框架，通过确保有界比率来增强训练灵活性和泛化能力。主要实验结果表明，在基于Qwen3大型语言模型的数学推理任务中，ABC-GRPO相比标准GRPO取得了更优性能，同时在训练全程保持显著更高的熵值，从而保留了模型的探索能力并缓解了过早收敛问题。</div>
</details>
</div>
<div class="card">
<div class="title">IndexTTS 2.5 Technical Report</div>
<div class="meta-line">Authors: Yunpei Li, Xun Zhou, Jinchao Wang, Lu Wang, Yong Wu, Siyi Zhou, Yiquan Zhou, Jingchen Shu</div>
<div class="meta-line">First: 2026-01-07T12:58:16+00:00 · Latest: 2026-01-07T12:58:16+00:00</div>
<div class="meta-line">Comments: 11 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03888v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03888v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In prior work, we introduced IndexTTS 2, a zero-shot neural text-to-speech foundation model comprising two core components: a transformer-based Text-to-Semantic (T2S) module and a non-autoregressive Semantic-to-Mel (S2M) module, which together enable faithful emotion replication and establish the first autoregressive duration-controllable generative paradigm. Building upon this, we present IndexTTS 2.5, which significantly enhances multilingual coverage, inference speed, and overall synthesis quality through four key improvements: 1) Semantic Codec Compression: we reduce the semantic codec frame rate from 50 Hz to 25 Hz, halving sequence length and substantially lowering both training and inference costs; 2) Architectural Upgrade: we replace the U-DiT-based backbone of the S2M module with a more efficient Zipformer-based modeling architecture, achieving notable parameter reduction and faster mel-spectrogram generation; 3) Multilingual Extension: We propose three explicit cross-lingual modeling strategies, boundary-aware alignment, token-level concatenation, and instruction-guided generation, establishing practical design principles for zero-shot multilingual emotional TTS that supports Chinese, English, Japanese, and Spanish, and enables robust emotion transfer even without target-language emotional training data; 4) Reinforcement Learning Optimization: we apply GRPO in post-training of the T2S module, improving pronunciation accuracy and natrualness. Experiments show that IndexTTS 2.5 not only supports broader language coverage but also replicates emotional prosody in unseen languages under the same zero-shot setting. IndexTTS 2.5 achieves a 2.28 times improvement in RTF while maintaining comparable WER and speaker similarity to IndexTTS 2.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IndexTTS 2.5 技术报告</div>
<div class="mono" style="margin-top:8px">在先前工作中，我们提出了 IndexTTS 2——一个零样本神经文本转语音基础模型，包含两个核心组件：基于 Transformer 的文本到语义（T2S）模块和非自回归的语义到梅尔频谱（S2M）模块，二者共同实现了忠实的情感复现，并建立了首个自回归时长可控的生成范式。在此基础上，我们推出 IndexTTS 2.5，通过四项关键改进显著提升了多语言覆盖范围、推理速度和整体合成质量：1）语义编解码器压缩：将语义编解码器帧率从 50 Hz 降至 25 Hz，序列长度减半，大幅降低训练和推理成本；2）架构升级：将 S2M 模块基于 U-DiT 的主干网络替换为更高效的基于 Zipformer 的建模架构，实现显著参数减少和更快的梅尔频谱生成；3）多语言扩展：提出三种显式跨语言建模策略——边界感知对齐、词元级拼接和指令引导生成，为零样本多语言情感 TTS（支持中文、英文、日文和西班牙文）建立了实用设计原则，即使没有目标语言情感训练数据也能实现鲁棒的情感迁移；4）强化学习优化：在 T2S 模块的后训练中应用 GRPO，提升了发音准确性和自然度。实验表明，IndexTTS 2.5 不仅支持更广泛的语言覆盖，还能在相同零样本设置下复现未见语言的情感韵律。IndexTTS 2.5 在保持与 IndexTTS 2 相当的词错误率和说话人相似度的同时，实现了 2.28 倍的实时因子提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work introduces IndexTTS 2.5, an enhanced zero-shot text-to-speech foundation model motivated by the need to improve upon its predecessor&#x27;s multilingual coverage, inference speed, and synthesis quality. The method incorporates four key improvements: compressing the semantic codec frame rate to reduce sequence length, upgrading the mel-spectrogram generation architecture to a more efficient Zipformer-based design, implementing explicit cross-lingual modeling strategies for multilingual support, and applying reinforcement learning to optimize pronunciation. Experimental results demonstrate that IndexTTS 2.5 successfully extends language coverage to Chinese, English, Japanese, and Spanish, enables robust zero-shot emotion transfer across languages, and achieves a 2.28x improvement in real-time factor while maintaining word error rate and speaker similarity comparable to the prior version.</div>
<div class="mono" style="margin-top:8px">本研究提出了IndexTTS 2.5，这是一个增强的零样本文本到语音基础模型，其动机是在其前代模型的基础上提升多语言覆盖、推理速度和合成质量。方法包含四项关键改进：压缩语义编解码器帧率以缩短序列长度，将梅尔频谱图生成架构升级为更高效的基于Zipformer的设计，实施显式的跨语言建模策略以支持多语言，并应用强化学习来优化发音。实验结果表明，IndexTTS 2.5成功将语言覆盖扩展至中文、英文、日文和西班牙文，能够在零样本设置下实现跨语言的鲁棒情感迁移，并在保持与先前版本相当的词错误率和说话人相似度的同时，将实时因子提升了2.28倍。</div>
</details>
</div>
<div class="card">
<div class="title">Practitioner Motives to Use Different Hyperparameter Optimization Methods</div>
<div class="meta-line">Authors: Niclas Kannengießer, Niklas Hasebrook, Felix Morsbach, Marc-André Zöller, Jörg Franke, Marius Lindauer, Frank Hutter, Ali Sunyaev</div>
<div class="meta-line">Venue: ACM Transactions on Computer-Human Interaction, Volume 32, Issue 6, 2025</div>
<div class="meta-line">First: 2022-03-03T13:55:38+00:00 · Latest: 2026-01-07T12:27:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2203.01717v5">Abs</a> · <a href="https://arxiv.org/pdf/2203.01717v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Programmatic hyperparameter optimization (HPO) methods, such as Bayesian optimization and evolutionary algorithms, are highly sample-efficient in identifying optimal hyperparameter configurations for machine learning (ML) models. However, practitioners frequently use less efficient methods, such as grid search, which can lead to under-optimized models. We suspect this behavior is driven by a range of practitioner-specific motives. Practitioner motives, however, still need to be clarified to enhance user-centered development of HPO tools. To uncover practitioner motives to use different HPO methods, we conducted 20 semi-structured interviews and an online survey with 49 ML experts. By presenting main goals (e.g., increase ML model understanding) and contextual factors affecting practitioners&#x27; selection of HPO methods (e.g., available computer resources), this study offers a conceptual foundation to better understand why practitioners use different HPO methods, supporting development of more user-centered and context-adaptive HPO tools in automated ML.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从业者采用不同超参数优化方法的动机</div>
<div class="mono" style="margin-top:8px">程序化超参数优化（HPO）方法，如贝叶斯优化和进化算法，在寻找机器学习模型最优超参数配置时具有极高的样本效率。然而，从业者常使用效率较低的方法（如网格搜索），可能导致模型优化不足。我们推测这一行为源于一系列从业者特有的动机。为促进以用户为中心的HPO工具开发，需明确这些动机。为此，我们通过20次半结构化访谈和一项针对49位机器学习专家的在线调查，揭示了从业者选择不同HPO方法的动机。本研究通过呈现主要目标（如提升对机器学习模型的理解）及影响HPO方法选择的背景因素（如可用计算资源），为理解从业者行为提供了概念基础，有助于开发更以用户为中心、适应上下文的自动化机器学习HPO工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the discrepancy between the sample efficiency of advanced hyperparameter optimization (HPO) methods like Bayesian optimization and the continued practitioner preference for less efficient methods such as grid search, this study aims to clarify the underlying practitioner motives to inform more user-centered HPO tool development. The method involved conducting 20 semi-structured interviews and an online survey with 49 machine learning experts to systematically uncover the goals and contextual factors influencing HPO method selection. The main experimental results identify key practitioner motives, including the desire to increase model understanding and constraints like available computational resources, providing a conceptual foundation for developing more context-adaptive automated ML tools.</div>
<div class="mono" style="margin-top:8px">本研究动机源于先进的超参数优化方法（如贝叶斯优化）虽具有高样本效率，但从业者仍常使用网格搜索等低效方法，为理解这一行为背后的动机以开发更以用户为中心的工具。研究方法包括对49位机器学习专家进行20次半结构化访谈和在线调查，以系统揭示影响超参数优化方法选择的目标和情境因素。主要实验结果明确了从业者的关键动机，如增强模型理解的需求以及可用计算资源等约束条件，为开发更具情境适应性的自动化机器学习工具提供了概念基础。</div>
</details>
</div>
<div class="card">
<div class="title">Parametric Expensive Multi-Objective Optimization via Generative Solution Modeling</div>
<div class="meta-line">Authors: Tingyang Wei, Jiao Liu, Abhishek Gupta, Chin Chun Ooi, Puay Siew Tan, Yew-Soon Ong</div>
<div class="meta-line">First: 2025-11-12T15:13:27+00:00 · Latest: 2026-01-07T12:19:33+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.09598v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.09598v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many real-world applications require solving families of expensive multi-objective optimization problems~(EMOPs) under varying operational conditions. This gives rise to parametric expensive multi-objective optimization problems (P-EMOPs) where each task parameter defines a distinct optimization instance. Current multi-objective Bayesian optimization methods have been widely used for finding finite sets of Pareto optimal solutions for individual tasks. However, P-EMOPs present a fundamental challenge: the continuous task parameter space can contain infinite distinct problems, each requiring separate expensive evaluations. This demands learning an inverse model that can directly predict optimized solutions for any task-preference query without expensive re-evaluation. This paper introduces a novel parametric multi-task multi-objective Bayesian optimizer that learns this inverse model by alternating between (1) acquisition-driven search leveraging inter-task synergies and (2) generative solution sampling via conditional generative models. This approach enables efficient optimization across related tasks and finally achieves direct solution prediction for unseen parameterized EMOPs without additional expensive evaluations. We theoretically justify the faster convergence by leveraging inter-task synergies through task-aware Gaussian processes. Meanwhile, based on that, empirical studies of our optimizer and inverse model in synthetic and real-world benchmarks further verify the effectiveness of the proposed generative alternating framework.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于生成式解建模的参数化昂贵多目标优化</div>
<div class="mono" style="margin-top:8px">许多实际应用需要在不同操作条件下求解一系列昂贵的多目标优化问题（EMOPs），由此产生了参数化昂贵多目标优化问题（P-EMOPs），其中每个任务参数定义了一个独立的优化实例。当前的多目标贝叶斯优化方法已广泛用于为单个任务寻找有限的帕累托最优解集。然而，P-EMOPs面临一个根本性挑战：连续的任务参数空间可能包含无限个不同问题，每个问题都需要独立的昂贵评估。这要求学习一个能够直接预测任意任务-偏好查询的优化解而无需昂贵重评估的逆模型。本文提出了一种新颖的参数化多任务多目标贝叶斯优化器，通过交替进行（1）利用任务间协同性的采集驱动搜索与（2）基于条件生成模型的生成式解采样，来学习该逆模型。该方法实现了跨相关任务的高效优化，并最终能够对未见过的参数化EMOPs进行直接解预测，无需额外昂贵评估。我们通过任务感知高斯过程利用任务间协同性，从理论上证明了更快的收敛速度。同时，基于此，我们在合成与真实基准测试中对优化器及逆模型进行的实证研究进一步验证了所提出的生成式交替框架的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of parametric expensive multi-objective optimization problems (P-EMOPs), where infinite distinct optimization instances arise from varying task parameters, each requiring costly evaluations. To avoid expensive re-evaluation for each new task, the authors propose a parametric multi-task Bayesian optimizer that alternates between acquisition-driven search, leveraging inter-task synergies via task-aware Gaussian processes for faster convergence, and generative solution sampling using conditional generative models to learn an inverse model. Experimental results on synthetic and real-world benchmarks demonstrate that this generative alternating framework effectively enables direct prediction of optimized solutions for unseen parameterized problems without additional expensive evaluations.</div>
<div class="mono" style="margin-top:8px">本文针对参数化昂贵多目标优化问题（P-EMOPs）的挑战展开研究，该问题中任务参数变化会产生无限个不同的优化实例，且每个实例都需要昂贵的评估。为避免为每个新任务进行昂贵重评估，作者提出一种参数化多任务贝叶斯优化器，该方法交替进行利用任务间协同效应的采集驱动搜索（通过任务感知高斯过程实现更快收敛）和使用条件生成模型的生成式解采样，以学习一个逆模型。在合成和真实世界基准测试中的实验结果表明，该生成式交替框架能有效实现对未见参数化问题的优化解的直接预测，而无需额外的昂贵评估。</div>
</details>
</div>
<div class="card">
<div class="title">ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition</div>
<div class="meta-line">Authors: Muyang Zhao, Qi Qi, Hao Sun</div>
<div class="meta-line">First: 2026-01-07T11:30:55+00:00 · Latest: 2026-01-07T11:30:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03822v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03822v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) can achieve strong reasoning performance with sufficient computation, but they do not inherently know how much computation a task requires. We study budgeted inference-time reasoning for multiple tasks under a strict global token constraint and formalize it as a Ordered Stochastic Multiple-Choice Knapsack Problem(OS-MCKP). This perspective highlights a meta-cognitive requirement -- anticipating task difficulty, estimating return over investment (ROI), and allocating computation strategically. We propose ROI-Reasoning, a two-stage framework that endows LLMs with intrinsic, budget-aware rationality. In the first stage, Meta-Cognitive Fine-Tuning teaches models to predict reasoning cost and expected utility before generation, enabling explicit solve-or-skip decisions. Next, Rationality-Aware Reinforcement Learning optimizes sequential decision making under a hard token budget, allowing models to learn long-horizon allocation strategies. Across budgeted mathematical reasoning benchmarks, ROI-Reasoning consistently improves overall score while substantially reducing regret under tight computation budgets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ROI推理：通过预计算元认知实现推理过程的理性优化</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）在充足计算资源下可实现强大的推理性能，但其本身无法预知任务所需计算量。本研究在严格全局令牌约束下探索多任务的预算推理问题，并将其形式化为有序随机多选择背包问题（OS-MCKP）。该视角揭示了元认知需求——预测任务难度、估算投资回报率（ROI）并策略性分配计算资源。我们提出ROI推理框架，通过两阶段机制赋予LLMs内在的预算感知理性能力：第一阶段通过元认知微调使模型在生成前预测推理成本与预期效用，实现显式的求解/跳过决策；第二阶段通过理性感知强化学习在硬性令牌预算下优化序列决策，使模型习得长期视野的资源分配策略。在多个预算数学推理基准测试中，ROI推理框架在严格计算预算下持续提升总体得分，同时显著降低决策遗憾。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this work is that while large language models (LLMs) can perform well with ample computation, they lack an intrinsic ability to judge how much computational effort a given reasoning task requires, which becomes critical under strict global token budgets. The method, named ROI-Reasoning, addresses this by formalizing budgeted inference as an Ordered Stochastic Multiple-Choice Knapsack Problem and introduces a two-stage framework: first, Meta-Cognitive Fine-Tuning teaches models to pre-compute predictions of reasoning cost and expected utility to make solve-or-skip decisions; second, Rationality-Aware Reinforcement Learning optimizes sequential decision-making under a hard token budget to learn long-horizon allocation strategies. The main experimental results show that across budgeted mathematical reasoning benchmarks, ROI-Reasoning consistently improves overall scores while significantly reducing regret, especially under tight computation constraints.</div>
<div class="mono" style="margin-top:8px">这项研究的动机在于，尽管大语言模型（LLMs）在充足计算资源下能表现出强大的推理能力，但它们缺乏内在能力来判断不同任务所需的计算量，这在严格的全局令牌预算下尤为关键。该方法名为ROI-Reasoning，通过将预算推理形式化为有序随机多选择背包问题，提出了一个两阶段框架：首先，元认知微调教导模型在生成前预测推理成本和预期效用，从而做出解决或跳过的明确决策；其次，理性感知强化学习在硬令牌预算下优化序列决策，使模型学习长期分配策略。主要实验结果表明，在多个预算数学推理基准测试中，ROI-Reasoning持续提高了总体得分，同时在紧张的计算预算下显著减少了遗憾。</div>
</details>
</div>
<div class="card">
<div class="title">WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks</div>
<div class="meta-line">Authors: Hao Bai, Alexey Taymanov, Tong Zhang, Aviral Kumar, Spencer Whitehead</div>
<div class="meta-line">First: 2026-01-05T09:35:11+00:00 · Latest: 2026-01-07T11:21:44+00:00</div>
<div class="meta-line">Comments: Slightly modified format; added Table 3 for better illustration of the scaling results</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02439v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.02439v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present WebGym, the largest-to-date open-source environment for training realistic visual web agents. Real websites are non-stationary and diverse, making artificial or small-scale task sets insufficient for robust policy learning. WebGym contains nearly 300,000 tasks with rubric-based evaluations across diverse, real-world websites and difficulty levels. We train agents with a simple reinforcement learning (RL) recipe, which trains on the agent&#x27;s own interaction traces (rollouts), using task rewards as feedback to guide learning. To enable scaling RL, we speed up sampling of trajectories in WebGym by developing a high-throughput asynchronous rollout system, designed specifically for web agents. Our system achieves a 4-5x rollout speedup compared to naive implementations. Second, we scale the task set breadth, depth, and size, which results in continued performance improvement. Fine-tuning a strong base vision-language model, Qwen-3-VL-8B-Instruct, on WebGym results in an improvement in success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models such as GPT-4o and GPT-5-Thinking that achieve 27.1% and 29.8%, respectively. This improvement is substantial because our test set consists only of tasks on websites never seen during training, unlike many other prior works on training visual web agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WebGym：面向视觉网页代理的现实任务训练环境规模化构建</div>
<div class="mono" style="margin-top:8px">我们推出WebGym，这是迄今为止规模最大的开源视觉网页代理训练环境。真实网站具有非稳态和多样性特征，使得人工或小规模任务集难以支撑稳健的策略学习。WebGym包含近30万个任务，通过基于量规的评估体系覆盖多样化真实网站及不同难度层级。我们采用简洁的强化学习方案训练代理：利用代理自身交互轨迹（rollouts）进行训练，以任务奖励作为学习反馈。为实现强化学习的规模化，我们专门为网页代理开发了高吞吐异步轨迹采样系统，将WebGym的轨迹采样速度较原始实现提升4-5倍。其次，我们通过拓展任务集的广度、深度和规模实现持续性能提升。在WebGym上对强基线视觉语言模型Qwen-3-VL-8B-Instruct进行微调后，其在分布外测试集上的成功率从26.2%提升至42.9%，显著优于基于GPT-4o（27.1%）和GPT-5-Thinking（29.8%）等专有模型的代理。该提升具有实质性意义，因为我们的测试集完全由训练阶段未见的网站任务构成，这与多数现有视觉网页代理研究形成鲜明对比。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for WebGym is to address the insufficiency of artificial or small-scale task sets for training robust visual web agents, given the non-stationary and diverse nature of real websites. The method involves creating a large-scale open-source environment with nearly 300,000 tasks across real-world websites, using rubric-based evaluations, and training agents with a reinforcement learning recipe that leverages interaction traces and task rewards; to scale RL, a high-throughput asynchronous rollout system is developed to speed up trajectory sampling. Main experimental results show a 4-5x rollout speedup compared to naive implementations, and fine-tuning the Qwen-3-VL-8B-Instruct model on WebGym improves success rates on an out-of-distribution test set from 26.2% to 42.9%, outperforming agents based on proprietary models like GPT-4o and GPT-5-Thinking.</div>
<div class="mono" style="margin-top:8px">WebGym的动机在于解决人工或小规模任务集在训练鲁棒视觉网络代理方面的不足，因为真实网站具有非平稳性和多样性。方法包括创建一个大规模开源环境，包含近30万个跨真实网站的任务，采用基于量规的评估，并通过强化学习配方训练代理，利用交互轨迹和任务奖励；为扩展强化学习，开发了高吞吐量异步轨迹采样系统以加速采样。主要实验结果显示，相比简单实现，轨迹采样速度提升4-5倍，在WebGym上微调Qwen-3-VL-8B-Instruct模型后，在分布外测试集上的成功率从26.2%提升至42.9%，优于基于GPT-4o和GPT-5-Thinking等专有模型的代理。</div>
</details>
</div>
<div class="card">
<div class="title">From Brute Force to Semantic Insight: Performance-Guided Data Transformation Design with LLMs</div>
<div class="meta-line">Authors: Usha Shrestha, Dmitry Ignatov, Radu Timofte</div>
<div class="meta-line">First: 2026-01-07T11:13:02+00:00 · Latest: 2026-01-07T11:13:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03808v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03808v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have achieved notable performance in code synthesis; however, data-aware augmentation remains a limiting factor, handled via heuristic design or brute-force approaches. We introduce a performance-aware, closed-loop solution in the NNGPT ecosystem of projects that enables LLMs to autonomously engineer optimal transformations by internalizing empirical performance cues. We fine-tune LLMs with Low-Rank Adaptation on a novel repository of more than 6,000 empirically evaluated PyTorch augmentation functions, each annotated solely by downstream model accuracy. Training uses pairwise performance ordering (better-worse transformations), enabling alignment through empirical feedback without reinforcement learning, reward models, or symbolic objectives. This reduces the need for exhaustive search, achieving up to 600x times fewer evaluated candidates than brute-force discovery while maintaining competitive peak accuracy and shifting generation from random synthesis to task-aligned design. Ablation studies show that structured Chain-of-Thought prompting introduces syntactic noise and degrades performance, whereas direct prompting ensures stable optimization in performance-critical code tasks. Qualitative and quantitative analyses demonstrate that the model internalizes semantic performance cues rather than memorizing syntax. These results show that LLMs can exhibit task-level reasoning through non-textual feedback loops, bypassing explicit symbolic rewards.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从暴力搜索到语义洞察：基于性能引导的LLM数据转换设计</div>
<div class="mono" style="margin-top:8px">大语言模型在代码合成方面已取得显著性能，但数据感知增强仍受限于启发式设计或暴力搜索方法。我们在NNGPT项目生态中提出一种性能感知的闭环解决方案，使LLM能够通过内化实证性能指标自主设计最优转换。我们采用低秩适应方法，基于包含6000多个经实证评估的PyTorch增强函数库对LLM进行微调，每个函数仅通过下游模型准确率进行标注。训练采用成对性能排序（优劣转换对比），通过实证反馈实现对齐，无需强化学习、奖励模型或符号目标。该方法大幅减少穷举搜索需求，相比暴力搜索评估候选方案减少高达600倍，同时保持竞争力的峰值准确率，并将生成模式从随机合成转向任务对齐设计。消融研究表明结构化思维链提示会引入语法噪声降低性能，而直接提示能确保性能关键代码任务的稳定优化。定性与定量分析表明模型内化了语义性能线索而非记忆语法。这些结果证明LLM可通过非文本反馈循环展现任务级推理能力，无需依赖显式符号奖励。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of heuristic or brute-force methods in data-aware code augmentation, this paper introduces a performance-guided, closed-loop approach within the NNGPT ecosystem that enables large language models (LLMs) to autonomously design optimal data transformations by internalizing empirical performance feedback. The method involves fine-tuning LLMs using Low-Rank Adaptation on a novel repository of over 6,000 PyTorch augmentation functions, each annotated solely by downstream model accuracy, and training them via pairwise performance ordering to align transformations with task performance without reinforcement learning or symbolic objectives. Experimental results show that this approach reduces the need for exhaustive search by up to 600 times compared to brute-force methods while maintaining competitive peak accuracy, shifting generation from random synthesis to task-aligned design, and ablation studies indicate that direct prompting outperforms structured Chain-of-Thought prompting by avoiding syntactic noise, with qualitative and quantitative analyses confirming that the model internalizes semantic performance cues rather than memorizing syntax.</div>
<div class="mono" style="margin-top:8px">针对数据感知代码增强中启发式或暴力方法的局限性，本文在NNGPT生态系统中提出了一种性能引导的闭环方法，使大语言模型能够通过内化实证性能反馈自主设计最优数据转换。该方法基于一个包含6000多个仅以下游模型准确率标注的PyTorch增强函数的新仓库，使用低秩适应对大语言模型进行微调，并通过成对性能排序进行训练，从而无需强化学习或符号目标即可使转换与任务性能对齐。实验结果表明，与暴力方法相比，该方法将搜索需求减少了高达600倍，同时保持了有竞争力的峰值准确率，实现了从随机合成到任务对齐设计的转变；消融研究显示直接提示优于结构化思维链提示，避免了句法噪声，定性与定量分析证实模型内化了语义性能线索而非记忆句法。</div>
</details>
</div>
<div class="card">
<div class="title">NeoAMT: Neologism-Aware Agentic Machine Translation with Reinforcement Learning</div>
<div class="meta-line">Authors: Zhongtao Miao, Kaiyan Zhao, Masaaki Nagata, Yoshimasa Tsuruoka</div>
<div class="meta-line">First: 2026-01-07T10:49:00+00:00 · Latest: 2026-01-07T10:49:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03790v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03790v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neologism-aware machine translation aims to translate source sentences containing neologisms into target languages. This field remains underexplored compared with general machine translation (MT). In this paper, we propose an agentic framework, NeoAMT, for neologism-aware machine translation using a Wiktionary search tool. Specifically, we first create a new dataset for neologism-aware machine translation and develop a search tool based on Wiktionary. The new dataset covers 16 languages and 75 translation directions and is derived from approximately 10 million records of an English Wiktionary dump. The retrieval corpus of the search tool is also constructed from around 3 million cleaned records of the Wiktionary dump. We then use it for training the translation agent with reinforcement learning (RL) and evaluating the accuracy of neologism-aware machine translation. Based on this, we also propose an RL training framework that contains a novel reward design and an adaptive rollout generation approach by leveraging &quot;translation difficulty&quot; to further improve the translation quality of translation agents using our search tool.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NeoAMT：基于强化学习的新词感知智能机器翻译框架</div>
<div class="mono" style="margin-top:8px">新词感知机器翻译旨在将包含新词的源语句翻译为目标语言。与通用机器翻译相比，该领域研究尚不充分。本文提出一种基于维基词典检索工具的智能框架NeoAMT，用于新词感知机器翻译。具体而言，我们首先构建了覆盖16种语言、75个翻译方向的新词感知机器翻译数据集，该数据集源自约1000万条英文维基词典记录；同时基于约300万条清洗后的维基词典记录构建了检索工具的语料库。随后利用该工具，通过强化学习训练翻译智能体，并评估新词感知机器翻译的准确性。在此基础上，进一步提出一种强化学习训练框架，通过引入“翻译难度”设计新型奖励机制与自适应推演生成方法，以提升使用本检索工具的翻译智能体的译文质量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the underexplored challenge of translating neologisms in machine translation, this paper introduces NeoAMT, an agentic framework that integrates a Wiktionary-based search tool with reinforcement learning. The method involves creating a new multilingual dataset from English Wiktionary and developing a retrieval tool, which is then used to train a translation agent via a novel RL approach featuring reward design and adaptive rollout generation based on translation difficulty. Experimental results demonstrate that this framework effectively improves translation accuracy for sentences containing neologisms across 16 languages and 75 translation directions.</div>
<div class="mono" style="margin-top:8px">针对机器翻译中未充分探索的新词翻译难题，本文提出了NeoAMT框架，该框架通过结合基于维基词典的搜索工具和强化学习来实现新词感知的机器翻译。方法包括利用英语维基词典构建新的多语言数据集和检索工具，并采用一种新颖的强化学习训练框架，其中设计了奖励机制和基于翻译难度的自适应生成策略。实验结果表明，该框架在涵盖16种语言和75个翻译方向的任务中，有效提升了包含新词的句子的翻译准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Differential Evolution via Nonlinear Population Size Reduction and Adaptive Restart: The ARRDE Algorithm</div>
<div class="meta-line">Authors: Khoirul Faiq Muzakka, Ahsani Hafizhu Shali, Haris Suhendar, Sören Möller, Martin Finsterbusch</div>
<div class="meta-line">First: 2025-11-23T12:50:25+00:00 · Latest: 2026-01-07T10:20:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.18429v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.18429v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study is motivated by a robustness issue in numerical optimization of bound-constrained problems: many algorithms that perform well on a particular benchmark suite, such as the IEEE CEC2017 problems, struggle to maintain the same level of performance when applied to other suites that differ in dimensionality, landscape complexity, or the maximum number of function evaluations ($N_{\text{max}}$). To address this, we propose the Adaptive Restart-Refine Differential Evolution (ARRDE) algorithm, a new variant of Differential Evolution (DE). ARRDE builds upon the LSHADE algorithm, incorporates key mechanisms from jSO, and introduces a nonlinear population-size reduction strategy combined with an adaptive restart-refine mechanism.
  We evaluate ARRDE on five benchmark suites (CEC2011, CEC2017, CEC2019, CEC2020, and CEC2022) which, to the best of our knowledge, constitutes the most extensive experimental study to date in the context of algorithmic comparison, as most prior works consider only one or two suites. This broad evaluation enables a rigorous assessment of generalization across markedly different problem characteristics. To further support fair cross-suite comparisons, we also introduce a bounded accuracy-based scoring metric derived from relative error. Using both rank-based and accuracy-based metrics, and comparing against algorithms that perform strongly on CEC2017 (e.g., jSO and LSHADE-cnEpSin) as well as those that excel on CEC2020 (e.g., j2020 and NLSHADE-RSP), ARRDE consistently demonstrates top-tier performance, ranking first across all benchmark suites considered. These results highlight ARRDE&#x27;s robustness and its superior generalization capability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于非线性种群规模缩减与自适应重启的鲁棒差分进化算法：ARRDE算法</div>
<div class="mono" style="margin-top:8px">本研究针对边界约束问题数值优化中的鲁棒性问题：许多在特定基准测试集（如IEEE CEC2017问题集）表现优异的算法，在应用于维度、景观复杂度或最大函数评估次数（$N_{\text{max}}$）不同的其他测试集时，难以保持同等性能。为此，我们提出自适应重启-精化差分进化（ARRDE）算法，这是差分进化（DE）的一种新变体。ARRDE基于LSHADE算法，融合了jSO的关键机制，并引入了非线性种群规模缩减策略与自适应重启-精化机制。我们在五个基准测试集（CEC2011、CEC2017、CEC2019、CEC2020和CEC2022）上评估ARRDE，据我们所知，这是迄今为止算法比较领域最广泛的实验研究（多数先前工作仅考虑一至两个测试集）。这种广泛评估能严格检验算法在显著不同问题特征间的泛化能力。为支持公平的跨测试集比较，我们还提出基于相对误差的有界精度评分指标。通过使用基于排名和精度的指标，并与在CEC2017表现优异的算法（如jSO和LSHADE-cnEpSin）及在CEC2020领先的算法（如j2020和NLSHADE-RSP）对比，ARRDE在所有测试集中均展现顶级性能，排名首位。这些结果凸显了ARRDE的鲁棒性和卓越的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the lack of robustness in numerical optimization algorithms when applied across different benchmark suites with varying dimensions and complexities, this paper proposes the Adaptive Restart-Refine Differential Evolution (ARRDE) algorithm. The method builds on LSHADE and jSO, introducing a nonlinear population-size reduction strategy and an adaptive restart-refine mechanism to enhance generalization. Experimental results on five CEC benchmark suites (2011, 2017, 2019, 2020, 2022) show that ARRDE consistently achieves top-tier performance, ranking first across all suites and demonstrating superior robustness and generalization capability compared to state-of-the-art algorithms.</div>
<div class="mono" style="margin-top:8px">本文针对数值优化算法在不同维度、复杂性和评估次数限制的基准测试集上表现不稳健的问题，提出了自适应重启-精化差分进化（ARRDE）算法。该方法基于LSHADE和jSO算法，引入了非线性种群规模缩减策略和自适应重启-精化机制以提升泛化能力。在五个CEC基准测试集（2011、2017、2019、2020、2022）上的实验结果表明，ARRDE在所有测试集中均排名第一，相比现有先进算法展现出卓越的稳健性和泛化性能。</div>
</details>
</div>
<div class="card">
<div class="title">MDAgent2: Large Language Model for Code Generation and Knowledge Q&amp;A in Molecular Dynamics</div>
<div class="meta-line">Authors: Zhuofan Shi, Hubao A, Yufei Shao, Dongliang Huang, Hongxu An, Chunxiao Xin, Haiyang Shen, Zhenyu Wang, Yunshan Na, Gang Huang, Xiang Jing</div>
<div class="meta-line">First: 2026-01-05T12:56:51+00:00 · Latest: 2026-01-07T10:06:36+00:00</div>
<div class="meta-line">Comments: 24 pages,4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02075v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.02075v3">PDF</a> · <a href="https://github.com/FredericVAN/PKU_MDAgent2">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&amp;A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MDAgent2：面向分子动力学代码生成与知识问答的大语言模型</div>
<div class="mono" style="margin-top:8px">分子动力学模拟在材料科学原子尺度行为研究中至关重要，但编写LAMMPS脚本仍属高度专业化且耗时的任务。尽管大语言模型在代码生成和领域问答中展现出潜力，其在分子动力学场景的性能受限于领域数据稀缺、前沿大模型部署成本高昂及代码可执行率低等问题。基于前期MDAgent工作，我们提出首个能在分子动力学领域同时执行知识问答与代码生成的端到端框架MDAgent2。通过构建领域专用数据生成流程，产出涵盖分子动力学知识、问答与代码生成的三类高质量数据集。基于这些数据集，采用三阶段后训练策略——持续预训练、监督微调与强化学习——训练出两个领域适配模型MD-Instruct与MD-Code。进一步提出MD-GRPO强化学习方法，以模拟结果为奖励信号并循环利用低奖励轨迹实现持续优化。同时构建可部署的多智能体系统MDAgent2-RUNTIME，集成代码生成、执行、评估与自修正功能。结合本文提出的首个LAMMPS代码生成与问答基准MD-EvalBench，我们的模型与系统性能超越多个强基线。本工作系统论证了大语言模型在工业仿真任务中的适应性与泛化能力，为AI for Science及工业级仿真的自动化代码生成奠定方法论基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for MDAgent2 stems from the specialized and time-consuming nature of writing LAMMPS scripts for molecular dynamics (MD) simulations, coupled with the limitations of existing large language models (LLMs) in this domain due to scarce data, high deployment costs, and low code executability. The method involves constructing three high-quality MD datasets and employing a three-stage post-training strategy—continued pre-training, supervised fine-tuning, and reinforcement learning—to train domain-adapted models (MD-Instruct and MD-Code), along with introducing MD-GRPO, a closed-loop RL method that uses simulation outcomes as rewards. The main experimental results, evaluated on the proposed MD-EvalBench benchmark, show that the models and integrated multi-agent system (MDAgent2-RUNTIME) surpass several strong baselines in both code generation and knowledge question answering, demonstrating effective adaptability for industrial simulation tasks.</div>
<div class="mono" style="margin-top:8px">MDAgent2的研发动机源于分子动力学模拟中编写LAMMPS脚本的专业性和耗时性，以及现有大语言模型在该领域因数据稀缺、部署成本高和代码可执行性低而受限。方法上，本研究构建了三个高质量的分子动力学数据集，并采用三阶段后训练策略——持续预训练、监督微调和强化学习——来训练领域适应模型（MD-Instruct和MD-Code），同时引入了MD-GRPO，这是一种利用模拟结果作为奖励信号的闭环强化学习方法。主要实验结果在提出的MD-EvalBench基准测试中显示，该模型及集成的多智能体系统（MDAgent2-RUNTIME）在代码生成和知识问答任务上均超越多个强基线，证明了其在工业模拟任务中的有效适应性和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">O-Researcher: An Open Ended Deep Research Model via Multi-Agent Distillation and Agentic RL</div>
<div class="meta-line">Authors: Yi Yao, He Zhu, Piaohong Wang, Jincheng Ren, Xinlong Yang, Qianben Chen, Xiaowan Li, Dingfeng Shi, Jiaxian Li, Qiexiang Wang, Sinuo Wang, Xinpeng Liu, Jiaqi Wu, Minghao Liu, Wangchunshu Zhou</div>
<div class="meta-line">First: 2026-01-07T09:31:10+00:00 · Latest: 2026-01-07T09:31:10+00:00</div>
<div class="meta-line">Comments: 22 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03743v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03743v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The performance gap between closed-source and open-source large language models (LLMs) is largely attributed to disparities in access to high-quality training data. To bridge this gap, we introduce a novel framework for the automated synthesis of sophisticated, research-grade instructional data. Our approach centers on a multi-agent workflow where collaborative AI agents simulate complex tool-integrated reasoning to generate diverse and high-fidelity data end-to-end. Leveraging this synthesized data, we develop a two-stage training strategy that integrates supervised fine-tuning with a novel reinforcement learning method, designed to maximize model alignment and capability. Extensive experiments demonstrate that our framework empowers open-source models across multiple scales, enabling them to achieve new state-of-the-art performance on the major deep research benchmark. This work provides a scalable and effective pathway for advancing open-source LLMs without relying on proprietary data or models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>O-Researcher：基于多智能体蒸馏与智能体强化学习的开放式深度研究模型</div>
<div class="mono" style="margin-top:8px">闭源与开源大语言模型（LLMs）的性能差距主要源于高质量训练数据的获取差异。为弥合这一差距，我们提出了一种自动化合成复杂研究级指令数据的新框架。该方法采用多智能体工作流，通过协作的AI智能体模拟集成工具的复杂推理过程，端到端生成多样化、高保真度的数据。基于此合成数据，我们开发了融合监督微调与新型强化学习方法的两阶段训练策略，旨在最大化模型对齐与能力。大量实验表明，该框架能有效赋能多规模开源模型，使其在主流深度研究基准测试中达到新的最优性能。本工作为不依赖专有数据或模型的开源LLMs发展提供了可扩展的有效路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the performance gap between closed-source and open-source LLMs due to disparities in training data, this paper introduces a novel framework for automated synthesis of high-quality, research-grade instructional data. The method employs a multi-agent workflow where collaborative AI agents simulate complex, tool-integrated reasoning to generate data, followed by a two-stage training strategy combining supervised fine-tuning with a novel reinforcement learning approach to enhance model alignment and capability. The main experimental results show that this framework enables open-source models of various scales to achieve new state-of-the-art performance on a major deep research benchmark, providing a scalable pathway for advancement without proprietary resources.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决因训练数据差异导致的闭源与开源大语言模型之间的性能差距，为此引入了一个自动化合成高质量研究级指令数据的新框架。该方法采用多智能体工作流，通过协作的AI智能体模拟复杂的工具集成推理来生成数据，并实施了一个结合监督微调与新型强化学习的两阶段训练策略，以提升模型的对齐性和能力。主要实验结果表明，该框架使不同规模的开源模型在核心深度研究基准测试中取得了新的最先进性能，为不依赖专有数据或模型的进步提供了一条可扩展的有效路径。</div>
</details>
</div>
<div class="card">
<div class="title">EDCO: Dynamic Curriculum Orchestration for Domain-specific Large Language Model Fine-tuning</div>
<div class="meta-line">Authors: Jing-Cheng Pang, Liu Sun, Chang Zhou, Xian Tang, Haichuan Ma, Kun Jiang, Jianlong Wang, Kai Zhang, Sijie Wu, Haoran Cai, Chenwei Wu, Xubin Li, Xin Chen</div>
<div class="meta-line">First: 2026-01-07T09:20:05+00:00 · Latest: 2026-01-07T09:20:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03725v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03725v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Domain-specific large language models (LLMs), typically developed by fine-tuning a pre-trained general-purpose LLM on specialized datasets, represent a significant advancement in applied AI. A common strategy in LLM fine-tuning is curriculum learning, which pre-orders training samples based on metrics like difficulty to improve learning efficiency compared to a random sampling strategy. However, most existing methods for LLM fine-tuning rely on a static curriculum, designed prior to training, which lacks adaptability to the model&#x27;s evolving needs during fine-tuning. To address this, we propose EDCO, a novel framework based on two key concepts: inference entropy and dynamic curriculum orchestration. Inspired by recent findings that maintaining high answer entropy benefits long-term reasoning gains, EDCO prioritizes samples with high inference entropy in a continuously adapted curriculum. EDCO integrates three core components: an efficient entropy estimator that uses prefix tokens to approximate full-sequence entropy, an entropy-based curriculum generator that selects data points with the highest inference entropy, and an LLM trainer that optimizes the model on the selected curriculum. Comprehensive experiments in communication, medicine and law domains, EDCO outperforms traditional curriculum strategies for fine-tuning Qwen3-4B and Llama3.2-3B models under supervised and reinforcement learning settings. Furthermore, the proposed efficient entropy estimation reduces computational time by 83.5% while maintaining high accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EDCO：面向领域特定大语言模型微调的动态课程编排框架</div>
<div class="mono" style="margin-top:8px">领域特定大语言模型通常通过在专业数据集上微调预训练的通用大语言模型实现，是应用人工智能的重要进展。课程学习是常见的微调策略，其根据难度等指标预先排序训练样本，相比随机采样能提升学习效率。然而现有方法多采用训练前设计的静态课程，难以适应微调过程中模型动态变化的需求。为此，我们提出基于推理熵与动态课程编排两大核心概念的EDCO框架。受近期研究发现高答案熵有利于长期推理能力提升的启发，EDCO在持续调整的课程中优先选择高推理熵样本。该框架包含三个核心组件：利用前缀词元近似全序列熵的高效熵估计器、选择最高推理熵数据点的基于熵的课程生成器，以及对选定课程进行模型优化的LLM训练器。在通信、医学和法律领域的综合实验中，EDCO在监督学习与强化学习设置下微调Qwen3-4B和Llama3.2-3B模型时均优于传统课程策略。所提出的高效熵估计方法在保持高精度的同时将计算时间降低了83.5%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the limitations of static curriculum learning in fine-tuning domain-specific large language models (LLMs), which lacks adaptability to the model&#x27;s evolving needs during training. To address this, the authors propose EDCO, a framework that dynamically orchestrates the training curriculum based on inference entropy, prioritizing samples with high entropy to potentially enhance reasoning gains. The method integrates an efficient entropy estimator using prefix tokens, an entropy-based curriculum generator, and an LLM trainer. Experimental results across communication, medicine, and law domains show that EDCO outperforms traditional curriculum strategies for models like Qwen3-4B and Llama3.2-3B in both supervised and reinforcement learning settings, while the efficient entropy estimation reduces computational time by 83.5% with maintained accuracy.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于针对特定领域大语言模型微调中静态课程学习的局限性，该方法无法适应训练过程中模型不断变化的需求。为此，作者提出了EDCO框架，该框架基于推理熵动态编排训练课程，优先选择高熵样本来提升推理收益。该方法整合了三个核心组件：使用前缀令牌的高效熵估计器、基于熵的课程生成器以及大语言模型训练器。在通信、医学和法律领域的综合实验中，EDCO在监督学习和强化学习设置下，对Qwen3-4B和Llama3.2-3B等模型的微调效果均优于传统课程策略，同时其高效熵估计方法在保持高精度的前提下将计算时间减少了83.5%。</div>
</details>
</div>
<div class="card">
<div class="title">ETR: Outcome-Guided Elastic Trust Regions for Policy Optimization</div>
<div class="meta-line">Authors: Shijie Zhang, Kevin Zhang, Zheyuan Gu, Xiang Guo, Rujun Guo, Shaoyu Liu, Guanjun Jiang, Xiaozhao Wang</div>
<div class="meta-line">First: 2026-01-07T09:19:53+00:00 · Latest: 2026-01-07T09:19:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03723v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03723v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an important paradigm for unlocking reasoning capabilities in large language models, exemplified by the success of OpenAI o1 and DeepSeek-R1. Currently, Group Relative Policy Optimization (GRPO) stands as the dominant algorithm in this domain due to its stable training and critic-free efficiency. However, we argue that GRPO suffers from a structural limitation: it imposes a uniform, static trust region constraint across all samples. This design implicitly assumes signal homogeneity, a premise misaligned with the heterogeneous nature of outcome-driven learning, where advantage magnitudes and variances fluctuate significantly. Consequently, static constraints fail to fully exploit high-quality signals while insufficiently suppressing noise, often precipitating rapid entropy collapse. To address this, we propose \textbf{E}lastic \textbf{T}rust \textbf{R}egions (\textbf{ETR}), a dynamic mechanism that aligns optimization constraints with signal quality. ETR constructs a signal-aware landscape through dual-level elasticity: at the micro level, it scales clipping boundaries based on advantage magnitude to accelerate learning from high-confidence paths; at the macro level, it leverages group variance to implicitly allocate larger update budgets to tasks in the optimal learning zone. Extensive experiments on AIME and MATH benchmarks demonstrate that ETR consistently outperforms GRPO, achieving superior accuracy while effectively mitigating policy entropy degradation to ensure sustained exploration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ETR：面向策略优化的结果导向弹性置信域方法</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习已成为解锁大语言模型推理能力的重要范式，以OpenAI o1和DeepSeek-R1的成功为代表。当前，组相对策略优化凭借其稳定的训练效率和无需价值函数的特性，成为该领域的主导算法。然而，我们认为GRPO存在结构性局限：它对所有样本施加统一、静态的置信域约束。这种设计隐含了信号同质性的假设，与结果驱动学习中优势值幅度和方差显著波动的异质性本质相悖。静态约束既无法充分利用高质量信号，又难以有效抑制噪声，常导致策略熵快速崩溃。为此，我们提出弹性置信域方法，这是一种将优化约束与信号质量动态对齐的机制。ETR通过双层弹性构建信号感知的优化空间：在微观层面，根据优势值幅度动态调整截断边界，加速高置信度路径的学习；在宏观层面，利用组方差隐式分配更大更新预算给处于最优学习区间的任务。在AIME和MATH基准上的大量实验表明，ETR持续超越GRPO，在实现更高准确率的同时有效缓解策略熵衰减，确保持续探索能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the structural limitation of Group Relative Policy Optimization (GRPO) in reinforcement learning with verifiable rewards, which uses a uniform, static trust region constraint that misaligns with the heterogeneous nature of outcome-driven signals, often leading to rapid entropy collapse. To address this, the authors propose Elastic Trust Regions (ETR), a dynamic method that aligns optimization constraints with signal quality through dual-level elasticity: micro-level scaling of clipping boundaries based on advantage magnitude to accelerate learning from high-confidence paths, and macro-level use of group variance to implicitly allocate larger update budgets to tasks in the optimal learning zone. Experimental results on AIME and MATH benchmarks show that ETR consistently outperforms GRPO, achieving superior accuracy while effectively mitigating policy entropy degradation to ensure sustained exploration.</div>
<div class="mono" style="margin-top:8px">本文的动机源于可验证奖励强化学习中组相对策略优化（GRPO）的结构性局限，即其采用均匀、静态的信任区域约束，这与结果驱动信号的非均匀性不匹配，常导致策略熵快速崩溃。为解决此问题，作者提出了弹性信任区域（ETR），这是一种动态方法，通过双重弹性机制使优化约束与信号质量对齐：在微观层面，基于优势幅度缩放裁剪边界以加速从高置信度路径学习；在宏观层面，利用组方差隐式地为处于最优学习区的任务分配更大更新预算。在AIME和MATH基准上的大量实验表明，ETR持续优于GRPO，在实现更高准确性的同时有效缓解策略熵退化，确保了持续的探索。</div>
</details>
</div>
<div class="card">
<div class="title">Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail</div>
<div class="meta-line">Authors: NVIDIA, :, Yan Wang, Wenjie Luo, Junjie Bai, Yulong Cao, Tong Che, Ke Chen, Yuxiao Chen, Jenna Diamond, Yifan Ding, Wenhao Ding, Liang Feng, Greg Heinrich, Jack Huang, Peter Karkus, Boyi Li, Pinyi Li, Tsung-Yi Lin, Dongran Liu, Ming-Yu Liu, Langechuan Liu, Zhijian Liu, Jason Lu, Yunxiang Mao, Pavlo Molchanov, Lindsey Pavao, Zhenghao Peng, Mike Ranzinger, Ed Schmerling, Shida Shen, Yunfei Shi, Sarah Tariq, Ran Tian, Tilman Wekel, Xinshuo Weng, Tianjun Xiao, Eric Yang, Xiaodong Yang, Yurong You, Xiaohui Zeng, Wenyuan Zhang, Boris Ivanovic, Marco Pavone</div>
<div class="meta-line">First: 2025-10-30T01:25:34+00:00 · Latest: 2026-01-07T09:09:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.00088v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.00088v2">PDF</a> · <a href="https://huggingface.co/nvidia/Alpamayo-R1-10B">Code1</a> · <a href="https://github.com/NVlabs/alpamayo">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">End-to-end architectures trained via imitation learning have advanced autonomous driving by scaling model size and data, yet performance remains brittle in safety-critical long-tail scenarios where supervision is sparse and causal understanding is limited. We introduce Alpamayo-R1 (AR1), a vision-language-action model (VLA) that integrates Chain of Causation reasoning with trajectory planning for complex driving scenarios. Our approach features three key innovations: (1) the Chain of Causation (CoC) dataset, built through a hybrid auto-labeling and human-in-the-loop pipeline producing decision-grounded, causally linked reasoning traces aligned with driving behaviors; (2) a modular VLA architecture combining Cosmos-Reason, a vision-language model pre-trained for Physical AI, with a diffusion-based trajectory decoder that generates dynamically feasible trajectories in real time; (3) a multi-stage training strategy using supervised fine-tuning to elicit reasoning and reinforcement learning (RL) to enforce reasoning-action consistency and optimize reasoning quality. AR1 achieves up to a 12% improvement in planning accuracy on challenging cases compared to a trajectory-only baseline, with a 35% reduction in close encounter rate in closed-loop simulation. RL post-training improves reasoning quality by 45% and reasoning-action consistency by 37%. Model scaling from 0.5B to 7B parameters shows consistent improvements. On-vehicle road tests confirm real-time performance (99 ms latency) and successful urban deployment. By bridging interpretable reasoning with precise control, AR1 demonstrates a practical path towards Level 4 autonomous driving. Model weights are available at https://huggingface.co/nvidia/Alpamayo-R1-10B with inference code at https://github.com/NVlabs/alpamayo.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Alpamayo-R1：通过因果推理与行为预测融合实现长尾场景下可泛化的自动驾驶</div>
<div class="mono" style="margin-top:8px">基于模仿学习的端到端架构通过扩大模型规模与数据量推动了自动驾驶发展，但在安全关键的长尾场景中，由于监督稀疏且因果理解有限，其性能仍显脆弱。我们提出Alpamayo-R1（AR1）——一种融合因果推理链与轨迹规划的视觉-语言-动作模型，针对复杂驾驶场景实现三大创新：（1）构建因果链数据集，通过混合自动标注与人机协同流程生成与驾驶行为对齐的、基于决策的因果推理轨迹；（2）采用模块化VLA架构，将预训练物理AI视觉语言模型Cosmos-Reason与基于扩散的轨迹解码器结合，实时生成动态可行轨迹；（3）设计多阶段训练策略，通过监督微调激发推理能力，并利用强化学习强化推理-动作一致性及优化推理质量。相比纯轨迹基线，AR1在挑战性案例中规划准确率提升达12%，闭环仿真中近距离接触率降低35%。强化学习后训练使推理质量提升45%，推理-动作一致性提高37%。模型参数从0.5B扩展至7B时性能持续提升。实车道路测试证实其实时性（99毫秒延迟）与城市部署可行性。通过可解释推理与精准控制的结合，AR1为迈向L4级自动驾驶提供了可行路径。模型权重与推理代码已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the brittleness of end-to-end imitation learning models in autonomous driving, particularly in long-tail safety-critical scenarios where data is sparse and causal understanding is limited. It introduces Alpamayo-R1 (AR1), a vision-language-action model that integrates Chain of Causation reasoning with trajectory planning through three innovations: a CoC dataset with causally linked reasoning traces, a modular architecture combining a pre-trained vision-language model with a diffusion-based trajectory decoder, and a multi-stage training strategy using supervised fine-tuning and reinforcement learning. Experimental results show AR1 achieves up to a 12% improvement in planning accuracy, a 35% reduction in close encounter rates in simulation, and RL post-training boosts reasoning quality by 45% and consistency by 37%, with scaling from 0.5B to 7B parameters yielding consistent gains and on-vehicle tests confirming real-time performance and urban deployment success.</div>
<div class="mono" style="margin-top:8px">本文针对端到端模仿学习模型在自动驾驶中，特别是在数据稀疏、因果理解有限的尾部安全关键场景中的脆弱性问题，提出了Alpamayo-R1（AR1）模型。该模型是一种视觉-语言-动作模型，通过三项创新将因果链推理与轨迹规划相结合：构建了具有因果关联推理轨迹的CoC数据集，采用结合预训练视觉语言模型与扩散轨迹解码器的模块化架构，以及使用监督微调和强化学习的多阶段训练策略。实验结果表明，AR1在规划精度上最高提升12%，在闭环仿真中近距离遭遇率降低35%，强化学习后训练使推理质量提升45%、推理-行动一致性提高37%，参数从0.5B扩展到7B时性能持续改善，实车道路测试验证了其实时性能和城市部署能力。</div>
</details>
</div>
<div class="card">
<div class="title">R$^3$L: Reflect-then-Retry Reinforcement Learning with Language-Guided Exploration, Pivotal Credit, and Positive Amplification</div>
<div class="meta-line">Authors: Weijie Shi, Yanxi Chen, Zexi Li, Xuchen Pan, Yuchang Sun, Jiajie Xu, Xiaofang Zhou, Yaliang Li</div>
<div class="meta-line">First: 2026-01-07T09:04:52+00:00 · Latest: 2026-01-07T09:04:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03715v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03715v1">PDF</a> · <a href="https://github.com/shiweijiezero/R3L">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning drives recent advances in LLM reasoning and agentic capabilities, yet current approaches struggle with both exploration and exploitation. Exploration suffers from low success rates on difficult tasks and high costs of repeated rollouts from scratch. Exploitation suffers from coarse credit assignment and training instability: Trajectory-level rewards penalize valid prefixes for later errors, and failure-dominated groups overwhelm the few positive signals, leaving optimization without constructive direction. To this end, we propose R$^3$L, Reflect-then-Retry Reinforcement Learning with Language-Guided Exploration, Pivotal Credit, and Positive Amplification. To synthesize high-quality trajectories, R$^3$L shifts from stochastic sampling to active synthesis via reflect-then-retry, leveraging language feedback to diagnose errors, transform failed attempts into successful ones, and reduce rollout costs by restarting from identified failure points. With errors diagnosed and localized, Pivotal Credit Assignment updates only the diverging suffix where contrastive signals exist, excluding the shared prefix from gradient update. Since failures dominate on difficult tasks and reflect-then-retry produces off-policy data, risking training instability, Positive Amplification upweights successful trajectories to ensure positive signals guide the optimization process. Experiments on agentic and reasoning tasks demonstrate 5\% to 52\% relative improvements over baselines while maintaining training stability. Our code is released at https://github.com/shiweijiezero/R3L.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>R$^3$L：基于语言引导探索、关键信用分配与正向放大的反思重试强化学习</div>
<div class="mono" style="margin-top:8px">强化学习推动了大型语言模型推理与智能体能力的近期进展，但现有方法在探索与利用两方面均面临挑战。探索方面，困难任务成功率低且从头重复执行成本高昂；利用方面，存在信用分配粗糙与训练不稳定问题：轨迹级奖励会因后续错误惩罚有效前缀，失败主导的样本群淹没少量正向信号，导致优化缺乏建设性方向。为此，我们提出R$^3$L——融合语言引导探索、关键信用分配与正向放大的反思重试强化学习。为生成高质量轨迹，R$^3$L通过“反思-重试”机制将随机采样转为主动合成：利用语言反馈诊断错误，将失败尝试转化为成功轨迹，并通过从识别出的失败点重启降低执行成本。在错误被诊断定位后，关键信用分配仅更新存在对比信号的分歧后缀，将共享前缀排除在梯度更新外。鉴于困难任务中失败样本占主导且反思重试产生离线策略数据可能引发训练不稳定，正向放大机制通过加权成功轨迹确保正向信号引导优化过程。在智能体与推理任务上的实验表明，该方法相对基线获得5%至52%的性能提升，同时保持训练稳定性。代码已发布于https://github.com/shiweijiezero/R3L。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses challenges in reinforcement learning for large language models, where current methods struggle with inefficient exploration due to low success rates on hard tasks and high costs of repeated rollouts, and poor exploitation due to coarse credit assignment and training instability from failure-dominated data. To overcome these issues, the authors propose R³L, a method that enhances exploration through language-guided reflect-then-retry synthesis, which uses feedback to diagnose errors and restart from failure points, reduces rollout costs, and improves exploitation via pivotal credit assignment that updates only diverging trajectory suffixes and positive amplification to upweight successful trajectories for stable optimization. Experimental results on agentic and reasoning tasks show relative improvements of 5% to 52% over baselines while maintaining training stability.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型强化学习中的挑战，现有方法在探索方面因困难任务成功率低和重复从头开始的高成本而效率低下，在利用方面因粗粒度信用分配和失败主导数据导致的训练不稳定而表现不佳。为解决这些问题，作者提出了R³L方法，该方法通过语言引导的反思-重试合成增强探索，利用反馈诊断错误并从失败点重启以降低计算成本，并通过关键信用分配仅更新轨迹中发散的后缀，以及正信号放大加权成功轨迹以实现稳定优化，从而改善利用。在代理和推理任务上的实验结果显示，相比基线方法，相对性能提升了5%至52%，同时保持了训练稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">TreeAdv: Tree-Structured Advantage Redistribution for Group-Based RL</div>
<div class="meta-line">Authors: Lang Cao, Hui Ruan, Yongqian Li, Peng Chao, Wu Ning, Haonan Song, Renhong Chen, Yitong Li</div>
<div class="meta-line">First: 2026-01-07T08:42:14+00:00 · Latest: 2026-01-07T08:42:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03703v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03703v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with group-based objectives, such as Group Relative Policy Optimization (GRPO), is a common framework for aligning large language models on complex reasoning tasks. However, standard GRPO treats each rollout trajectory as an independent flat sequence and assigns a single sequence-level advantage to all tokens, which leads to sample inefficiency and a length bias toward verbose, redundant chains of thought without improving logical depth. We introduce TreeAdv (Tree-Structured Advantage Redistribution for Group-Based RL), which makes the tree structure of group rollouts explicit for both exploration and advantage assignment. Specifically, TreeAdv builds a group of trees (a forest) based on an entropy-driven sampling method where each tree branches at high-uncertainty decisions while sharing low-uncertainty tokens across rollouts. Then, TreeAdv aggregates token-level advantages for internal tree segments by redistributing the advantages of complete rollouts (all leaf nodes), and TreeAdv can easily apply to group-based objectives such as GRPO or GSPO. Across 10 math reasoning benchmarks, TreeAdv consistently outperforms GRPO and GSPO, while using substantially fewer generated tokens under identical supervision, data, and decoding budgets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TreeAdv：面向分组强化学习的树形优势重分配方法</div>
<div class="mono" style="margin-top:8px">基于分组目标的强化学习（如组相对策略优化GRPO）是对齐大语言模型处理复杂推理任务的常用框架。然而标准GRPO将每次轨迹展开视为独立扁平序列，并为所有词元分配单一序列级优势，这导致样本效率低下，且产生偏向冗长冗余思维链的长度偏差，却未提升逻辑深度。本文提出TreeAdv（面向分组强化学习的树形优势重分配方法），在探索与优势分配中显式构建分组展开的树形结构。具体而言，TreeAdv基于熵驱动采样方法构建树群（森林），每棵树在高不确定性决策点分叉，同时在展开轨迹间共享低不确定性词元。随后，TreeAdv通过重分配完整展开轨迹（所有叶节点）的优势值，聚合内部树段词元级优势，该方法可轻松适配GRPO、GSPO等分组目标。在10项数学推理基准测试中，TreeAdv在相同监督、数据和解码预算下，使用显著更少的生成词元，始终优于GRPO与GSPO方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the sample inefficiency and length bias in group-based reinforcement learning for language models, where standard methods like GRPO treat trajectories as flat sequences and assign uniform advantages. To address this, the authors propose TreeAdv, a method that explicitly leverages the tree structure of group rollouts by constructing a forest through entropy-driven sampling, branching at high-uncertainty decisions while sharing low-uncertainty tokens, and redistributing token-level advantages from complete rollouts to internal segments. Experimental results across 10 math reasoning benchmarks show that TreeAdv consistently outperforms GRPO and GSPO while using significantly fewer generated tokens under the same supervision, data, and decoding budgets.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决基于群体的强化学习在语言模型中的样本效率低下和长度偏差问题，其中标准方法如GRPO将轨迹视为扁平序列并分配统一的优势值。为此，作者提出了TreeAdv方法，该方法通过基于熵驱动的采样构建树结构森林，在高不确定性决策处进行分支并跨轨迹共享低不确定性标记，从而将完整轨迹的优势值重新分配到内部片段。在10个数学推理基准测试中，实验结果表明TreeAdv在相同监督、数据和解码预算下，始终优于GRPO和GSPO，同时显著减少了生成的标记数量。</div>
</details>
</div>
<div class="card">
<div class="title">Sandwich Reasoning: An Answer-Reasoning-Answer Approach for Low-Latency Query Correction</div>
<div class="meta-line">Authors: Chen Zhang, Kepu Zhang, Jiatong Zhang, Xiao Zhang, Jun Xu</div>
<div class="meta-line">First: 2026-01-07T07:52:30+00:00 · Latest: 2026-01-07T07:52:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03672v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03672v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Query correction is a critical entry point in modern search pipelines, demanding high accuracy strictly within real-time latency constraints. Chain-of-Thought (CoT) reasoning improves accuracy but incurs prohibitive latency for real-time query correction. A potential solution is to output an answer before reasoning to reduce latency; however, under autoregressive decoding, the early answer is independent of subsequent reasoning, preventing the model from leveraging its reasoning capability to improve accuracy. To address this issue, we propose Sandwich Reasoning (SandwichR), a novel approach that explicitly aligns a fast initial answer with post-hoc reasoning, enabling low-latency query correction without sacrificing reasoning-aware accuracy. SandwichR follows an Answer-Reasoning-Answer paradigm, producing an initial correction, an explicit reasoning process, and a final refined correction. To align the initial answer with post-reasoning insights, we design a consistency-aware reinforcement learning (RL) strategy: a dedicated consistency reward enforces alignment between the initial and final corrections, while margin-based rejection sampling prioritizes borderline samples where reasoning drives the most impactful corrective gains. Additionally, we construct a high-quality query correction dataset, addressing the lack of specialized benchmarks for complex query correction. Experimental results demonstrate that SandwichR achieves SOTA accuracy comparable to standard CoT while delivering a 40-70% latency reduction, resolving the latency-accuracy trade-off in online search.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>三明治推理：一种面向低延迟查询纠错的答案-推理-答案方法</div>
<div class="mono" style="margin-top:8px">查询纠错是现代搜索流程的关键入口，需在实时延迟约束下确保高准确率。思维链推理虽能提升准确率，但其延迟过高难以满足实时纠错需求。一种潜在方案是在推理前先输出答案以降低延迟，但在自回归解码框架下，早期答案独立于后续推理，导致模型无法利用推理能力提升准确率。为解决此问题，我们提出三明治推理——一种通过显式对齐快速初始答案与事后推理的新方法，在保持推理感知准确率的同时实现低延迟查询纠错。该方法遵循答案-推理-答案范式，依次生成初始纠错结果、显式推理过程和最终优化纠错结果。为对齐初始答案与推理后认知，我们设计了基于一致性的强化学习策略：专用一致性奖励机制强制初始与最终纠错结果对齐，而基于边界的拒绝采样则优先处理推理能带来最显著纠错增益的临界样本。此外，我们构建了高质量的查询纠错数据集，以填补复杂查询纠错领域专用基准数据的空白。实验结果表明，三明治推理在取得与标准思维链推理相当的最优准确率的同时，实现了40-70%的延迟降低，有效解决了在线搜索中的延迟-准确率权衡难题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the latency-accuracy trade-off in real-time query correction, where Chain-of-Thought reasoning improves accuracy but introduces prohibitive latency. The method proposed, Sandwich Reasoning (SandwichR), introduces an Answer-Reasoning-Answer paradigm that produces an initial correction, an explicit reasoning process, and a final refined correction; it employs a consistency-aware reinforcement learning strategy with a dedicated reward and margin-based rejection sampling to align the initial answer with post-reasoning insights. Experimental results show that SandwichR achieves state-of-the-art accuracy comparable to standard CoT while reducing latency by 40-70%, effectively resolving the trade-off for online search applications.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决实时查询纠正中的延迟与准确性权衡问题，其中思维链推理能提升准确性但会导致难以接受的延迟。所提出的方法——三明治推理（SandwichR）——采用答案-推理-答案范式，生成初始纠正、显式推理过程和最终优化纠正；它通过一致性感知的强化学习策略，结合专用奖励和基于边界的拒绝采样，使初始答案与推理后洞察保持一致。实验结果表明，SandwichR在达到与标准思维链推理相当的最先进准确性的同时，将延迟降低了40-70%，有效解决了在线搜索中的延迟-准确性权衡。</div>
</details>
</div>
<div class="card">
<div class="title">AMIR-GRPO: Inducing Implicit Preference Signals into GRPO</div>
<div class="meta-line">Authors: Amir Hossein Yari, Fajri Koto</div>
<div class="meta-line">First: 2026-01-07T07:22:58+00:00 · Latest: 2026-01-07T07:22:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03661v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03661v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning has become the primary paradigm for aligning large language models (LLMs) on complex reasoning tasks, with group relative policy optimization (GRPO) widely used in large-scale post-training. However, GRPO faces structural limitations in reasoning-heavy settings: sequence-level advantage normalization introduces systematic length bias, penalties for low-quality trajectories are diluted, and the scalar objective discards rich pairwise preference information embedded in within-group reward rankings. As a result, valuable supervision from costly rollouts remains underutilized.
  We propose AMIR-GRPO, which augments GRPO with an implicit DPO-style contrastive regularizer constructed directly from intra-group reward rankings, requiring no additional annotations. This mechanism amplifies suppression of low-reward trajectories, attenuates response-level length bias, and transforms each rollout group into a denser set of supervision constraints. Across multiple mathematical reasoning benchmarks, AMIR-GRPO consistently outperforms strong GRPO baselines, yields clearer separation between correct and incorrect reasoning chains, and delivers broader coverage gains beyond the subset of instances solved by standard GRPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AMIR-GRPO：将隐式偏好信号引入GRPO</div>
<div class="mono" style="margin-top:8px">强化学习已成为在复杂推理任务上对齐大语言模型（LLMs）的主要范式，其中组相对策略优化（GRPO）被广泛用于大规模后训练。然而，GRPO在推理密集型场景中存在结构性局限：序列级优势归一化会引入系统性长度偏差，低质量轨迹的惩罚被稀释，且标量目标函数丢弃了组内奖励排序所蕴含的丰富成对偏好信息。这导致来自高成本轨迹生成的有价值监督未能被充分利用。
我们提出AMIR-GRPO，通过直接从组内奖励排序构建隐式DPO风格对比正则化器来增强GRPO，无需额外标注。该机制强化了对低奖励轨迹的抑制，减弱了响应级别的长度偏差，并将每个轨迹组转化为更密集的监督约束集合。在多个数学推理基准测试中，AMIR-GRPO持续优于强GRPO基线，在正确与错误推理链之间产生更清晰的区分，并在标准GRPO已解决的实例子集之外实现了更广泛的覆盖增益。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by structural limitations in Group Relative Policy Optimization (GRPO) for aligning large language models on reasoning tasks, where sequence-level advantage normalization introduces length bias, penalties are diluted, and scalar objectives discard rich pairwise preference information from reward rankings. The method, AMIR-GRPO, augments GRPO with an implicit DPO-style contrastive regularizer constructed directly from intra-group reward rankings, requiring no extra annotations, to amplify suppression of low-reward trajectories, attenuate length bias, and transform rollout groups into denser supervision constraints. Experimental results across multiple mathematical reasoning benchmarks show that AMIR-GRPO consistently outperforms strong GRPO baselines, yields clearer separation between correct and incorrect reasoning chains, and achieves broader coverage gains beyond instances solved by standard GRPO.</div>
<div class="mono" style="margin-top:8px">本文的动机源于群组相对策略优化（GRPO）在大型语言模型推理任务对齐中的结构局限性，包括序列级优势归一化引入的长度偏差、低质量轨迹惩罚被稀释，以及标量目标丢弃了奖励排名中丰富的成对偏好信息。所提出的方法AMIR-GRPO通过引入一个基于群组内奖励排名构建的隐式DPO风格对比正则化器来增强GRPO，无需额外标注，从而强化对低奖励轨迹的抑制、减轻响应级长度偏差，并将每个rollout群组转化为更密集的监督约束。在多个数学推理基准上的实验结果表明，AMIR-GRPO持续优于强GRPO基线，在正确与错误推理链之间产生更清晰的区分，并在标准GRPO已解决的实例之外实现了更广泛的覆盖增益。</div>
</details>
</div>
<div class="card">
<div class="title">ReLA: Representation Learning and Aggregation for Job Scheduling with Reinforcement Learning</div>
<div class="meta-line">Authors: Zhengyi Kwan, Zhang Wei, Aik Beng Ng, Zhengkui Wang, Simon See</div>
<div class="meta-line">First: 2026-01-07T06:50:56+00:00 · Latest: 2026-01-07T06:50:56+00:00</div>
<div class="meta-line">Comments: 15 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03646v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03646v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Job scheduling is widely used in real-world manufacturing systems to assign ordered job operations to machines under various constraints. Existing solutions remain limited by long running time or insufficient schedule quality, especially when problem scale increases. In this paper, we propose ReLA, a reinforcement-learning (RL) scheduler built on structured representation learning and aggregation. ReLA first learns diverse representations from scheduling entities, including job operations and machines, using two intra-entity learning modules with self-attention and convolution and one inter-entity learning module with cross-attention. These modules are applied in a multi-scale architecture, and their outputs are aggregated to support RL decision-making. Across experiments on small, medium, and large job instances, ReLA achieves the best makespan in most tested settings over the latest solutions. On non-large instances, ReLA reduces the optimality gap of the SOTA baseline by 13.0%, while on large-scale instances it reduces the gap by 78.6%, with the average optimality gaps lowered to 7.3% and 2.1%, respectively. These results confirm that ReLA&#x27;s learned representations and aggregation provide strong decision support for RL scheduling, and enable fast job completion and decision-making for real-world applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReLA：基于强化学习的作业调度中的表示学习与聚合方法</div>
<div class="mono" style="margin-top:8px">作业调度在现实制造系统中广泛应用，用于在多种约束下将有序作业工序分配给机器。现有解决方案仍受限于运行时间过长或调度质量不足，尤其在问题规模增大时更为明显。本文提出ReLA，一种基于结构化表示学习与聚合的强化学习调度器。ReLA首先通过两个采用自注意力和卷积的实体内部学习模块，以及一个采用交叉注意力的实体间学习模块，从调度实体（包括作业工序和机器）中学习多样化表示。这些模块应用于多尺度架构中，其输出经聚合后支持强化学习决策。在小型、中型和大型作业实例的实验中，ReLA在多数测试设定下取得了优于最新解决方案的最短完工时间。在非大型实例上，ReLA将SOTA基准的最优性差距降低了13.0%；而在大规模实例上，该差距降低了78.6%，平均最优性差距分别降至7.3%和2.1%。这些结果证实，ReLA学习的表示与聚合为强化学习调度提供了强有力的决策支持，并能实现快速作业完成与实时应用决策。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing job scheduling solutions in terms of runtime and schedule quality at scale, this paper proposes ReLA, a reinforcement learning scheduler that employs structured representation learning and aggregation. The method learns diverse representations from scheduling entities like job operations and machines using intra-entity modules with self-attention and convolution and an inter-entity module with cross-attention within a multi-scale architecture, aggregating their outputs to inform RL decisions. Experimental results across small, medium, and large job instances show that ReLA achieves the best makespan in most settings, reducing the optimality gap of the state-of-the-art baseline by 13.0% on non-large instances and 78.6% on large-scale instances, with average gaps lowered to 7.3% and 2.1%, respectively, confirming its effectiveness for fast and high-quality scheduling.</div>
<div class="mono" style="margin-top:8px">针对现有作业调度解决方案在运行时间和大规模调度质量方面的不足，本文提出了ReLA，一种基于强化学习的调度器，采用结构化表示学习与聚合方法。该方法通过结合自注意力和卷积的实体内部学习模块以及跨注意力的实体间学习模块，在多尺度架构中从作业操作和机器等调度实体学习多样化表示，并聚合输出以支持强化学习决策。在小型、中型和大型作业实例上的实验结果表明，ReLA在大多数测试设置中实现了最佳完工时间，将最先进基线的优化差距在非大型实例上降低了13.0%，在大型实例上降低了78.6%，平均优化差距分别降至7.3%和2.1%，证实了其在实际应用中实现快速、高质量调度的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Interleaved Tool-Call Reasoning for Protein Function Understanding</div>
<div class="meta-line">Authors: Chuanliu Fan, Zicheng Ma, Huanran Meng, Aijia Zhang, Wenjie Du, Jun Zhang, Yi Qin Gao, Ziqiang Cao, Guohong Fu</div>
<div class="meta-line">First: 2026-01-07T05:34:38+00:00 · Latest: 2026-01-07T05:34:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03604v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03604v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large language models (LLMs) have highlighted the effectiveness of chain-of-thought reasoning in symbolic domains such as mathematics and programming. However, our study shows that directly transferring such text-based reasoning paradigms to protein function understanding is ineffective: reinforcement learning mainly amplifies superficial keyword patterns while failing to introduce new biological knowledge, resulting in limited generalization. We argue that protein function prediction is a knowledge-intensive scientific task that fundamentally relies on external biological priors and computational tools rather than purely internal reasoning. To address this gap, we propose PFUA, a tool-augmented protein reasoning agent that unifies problem decomposition, tool invocation, and grounded answer generation. Instead of relying on long unconstrained reasoning traces, PFUA integrates domain-specific tools to produce verifiable intermediate evidence. Experiments on four benchmarks demonstrate that PFUA consistently outperforms text-only reasoning models with an average performance improvement of 103%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>交错式工具调用推理用于蛋白质功能理解</div>
<div class="mono" style="margin-top:8px">近期大语言模型（LLMs）的进展突显了思维链推理在数学和编程等符号领域的有效性。然而，我们的研究表明，直接将此类基于文本的推理范式迁移到蛋白质功能理解中是无效的：强化学习主要放大了表面的关键词模式，而未能引入新的生物学知识，导致泛化能力有限。我们认为蛋白质功能预测是一项知识密集型的科学任务，其根本上依赖于外部生物学先验知识和计算工具，而非纯粹的内部推理。为弥补这一差距，我们提出了PFUA——一种工具增强的蛋白质推理智能体，它统一了问题分解、工具调用和基于证据的答案生成。PFUA不依赖冗长无约束的推理轨迹，而是整合领域专用工具以生成可验证的中间证据。在四个基准测试上的实验表明，PFUA始终优于纯文本推理模型，平均性能提升达103%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study is motivated by the ineffectiveness of directly applying text-based chain-of-thought reasoning from symbolic domains to protein function understanding, as it often amplifies superficial patterns without introducing new biological knowledge, limiting generalization. To address this, the authors propose PFUA, a tool-augmented protein reasoning agent that unifies problem decomposition, tool invocation, and grounded answer generation by integrating domain-specific tools to produce verifiable intermediate evidence instead of relying on unconstrained reasoning traces. Experimental results on four benchmarks show that PFUA consistently outperforms text-only reasoning models with an average performance improvement of 103%.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于，直接将基于文本的思维链推理从数学等符号领域迁移到蛋白质功能理解中效果不佳，因为强化学习主要放大了表面的关键词模式而未能引入新的生物学知识，导致泛化能力有限。为解决这一问题，作者提出了PFUA，一种工具增强的蛋白质推理代理，它通过整合领域特定工具来生成可验证的中间证据，统一了问题分解、工具调用和基于证据的答案生成，而不是依赖无约束的推理轨迹。在四个基准测试上的实验结果表明，PFUA始终优于纯文本推理模型，平均性能提升了103%。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning for Tool-Integrated Interleaved Thinking towards Cross-Domain Generalization</div>
<div class="meta-line">Authors: Zhengyu Chen, Jinluan Yang, Teng Xiao, Ruochen Zhou, Luan Zhang, Xiangyu Xi, Xiaowei Shi, Wei Wang, Jinggang Wang</div>
<div class="meta-line">First: 2025-10-13T09:19:13+00:00 · Latest: 2026-01-07T04:36:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.11184v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.11184v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large language models (LLMs) have demonstrated remarkable capabilities in reasoning and tool utilization. However, the generalization of tool-augmented reinforcement learning (RL) across diverse domains remains a significant challenge. Standard paradigms often treat tool usage as a linear or isolated event, which becomes brittle when transferring skills from restricted domains (e.g., mathematics) to open-ended tasks. In this work, we investigate the cross-domain generalization of an LLM agent trained exclusively on mathematical problem-solving. To facilitate robust skill transfer, we propose a {\textbf{R}einforcement Learning for \textbf{I}nterleaved \textbf{T}ool \textbf{E}xecution (RITE)}. Unlike traditional methods, RITE enforces a continuous ``Plan-Action-Reflection&#x27;&#x27; cycle, allowing the model to ground its reasoning in intermediate tool outputs and self-correct during long-horizon tasks. To effectively train this complex interleaved policy, we introduce {Dr. GRPO}, a robust optimization objective that utilizes token-level loss aggregation with importance sampling to mitigate reward sparsity and high-variance credit assignment. Furthermore, we employ a dual-component reward system and dynamic curriculum via online rollout filtering to ensure structural integrity and sample efficiency. Extensive experiments reveal that our approach, despite being trained solely on math tasks, achieves state-of-the-art performance across diverse reasoning domains, demonstrating high token efficiency and strong generalization capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向跨领域泛化的工具集成交错思维强化学习</div>
<div class="mono" style="margin-top:8px">大语言模型在推理与工具利用方面展现出卓越能力，但工具增强强化学习在不同领域的泛化仍面临挑战。传统范式常将工具使用视为线性或孤立事件，导致从受限领域向开放任务迁移时表现脆弱。本研究探索了仅接受数学问题求解训练的LLM智能体的跨领域泛化能力。为实现鲁棒的技能迁移，我们提出交错工具执行强化学习方法。与传统方法不同，该方法通过持续的“规划-执行-反思”循环，使模型能够基于中间工具输出进行推理并在长程任务中自我修正。为有效训练这一复杂交错策略，我们引入基于重要性采样的词元级损失聚合优化目标，以缓解奖励稀疏性与高方差信用分配问题。此外，采用双组件奖励系统与在线推演筛选的动态课程学习机制，确保结构完整性与样本效率。实验表明，尽管仅接受数学任务训练，该方法在多样化推理领域均达到最先进性能，展现出高效的词元利用率与强大的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of cross-domain generalization for tool-augmented reinforcement learning agents, motivated by the brittleness of standard linear tool-use paradigms when transferring skills from restricted domains like mathematics to open-ended tasks. The method introduces RITE (Reinforcement Learning for Interleaved Tool Execution), which enforces a continuous Plan-Action-Reflection cycle to ground reasoning in tool outputs and enable self-correction, and Dr. GRPO, an optimization objective using token-level loss aggregation with importance sampling to mitigate reward sparsity and credit assignment issues, alongside a dual-component reward system and dynamic curriculum. Experimental results show that the approach, trained solely on math tasks, achieves state-of-the-art performance across diverse reasoning domains with high token efficiency and strong generalization.</div>
<div class="mono" style="margin-top:8px">本文针对工具增强强化学习智能体在跨领域泛化中的挑战，动机在于标准线性工具使用范式在从数学等受限领域迁移技能到开放任务时表现脆弱。方法提出了RITE（交错工具执行的强化学习），通过强制连续的“计划-行动-反思”循环使推理基于工具输出并实现自我修正，以及Dr. GRPO优化目标，利用令牌级损失聚合和重要性采样来缓解奖励稀疏性和信用分配问题，同时结合双组件奖励系统和动态课程学习。实验结果表明，该方法仅在数学任务上训练，就在多样推理领域实现了最先进的性能，表现出高令牌效率和强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">CatCMA with Margin for Single- and Multi-Objective Mixed-Variable Black-Box Optimization</div>
<div class="meta-line">Authors: Ryoki Hamano, Masahiro Nomura, Shota Saito, Kento Uchida, Shinichi Shirakawa</div>
<div class="meta-line">First: 2025-04-10T15:59:22+00:00 · Latest: 2026-01-07T03:51:09+00:00</div>
<div class="meta-line">Comments: v6: Extended journal version. Adds a multi-objective extension (Section 7) and bi-objective mixed-variable experiments. v5 corresponds to the GECCO&#x27;25 conference version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.07884v6">Abs</a> · <a href="https://arxiv.org/pdf/2504.07884v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study focuses on mixed-variable black-box optimization (MV-BBO), addressing continuous, integer, and categorical variables. Many real-world MV-BBO problems involve dependencies among these different types of variables, requiring efficient methods to optimize them simultaneously. Recently, stochastic optimization methods leveraging the mechanism of the covariance matrix adaptation evolution strategy have shown promising results in mixed-integer or mixed-category optimization. However, such methods cannot handle the three types of variables simultaneously. In this study, we propose CatCMA with Margin (CatCMAwM), a stochastic optimization method for MV-BBO that jointly optimizes continuous, integer, and categorical variables. CatCMAwM is developed by incorporating novel integer handling into CatCMA, a mixed-category black-box optimization method employing a joint distribution of multivariate Gaussian and categorical distributions. The proposed integer handling is carefully designed by reviewing existing integer handling and following the design principles of CatCMA. Furthermore, we extend CatCMAwM to multi-objective MV-BBO by instantiating it within the Sofomore framework, obtaining a multi-objective optimizer termed COMO-CatCMA with Margin (COMO-CatCMAwM). Numerical experiments on single-objective MV-BBO problems show that CatCMAwM effectively handles the three types of variables, outperforming state-of-the-art Bayesian optimization methods and baselines that simply incorporate existing integer handlings into CatCMA. Moreover, on bi-objective MV-BBO benchmarks, COMO-CatCMAwM achieves competitive or superior hypervolume compared to representative baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>带边界机制的CatCMA方法：面向单目标与多目标混合变量黑盒优化</div>
<div class="mono" style="margin-top:8px">本研究聚焦混合变量黑盒优化问题，涉及连续、整数与类别变量。现实中的混合变量优化常存在跨类型变量依赖，需高效同步优化方法。近期基于协方差矩阵自适应进化策略的随机优化方法在混合整数/类别优化中表现突出，但无法同时处理三类变量。本文提出带边界机制的CatCMA方法——一种能同步优化连续、整数与类别变量的随机优化算法。该方法通过将新型整数处理机制融入CatCMA（一种采用多元高斯与类别联合分布的混合类别黑盒优化方法）实现，其整数处理模块在系统评估现有方法后严格遵循CatCMA设计原则构建。进一步地，通过将CatCMAwM实例化于Sofomore框架，我们将其扩展至多目标混合变量优化领域，得到名为COMO-CatCMAwM的多目标优化器。在单目标混合变量问题上的数值实验表明，CatCMAwM能有效处理三类变量，其性能优于先进贝叶斯优化方法及简单整合现有整数处理机制的基线方法。在双目标混合变量基准测试中，COMO-CatCMAwM获得的超体积指标与代表性基线方法相比具有竞争力或更优表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses mixed-variable black-box optimization (MV-BBO) involving continuous, integer, and categorical variables, motivated by the need to handle dependencies among these variable types efficiently in real-world problems. The method proposes CatCMA with Margin (CatCMAwM), which extends the CatCMA approach by incorporating a novel integer-handling mechanism to jointly optimize all three variable types using a distribution combining multivariate Gaussian and categorical models; it is further extended to multi-objective optimization via the Sofomore framework, resulting in COMO-CatCMAwM. Experimental results on single-objective problems demonstrate that CatCMAwM outperforms state-of-the-art Bayesian optimization methods and baselines, while in bi-objective benchmarks, COMO-CatCMAwM achieves competitive or superior hypervolume compared to representative alternatives.</div>
<div class="mono" style="margin-top:8px">本研究针对包含连续、整数和类别变量的混合变量黑盒优化问题，其动机在于现实问题中这些变量类型间常存在依赖关系，需要高效方法进行同步优化。该方法提出了CatCMA with Margin（CatCMAwM），通过引入新颖的整数处理机制扩展了CatCMA方法，利用多元高斯与类别分布的联合分布来共同优化三类变量，并基于Sofomore框架将其扩展至多目标优化，得到COMO-CatCMAwM。在单目标问题上的实验结果表明，CatCMAwM优于先进的贝叶斯优化方法和基线；在双目标基准测试中，COMO-CatCMAwM相比代表性基线取得了竞争性或更优的超体积指标。</div>
</details>
</div>
<div class="card">
<div class="title">Active operator learning with predictive uncertainty quantification for partial differential equations</div>
<div class="meta-line">Authors: Nick Winovich, Mitchell Daneker, Lu Lu, Guang Lin</div>
<div class="meta-line">First: 2025-03-05T04:48:14+00:00 · Latest: 2026-01-07T03:50:11+00:00</div>
<div class="meta-line">Comments: Submitted to the Journal of Computational Physics</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.03178v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.03178v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the increased prevalence of neural operators being used to provide rapid solutions to partial differential equations (PDEs), understanding the accuracy of model predictions and the associated error levels is necessary for deploying reliable surrogate models in scientific applications. Existing uncertainty quantification (UQ) frameworks employ ensembles or Bayesian methods, which can incur substantial computational costs during both training and inference. We propose a lightweight predictive UQ method tailored for Deep operator networks (DeepONets) that also generalizes to other operator networks. Numerical experiments on linear and nonlinear PDEs demonstrate that the framework&#x27;s uncertainty estimates are unbiased and provide accurate out-of-distribution uncertainty predictions with a sufficiently large training dataset. Our framework provides fast inference and uncertainty estimates that can efficiently drive outer-loop analyses that would be prohibitively expensive with conventional solvers. We demonstrate how predictive uncertainties can be used in the context of Bayesian optimization and active learning problems to yield improvements in accuracy and data-efficiency for outer-loop optimization procedures. In the active learning setup, we extend the framework to Fourier Neural Operators (FNO) and describe a generalized method for other operator networks. To enable real-time deployment, we introduce an inference strategy based on precomputed trunk outputs and a sparse placement matrix, reducing evaluation time by more than a factor of five. Our method provides a practical route to uncertainty-aware operator learning in time-sensitive settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>偏微分方程主动算子学习与预测不确定性量化</div>
<div class="mono" style="margin-top:8px">随着神经算子被广泛用于快速求解偏微分方程，理解模型预测的准确性及相关误差水平对于在科学应用中部署可靠的代理模型至关重要。现有不确定性量化框架采用集成或贝叶斯方法，在训练和推断阶段均会产生高昂计算成本。本文提出一种专为深度算子网络设计的轻量级预测不确定性量化方法，该方法也可推广至其他算子网络。在线性与非线性偏微分方程上的数值实验表明，在训练数据集足够大的情况下，该框架的不确定性估计具有无偏性，并能提供准确的分布外不确定性预测。本框架提供快速推断与不确定性估计，可高效驱动传统求解器难以承担的外循环分析。我们展示了如何将预测不确定性应用于贝叶斯优化和主动学习问题，以提升外循环优化过程的精度与数据效率。在主动学习场景中，我们将框架扩展至傅里叶神经算子，并描述了适用于其他算子网络的通用方法。为实现实时部署，我们引入基于预计算主干输出和稀疏放置矩阵的推断策略，将评估时间缩短五倍以上。该方法为时间敏感场景下的不确定性感知算子学习提供了实用路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for reliable uncertainty quantification (UQ) in neural operator models for solving partial differential equations (PDEs), as existing UQ methods are often computationally expensive. The authors propose a lightweight predictive UQ framework tailored for DeepONets, which also generalizes to other operator networks like Fourier Neural Operators, emphasizing efficiency through techniques such as precomputed trunk outputs to speed up inference. Experimental results on linear and nonlinear PDEs show that the method provides unbiased uncertainty estimates and accurate out-of-distribution predictions with sufficient training data, enabling applications in Bayesian optimization and active learning to improve accuracy and data-efficiency in outer-loop analyses.</div>
<div class="mono" style="margin-top:8px">本文针对求解偏微分方程的神经算子模型需要可靠的不确定性量化问题，因为现有方法通常计算成本高昂。作者提出了一种专为DeepONets设计的轻量级预测性不确定性量化框架，并可推广到其他算子网络如傅里叶神经算子，通过预计算主干输出等技术提高推理效率。在线性和非线性偏微分方程上的实验结果表明，该方法在足够训练数据下能提供无偏的不确定性估计和准确的分布外预测，从而支持贝叶斯优化和主动学习等应用，提升外循环分析的准确性和数据效率。</div>
</details>
</div>
<div class="card">
<div class="title">SCRIBE: Structured Mid-Level Supervision for Tool-Using Language Models</div>
<div class="meta-line">Authors: Yuxuan Jiang, Francis Ferraro</div>
<div class="meta-line">First: 2026-01-07T03:49:48+00:00 · Latest: 2026-01-07T03:49:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03555v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03555v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training reliable tool-augmented agents remains a significant challenge, largely due to the difficulty of credit assignment in multi-step reasoning. While process-level reward models offer a promising direction, existing LLM-based judges often produce noisy and inconsistent signals because they lack fine-grained, task-specific rubrics to distinguish high-level planning from low-level execution. In this work, we introduce SCRIBE (Skill-Conditioned Reward with Intermediate Behavioral Evaluation), a reinforcement learning framework that intervenes at a novel mid-level abstraction. SCRIBE grounds reward modeling in a curated library of skill prototypes, transforming open-ended LLM evaluation into a constrained verification problem. By routing each subgoal to a corresponding prototype, the reward model is equipped with precise, structured rubrics that substantially reduce reward variance.
  Experimental results show that SCRIBE achieves state-of-the-art performance across a range of reasoning and tool-use benchmarks. In particular, it improves the AIME25 accuracy of a Qwen3-4B model from 43.3% to 63.3%, and significantly increases success rates in complex multi-turn tool interactions.
  Further analysis of training dynamics reveals a co-evolution across abstraction levels, where mastery of mid-level skills consistently precedes the emergence of effective high-level planning behaviors. Finally, we demonstrate that SCRIBE is additive to low-level tool optimizations, providing a scalable and complementary pathway toward more autonomous and reliable tool-using agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SCRIBE：面向工具调用语言模型的结构化中层监督框架</div>
<div class="mono" style="margin-top:8px">训练可靠的工具增强智能体仍是重大挑战，主要源于多步推理中的信用分配难题。虽然过程级奖励模型提供了可行方向，但现有基于大语言模型的评估器常因缺乏区分高层规划与底层执行的细粒度任务标准，而产生噪声大且不一致的信号。本研究提出SCRIBE（基于技能条件与中间行为评估的强化学习框架），该框架通过创新的中层抽象进行干预。SCRIBE将奖励模型建立在精心构建的技能原型库基础上，将开放式大语言模型评估转化为约束性验证问题。通过将每个子目标路由至对应原型，奖励模型获得精确的结构化评估标准，从而显著降低奖励方差。实验结果表明，SCRIBE在多项推理与工具使用基准测试中达到最先进性能：其将Qwen3-4B模型在AIME25的准确率从43.3%提升至63.3%，并显著提高复杂多轮工具交互的成功率。训练动态的进一步分析揭示了抽象层级间的协同演化规律——中层技能的掌握始终先于有效高层规划行为的形成。最后，我们证明SCRIBE可与底层工具优化形成互补，为构建更自主可靠的工具调用智能体提供可扩展的增强路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of credit assignment in multi-step tool-use tasks, where existing LLM-based reward models produce noisy signals due to a lack of fine-grained rubrics, this paper introduces SCRIBE, a reinforcement learning framework that provides structured mid-level supervision. The method grounds reward modeling in a library of skill prototypes, transforming open-ended evaluation into a constrained verification problem to reduce reward variance. Experimental results show SCRIBE achieves state-of-the-art performance on reasoning and tool-use benchmarks, notably improving a Qwen3-4B model&#x27;s AIME25 accuracy from 43.3% to 63.3% and boosting success in complex multi-turn interactions, with analysis revealing that mid-level skill mastery precedes effective high-level planning.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决多步骤工具使用任务中的信用分配难题，现有基于大语言模型的奖励模型因缺乏细粒度评估标准而产生噪声信号。为此，论文提出了SCRIBE强化学习框架，通过引入结构化的中层监督，将奖励建模建立在技能原型库上，将开放式评估转化为受限验证问题以降低奖励方差。实验结果表明，SCRIBE在推理和工具使用基准测试中达到了最先进的性能，特别是将Qwen3-4B模型在AIME25上的准确率从43.3%提升至63.3%，并显著提高了复杂多轮交互的成功率，分析进一步揭示中层技能掌握先于有效高层规划行为的出现。</div>
</details>
</div>
<div class="card">
<div class="title">DRA-GRPO: Your GRPO Needs to Know Diverse Reasoning Paths for Mathematical Reasoning</div>
<div class="meta-line">Authors: Xiwen Chen, Wenhui Zhu, Peijie Qiu, Xuanzhao Dong, Hao Wang, Haiyu Wu, Huayu Li, Aristeidis Sotiras, Yalin Wang, Abolfazl Razi</div>
<div class="meta-line">First: 2025-05-14T02:02:32+00:00 · Latest: 2026-01-07T03:24:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.09655v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.09655v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Post-training LLMs with Reinforcement Learning, specifically Group Relative Policy Optimization (GRPO), has emerged as a paradigm for enhancing mathematical reasoning. However, standard GRPO relies on scalar correctness rewards that are often non-injective with respect to semantic content: distinct reasoning paths receive identical rewards. This leads to a Diversity-Quality Inconsistency, where the policy collapses into a narrow set of dominant modes while ignoring equally valid but structurally novel strategies. To bridge this gap, we propose Diversity-aware Reward Adjustment (DRA), a theoretically grounded framework that calibrates the reward signal using the semantic density of sampled groups. By leveraging Submodular Mutual Information (SMI), DRA implements an Inverse Propensity Scoring (IPS) mechanism that effectively de-biases the gradient estimation. This creates a repulsive force against redundancy, driving the policy to achieve better coverage of the high-reward landscape. Our method is plug-and-play and integrates seamlessly with GRPO variants. Empirical evaluations on five math benchmarks demonstrate that DRA-GRPO consistently outperforms strong baselines, achieving an average accuracy of 58.2% on DeepSeek-R1-Distill-Qwen-1.5B with only 7,000 training samples and $55 cost, highlighting the critical role of diversity calibration in data-efficient alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DRA-GRPO：数学推理中GRPO需要掌握多样化推理路径</div>
<div class="mono" style="margin-top:8px">基于强化学习（特别是组相对策略优化GRPO）的大语言模型后训练已成为提升数学推理能力的范式。然而，标准GRPO依赖的标量正确性奖励常与语义内容非单射关联：不同推理路径获得相同奖励。这导致多样性-质量不一致性，使策略坍缩至狭窄的主导模式，忽略结构新颖但同等有效的策略。为弥合此差距，我们提出理论基础的多样性感知奖励调整框架，通过采样组的语义密度校准奖励信号。该框架利用子模互信息实现逆倾向评分机制，有效消除梯度估计偏差，形成针对冗余的排斥力，驱动策略更好覆盖高奖励空间。本方法即插即用，可与GRPO变体无缝集成。在五项数学基准测试中，DRA-GRPO始终优于强基线模型，仅用7,000训练样本和55美元成本即在DeepSeek-R1-Distill-Qwen-1.5B上实现58.2%平均准确率，凸显了多样性校准在数据高效对齐中的关键作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the Diversity-Quality Inconsistency in standard Group Relative Policy Optimization (GRPO) for mathematical reasoning, where scalar correctness rewards fail to distinguish between distinct but valid reasoning paths, causing policy collapse. To address this, the authors propose Diversity-aware Reward Adjustment (DRA), a plug-and-play framework that calibrates rewards using Submodular Mutual Information to measure semantic density and applies Inverse Propensity Scoring to de-bias gradient estimation, thereby encouraging diverse reasoning strategies. Experimental results on five math benchmarks show that DRA-GRPO outperforms baselines, achieving an average accuracy of 58.2% with high data efficiency and low cost, demonstrating the importance of diversity calibration in alignment.</div>
<div class="mono" style="margin-top:8px">本文的动机在于标准群组相对策略优化（GRPO）在数学推理中存在的多样性-质量不一致问题，即标量正确性奖励无法区分不同但有效的推理路径，导致策略坍缩。为解决此问题，作者提出了多样性感知奖励调整（DRA），这是一个即插即用的框架，利用子模互信息度量语义密度并应用逆倾向评分来校正梯度估计偏差，从而鼓励多样化的推理策略。在五个数学基准测试上的实验结果表明，DRA-GRPO优于基线方法，以高数据效率和低成本实现了58.2%的平均准确率，证明了多样性校准在对齐中的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">Think Outside the Policy: In-Context Steered Policy Optimization</div>
<div class="meta-line">Authors: Hsiu-Yuan Huang, Chenming Tang, Weijie Liu, Clive Bai, Saiyong Yang, Yunfang Wu</div>
<div class="meta-line">First: 2025-10-30T14:14:15+00:00 · Latest: 2026-01-07T03:04:54+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.26519v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.26519v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing Reinforcement Learning from Verifiable Rewards (RLVR) methods, such as Group Relative Policy Optimization (GRPO), have achieved remarkable progress in improving the reasoning capabilities of Large Reasoning Models (LRMs). However, they exhibit limited exploration due to reliance on on-policy rollouts which are confined to the current policy&#x27;s distribution, resulting in narrow trajectory diversity. Recent approaches attempt to expand policy coverage by incorporating trajectories generated from stronger expert models, yet this reliance increases computational cost and such advanced models are often inaccessible. To address these issues, we propose In-Context Steered Policy Optimization (ICPO), a unified framework that leverages the inherent in-context learning capability of LRMs to provide expert guidance using existing datasets. ICPO introduces mixed-policy GRPO with implicit expert forcing, which expands exploration beyond the current policy distribution without requiring advanced LRM trajectories. To further stabilize optimization, ICPO integrates expert region reject sampling to filter unreliable off-policy trajectories and annealed expert-bonus reward shaping to balance early expert guidance with later autonomous improvement. Results demonstrate that ICPO consistently enhances RLVR performance and training stability on mathematical reasoning benchmarks, revealing a scalable and effective RLVR paradigm for LRMs. Our code is available at https://anonymous.4open.science/r/ICPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越策略边界：上下文引导的策略优化</div>
<div class="mono" style="margin-top:8px">现有的可验证奖励强化学习方法，如组相对策略优化，在提升大型推理模型的推理能力方面取得了显著进展。然而，由于依赖局限于当前策略分布的在线策略轨迹，这些方法探索能力有限，导致轨迹多样性不足。近期研究尝试通过引入更强专家模型生成的轨迹来扩展策略覆盖范围，但这种依赖增加了计算成本，且此类先进模型往往难以获取。为解决这些问题，我们提出了上下文引导的策略优化，这是一个统一框架，利用大型推理模型固有的上下文学习能力，通过现有数据集提供专家指导。该方法引入了带有隐式专家强化的混合策略组相对策略优化，能够在无需高级模型轨迹的情况下，将探索范围扩展至当前策略分布之外。为进一步稳定优化过程，该方法整合了专家区域拒绝采样以过滤不可靠的离线策略轨迹，并采用退火式专家奖励塑形来平衡早期专家指导与后期自主改进。实验结果表明，在数学推理基准测试中，该方法持续提升了可验证奖励强化学习的性能与训练稳定性，为大型推理模型揭示了一种可扩展且有效的强化学习范式。代码发布于 https://anonymous.4open.science/r/ICPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limited exploration in existing Reinforcement Learning from Verifiable Rewards (RLVR) methods like GRPO, which rely on on-policy rollouts that constrain trajectory diversity. To overcome this without depending on computationally expensive or inaccessible expert models, the authors propose In-Context Steered Policy Optimization (ICPO), a framework that leverages the in-context learning of Large Reasoning Models to provide expert guidance from existing datasets. ICPO employs mixed-policy GRPO with implicit expert forcing to broaden exploration, along with expert region reject sampling and annealed expert-bonus reward shaping to stabilize training. Experimental results on mathematical reasoning benchmarks show that ICPO consistently improves RLVR performance and training stability, offering a scalable and effective paradigm for enhancing LRMs.</div>
<div class="mono" style="margin-top:8px">本文针对现有可验证奖励强化学习方法（如GRPO）探索有限的问题，这些方法依赖当前策略分布内的轨迹，导致多样性不足。为避免依赖计算成本高或难以获取的专家模型，作者提出了上下文引导策略优化（ICPO），该框架利用大型推理模型的上下文学习能力，从现有数据集中提供专家指导。ICPO采用混合策略GRPO与隐式专家强制来扩展探索，并结合专家区域拒绝采样和退火专家奖励塑形以稳定训练。在数学推理基准测试上的实验结果表明，ICPO能持续提升RLVR性能和训练稳定性，为增强大型推理模型提供了一种可扩展且有效的范式。</div>
</details>
</div>
<div class="card">
<div class="title">Boosting In-Silicon Directed Evolution with Fine-Tuned Protein Language Model and Tree Search</div>
<div class="meta-line">Authors: Yaodong Yang, Yang Wang, Jinpeng Li, Pei Guo, Da Han, Guangyong Chen, Pheng-Ann Heng</div>
<div class="meta-line">First: 2025-11-13T03:00:52+00:00 · Latest: 2026-01-07T02:45:43+00:00</div>
<div class="meta-line">Comments: working in progress, 20 pages, 6 figures, 16 tables, updating template</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.09900v4">Abs</a> · <a href="https://arxiv.org/pdf/2511.09900v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Protein evolution through amino acid mutations is a cornerstone of life sciences. Recent advances in protein language models have shown rich evolutionary patterns, offering unprecedented potential for in-silicon directed evolution. However, existing directed evolution methods largely rely on heuristic evolution strategies and have yet to efficiently integrate the transformative protein language models with advanced optimization techniques, such as reinforcement learning, to adaptively learn superior evolution policies. To bridge this gap, we propose AlphaDE, a novel framework that evolves protein sequences by harnessing the innovative paradigms of large language models, such as fine-tuning and test-time inference. First, AlphaDE fine-tunes pretrained protein language models using masked language modeling on homologous protein sequences to activate the evolutionary plausibility of the interested protein family. Second, AlphaDE introduces test-time inference based on Monte Carlo tree search, which effectively evolves proteins with evolutionary guidance from the fine-tuned protein language model. Extensive benchmark experiments show that AlphaDE remarkably outperforms previous state-of-the-art methods even with few-shot fine-tuning. A case study further demonstrates that AlphaDE supports condensing the protein sequence space of avGFP through computational evolution.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于微调蛋白质语言模型与树搜索增强硅基定向进化</div>
<div class="mono" style="margin-top:8px">通过氨基酸突变实现蛋白质进化是生命科学的基石。近期蛋白质语言模型的进展揭示了丰富的进化模式，为硅基定向进化提供了前所未有的潜力。然而，现有定向进化方法多依赖启发式进化策略，尚未有效整合变革性的蛋白质语言模型与强化学习等先进优化技术，以自适应学习更优进化策略。为填补这一空白，我们提出AlphaDE框架，该框架利用大语言模型的创新范式（如微调和测试时推理）进化蛋白质序列。首先，AlphaDE通过同源蛋白质序列的掩码语言建模微调预训练蛋白质语言模型，以激活目标蛋白质家族的进化合理性。其次，AlphaDE引入基于蒙特卡洛树搜索的测试时推理，在微调蛋白质语言模型的进化指导下高效进化蛋白质。大量基准实验表明，即使仅进行少样本微调，AlphaDE仍显著优于现有最优方法。案例研究进一步证明，AlphaDE可通过计算进化压缩avGFP的蛋白质序列空间。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to better integrate protein language models with advanced optimization for directed evolution, this paper introduces AlphaDE, a framework that fine-tunes a pretrained protein language model on homologous sequences to capture evolutionary plausibility and then employs Monte Carlo tree search for test-time inference to guide protein evolution. The method adaptively learns superior evolution policies, moving beyond heuristic strategies. Experimental results on benchmarks demonstrate that AlphaDE significantly outperforms prior state-of-the-art methods even with few-shot fine-tuning, and a case study shows its effectiveness in condensing the sequence space of avGFP through computational evolution.</div>
<div class="mono" style="margin-top:8px">本文的动机在于将蛋白质语言模型与先进的优化技术更有效地结合以改进定向进化，为此提出了AlphaDE框架，该方法首先通过在同源蛋白质序列上进行掩码语言建模来微调预训练的蛋白质语言模型，以激活目标蛋白质家族的进化合理性，随后引入基于蒙特卡洛树搜索的测试时推理，在微调模型的指导下进化蛋白质。该方法超越了传统的启发式策略，能够自适应地学习更优的进化策略。大量基准实验表明，即使进行少样本微调，AlphaDE也显著优于以往的最先进方法，案例研究进一步证明其能够通过计算进化有效压缩avGFP的蛋白质序列空间。</div>
</details>
</div>
<div class="card">
<div class="title">VeRPO: Verifiable Dense Reward Policy Optimization for Code Generation</div>
<div class="meta-line">Authors: Longwen Wang, Xuan&#x27;er Wu, Xiaohui Hu, Yirui Liu, Yuankai Fan, Kaidong Yu, Qizhen Weng, Wei Xi, Xuelong Li</div>
<div class="meta-line">First: 2026-01-07T02:29:49+00:00 · Latest: 2026-01-07T02:29:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03525v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03525v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Effective reward design is a central challenge in Reinforcement Learning (RL) for code generation. Mainstream pass/fail outcome rewards enforce functional correctness via executing unit tests, but the resulting sparsity limits potential performance gains. While recent work has explored external Reward Models (RM) to generate richer, continuous rewards, the learned RMs suffer from reward misalignment and prohibitive computational cost. In this paper, we introduce \textbf{VeRPO} (\textbf{V}erifiable D\textbf{e}nse \textbf{R}eward \textbf{P}olicy \textbf{O}ptimization), a novel RL framework for code generation that synthesizes \textit{robust and dense rewards fully grounded in verifiable execution feedback}. The core idea of VeRPO is constructing dense rewards from weighted partial success: by dynamically estimating the difficulty weight of each unit test based on the execution statistics during training, a dense reward is derived from the sum of weights of the passed unit tests. To solidify the consistency between partial success and end-to-end functional correctness, VeRPO further integrates the dense signal with global execution outcomes, establishing a robust and dense reward paradigm relying solely on verifiable execution feedback. Extensive experiments across diverse benchmarks and settings demonstrate that VeRPO consistently outperforms outcome-driven and RM-based baselines, achieving up to +8.83\% gain in pass@1 with negligible time cost (&lt; 0.02\%) and zero GPU memory overhead.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VeRPO：面向代码生成的可验证密集奖励策略优化</div>
<div class="mono" style="margin-top:8px">有效的奖励设计是代码生成强化学习（RL）的核心挑战。主流基于通过/失败结果的奖励通过执行单元测试来确保功能正确性，但其稀疏性限制了性能提升潜力。尽管近期研究探索使用外部奖励模型（RM）生成更丰富的连续奖励，但学习得到的RM存在奖励失准和计算成本过高的问题。本文提出\textbf{VeRPO}（\textbf{可验证密集奖励策略优化}），这是一种用于代码生成的新型RL框架，能够基于\textit{完全可验证的执行反馈}合成\textit{鲁棒且密集的奖励}。VeRPO的核心思想是通过加权部分成功构建密集奖励：在训练过程中根据执行统计数据动态估计每个单元测试的难度权重，从通过的单元测试权重总和中推导出密集奖励。为强化部分成功与端到端功能正确性的一致性，VeRPO进一步将密集信号与全局执行结果整合，建立了仅依赖可验证执行反馈的鲁棒密集奖励范式。跨多基准和设置的广泛实验表明，VeRPO始终优于结果驱动和基于RM的基线方法，在pass@1指标上最高提升+8.83%，且时间成本可忽略（&lt;0.02%），GPU内存开销为零。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of sparse rewards in reinforcement learning for code generation, where traditional pass/fail rewards based on unit test execution limit performance. The proposed method, VeRPO, constructs dense rewards by weighting partial success: it dynamically estimates the difficulty of each unit test using execution statistics during training and sums the weights of passed tests, integrating this with global execution outcomes to ensure alignment with functional correctness. Experimental results across various benchmarks show that VeRPO consistently outperforms both outcome-driven and reward model-based baselines, achieving up to +8.83% improvement in pass@1 with minimal time cost and no GPU memory overhead.</div>
<div class="mono" style="margin-top:8px">本文针对代码生成中强化学习的稀疏奖励挑战，传统基于单元测试执行的通过/失败奖励限制了性能提升。提出的方法VeRPO通过加权部分成功构建密集奖励：在训练中利用执行统计动态估计每个单元测试的难度，并汇总通过测试的权重，同时结合全局执行结果以确保与功能正确性一致。在多个基准测试中的实验结果表明，VeRPO持续优于基于结果和奖励模型的基线方法，在pass@1指标上最高提升+8.83%，且时间成本极低，无GPU内存开销。</div>
</details>
</div>
<div class="card">
<div class="title">A Reinforcement Learning-Based Model for Mapping and Goal-Directed Navigation Using Multiscale Place Fields</div>
<div class="meta-line">Authors: Bekarys Dukenbaev, Andrew Gerstenslager, Alexander Johnson, Ali A. Minai</div>
<div class="meta-line">First: 2026-01-07T02:10:52+00:00 · Latest: 2026-01-07T02:10:52+00:00</div>
<div class="meta-line">Comments: 11 pages, 8 figures. Submitted to IEEE Transactions on Cognitive and Developmental Systems</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03520v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03520v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous navigation in complex and partially observable environments remains a central challenge in robotics. Several bio-inspired models of mapping and navigation based on place cells in the mammalian hippocampus have been proposed. This paper introduces a new robust model that employs parallel layers of place fields at multiple spatial scales, a replay-based reward mechanism, and dynamic scale fusion. Simulations show that the model improves path efficiency and accelerates learning compared to single-scale baselines, highlighting the value of multiscale spatial representations for adaptive robot navigation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习的多尺度位置场映射与目标导向导航模型</div>
<div class="mono" style="margin-top:8px">在复杂且部分可观测环境中的自主导航仍是机器人技术的核心挑战。已有多种基于哺乳动物海马体位置细胞的仿生映射与导航模型被提出。本文提出一种新型鲁棒模型，采用多空间尺度的并行位置场层、基于经验回放的奖励机制及动态尺度融合技术。仿真实验表明，相较于单尺度基准模型，该模型能提升路径效率并加速学习进程，凸显了多尺度空间表征对自适应机器人导航的重要价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of autonomous navigation in complex, partially observable environments, this paper introduces a bio-inspired model that uses parallel layers of multiscale place fields, a replay-based reward mechanism, and dynamic scale fusion to enhance mapping and goal-directed navigation. The method leverages hierarchical spatial representations akin to hippocampal place cells. Experimental simulations demonstrate that the model achieves improved path efficiency and faster learning rates compared to single-scale baseline approaches, underscoring the utility of multiscale representations for adaptive robotic navigation.</div>
<div class="mono" style="margin-top:8px">针对复杂、部分可观测环境中自主导航的挑战，本文提出一种受生物启发的模型，该模型采用多尺度位置场的并行层、基于经验回放的奖励机制和动态尺度融合，以增强建图与目标导向导航。该方法利用了类似海马体位置细胞的分层空间表征。实验仿真结果表明，与单尺度基线方法相比，该模型提高了路径效率并加速了学习过程，从而凸显了多尺度空间表征对于自适应机器人导航的价值。</div>
</details>
</div>
<div class="card">
<div class="title">DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search</div>
<div class="meta-line">Authors: Fang Wu, Weihao Xuan, Heli Qi, Ximing Lu, Aaron Tu, Li Erran Li, Yejin Choi</div>
<div class="meta-line">First: 2025-09-29T20:00:29+00:00 · Latest: 2026-01-07T02:08:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.25454v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.25454v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although RLVR has become an essential component for developing advanced reasoning skills in language models, contemporary studies have documented training plateaus after thousands of optimization steps, i.e., notable decreases in performance gains despite increased computational investment. This limitation stems from the sparse exploration patterns inherent in current RLVR practices, where models rely on limited rollouts that often miss critical reasoning paths and fail to provide systematic coverage of the solution space. We present DeepSearch, a framework that integrates Monte Carlo Tree Search (MCTS) directly into RLVR training. In contrast to existing methods that rely on tree search only at inference, DeepSearch embeds structured search into the training loop, enabling systematic exploration and fine-grained credit assignment across reasoning steps. Through training-time exploration, DeepSearch addresses the fundamental bottleneck of insufficient exploration, which leads to diminishing performance improvements over prolonged training steps. Our contributions include: (1) a global frontier selection strategy that prioritizes promising nodes across the search tree, (2) selection with entropy-based guidance that identifies confident paths for supervision, and (3) adaptive replay buffer training with solution caching for efficiency. Experiments on mathematical reasoning benchmarks show that DeepSearch achieves 62.95% average accuracy and establishes a new state-of-the-art for 1.5B reasoning models, while using 5.7x fewer GPU hours than extended training approaches. These results highlight the importance of strategic exploration over brute-force scaling and demonstrate the promise of algorithmic innovation for advancing RLVR methodologies. DeepSearch establishes a new direction for scaling reasoning capabilities through systematic search rather than prolonged computation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeepSearch：通过蒙特卡洛树搜索突破强化学习可验证奖励的瓶颈</div>
<div class="mono" style="margin-top:8px">尽管强化学习可验证奖励已成为提升语言模型高级推理能力的关键技术，但近期研究表明，经过数千步优化后常出现训练平台期——即计算资源持续增加而性能增益显著下降。这一局限源于当前强化学习可验证奖励实践中固有的稀疏探索模式：模型依赖有限次数的推演，往往错过关键推理路径，且无法系统覆盖解空间。我们提出DeepSearch框架，将蒙特卡洛树搜索直接整合到强化学习可验证奖励训练中。与现有仅在推理阶段使用树搜索的方法不同，DeepSearch将结构化搜索嵌入训练循环，实现系统化探索和推理步骤间的细粒度信用分配。通过训练阶段的探索，DeepSearch从根本上解决了探索不足导致的性能提升瓶颈问题。我们的贡献包括：（1）全局前沿选择策略——优先探索搜索树中潜力节点；（2）基于熵的引导选择机制——识别高置信路径用于监督；（3）自适应回放缓冲区训练与解缓存技术以提升效率。数学推理基准测试表明，DeepSearch在15亿参数推理模型上取得62.95%的平均准确率，刷新当前最优性能记录，同时比扩展训练方法减少5.7倍GPU时耗。这些结果凸显了策略性探索相对于暴力扩展的重要性，并为推进强化学习可验证奖励方法论展示了算法创新的潜力。DeepSearch通过系统化搜索而非延长计算时间，为扩展推理能力开辟了新方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the training plateaus and sparse exploration in Reinforcement Learning with Verifiable Rewards (RLVR), which limit performance gains despite increased computation, this paper introduces DeepSearch, a framework that integrates Monte Carlo Tree Search (MCTS) directly into the RLVR training loop to enable systematic exploration and fine-grained credit assignment across reasoning steps. The method employs a global frontier selection strategy, entropy-based guidance for path selection, and adaptive replay buffer training with solution caching. Experimental results on mathematical reasoning benchmarks demonstrate that DeepSearch achieves a state-of-the-art average accuracy of 62.95% for 1.5B parameter models while using 5.7 times fewer GPU hours than extended training approaches, highlighting the efficacy of strategic exploration over brute-force scaling.</div>
<div class="mono" style="margin-top:8px">针对可验证奖励的强化学习（RLVR）中存在的训练瓶颈和探索稀疏性问题，即尽管计算投入增加但性能提升有限，本文提出了DeepSearch框架，它将蒙特卡洛树搜索（MCTS）直接集成到RLVR训练循环中，以实现对推理步骤的系统性探索和细粒度信用分配。该方法采用了全局前沿选择策略、基于熵的路径选择指导以及带解决方案缓存的适应性回放缓冲区训练。在数学推理基准测试上的实验结果表明，DeepSearch在1.5B参数模型上达到了62.95%的平均准确率，创下了新的最先进水平，同时比延长训练的方法减少了5.7倍的GPU小时使用，证明了策略性探索相比暴力扩展的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">The Invisible Leash: Why RLVR May or May Not Escape Its Origin</div>
<div class="meta-line">Authors: Fang Wu, Weihao Xuan, Ximing Lu, Mingjie Liu, Yi Dong, Zaid Harchaoui, Yejin Choi</div>
<div class="meta-line">First: 2025-07-20T07:04:08+00:00 · Latest: 2026-01-07T01:59:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.14843v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.14843v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in LLMs highlight Reinforcement Learning with Verifiable Rewards (RLVR) as a promising method for enhancing AI capabilities, particularly in solving complex logical tasks. However, it remains unclear whether the current practice of RLVR truly expands a model&#x27;s reasoning boundary or mainly amplifies high-reward outputs that the base model already knows, leading to improved precision. This study presents an empirical investigation that provides new insights into the potential limits of the common RLVR recipe. We examine how, under current training conditions, RLVR can operate as a support-constrained optimization mechanism that may restrict the discovery of entirely novel solutions, remaining constrained by the base model&#x27;s initial distribution. We also identify an entropy-reward trade-off: while the current RLVR recipe reliably enhances precision, it may progressively narrow exploration and potentially overlook correct yet underrepresented solutions. Extensive empirical experiments show that although the current RLVR recipe consistently improves pass@1, the shrinkage of empirical support generally outweighs the expansion of empirical support under larger sampling budgets, failing to recover correct answers that were previously accessible to the base model. Interestingly, we also observe that while RLVR sometimes increases token-level entropy, it leads to greater uncertainty at each generation step but declining answer-level entropy. This suggests that these seemingly more uncertain generation paths ultimately converge onto a smaller set of distinct answers. Taken together, our findings reveal potential limits of the current RLVR recipe in extending reasoning horizons. Breaking this invisible leash may require future algorithmic innovations, such as explicit exploration mechanisms or hybrid strategies that allocate probability mass to underrepresented solution regions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无形之链：为何RLVR可能无法突破其起源局限</div>
<div class="mono" style="margin-top:8px">近期大语言模型的发展突显了可验证奖励强化学习（RLVR）作为提升AI能力、特别是解决复杂逻辑任务的有效方法。然而，当前RLVR实践究竟是真正拓展了模型的推理边界，还是主要放大了基础模型已知的高奖励输出从而提升精度，仍不明确。本研究通过实证分析，对常见RLVR方法的潜在局限提供了新见解。我们揭示了在当前训练条件下，RLVR可能作为一种支持受限的优化机制运行，限制全新解决方案的发现，始终受限于基础模型的初始分布。同时，我们发现了熵-奖励权衡：虽然当前RLVR方法能可靠提升精度，但可能逐步缩小探索空间，忽略正确但未被充分代表的解决方案。大量实验表明，尽管当前RLVR方法持续提升pass@1指标，但在更大采样预算下，经验支持集的收缩通常超过其扩展，未能恢复基础模型原本可获得的正确答案。有趣的是，我们观察到RLVR有时虽会增加词元级熵值，导致每个生成步骤的不确定性上升，但答案级熵值却在下降。这表明这些看似更不确定的生成路径最终会收敛到更小的答案集合中。综合来看，我们的研究揭示了当前RLVR方法在拓展推理视野方面的潜在局限。要突破这条无形之链，可能需要未来的算法创新，例如显式探索机制或将概率质量分配给未被充分代表解区域的混合策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates whether Reinforcement Learning with Verifiable Rewards (RLVR) genuinely expands the reasoning capabilities of large language models or merely refines existing knowledge, motivated by concerns that current RLVR practices may not foster novel solutions. The method involves an empirical analysis of RLVR as a support-constrained optimization mechanism, examining its tendency to narrow exploration due to an entropy-reward trade-off. Experimental results show that while RLVR consistently improves precision metrics like pass@1, it often shrinks the empirical support of outputs, failing to recover correct answers previously accessible to the base model and leading to declining answer-level entropy despite increased token-level uncertainty, thereby revealing limits in extending reasoning boundaries.</div>
<div class="mono" style="margin-top:8px">本研究探讨了基于可验证奖励的强化学习（RLVR）是真正扩展了大语言模型的推理能力，还是仅优化了已有知识，其动机在于当前RLVR实践可能无法催生新颖解决方案的担忧。方法上，通过实证分析将RLVR视为一种支持受限的优化机制，考察其因熵-奖励权衡而导致的探索收窄倾向。实验结果表明，尽管RLVR持续提升了如pass@1等精度指标，但它往往缩小了输出的经验支持范围，无法恢复基础模型先前可获取的正确答案，并在词元级熵增的同时导致答案级熵减，从而揭示了其在拓展推理边界方面的潜在局限。</div>
</details>
</div>
<div class="card">
<div class="title">Multiplayer Nash Preference Optimization</div>
<div class="meta-line">Authors: Fang Wu, Xu Huang, Weihao Xuan, Zhiwei Zhang, Yijia Xiao, Guancheng Wan, Xiaomin Li, Bing Hu, Peng Xia, Jure Leskovec, Yejin Choi</div>
<div class="meta-line">First: 2025-09-27T04:18:33+00:00 · Latest: 2026-01-07T01:54:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23102v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.23102v2">PDF</a> · <a href="https://github.com/smiles724/MNPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning from human feedback (RLHF) has emerged as the standard paradigm for aligning large language models with human preferences. However, reward-based methods built on the Bradley-Terry assumption struggle to capture the non-transitive and heterogeneous nature of real-world preferences. To address this, recent studies have reframed alignment as a two-player Nash game, giving rise to Nash learning from human feedback (NLHF). While this perspective has inspired algorithms such as INPO, ONPO, and EGPO with strong theoretical and empirical guarantees, they remain fundamentally restricted to two-player interactions, creating a single-opponent bias that fails to capture the full complexity of realistic preference structures. This work introduces Multiplayer Nash Preference Optimization (MNPO), a novel framework that generalizes NLHF to the multiplayer regime. It formulates alignment as an n-player game, where each policy competes against a population of opponents while being regularized toward a reference model. We demonstrate that MNPO inherits the equilibrium guarantees of two-player methods while enabling richer competitive dynamics and improved coverage of diverse preference structures. Comprehensive empirical evaluation shows that MNPO consistently outperforms existing NLHF baselines on instruction-following benchmarks, achieving superior alignment quality under heterogeneous annotator conditions and mixed-policy evaluation scenarios. Together, these results establish MNPO as a principled and scalable framework for aligning LLMs with complex, non-transitive human preferences. Code is available at https://github.com/smiles724/MNPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多人纳什偏好优化</div>
<div class="mono" style="margin-top:8px">基于人类反馈的强化学习已成为将大语言模型与人类偏好对齐的标准范式。然而，基于布拉德利-特里假设的奖励方法难以捕捉现实偏好的非传递性与异质性。为此，近期研究将对齐问题重构为双人纳什博弈，催生了基于人类反馈的纳什学习。尽管该视角已衍生出INPO、ONPO、EGPO等具备理论与实证保证的算法，其本质上仍受限于双人交互框架，存在单一对手偏差，无法捕捉现实偏好结构的完整复杂性。本研究提出多人纳什偏好优化框架，将NLHF推广至多人博弈体系，将对齐问题建模为n人博弈——每个策略在与对手群体竞争的同时，受参考模型的正则化约束。我们证明MNPO在继承双人方法均衡保证的基础上，能实现更丰富的竞争动态与更全面的偏好结构覆盖。系统实证评估表明，在指令遵循基准测试中，MNPO持续优于现有NLHF基线方法，在异质标注者条件与混合策略评估场景下均取得更优的对齐质量。这些成果共同确立了MNPO作为应对复杂非传递人类偏好的原则性可扩展对齐框架。代码已开源：https://github.com/smiles724/MNPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of two-player Nash learning from human feedback (NLHF) methods, which fail to capture the full complexity of real-world non-transitive and heterogeneous preferences due to a single-opponent bias, this paper introduces Multiplayer Nash Preference Optimization (MNPO) to generalize alignment to an n-player game. The method formulates alignment as a multiplayer competition where each policy competes against a population of opponents while being regularized toward a reference model, inheriting equilibrium guarantees from two-player approaches. Experimental results demonstrate that MNPO consistently outperforms existing NLHF baselines on instruction-following benchmarks, achieving superior alignment quality under heterogeneous annotator conditions and mixed-policy evaluation scenarios, establishing it as a principled framework for complex preference alignment.</div>
<div class="mono" style="margin-top:8px">针对现有基于两人纳什博弈的人类反馈学习（NLHF）方法因单一对手偏差而难以捕捉现实世界中非传递性和异质性偏好复杂性的问题，本文提出了多人纳什偏好优化（MNPO），将对齐问题推广至多人博弈框架。该方法将对齐建模为多人竞争，每个策略在与对手群体竞争的同时向参考模型正则化，继承了两人方法的均衡保证。实验结果表明，在指令遵循基准测试中，MNPO持续优于现有NLHF基线，在异质标注者条件和混合策略评估场景下实现了更优的对齐质量，从而确立为一个处理复杂偏好对齐的原则性框架。</div>
</details>
</div>
<div class="card">
<div class="title">Sensor to Pixels: Decentralized Swarm Gathering via Image-Based Reinforcement Learning</div>
<div class="meta-line">Authors: Yigal Koifman, Eran Iceland, Erez Koifman, Ariel Barel, Alfred M. Bruckstein</div>
<div class="meta-line">First: 2026-01-06T20:58:11+00:00 · Latest: 2026-01-06T20:58:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03413v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03413v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study highlights the potential of image-based reinforcement learning methods for addressing swarm-related tasks. In multi-agent reinforcement learning, effective policy learning depends on how agents sense, interpret, and process inputs. Traditional approaches often rely on handcrafted feature extraction or raw vector-based representations, which limit the scalability and efficiency of learned policies concerning input order and size. In this work we propose an image-based reinforcement learning method for decentralized control of a multi-agent system, where observations are encoded as structured visual inputs that can be processed by Neural Networks, extracting its spatial features and producing novel decentralized motion control rules. We evaluate our approach on a multi-agent convergence task of agents with limited-range and bearing-only sensing that aim to keep the swarm cohesive during the aggregation. The algorithm&#x27;s performance is evaluated against two benchmarks: an analytical solution proposed by Bellaiche and Bruckstein, which ensures convergence but progresses slowly, and VariAntNet, a neural network-based framework that converges much faster but shows medium success rates in hard constellations. Our method achieves high convergence, with a pace nearly matching that of VariAntNet. In some scenarios, it serves as the only practical alternative.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从传感器到像素：基于图像强化学习的去中心化集群聚集</div>
<div class="mono" style="margin-top:8px">本研究揭示了基于图像的强化学习方法在解决集群任务方面的潜力。在多智能体强化学习中，有效的策略学习取决于智能体如何感知、解释和处理输入。传统方法通常依赖手工特征提取或基于原始向量的表示，这限制了所学策略在输入顺序和规模方面的可扩展性与效率。本文提出一种基于图像的强化学习方法，用于多智能体系统的去中心化控制，其中观测被编码为结构化视觉输入，可由神经网络处理以提取空间特征，并生成新颖的去中心化运动控制规则。我们在多智能体聚集任务上评估该方法，该任务中的智能体具备有限距离和纯方位感知能力，旨在聚集过程中保持集群凝聚力。算法性能通过两个基准进行评估：一是Bellaiche和Bruckstein提出的确保收敛但进展缓慢的解析解，二是基于神经网络的VariAntNet框架——其收敛速度更快但在复杂构型中成功率中等。我们的方法实现了高收敛性，其速度几乎与VariAntNet相当，在某些场景下成为唯一可行的替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of traditional handcrafted or vector-based input representations in multi-agent reinforcement learning, which hinder scalability and efficiency, this paper proposes an image-based reinforcement learning method for decentralized swarm control. The method encodes agent observations as structured visual inputs, allowing neural networks to extract spatial features and generate novel decentralized motion policies. Experimental evaluation on a multi-agent convergence task with limited-range, bearing-only sensing shows the approach achieves high convergence rates at a pace nearly matching the fast VariAntNet benchmark, while outperforming a slower analytical solution, and serves as the only practical alternative in some challenging scenarios.</div>
<div class="mono" style="margin-top:8px">针对多智能体强化学习中传统手工特征或向量化输入表示可扩展性和效率不足的问题，本研究提出一种基于图像的强化学习方法用于去中心化集群控制。该方法将智能体观测编码为结构化视觉输入，利用神经网络提取空间特征并生成新的去中心化运动策略。在有限距离、仅测向感知的多智能体聚集任务上的实验表明，该方法实现了高收敛率，其速度接近快速的VariAntNet基准，同时优于较慢的解析解，并在某些困难场景中成为唯一可行的替代方案。</div>
</details>
</div>
<div class="card">
<div class="title">Exploration Through Introspection: A Self-Aware Reward Model</div>
<div class="meta-line">Authors: Michael Petrowski, Milica Gašić</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2026-01-06T19:53:33+00:00 · Latest: 2026-01-06T19:53:33+00:00</div>
<div class="meta-line">Comments: Accepted at AAAI-26 ToM4AI Workshop</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03389v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03389v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding how artificial agents model internal mental states is central to advancing Theory of Mind in AI. Evidence points to a unified system for self- and other-awareness. We explore this self-awareness by having reinforcement learning agents infer their own internal states in gridworld environments. Specifically, we introduce an introspective exploration component that is inspired by biological pain as a learning signal by utilizing a hidden Markov model to infer &quot;pain-belief&quot; from online observations. This signal is integrated into a subjective reward function to study how self-awareness affects the agent&#x27;s learning abilities. Further, we use this computational framework to investigate the difference in performance between normal and chronic pain perception models. Results show that introspective agents in general significantly outperform standard baseline agents and can replicate complex human-like behaviors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>内省式探索：一种自我感知的奖励模型</div>
<div class="mono" style="margin-top:8px">理解人工智能体如何建模内部心理状态是推进AI心智理论发展的核心。证据表明自我认知与他人认知存在统一系统。我们通过让强化学习智能体在网格世界中推断自身内部状态来探索这种自我感知能力。具体而言，我们引入受生物疼痛启发的内省式探索组件，利用隐马尔可夫模型从在线观测中推断“疼痛信念”，并将该信号整合到主观奖励函数中，以研究自我感知如何影响智能体的学习能力。进一步，我们运用该计算框架探究正常与慢性疼痛感知模型的性能差异。结果表明，内省式智能体总体显著优于标准基线智能体，并能复现类人的复杂行为。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to understand how artificial agents model internal mental states to advance Theory of Mind in AI, this paper introduces an introspective exploration method for reinforcement learning agents in gridworld environments. The method utilizes a hidden Markov model to infer a &quot;pain-belief&quot; state from online observations, inspired by biological pain as a learning signal, and integrates this into a subjective reward function to study self-awareness effects. Experimental results demonstrate that these introspective agents significantly outperform standard baselines and can replicate complex human-like behaviors, with the framework also used to investigate differences between normal and chronic pain perception models.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过理解人工智能代理如何建模内部心理状态来推进AI中的心智理论，为此在网格世界环境中为强化学习代理引入了一种内省探索方法。该方法受生物疼痛作为学习信号的启发，利用隐马尔可夫模型从在线观察中推断“疼痛信念”，并将其整合到主观奖励函数中以研究自我意识的影响。实验结果表明，这些内省代理在性能上显著优于标准基线代理，并能复现复杂类人行为，该计算框架还用于探究正常与慢性疼痛感知模型之间的差异。</div>
</details>
</div>
<div class="card">
<div class="title">Critic-Guided Reinforcement Unlearning in Text-to-Image Diffusion</div>
<div class="meta-line">Authors: Mykola Vysotskyi, Zahar Kohut, Mariia Shpir, Taras Rumezhak, Volodymyr Karpiv</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-06T17:52:02+00:00 · Latest: 2026-01-06T17:52:02+00:00</div>
<div class="meta-line">Comments: Preprint. Under review at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03213v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03213v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine unlearning in text-to-image diffusion models aims to remove targeted concepts while preserving overall utility. Prior diffusion unlearning methods typically rely on supervised weight edits or global penalties; reinforcement-learning (RL) approaches, while flexible, often optimize sparse end-of-trajectory rewards, yielding high-variance updates and weak credit assignment. We present a general RL framework for diffusion unlearning that treats denoising as a sequential decision process and introduces a timestep-aware critic with noisy-step rewards. Concretely, we train a CLIP-based reward predictor on noisy latents and use its per-step signal to compute advantage estimates for policy-gradient updates of the reverse diffusion kernel. Our algorithm is simple to implement, supports off-policy reuse, and plugs into standard text-to-image backbones. Across multiple concepts, the method achieves better or comparable forgetting to strong baselines while maintaining image quality and benign prompt fidelity; ablations show that (i) per-step critics and (ii) noisy-conditioned rewards are key to stability and effectiveness. We release code and evaluation scripts to facilitate reproducibility and future research on RL-based diffusion unlearning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>文本到图像扩散中的批评家引导强化学习遗忘</div>
<div class="mono" style="margin-top:8px">文本到图像扩散模型的机器遗忘旨在移除特定概念，同时保持整体效用。现有扩散遗忘方法通常依赖监督式权重编辑或全局惩罚；强化学习方法虽灵活，但常优化稀疏的轨迹末端奖励，导致高方差更新和弱信用分配。本文提出一种通用的强化学习框架，将去噪视为序列决策过程，并引入具有噪声步奖励的时间步感知批评家。具体而言，我们在噪声潜在空间上训练基于CLIP的奖励预测器，利用其每步信号计算优势估计，以更新反向扩散核的策略梯度。该算法实现简单，支持离策略重用，并可集成至标准文本到图像主干模型。在多种概念上的实验表明，本方法在保持图像质量和良性提示保真度的同时，遗忘效果优于或媲美强基线；消融研究证实：（i）每步批评家与（ii）噪声条件奖励是稳定性和有效性的关键。我们公开了代码与评估脚本，以促进基于强化学习的扩散遗忘研究的可复现性与未来探索。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of machine unlearning in text-to-image diffusion models, aiming to remove specific concepts without degrading overall model utility. The authors propose a reinforcement learning framework that treats the denoising process as a sequential decision problem, introducing a timestep-aware critic with noisy-step rewards to overcome the high-variance updates of prior RL methods. Experimental results demonstrate that their method achieves effective forgetting of targeted concepts, comparable to or better than strong baselines, while preserving image quality and fidelity to benign prompts, with ablations confirming the importance of per-step critics and noisy-conditioned rewards.</div>
<div class="mono" style="margin-top:8px">本文针对文本到图像扩散模型中的机器遗忘问题，旨在移除特定概念而不损害模型整体性能。作者提出一个强化学习框架，将去噪过程视为序列决策问题，引入了具有噪声步奖励的时间步感知评论家，以克服先前强化学习方法的高方差更新问题。实验结果表明，该方法能有效遗忘目标概念，性能与强基线相当或更优，同时保持了图像质量和对良性提示的保真度，消融研究证实了每步评论家和噪声条件奖励对稳定性和有效性的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">UltraLogic: Enhancing LLM Reasoning through Large-Scale Data Synthesis and Bipolar Float Reward</div>
<div class="meta-line">Authors: Yile Liu, Yixian Liu, Zongwei Li, Yufei Huang, Xinhua Feng, Zhichao Hu, Jinglu Hu, Jianfeng Yan, Fengzong Lian, Yuhong Liu</div>
<div class="meta-line">First: 2026-01-06T17:41:32+00:00 · Latest: 2026-01-06T17:41:32+00:00</div>
<div class="meta-line">Comments: 19 pages, 6 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03205v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03205v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Language Models (LLMs) have demonstrated significant potential in natural language processing , complex general-purpose reasoning requiring multi-step logic, planning, and verification remains a critical bottleneck. Although Reinforcement Learning with Verifiable Rewards (RLVR) has succeeded in specific domains , the field lacks large-scale, high-quality, and difficulty-calibrated data for general reasoning. To address this, we propose UltraLogic, a framework that decouples the logical core of a problem from its natural language expression through a Code-based Solving methodology to automate high-quality data production. The framework comprises hundreds of unique task types and an automated calibration pipeline across ten difficulty levels. Furthermore, to mitigate binary reward sparsity and the Non-negative Reward Trap, we introduce the Bipolar Float Reward (BFR) mechanism, utilizing graded penalties to effectively distinguish perfect responses from those with logical flaws. Our experiments demonstrate that task diversity is the primary driver for reasoning enhancement , and that BFR, combined with a difficulty matching strategy, significantly improves training efficiency, guiding models toward global logical optima.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UltraLogic：通过大规模数据合成与双极浮动奖励增强大语言模型推理能力</div>
<div class="mono" style="margin-top:8px">尽管大语言模型在自然语言处理领域展现出巨大潜力，但需要多步逻辑、规划与验证的复杂通用推理仍是关键瓶颈。虽然可验证奖励的强化学习在特定领域取得成功，但通用推理领域仍缺乏大规模、高质量且难度分级的数据。为此，我们提出UltraLogic框架，通过基于代码的求解方法将问题的逻辑核心与其自然语言表达解耦，实现高质量数据的自动化生成。该框架包含数百种独特任务类型及跨十个难度级别的自动校准流程。此外，为缓解二元奖励稀疏性与非负奖励陷阱，我们引入双极浮动奖励机制，利用分级惩罚有效区分完美响应与存在逻辑缺陷的响应。实验表明，任务多样性是提升推理能力的主要驱动力，而双极浮动奖励结合难度匹配策略能显著提高训练效率，引导模型趋向全局逻辑最优解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind UltraLogic is to address the bottleneck in complex general-purpose reasoning of Large Language Models (LLMs), which lack large-scale, high-quality, and difficulty-calibrated data. The method involves a framework that decouples logical problem cores from natural language using a Code-based Solving approach to automate data synthesis across hundreds of task types and ten difficulty levels, and introduces a Bipolar Float Reward (BFR) mechanism with graded penalties to overcome binary reward limitations. Experimental results show that task diversity is key to enhancing reasoning, and BFR combined with difficulty matching significantly boosts training efficiency, guiding models toward global logical optima.</div>
<div class="mono" style="margin-top:8px">UltraLogic的动机是解决大型语言模型在复杂通用推理中的瓶颈，即缺乏大规模、高质量且难度校准的数据。该方法通过基于代码求解的方法，将问题的逻辑核心与自然语言表达解耦，以自动化生成涵盖数百种任务类型和十个难度级别的数据，并引入带有分级惩罚的双极浮动奖励机制来克服二元奖励的稀疏性。实验结果表明，任务多样性是提升推理能力的主要驱动力，而双极浮动奖励结合难度匹配策略显著提高了训练效率，引导模型实现全局逻辑优化。</div>
</details>
</div>
<div class="card">
<div class="title">DisCO: Reinforcing Large Reasoning Models with Discriminative Constrained Optimization</div>
<div class="meta-line">Authors: Gang Li, Ming Lin, Tomer Galanti, Zhengzhong Tu, Tianbao Yang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-18T11:08:32+00:00 · Latest: 2026-01-06T16:40:19+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.12366v5">Abs</a> · <a href="https://arxiv.org/pdf/2505.12366v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The recent success and openness of DeepSeek-R1 have brought widespread attention to Group Relative Policy Optimization (GRPO) as a reinforcement learning method for large reasoning models (LRMs). In this work, we analyze the GRPO objective under a binary reward setting and reveal an inherent limitation of question-level difficulty bias. We also identify a connection between GRPO and traditional discriminative methods in supervised learning. Motivated by these insights, we introduce a new Discriminative Constrained Optimization (DisCO) framework for reinforcing LRMs, grounded in the principle of discriminative learning. The main differences between DisCO and GRPO and its recent variants are: (1) it replaces the group relative objective with a discriminative objective defined by a scoring function; (2) it abandons clipping-based surrogates in favor of non-clipping RL surrogate objectives used as scoring functions; (3) it employs a simple yet effective constrained optimization approach to enforce the KL divergence constraint. As a result, DisCO offers notable advantages over GRPO and its variants: (i) it completely eliminates difficulty bias by adopting discriminative objectives; (ii) it addresses the entropy instability in GRPO and its variants through the use of non-clipping scoring functions and a constrained optimization approach, yielding long and stable training dynamics; (iii) it allows the incorporation of advanced discriminative learning techniques to address data imbalance, where a significant number of questions have more negative than positive generated answers during training. Our experiments on enhancing the mathematical reasoning capabilities of SFT-finetuned models show that DisCO significantly outperforms GRPO and its improved variants such as DAPO, achieving average gains of 7\% over GRPO and 6\% over DAPO across six benchmark tasks for a 1.5B model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DisCO：基于判别式约束优化强化大型推理模型</div>
<div class="mono" style="margin-top:8px">DeepSeek-R1 近期的成功与开源使群体相对策略优化（GRPO）作为大型推理模型（LRM）的强化学习方法受到广泛关注。本研究在二元奖励设定下分析了 GRPO 目标函数，揭示了其固有的问题级难度偏差局限，并发现了 GRPO 与监督学习中传统判别式方法的关联。基于这些洞察，我们提出了基于判别学习原理的判别式约束优化（DisCO）框架，用于强化 LRM。DisCO 与 GRPO 及其近期变体的主要区别在于：（1）用评分函数定义的判别目标替代群体相对目标；（2）放弃基于裁剪的代理目标，采用非裁剪强化学习代理目标作为评分函数；（3）通过简洁有效的约束优化方法实施 KL 散度约束。因此，DisCO 相比 GRPO 及其变体具有显著优势：（i）通过判别目标完全消除难度偏差；（ii）利用非裁剪评分函数与约束优化方法解决 GRPO 及其变体的熵不稳定问题，实现长期稳定的训练动态；（iii）可结合先进判别学习技术处理数据不平衡问题（训练中大量问题生成的负例答案多于正例）。在增强 SFT 微调模型数学推理能力的实验中，DisCO 显著优于 GRPO 及其改进变体（如 DAPO），在 1.5B 参数的模型上，六项基准任务平均提升分别达 7%（对比 GRPO）和 6%（对比 DAPO）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by an analysis revealing a difficulty bias limitation in the Group Relative Policy Optimization (GRPO) method for large reasoning models, this paper introduces Discriminative Constrained Optimization (DisCO). The method replaces GRPO&#x27;s group-relative objective with a discriminative scoring function, employs non-clipping reinforcement learning surrogates, and uses a constrained optimization approach to enforce KL divergence constraints. Experimental results on enhancing mathematical reasoning show that DisCO significantly outperforms GRPO and its variant DAPO, achieving average gains of 7% and 6% respectively across six benchmark tasks for a 1.5B model, while offering more stable training.</div>
<div class="mono" style="margin-top:8px">本文的动机源于对大型推理模型强化学习方法GRPO的分析，揭示了其存在难度偏差的固有局限。为此，作者提出了判别式约束优化（DisCO）框架，该方法用判别式评分函数取代GRPO的组相对目标，采用非裁剪的强化学习替代目标，并通过约束优化来实施KL散度约束。在提升数学推理能力的实验中，DisCO显著优于GRPO及其改进变体DAPO，对于一个15亿参数的模型，在六项基准任务上平均分别取得了7%和6%的性能提升，同时训练过程更稳定。</div>
</details>
</div>
<div class="card">
<div class="title">Unified Thinker: A General Reasoning Modular Core for Image Generation</div>
<div class="meta-line">Authors: Sashuai Zhou, Qiang Zhou, Jijin Hu, Hanqing Yang, Yue Cao, Junpeng Ma, Yinchao Ma, Jun Song, Tiezheng Ge, Cheng Yu, Bo Zheng, Zhou Zhao</div>
<div class="meta-line">First: 2026-01-06T15:59:33+00:00 · Latest: 2026-01-06T15:59:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03127v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03127v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite impressive progress in high-fidelity image synthesis, generative models still struggle with logic-intensive instruction following, exposing a persistent reasoning--execution gap. Meanwhile, closed-source systems (e.g., Nano Banana) have demonstrated strong reasoning-driven image generation, highlighting a substantial gap to current open-source models. We argue that closing this gap requires not merely better visual generators, but executable reasoning: decomposing high-level intents into grounded, verifiable plans that directly steer the generative process. To this end, we propose Unified Thinker, a task-agnostic reasoning architecture for general image generation, designed as a unified planning core that can plug into diverse generators and workflows. Unified Thinker decouples a dedicated Thinker from the image Generator, enabling modular upgrades of reasoning without retraining the entire generative model. We further introduce a two-stage training paradigm: we first build a structured planning interface for the Thinker, then apply reinforcement learning to ground its policy in pixel-level feedback, encouraging plans that optimize visual correctness over textual plausibility. Extensive experiments on text-to-image generation and image editing show that Unified Thinker substantially improves image reasoning and generation quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>统一思考者：面向图像生成的通用推理模块化核心</div>
<div class="mono" style="margin-top:8px">尽管高保真图像合成已取得显著进展，生成模型在遵循逻辑密集型指令方面仍存在困难，暴露出持续的推理-执行鸿沟。与此同时，闭源系统（如Nano Banana）已展现出强大的推理驱动图像生成能力，突显了与当前开源模型间的显著差距。我们认为，弥合这一差距不仅需要更优的视觉生成器，更需可执行的推理能力：将高层意图分解为可直接指导生成过程的、可验证的具象化规划。为此，我们提出统一思考者——一种面向通用图像生成的任务无关推理架构，其设计为可接入多样化生成器与工作流的统一规划核心。该架构将专用思考者模块与图像生成器解耦，实现无需重新训练整个生成模型的推理模块化升级。我们进一步引入两阶段训练范式：首先为思考者构建结构化规划接口，随后应用强化学习使其策略基于像素级反馈进行具象化调整，从而推动规划更侧重于视觉正确性而非文本合理性。在文本到图像生成与图像编辑任务上的大量实验表明，统一思考者显著提升了图像推理与生成质量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the persistent reasoning-execution gap in generative models and the strong reasoning-driven image generation of closed-source systems, this paper proposes Unified Thinker, a task-agnostic reasoning architecture designed as a modular core to decompose high-level intents into grounded plans for steering diverse image generators. The method decouples a dedicated Thinker module from the Generator and employs a two-stage training paradigm involving a structured planning interface followed by reinforcement learning with pixel-level feedback to ground the policy in visual correctness. Experimental results on text-to-image generation and image editing demonstrate that Unified Thinker substantially improves image reasoning and generation quality.</div>
<div class="mono" style="margin-top:8px">本文的动机是生成模型在逻辑密集型指令遵循上存在持续的推理-执行差距，而闭源系统已展现出强大的推理驱动图像生成能力。为此，论文提出了Unified Thinker，一种任务无关的推理架构，作为统一规划核心，可将高层意图分解为可验证的计划以引导各种图像生成器；其方法是将专用的Thinker模块与Generator解耦，并采用两阶段训练范式：先为Thinker构建结构化规划接口，再通过强化学习结合像素级反馈来使其策略基于视觉正确性。在文本到图像生成和图像编辑上的大量实验表明，Unified Thinker显著提升了图像推理和生成质量。</div>
</details>
</div>
<div class="card">
<div class="title">Gradient Coupling: The Hidden Barrier to Generalization in Agentic Reinforcement Learning</div>
<div class="meta-line">Authors: Jingyu Liu, Xiaopeng Wu, Jingquan Peng, Kehan Chen, Chuan Yu, Lizhong Ding, Yong Liu</div>
<div class="meta-line">First: 2025-09-28T13:24:38+00:00 · Latest: 2026-01-06T15:48:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23870v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.23870v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is a dominant paradigm for training autonomous agents, yet these agents often exhibit poor generalization, failing to adapt to scenarios not seen during training. In this work, we identify a fundamental cause of this brittleness, a phenomenon which we term &quot;gradient coupling.&quot; We hypothesize that in complex agentic tasks, the high similarity between distinct states leads to destructive interference between gradients. Specifically, a gradient update that reinforces an optimal action in one state can inadvertently increase the likelihood of a suboptimal action in a similar, yet different, state. To solve this, we propose a novel objective where the actor is trained to simultaneously function as a classifier that separates good and bad actions. This auxiliary pressure compels the model to learn disentangled embeddings for positive and negative actions, which mitigates negative gradient interference and improve the generalization performance. Extensive experiments demonstrate the effectiveness of our method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>梯度耦合：智能体强化学习中泛化能力的隐性障碍</div>
<div class="mono" style="margin-top:8px">强化学习是训练自主智能体的主流范式，但这些智能体常表现出较差的泛化能力，难以适应训练时未见的场景。本研究揭示了导致这种脆弱性的根本原因——我们称之为“梯度耦合”的现象。我们假设在复杂的智能体任务中，不同状态间的高度相似性会导致梯度间的破坏性干扰。具体而言，强化某一状态最优动作的梯度更新，可能无意中增加相似但不同状态下次优动作的选择概率。为解决此问题，我们提出一种新颖的目标函数：训练执行器同时作为区分优劣动作的分类器。这种辅助压力迫使模型学习正负动作的解耦嵌入表示，从而减轻负面梯度干扰并提升泛化性能。大量实验验证了本方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the poor generalization of reinforcement learning agents, attributing it to a phenomenon called &#x27;gradient coupling,&#x27; where high similarity between states causes gradient updates for optimal actions in one state to negatively interfere with action selection in similar states. To address this, the authors propose a novel training objective that compels the actor network to also function as a classifier separating good from bad actions, thereby learning disentangled embeddings that mitigate destructive gradient interference. Experimental results demonstrate that this method effectively improves the agent&#x27;s generalization performance.</div>
<div class="mono" style="margin-top:8px">本文研究了强化学习智能体泛化能力差的问题，将其归因于一种称为&#x27;梯度耦合&#x27;的现象，即状态间的高相似性导致一个状态中优化动作的梯度更新会干扰相似但不同状态中的动作选择。为解决此问题，作者提出了一种新颖的训练目标，迫使执行器网络同时充当区分好坏动作的分类器，从而学习解耦的表征以减轻破坏性的梯度干扰。大量实验结果表明，该方法有效提升了智能体的泛化性能。</div>
</details>
</div>
<div class="card">
<div class="title">One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling</div>
<div class="meta-line">Authors: Yiyuan Li, Zhen Huang, Yanan Wu, Weixun Wang, Xuefeng Li, Yijia Luo, Wenbo Su, Bo Zheng, Pengfei Liu</div>
<div class="meta-line">First: 2026-01-06T15:41:35+00:00 · Latest: 2026-01-06T15:41:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03111v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03111v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一统万样的单样本：强化学习扩展中的极致数据效率</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）的推理能力可通过强化学习（RL）释放（OpenAI, 2024；深度求索AI等, 2025a；Zeng等, 2025）。现有LLM强化学习尝试的成功通常依赖于数千乃至更多的高质量样本。本文通过展示单样本学习的显著有效性，挑战了关于LLM强化学习中数据需求的基本假设。具体而言，我们提出博学学习框架，用于设计能引发多学科影响的单一训练样本。我们呈现三项关键发现：（1）单个经策略性选择的数学推理样本可在物理、化学、生物等多领域通过强化学习产生显著的性能提升；（2）推理中关键的数学技能揭示了最优博学样本的特征；（3）整合多学科要素的工程化合成样本，其训练效果优于使用自然产生的独立样本。我们的方法在多种推理基准测试中均优于使用更大数据集的训练，表明样本质量与设计（而非数量）可能是解锁语言模型增强推理能力的关键。研究结果预示着一场被称为“样本工程”的范式转变——从单纯增加数据量转向对训练样本的精准设计。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the high data demands of existing reinforcement learning (RL) approaches for large language models (LLMs), this paper challenges the assumption that thousands of samples are necessary by demonstrating the extreme data efficiency of one-shot learning. The method introduces &#x27;polymath learning&#x27;, a framework for designing a single, strategically engineered training sample to elicit broad, multidisciplinary improvements. Key experimental results show that one such sample, particularly in math reasoning, can significantly boost performance across physics, chemistry, and biology domains via RL; an engineered synthetic sample integrating multidisciplinary elements outperforms training with multiple natural samples; and this approach surpasses training with larger datasets on various reasoning benchmarks, indicating that sample quality and design are more critical than quantity.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型强化学习通常需要数千高质量样本的高数据需求，挑战了这一基本假设，展示了单样本学习的极高数据效率。其方法引入了&#x27;博学学习&#x27;框架，旨在设计单个经过战略工程化的训练样本来引发广泛的跨学科能力提升。主要实验结果表明：一个精心挑选的数学推理样本，通过强化学习就能在物理、化学和生物等多个领域带来显著的性能改进；一个融合多学科元素的工程化合成样本，其训练效果优于使用多个自然样本；该方法在各种推理基准测试中的表现超越了使用更大数据集训练的结果，表明样本质量与设计比单纯的数据量更为关键。</div>
</details>
</div>
<div class="card">
<div class="title">A Comedy of Estimators: On KL Regularization in RL Training of LLMs</div>
<div class="meta-line">Authors: Vedant Shah, Johan Obando-Ceron, Vineet Jain, Brian Bartoldson, Bhavya Kailkhura, Sarthak Mittal, Glen Berseth, Pablo Samuel Castro, Yoshua Bengio, Nikolay Malkin, Moksh Jain, Siddarth Venkatraman, Aaron Courville</div>
<div class="meta-line">First: 2025-12-26T04:20:58+00:00 · Latest: 2026-01-06T15:07:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21852v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.21852v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The reasoning performance of large language models (LLMs) can be substantially improved by training them with reinforcement learning (RL). The RL objective for LLM training involves a regularization term, which is the reverse Kullback-Leibler (KL) divergence between the trained policy and the reference policy. Since computing the KL divergence exactly is intractable, various estimators are used in practice to estimate it from on-policy samples. Despite its wide adoption, including in several open-source libraries, there is no systematic study analyzing the numerous ways of incorporating KL estimators in the objective and their effect on the downstream performance of RL-trained models. Recent works show that prevailing practices for incorporating KL regularization do not provide correct gradients for stated objectives, creating a discrepancy between the objective and its implementation. In this paper, we further analyze these practices and study the gradients of several estimators configurations, revealing how design choices shape gradient bias. We substantiate these findings with empirical observations by RL fine-tuning \texttt{Qwen2.5-7B}, \texttt{Llama-3.1-8B-Instruct} and \texttt{Qwen3-4B-Instruct-2507} with different configurations and evaluating their performance on both in- and out-of-distribution tasks. Through our analysis, we observe that, in on-policy settings: (1) estimator configurations with biased gradients can result in training instabilities; and (2) using estimator configurations resulting in unbiased gradients leads to better performance on in-domain as well as out-of-domain tasks. We also investigate the performance resulting from different KL configurations in off-policy settings and observe that KL regularization can help stabilize off-policy RL training resulting from asynchronous setups.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>估计量的喜剧：论大语言模型强化学习训练中的KL正则化</div>
<div class="mono" style="margin-top:8px">通过强化学习训练可大幅提升大语言模型的推理性能。其训练目标包含一个正则化项，即训练策略与参考策略之间的反向KL散度。由于精确计算KL散度不可行，实践中常使用各种估计器基于在线策略样本进行估计。尽管该方法已被广泛采用（包括多个开源库），但尚未有系统研究分析KL估计器在目标函数中的多种整合方式及其对RL训练模型下游性能的影响。近期研究表明，当前主流的KL正则化实践并未为既定目标提供正确的梯度，导致目标与实现之间存在偏差。本文进一步分析这些实践，研究多种估计器配置的梯度特性，揭示设计选择如何影响梯度偏差。我们通过RL微调\texttt{Qwen2.5-7B}、\texttt{Llama-3.1-8B-Instruct}和\texttt{Qwen3-4B-Instruct-2507}模型的不同配置，并在分布内与分布外任务上评估其性能，以实证结果支撑上述发现。分析表明，在在线策略设置中：（1）具有偏差梯度的估计器配置可能导致训练不稳定；（2）使用产生无偏梯度的估计器配置能在领域内及跨领域任务上获得更优性能。同时，我们还研究了离线策略设置中不同KL配置的性能表现，发现KL正则化有助于稳定异步设置导致的离线策略RL训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the practical implementation of KL divergence regularization in reinforcement learning (RL) fine-tuning of large language models (LLMs), motivated by the widespread but unanalyzed use of various estimators for this intractable term in RL objectives. The method involves a systematic analysis of different configurations for incorporating KL estimators, examining their gradient properties to reveal how design choices introduce bias. Experimental results from RL fine-tuning of models like Qwen2.5-7B and Llama-3.1-8B show that configurations with biased gradients cause training instability, whereas those with unbiased gradients lead to better performance on both in-distribution and out-of-distribution tasks, and KL regularization also helps stabilize off-policy RL training.</div>
<div class="mono" style="margin-top:8px">本文研究了大型语言模型（LLM）强化学习微调中KL散度正则化的实际实现，其动机在于RL目标中这一难以精确计算项的各种估计器被广泛使用但缺乏系统分析。方法上，系统分析了整合KL估计器的不同配置，通过检验其梯度特性来揭示设计选择如何引入偏差。在Qwen2.5-7B和Llama-3.1-8B等模型的RL微调实验结果表明，具有偏差梯度的配置会导致训练不稳定，而使用无偏梯度配置的模型在分布内和分布外任务上表现更优，同时KL正则化也有助于稳定异步设置下的离策略RL训练。</div>
</details>
</div>
<div class="card">
<div class="title">EvoGPT: Leveraging LLM-Driven Seed Diversity to Improve Search-Based Test Suite Generation</div>
<div class="meta-line">Authors: Lior Broide, Roni Stern, Argaman Mordoch</div>
<div class="meta-line">First: 2025-05-18T13:48:53+00:00 · Latest: 2026-01-06T14:41:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.12424v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.12424v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Search-Based Software Testing (SBST) is a well-established approach for automated unit test generation, yet it often suffers from premature convergence and limited diversity in the generated test suites. Recently, Large Language Models (LLMs) have emerged as an alternative technique for unit test generation. We present EvoGPT, a hybrid test generation system that integrates LLM-based test generation with SBST-based test suite optimization. EvoGPT uses LLMs to generate an initial population of test suites, and uses an Evolutionary Algorithm (EA) to further optimize this test suite population. A distinguishing feature of EvoGPT is its explicit enforcement of diversity, achieved through the use of multiple temperatures and prompt instructions during test generation. In addition, each LLM-generated test is refined using a generation-repair loop and coverage-guided assertion generation. To address evolutionary plateaus, EvoGPT also detects stagnation during search and injects additional LLM-generated tests aimed at previously uncovered branches. Here too diversity is enforced using multiple temperatures and prompt instructions. We evaluate EvoGPT on Defects4J, a standard benchmark for test generation. The results show that EvoGPT achieves, on average, a 10\% improvement in both code coverage and mutation score metrics compared to TestART, an LLM-only baseline; and EvoSuite, a standard SBST baseline. An ablation study indicates that explicitly enforcing diversity both at initialization and during the search is key to effectively leveraging LLMs for automated unit test generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EvoGPT：利用大语言模型驱动的种子多样性改进基于搜索的测试套件生成</div>
<div class="mono" style="margin-top:8px">基于搜索的软件测试（SBST）是一种成熟的自动化单元测试生成方法，但常面临早熟收敛和生成测试套件多样性不足的问题。近期，大语言模型（LLM）已成为单元测试生成的替代技术。本文提出EvoGPT，一种将基于LLM的测试生成与基于SBST的测试套件优化相结合的混合测试生成系统。EvoGPT使用LLM生成初始测试套件种群，并采用进化算法（EA）进一步优化该种群。其显著特点是通过在测试生成阶段使用多温度参数和多样化提示指令，显式增强多样性。此外，每个LLM生成的测试均通过生成-修复循环和覆盖率引导的断言生成进行精炼。为应对进化停滞，EvoGPT在搜索过程中检测停滞状态，并注入针对未覆盖分支的额外LLM生成测试，此阶段同样通过多温度参数和提示指令强化多样性。在标准测试生成基准Defects4J上的评估表明：相较于纯LLM基线TestART和标准SBST基线EvoSuite，EvoGPT在代码覆盖率和变异分数指标上平均提升10%。消融实验证实，在初始化和搜索阶段显式增强多样性是有效利用LLM进行自动化单元测试生成的关键。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for EvoGPT stems from addressing the limitations of Search-Based Software Testing (SBST), such as premature convergence and low diversity, by integrating Large Language Models (LLMs) to enhance automated unit test generation. The method involves a hybrid system that uses LLMs with varied temperatures and prompts to generate a diverse initial population of test suites, which are then optimized via an Evolutionary Algorithm (EA), incorporating a generation-repair loop, coverage-guided assertions, and stagnation detection with additional LLM injections for uncovered branches. Experimental results on the Defects4J benchmark demonstrate that EvoGPT achieves approximately 10% higher code coverage and mutation scores compared to LLM-only and SBST baselines, with ablation studies confirming the importance of enforced diversity throughout the process.</div>
<div class="mono" style="margin-top:8px">EvoGPT的动机源于解决基于搜索的软件测试（SBST）在自动单元测试生成中存在的早熟收敛和多样性不足问题，通过集成大语言模型（LLMs）来提升生成效果。该方法采用混合系统，利用LLMs结合多种温度和提示指令生成多样化的初始测试套件种群，随后通过进化算法进行优化，包括生成-修复循环、覆盖率引导的断言生成，以及在搜索停滞时检测并注入针对未覆盖分支的LLM生成测试。在Defects4J基准上的实验结果表明，与纯LLM和SBST基线相比，EvoGPT平均提高了约10%的代码覆盖率和变异分数，消融研究进一步验证了在初始化和搜索过程中强制保持多样性的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">IBISAgent: Reinforcing Pixel-Level Visual Reasoning in MLLMs for Universal Biomedical Object Referring and Segmentation</div>
<div class="meta-line">Authors: Yankai Jiang, Qiaoru Li, Binlu Xu, Haoran Sun, Chao Ding, Junting Dong, Yuxiang Cai, Xuhong Zhang, Jianwei Yin</div>
<div class="meta-line">First: 2026-01-06T14:37:50+00:00 · Latest: 2026-01-06T14:37:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03054v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03054v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent research on medical MLLMs has gradually shifted its focus from image-level understanding to fine-grained, pixel-level comprehension. Although segmentation serves as the foundation for pixel-level understanding, existing approaches face two major challenges. First, they introduce implicit segmentation tokens and require simultaneous fine-tuning of both the MLLM and external pixel decoders, which increases the risk of catastrophic forgetting and limits generalization to out-of-domain scenarios. Second, most methods rely on single-pass reasoning and lack the capability to iteratively refine segmentation results, leading to suboptimal performance. To overcome these limitations, we propose a novel agentic MLLM, named IBISAgent, that reformulates segmentation as a vision-centric, multi-step decision-making process. IBISAgent enables MLLMs to generate interleaved reasoning and text-based click actions, invoke segmentation tools, and produce high-quality masks without architectural modifications. By iteratively performing multi-step visual reasoning on masked image features, IBISAgent naturally supports mask refinement and promotes the development of pixel-level visual reasoning capabilities. We further design a two-stage training framework consisting of cold-start supervised fine-tuning and agentic reinforcement learning with tailored, fine-grained rewards, enhancing the model&#x27;s robustness in complex medical referring and reasoning segmentation tasks. Extensive experiments demonstrate that IBISAgent consistently outperforms both closed-source and open-source SOTA methods. All datasets, code, and trained models will be released publicly.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IBISAgent：强化多模态大语言模型的像素级视觉推理能力，实现通用生物医学对象指代与分割</div>
<div class="mono" style="margin-top:8px">近期医学多模态大语言模型的研究重点正逐渐从图像级理解转向细粒度的像素级理解。尽管分割是像素级理解的基础，现有方法面临两大挑战：其一，它们引入隐式分割标记，并需同时微调多模态大语言模型与外部像素解码器，这会增加灾难性遗忘风险，并限制模型在域外场景的泛化能力；其二，多数方法依赖单次推理，缺乏迭代优化分割结果的能力，导致性能欠佳。为突破这些局限，我们提出一种新型智能体化多模态大语言模型IBISAgent，将分割重构为以视觉为中心的多步决策过程。该模型无需修改架构即可实现交错式推理与基于文本的点击操作、调用分割工具并生成高质量掩码。通过对掩码图像特征进行迭代式多步视觉推理，IBISAgent天然支持掩码优化，并促进像素级视觉推理能力的发展。我们进一步设计包含冷启动监督微调与细粒度奖励强化学习的双阶段训练框架，提升模型在复杂医学指代与推理分割任务中的鲁棒性。大量实验表明，IBISAgent在闭源与开源前沿方法中均取得持续优势。所有数据集、代码及训练模型将公开释放。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the shift towards fine-grained pixel-level understanding in medical MLLMs and the limitations of existing segmentation methods—such as catastrophic forgetting from joint fine-tuning and lack of iterative refinement—this paper introduces IBISAgent, an agentic MLLM that reformulates segmentation as a multi-step decision-making process. The method enables MLLMs to perform interleaved reasoning and text-based click actions to invoke segmentation tools without architectural changes, using a two-stage training framework with supervised fine-tuning and agentic reinforcement learning for robustness. Experimental results show that IBISAgent consistently outperforms state-of-the-art methods in universal biomedical object referring and segmentation tasks.</div>
<div class="mono" style="margin-top:8px">本文的动机源于医学多模态大模型研究正从图像级理解转向细粒度像素级理解，而现有分割方法存在两大挑战：联合微调易导致灾难性遗忘且泛化性差，以及缺乏迭代优化能力。为此，作者提出了IBISAgent，一种智能体化的多模态大模型，将分割重构为以视觉为中心的多步决策过程，使模型能通过交错推理和基于文本的点击动作调用分割工具，无需修改架构，并采用包含监督微调和智能体强化学习的两阶段训练框架以增强鲁棒性。主要实验结果表明，IBISAgent在通用生物医学对象指代和分割任务中持续优于当前最先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">Aligning Findings with Diagnosis: A Self-Consistent Reinforcement Learning Framework for Trustworthy Radiology Reporting</div>
<div class="meta-line">Authors: Kun Zhao, Siyuan Dai, Pan Wang, Jifeng Song, Hui Ji, Chenghua Lin, Liang Zhan, Haoteng Tang</div>
<div class="meta-line">First: 2026-01-06T14:17:44+00:00 · Latest: 2026-01-06T14:17:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03321v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03321v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) have shown strong potential for radiology report generation, yet their clinical translation is hindered by architectural heterogeneity and the prevalence of factual hallucinations. Standard supervised fine-tuning often fails to strictly align linguistic outputs with visual evidence, while existing reinforcement learning approaches struggle with either prohibitive computational costs or limited exploration. To address these challenges, we propose a comprehensive framework for self-consistent radiology report generation. First, we conduct a systematic evaluation to identify optimal vision encoder and LLM backbone configurations for medical imaging. Building on this foundation, we introduce a novel &quot;Reason-then-Summarize&quot; architecture optimized via Group Relative Policy Optimization (GRPO). This framework restructures generation into two distinct components: a think block for detailed findings and an answer block for structured disease labels. By utilizing a multi-dimensional composite reward function, we explicitly penalize logical discrepancies between the generated narrative and the final diagnosis. Extensive experiments on the MIMIC-CXR benchmark demonstrate that our method achieves state-of-the-art performance in clinical efficacy metrics and significantly reduces hallucinations compared to strong supervised baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>发现与诊断对齐：基于自洽强化学习的可信放射学报告生成框架</div>
<div class="mono" style="margin-top:8px">多模态大语言模型在放射学报告生成中展现出巨大潜力，但其临床转化受限于架构异构性与事实性幻觉的普遍存在。标准监督微调常难以严格对齐语言输出与视觉证据，而现有强化学习方法则面临计算成本过高或探索能力有限的问题。为此，我们提出一个自洽的放射学报告生成综合框架。首先，通过系统评估确定医学影像处理中最优的视觉编码器与大语言模型骨干配置。在此基础上，引入基于组相对策略优化的新型“先推理后总结”架构，将生成过程重构为两个独立模块：用于详细发现的思维模块与用于结构化疾病标签的应答模块。通过采用多维复合奖励函数，显式惩罚生成叙述与最终诊断间的逻辑不一致性。在MIMIC-CXR基准上的大量实验表明，本方法在临床效能指标上达到最优性能，相较于强监督基线模型显著减少了幻觉现象。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of aligning radiology report generation with clinical evidence to reduce factual hallucinations, which hinder the clinical adoption of Multimodal Large Language Models (MLLMs). The method involves first identifying optimal vision and language model backbones, then introducing a &quot;Reason-then-Summarize&quot; architecture optimized via Group Relative Policy Optimization (GRPO), which separates detailed findings generation from structured diagnosis labeling and uses a composite reward to penalize logical inconsistencies. Experimental results on the MIMIC-CXR benchmark show state-of-the-art performance in clinical metrics and a significant reduction in hallucinations compared to supervised baselines.</div>
<div class="mono" style="margin-top:8px">本文针对放射学报告生成中事实性幻觉阻碍多模态大语言模型临床应用的挑战，提出了一种自洽的强化学习框架。方法上，首先系统评估并选择最优的视觉编码器和语言模型骨干，然后引入基于组相对策略优化的“推理后总结”架构，将生成过程分解为详细发现和结构化疾病标签两部分，并利用多维复合奖励函数显式惩罚叙述与诊断间的逻辑不一致。在MIMIC-CXR基准上的实验表明，该方法在临床效能指标上达到领先水平，并相比强监督基线显著减少了幻觉现象。</div>
</details>
</div>
<div class="card">
<div class="title">Ratio-Variance Regularized Policy Optimization for Efficient LLM Fine-tuning</div>
<div class="meta-line">Authors: Yu Luo, Shuo Han, Yihan Hu, Dong Li, Jianye Hao</div>
<div class="meta-line">First: 2026-01-06T14:01:42+00:00 · Latest: 2026-01-06T14:01:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03320v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03320v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">On-policy reinforcement learning (RL), particularly Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO), has become the dominant paradigm for fine-tuning large language models (LLMs). While policy ratio clipping stabilizes training, this heuristic hard constraint incurs a fundamental cost: it indiscriminately truncates gradients from high-return yet high-divergence actions, suppressing rare but highly informative &quot;eureka moments&quot; in complex reasoning. Moreover, once data becomes slightly stale, hard clipping renders it unusable, leading to severe sample inefficiency. In this work, we revisit the trust-region objective in policy optimization and show that explicitly constraining the \emph{variance (second central moment) of the policy ratio} provides a principled and smooth relaxation of hard clipping. This distributional constraint stabilizes policy updates while preserving gradient signals from valuable trajectories. Building on this insight, we propose $R^2VPO$ (Ratio-Variance Regularized Policy Optimization), a novel primal-dual framework that supports stable on-policy learning and enables principled off-policy data reuse by dynamically reweighting stale samples rather than discarding them. We extensively evaluate $R^2VPO$ on fine-tuning state-of-the-art LLMs, including DeepSeek-Distill-Qwen-1.5B and the openPangu-Embedded series (1B and 7B), across challenging mathematical reasoning benchmarks. Experimental results show that $R^2VPO$ consistently achieves superior asymptotic performance, with average relative gains of up to 17% over strong clipping-based baselines, while requiring approximately 50% fewer rollouts to reach convergence. These findings establish ratio-variance control as a promising direction for improving both stability and data efficiency in RL-based LLM alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>比率-方差正则化策略优化：面向高效大语言模型微调</div>
<div class="mono" style="margin-top:8px">在线策略强化学习（RL），特别是近端策略优化（PPO）和组相对策略优化（GRPO），已成为微调大语言模型（LLM）的主流范式。虽然策略比率裁剪能稳定训练，但这种启发式硬约束存在根本性代价：它无差别地截断来自高回报但高差异度动作的梯度，抑制了复杂推理中罕见但信息量极大的“顿悟时刻”。此外，一旦数据略微过时，硬裁剪会使其无法使用，导致严重的样本效率低下。本研究重新审视策略优化中的信赖域目标，证明显式约束策略比率的方差（二阶中心矩）可为硬裁剪提供一种原则性且平滑的松弛方法。这种分布约束在稳定策略更新的同时，保留了来自高价值轨迹的梯度信号。基于此洞见，我们提出R²VPO（比率-方差正则化策略优化）——一种新颖的原对偶框架，支持稳定的在线策略学习，并通过动态重新加权而非丢弃过时样本，实现原则性的离线策略数据复用。我们在微调前沿LLM（包括DeepSeek-Distill-Qwen-1.5B及openPangu-Embedded系列（1B与7B））上，通过多个高难度数学推理基准对R²VPO进行广泛评估。实验结果表明，R²VPO始终获得更优的渐进性能，相较于基于裁剪的强基线平均相对提升达17%，且达到收敛所需采样轨迹数减少约50%。这些发现确立了比率-方差控制作为提升基于RL的LLM对齐任务稳定性与数据效率的可行方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of hard policy ratio clipping in on-policy reinforcement learning for LLM fine-tuning, which suppresses informative gradient signals from high-divergence actions and causes sample inefficiency by discarding stale data. The authors propose Ratio-Variance Regularized Policy Optimization (R²VPO), a primal-dual framework that replaces hard clipping with a soft constraint on the variance of the policy ratio, stabilizing updates while preserving valuable gradients and enabling principled off-policy data reuse through dynamic reweighting. Experiments on fine-tuning models like DeepSeek-Distill-Qwen-1.5B and openPangu-Embedded series across mathematical reasoning benchmarks show that R²VPO achieves up to 17% higher average performance than clipping-based baselines and requires about 50% fewer rollouts to converge, demonstrating improved stability and data efficiency.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型微调中基于策略的强化学习所采用的硬性策略比率裁剪的局限性，该方法会抑制来自高发散性动作的信息梯度信号，并因丢弃陈旧数据导致样本效率低下。作者提出了比率-方差正则化策略优化（R²VPO），这是一个原始-对偶框架，用对策略比率方差的软约束替代硬裁剪，在稳定更新的同时保留有价值的梯度，并通过动态重加权实现原则性的离策略数据重用。在DeepSeek-Distill-Qwen-1.5B和openPangu-Embedded系列模型上的数学推理基准实验表明，R²VPO相比基于裁剪的基线方法平均性能提升高达17%，且收敛所需采样轮次减少约50%，证明了其在稳定性和数据效率方面的改进。</div>
</details>
</div>
<div class="card">
<div class="title">Dementia-R1: Reinforced Pretraining and Reasoning from Unstructured Clinical Notes for Real-World Dementia Prognosis</div>
<div class="meta-line">Authors: Choonghan Kim, Hyunmin Hwang, Hangeol Chang, Jaemin Kim, Jinse Park, Jae-Sung Lim, Jong Chul Ye</div>
<div class="meta-line">First: 2026-01-06T13:44:04+00:00 · Latest: 2026-01-06T13:44:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03018v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03018v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Language Models (LLMs) have shown strong performance on clinical text understanding, they struggle with longitudinal prediction tasks such as dementia prognosis, which require reasoning over complex, non-monotonic symptom trajectories across multiple visits. Standard supervised training lacks explicit annotations for symptom evolution, while direct Reinforcement Learning (RL) is hindered by sparse binary rewards. To address this challenge, we introduce Dementia-R1, an RL-based framework for longitudinal dementia prognosis from unstructured clinical notes. Our approach adopts a Cold-Start RL strategy that pre-trains the model to predict verifiable clinical indices extracted from patient histories, enhancing the capability to reason about disease progression before determining the final clinical status. Extensive experiments demonstrate that Dementia-R1 achieves an F1 score of 77.03% on real-world unstructured clinical datasets. Notably, on the ADNI benchmark, our 7B model rivals GPT-4o, effectively capturing fluctuating cognitive trajectories. Code is available at https://anonymous.4open.science/r/dementiar1-CDB5</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Dementia-R1：基于无结构临床记录的强化预训练与推理实现真实世界痴呆症预后预测</div>
<div class="mono" style="margin-top:8px">尽管大语言模型在临床文本理解方面表现出色，但在痴呆症预后等需要跨多次就诊对复杂非单调症状轨迹进行推理的纵向预测任务中仍存在困难。标准监督训练缺乏症状演变的显式标注，而直接强化学习则受限于稀疏的二元奖励信号。为解决这一挑战，我们提出了Dementia-R1——一个基于强化学习的框架，用于从无结构临床记录中实现纵向痴呆症预后预测。该方法采用冷启动强化学习策略，通过预训练模型预测从患者历史中提取的可验证临床指标，从而在确定最终临床状态前增强对疾病进展的推理能力。大量实验表明，Dementia-R1在真实世界无结构临床数据集上取得了77.03%的F1分数。值得注意的是，在ADNI基准测试中，我们的70亿参数模型性能媲美GPT-4o，能有效捕捉波动的认知轨迹。代码发布于：https://anonymous.4open.science/r/dementiar1-CDB5</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the limitations of Large Language Models (LLMs) in longitudinal prediction tasks like dementia prognosis, which require reasoning over complex, non-monotonic symptom trajectories across multiple clinical visits, a challenge for standard supervised training and direct Reinforcement Learning (RL) due to sparse rewards. The method introduces Dementia-R1, an RL-based framework that employs a Cold-Start RL strategy, pre-training the model to predict verifiable clinical indices from patient histories to enhance reasoning about disease progression before determining the final clinical status. The main experimental results show that Dementia-R1 achieves an F1 score of 77.03% on real-world unstructured clinical datasets and, on the ADNI benchmark, its 7B model rivals GPT-4o in effectively capturing fluctuating cognitive trajectories.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大型语言模型在纵向预测任务（如痴呆症预后）中的局限性，这类任务需要跨多次临床就诊对复杂、非单调的症状轨迹进行推理，而标准监督训练和直接强化学习因奖励稀疏而难以应对。方法上提出了Dementia-R1，这是一个基于强化学习的框架，采用冷启动强化学习策略，通过预训练模型从患者历史中预测可验证的临床指标，以增强在确定最终临床状态前对疾病进展的推理能力。主要实验结果表明，Dementia-R1在真实世界非结构化临床数据集上取得了77.03%的F1分数，并且在ADNI基准测试中，其7B模型与GPT-4o表现相当，能有效捕捉波动的认知轨迹。</div>
</details>
</div>
<div class="card">
<div class="title">In-Context Reinforcement Learning through Bayesian Fusion of Context and Value Prior</div>
<div class="meta-line">Authors: Anaïs Berkes, Vincent Taboga, Donna Vakalis, David Rolnick, Yoshua Bengio</div>
<div class="meta-line">First: 2026-01-06T13:41:31+00:00 · Latest: 2026-01-06T13:41:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03015v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03015v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In-context reinforcement learning (ICRL) promises fast adaptation to unseen environments without parameter updates, but current methods either cannot improve beyond the training distribution or require near-optimal data, limiting practical adoption. We introduce SPICE, a Bayesian ICRL method that learns a prior over Q-values via deep ensemble and updates this prior at test-time using in-context information through Bayesian updates. To recover from poor priors resulting from training on sub-optimal data, our online inference follows an Upper-Confidence Bound rule that favours exploration and adaptation. We prove that SPICE achieves regret-optimal behaviour in both stochastic bandits and finite-horizon MDPs, even when pretrained only on suboptimal trajectories. We validate these findings empirically across bandit and control benchmarks. SPICE achieves near-optimal decisions on unseen tasks, substantially reduces regret compared to prior ICRL and meta-RL approaches while rapidly adapting to unseen tasks and remaining robust under distribution shift.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于贝叶斯融合上下文与价值先验的上下文强化学习</div>
<div class="mono" style="margin-top:8px">上下文强化学习（ICRL）无需参数更新即可快速适应未知环境，但现有方法要么无法超越训练分布提升性能，要么需要接近最优的数据，限制了实际应用。我们提出SPICE，一种贝叶斯ICRL方法，通过深度集成学习Q值的先验分布，并在测试时利用上下文信息通过贝叶斯更新调整该先验。为克服次优训练数据导致的先验偏差，我们的在线推理采用上置信界规则，以促进探索与适应。理论证明，即使在次优轨迹上预训练，SPICE在随机多臂赌博机和有限时域马尔可夫决策过程中均能实现遗憾最优行为。我们在赌博机与控制基准测试中实证验证了上述结论：SPICE在未知任务上做出接近最优的决策，相比现有ICRL与元强化学习方法显著降低遗憾值，同时快速适应新任务并保持分布偏移下的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of in-context reinforcement learning (ICRL), which struggles to improve beyond training distributions or requires optimal data, by proposing SPICE, a Bayesian ICRL method that learns a prior over Q-values via deep ensembles and updates it at test-time using in-context Bayesian updates. To handle suboptimal training data, it employs an online Upper-Confidence Bound rule to encourage exploration and adaptation. Theoretical analysis proves SPICE achieves regret-optimal behavior in stochastic bandits and finite-horizon MDPs, even with suboptimal pretraining, and empirical validation across benchmarks shows it makes near-optimal decisions on unseen tasks, significantly reducing regret compared to prior methods while adapting quickly and remaining robust to distribution shifts.</div>
<div class="mono" style="margin-top:8px">本文针对上下文强化学习（ICRL）难以超越训练分布或需要最优数据的局限性，提出了SPICE这一贝叶斯ICRL方法，它通过深度集成学习Q值的先验，并在测试时利用上下文信息进行贝叶斯更新。为处理次优训练数据，该方法采用在线上置信界规则以促进探索和适应。理论分析证明，即使在次优预训练下，SPICE也能在随机多臂赌博机和有限时域马尔可夫决策过程中实现遗憾最优行为；实证验证表明，在多个基准测试中，SPICE对未见任务做出接近最优的决策，相比现有ICRL和元强化学习方法显著降低遗憾，同时快速适应新任务并保持对分布偏移的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Multimodal oscillator networks learn to solve a classification problem</div>
<div class="meta-line">Authors: Daan de Bos, Marc Serra-Garcia</div>
<div class="meta-line">First: 2025-02-17T16:54:54+00:00 · Latest: 2026-01-06T13:24:21+00:00</div>
<div class="meta-line">Comments: 14 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.12020v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.12020v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We numerically demonstrate a network of coupled oscillators that can learn to solve a classification task from a set of examples -- performing both training and inference through the nonlinear evolution of the system. We accomplish this by combining three key elements to achieve learning: A long-term memory that stores learned responses, analogous to the synapses in biological brains; a short-term memory that stores the neural activations, similar to the firing patterns of neurons; and an evolution law that updates the synapses in response to novel examples, inspired by synaptic plasticity. Achieving all three elements in wave-based information processors such as metamaterials is a significant challenge. Here, we solve it by leveraging the material multistability to implement long-term memory, and harnessing symmetries and thermal noise to realize the learning rule. Our analysis reveals that the learning mechanism, although inspired by synaptic plasticity, also shares parallelisms with bacterial evolution strategies, where mutation rates increase in the presence of noxious stimuli.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多模态振荡器网络学习解决分类问题</div>
<div class="mono" style="margin-top:8px">我们通过数值模拟展示了一个耦合振荡器网络，该网络能够从一组示例中学习解决分类任务——通过系统的非线性演化同时完成训练和推理。我们通过结合三个关键要素实现学习：长期记忆（存储学习响应，类似于生物大脑中的突触）、短期记忆（存储神经激活状态，类似于神经元的放电模式），以及受突触可塑性启发的演化规律（根据新示例更新突触）。在超材料等基于波的信息处理器中同时实现这三个要素是一项重大挑战。本文通过利用材料多稳态性实现长期记忆，并借助对称性和热噪声实现学习规则，成功解决了这一难题。分析表明，该学习机制虽受突触可塑性启发，但也与细菌进化策略存在相似性——后者在有害刺激下会提高突变率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the challenge of implementing learning capabilities in wave-based physical systems like metamaterials, which lack conventional computing elements. The method introduces a network of coupled oscillators that learns a classification task by integrating three components: long-term memory for storing learned responses (analogous to synapses), short-term memory for neural activations, and an evolution law for updating synapses based on new examples, inspired by synaptic plasticity. The main experimental results, demonstrated numerically, show that the system successfully performs both training and inference through its nonlinear dynamics by leveraging material multistability for long-term memory and using symmetries and thermal noise to realize the learning rule, with the mechanism also showing parallels to bacterial evolution strategies.</div>
<div class="mono" style="margin-top:8px">本文的动机在于解决基于波的物理系统（如超材料）中实现学习能力的挑战，这些系统缺乏传统计算元件。方法提出了一种耦合振荡器网络，通过整合三个组件来学习分类任务：用于存储学习响应的长期记忆（类似于突触）、用于神经激活的短期记忆，以及受突触可塑性启发、基于新样本更新突触的演化定律。主要的数值实验结果表明，该系统通过非线性动力学成功执行了训练和推理，利用材料多稳态实现长期记忆，并借助对称性和热噪声来实现学习规则，该机制还显示出与细菌进化策略的相似性。</div>
</details>
</div>
<div class="card">
<div class="title">Reconsidering Overthinking: Penalizing Internal and External Redundancy in CoT Reasoning</div>
<div class="meta-line">Authors: Jialiang Hong, Taihang Zhen, Kai Chen, Jiaheng Liu, Junlan Feng, Wenpeng Zhu, Jing Huo, Yang Gao, Depeng Wang, Haitao Wan, Xi Yang, Boyan Wang, Fanyu Meng, Yuyao Zhang</div>
<div class="meta-line">First: 2025-08-04T08:22:14+00:00 · Latest: 2026-01-06T13:18:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.02178v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.02178v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Reasoning Models (LRMs) often suffer from overthinking, generating verbose reasoning traces that compromise both computational efficiency and interpretability. Unlike prior efforts that rely on global length-based rewards, we propose a semantic-aware decomposition of redundancy into two distinct forms: internal redundancy (informational stagnation within the reasoning process) and external redundancy (superfluous continuation after the final answer). We introduce a dual-penalty reinforcement learning framework that surgically targets these inefficiencies: a sliding-window semantic analysis is employed to penalize low-gain steps within the reasoning trajectory, while a normalized metric suppresses the post-answer tail. Extensive experiments demonstrate that our method significantly compresses Chain-of-Thought traces with minimal accuracy degradation, while maintaining strong generalization to out-of-domain tasks. Crucially, we reveal an asymmetry in redundancy: external redundancy can be safely eliminated without performance loss, whereas internal redundancy removal requires a calibrated trade-off to maintain reasoning fidelity. Our framework enables fine-grained, implicit control over reasoning length, paving the way for more concise and interpretable LRMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新审视过度思考：惩罚思维链推理中的内部与外部冗余</div>
<div class="mono" style="margin-top:8px">大型推理模型常因过度思考产生冗长的推理轨迹，损害计算效率与可解释性。区别于以往依赖全局长度奖励的方法，我们提出将冗余语义分解为两种形式：内部冗余（推理过程中的信息停滞）和外部冗余（最终答案后的冗余延续）。我们设计了双惩罚强化学习框架精准针对这些低效环节：通过滑动窗口语义分析惩罚推理轨迹中的低增益步骤，同时采用归一化指标抑制答案后冗余尾部。大量实验表明，该方法能显著压缩思维链轨迹且精度损失极小，并保持对领域外任务的强泛化能力。关键发现是冗余存在不对称性：外部冗余可安全消除而不影响性能，而内部冗余的消除需校准权衡以保持推理保真度。该框架实现了对推理长度的细粒度隐式控制，为构建更简洁可解释的大型推理模型开辟了新路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the issue of overthinking in Large Reasoning Models, which leads to inefficient and less interpretable verbose reasoning chains, this paper proposes a semantic-aware method to decompose redundancy into internal (stagnation within reasoning) and external (superfluous continuation after the answer) types. The method employs a dual-penalty reinforcement learning framework that uses sliding-window semantic analysis to penalize low-gain reasoning steps and a normalized metric to suppress post-answer tails. Experimental results show that this approach significantly compresses Chain-of-Thought traces with minimal accuracy loss, maintains strong out-of-domain generalization, and reveals that external redundancy can be safely removed while internal redundancy requires a calibrated trade-off to preserve reasoning fidelity.</div>
<div class="mono" style="margin-top:8px">针对大型推理模型中因过度思考导致推理链冗长、效率低下和可解释性差的问题，本文提出了一种语义感知的方法，将冗余分解为内部冗余（推理过程中的信息停滞）和外部冗余（答案后的多余延续）。该方法采用双重惩罚强化学习框架，利用滑动窗口语义分析惩罚低增益推理步骤，并通过归一化指标抑制答案后尾部。实验结果表明，该方法能显著压缩思维链轨迹且精度损失最小，保持强大的跨领域泛化能力，并揭示外部冗余可安全消除，而内部冗余的去除需权衡校准以维持推理保真度。</div>
</details>
</div>
<div class="card">
<div class="title">Interpretable All-Type Audio Deepfake Detection with Audio LLMs via Frequency-Time Reinforcement Learning</div>
<div class="meta-line">Authors: Yuankun Xie, Xiaoxuan Guo, Jiayi Zhou, Tao Wang, Jian Liu, Ruibo Fu, Xiaopeng Wang, Haonan Cheng, Long Ye</div>
<div class="meta-line">First: 2026-01-06T12:50:02+00:00 · Latest: 2026-01-06T12:50:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02983v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02983v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in audio large language models (ALLMs) have made high-quality synthetic audio widely accessible, increasing the risk of malicious audio deepfakes across speech, environmental sounds, singing voice, and music. Real-world audio deepfake detection (ADD) therefore requires all-type detectors that generalize across heterogeneous audio and provide interpretable decisions. Given the strong multi-task generalization ability of ALLMs, we first investigate their performance on all-type ADD under both supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). However, SFT using only binary real/fake labels tends to reduce the model to a black-box classifier, sacrificing interpretability. Meanwhile, vanilla RFT under sparse supervision is prone to reward hacking and can produce hallucinated, ungrounded rationales. To address this, we propose an automatic annotation and polishing pipeline that constructs Frequency-Time structured chain-of-thought (CoT) rationales, producing ~340K cold-start demonstrations. Building on CoT data, we propose Frequency Time-Group Relative Policy Optimization (FT-GRPO), a two-stage training paradigm that cold-starts ALLMs with SFT and then applies GRPO under rule-based frequency-time constraints. Experiments demonstrate that FT-GRPO achieves state-of-the-art performance on all-type ADD while producing interpretable, FT-grounded rationales. The data and code are available online.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于音频大语言模型与频时强化学习的可解释全类型音频深度伪造检测</div>
<div class="mono" style="margin-top:8px">音频大语言模型（ALLM）的进展使高质量合成音频广泛传播，增加了语音、环境声、歌声及音乐等恶意音频深度伪造的风险。现实场景的音频深度伪造检测（ADD）需具备跨异构音频泛化能力且提供可解释决策的全类型检测器。鉴于ALLM强大的多任务泛化能力，我们首先探究其在监督微调（SFT）与强化微调（RFT）下对全类型ADD的性能。然而，仅使用二元真伪标签的SFT易使模型退化为黑盒分类器，丧失可解释性；而稀疏监督下的原始RFT易出现奖励破解，产生虚假无依据的决策依据。为此，我们提出自动标注优化流程，构建频时结构化思维链（CoT）依据，生成约34万条冷启动示范数据。基于CoT数据，我们提出频时分组相对策略优化（FT-GRPO）：通过SFT冷启动ALLM，再在基于规则的频时约束下实施GRPO的两阶段训练范式。实验表明，FT-GRPO在全类型ADD中达到最优性能，同时生成可解释的频时锚定决策依据。数据与代码已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the growing accessibility of high-quality synthetic audio and the need for detectors that generalize across diverse audio types while offering interpretable decisions, this paper proposes a novel method for all-type audio deepfake detection. The method introduces an automatic pipeline to construct Frequency-Time structured chain-of-thought rationales, generating approximately 340K demonstrations, and employs a two-stage training paradigm called Frequency Time-Group Relative Policy Optimization (FT-GRPO) that combines supervised fine-tuning with reinforcement fine-tuning under frequency-time constraints. Experimental results show that FT-GRPO achieves state-of-the-art performance on all-type audio deepfake detection tasks while producing interpretable rationales grounded in frequency-time analysis.</div>
<div class="mono" style="margin-top:8px">本文的动机源于高质量合成音频的日益普及，以及需要能够泛化于多种音频类型并提供可解释决策的检测器。方法上，提出了一种自动构建频率-时间结构思维链推理的流程，生成约34万条初始演示数据，并采用名为频率时间-组相对策略优化（FT-GRPO）的两阶段训练范式，结合监督微调和在频率-时间约束下的强化微调。主要实验结果表明，FT-GRPO在全类型音频深度伪造检测任务中达到了最先进的性能，同时产生了基于频率-时间分析的可解释推理。</div>
</details>
</div>
<div class="card">
<div class="title">Learning mirror maps in policy mirror descent</div>
<div class="meta-line">Authors: Carlo Alfano, Sebastian Towers, Silvia Sapora, Chris Lu, Patrick Rebeschini</div>
<div class="meta-line">First: 2024-02-07T19:01:06+00:00 · Latest: 2026-01-06T12:33:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2402.05187v3">Abs</a> · <a href="https://arxiv.org/pdf/2402.05187v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Policy Mirror Descent (PMD) is a popular framework in reinforcement learning, serving as a unifying perspective that encompasses numerous algorithms. These algorithms are derived through the selection of a mirror map and enjoy finite-time convergence guarantees. Despite its popularity, the exploration of PMD&#x27;s full potential is limited, with the majority of research focusing on a particular mirror map -- namely, the negative entropy -- which gives rise to the renowned Natural Policy Gradient (NPG) method. It remains uncertain from existing theoretical studies whether the choice of mirror map significantly influences PMD&#x27;s efficacy. In our work, we conduct empirical investigations to show that the conventional mirror map choice (NPG) often yields less-than-optimal outcomes across several standard benchmark environments. Using evolutionary strategies, we identify more efficient mirror maps that enhance the performance of PMD. We first focus on a tabular environment, i.e. Grid-World, where we relate existing theoretical bounds with the performance of PMD for a few standard mirror maps and the learned one. We then show that it is possible to learn a mirror map that outperforms the negative entropy in more complex environments, such as the MinAtar suite. Additionally, we demonstrate that the learned mirror maps generalize effectively to different tasks by testing each map across various other environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>策略镜像下降中学习镜像映射</div>
<div class="mono" style="margin-top:8px">策略镜像下降（PMD）是强化学习中一个广受欢迎的框架，它作为一个统一视角涵盖了众多算法。这些算法通过选择镜像映射推导而来，并享有有限时间收敛保证。尽管PMD广受欢迎，但其全部潜力的探索仍有限，大多数研究聚焦于特定镜像映射——即负熵，这催生了著名的自然策略梯度（NPG）方法。现有理论研究尚不确定镜像映射的选择是否显著影响PMD的效能。在我们的工作中，我们通过实证研究表明，传统镜像映射选择（NPG）在多个标准基准环境中常产生次优结果。利用进化策略，我们识别出能提升PMD性能的更高效镜像映射。我们首先关注表格环境（如Grid-World），将现有理论界限与几种标准镜像映射及学习所得映射的PMD性能相关联。随后证明，在更复杂环境（如MinAtar套件）中，可以学习到优于负熵的镜像映射。此外，通过在不同环境中测试各映射，我们证明所学镜像映射能有效泛化至不同任务。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the observation that Policy Mirror Descent (PMD), a broad reinforcement learning framework, is predominantly studied with the negative entropy mirror map, which yields the Natural Policy Gradient (NPG) method, leaving its full potential unexplored and the impact of mirror map choice unclear. The method employs evolutionary strategies to empirically learn and identify more efficient mirror maps beyond the standard negative entropy. Key experimental results demonstrate that the conventional NPG often underperforms on standard benchmarks like Grid-World and the MinAtar suite, whereas the learned mirror maps significantly enhance PMD&#x27;s performance and show effective generalization across different tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于观察到策略镜像下降（PMD）这一广泛的强化学习框架主要使用负熵镜像映射进行研究，这产生了自然策略梯度方法，但其全部潜力尚未开发，且镜像映射选择的影响尚不明确。该方法采用进化策略，通过经验学习并识别出比标准负熵更高效的镜像映射。主要实验结果表明，在Grid-World和MinAtar套件等标准基准测试中，传统的自然策略梯度方法往往表现不佳，而学习到的镜像映射能显著提升PMD的性能，并在不同任务中展现出有效的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Correct, Concise and Complete: Multi-stage Training For Adaptive Reasoning</div>
<div class="meta-line">Authors: Nathanaël Carraz Rakotonirina, Ren Pang, Neha Anna John, Michael Bohlke-Schneider, Momchil Hardalov</div>
<div class="meta-line">First: 2026-01-06T12:31:51+00:00 · Latest: 2026-01-06T12:31:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02972v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02972v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The reasoning capabilities of large language models (LLMs) have improved substantially through increased test-time computation, typically in the form of intermediate tokens known as chain-of-thought (CoT). However, CoT often becomes unnecessarily long, increasing computation cost without actual accuracy gains or sometimes even degrading performance, a phenomenon known as ``overthinking&#x27;&#x27;. We propose a multi-stage efficient reasoning method that combines supervised fine-tuning -- via rejection sampling or reasoning trace reformatting -- with reinforcement learning using an adaptive length penalty. We introduce a lightweight reward function that penalizes tokens generated after the first correct answer but encouraging self-verification only when beneficial. We conduct a holistic evaluation across seven diverse reasoning tasks, analyzing the accuracy-response length trade-off. Our approach reduces response length by an average of 28\% for 8B models and 40\% for 32B models, while incurring only minor performance drops of 1.6 and 2.5 points, respectively. Despite its conceptual simplicity, it achieves a superior trade-off compared to more complex state-of-the-art efficient reasoning methods, scoring 76.6, in terms of the area under the Overthinking-Adjusted Accuracy curve ($\text{AUC}_{\text{OAA}}$) -- 5 points above the base model and 2.5 points above the second-best approach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>正确、简洁且完整：面向自适应推理的多阶段训练方法</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）的推理能力通过增加测试时计算量（通常以中间标记的形式，即思维链）已显著提升。然而，思维链常变得冗长，增加了计算成本却未提升准确率，甚至可能降低性能，这种现象称为“过度思考”。我们提出一种多阶段高效推理方法，结合监督微调（通过拒绝采样或推理轨迹重构）与采用自适应长度惩罚的强化学习。我们引入轻量级奖励函数，惩罚首次正确答案后生成的标记，但仅在有益时鼓励自我验证。我们在七项多样化推理任务中进行全面评估，分析准确率与响应长度的权衡。该方法使8B模型的平均响应长度减少28%，32B模型减少40%，而性能仅分别轻微下降1.6和2.5个点。尽管概念简单，其在过度思考调整准确率曲线下面积（$\text{AUC}_{\text{OAA}}$）指标上达到76.6分，较基础模型高5分，较次优方法高2.5分，实现了优于复杂前沿高效推理方法的权衡效果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the inefficiency of chain-of-thought reasoning in large language models, which often leads to unnecessarily long and sometimes performance-degrading outputs, this paper proposes a multi-stage training method to achieve adaptive reasoning. The method combines supervised fine-tuning via rejection sampling or trace reformatting with reinforcement learning that applies an adaptive length penalty, using a lightweight reward function to penalize superfluous tokens after a correct answer while encouraging beneficial self-verification. Experimental results across seven reasoning tasks show that this approach reduces response length by an average of 28% for 8B models and 40% for 32B models with only minor accuracy drops of 1.6 and 2.5 points, respectively, achieving a superior trade-off measured by the Overthinking-Adjusted Accuracy curve compared to other efficient reasoning methods.</div>
<div class="mono" style="margin-top:8px">针对大语言模型中思维链推理常产生冗长且可能降低性能的“过度思考”问题，本文提出了一种多阶段训练方法以实现自适应推理。该方法通过拒绝采样或推理轨迹重构进行监督微调，并结合使用自适应长度惩罚的强化学习，其轻量级奖励函数在鼓励有益自我验证的同时，对生成正确答案后的冗余标记进行惩罚。在七个推理任务上的实验结果表明，该方法使8B和32B模型的平均响应长度分别减少了28%和40%，而准确率仅轻微下降1.6和2.5个百分点，在过思考调整准确率曲线指标上优于其他高效推理方法，实现了更优的权衡。</div>
</details>
</div>
<div class="card">
<div class="title">The World is Not Mono: Enabling Spatial Understanding in Large Audio-Language Models</div>
<div class="meta-line">Authors: Yuhuan You, Lai Wei, Xihong Wu, Tianshu Qu</div>
<div class="meta-line">First: 2026-01-06T11:54:47+00:00 · Latest: 2026-01-06T11:54:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02954v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02954v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing large audio-language models perceive the world as &quot;mono&quot; -- a single stream of audio that ignores the critical spatial dimension (&quot;where&quot;) required for universal acoustic scene analysis. To bridge this gap, we first introduce a hierarchical framework for Auditory Scene Analysis (ASA). Guided by this framework, we introduce a system that enables models like Qwen2-Audio to understand and reason about the complex acoustic world. Our framework achieves this through three core contributions: First, we build a large-scale, synthesized binaural audio dataset to provide the rich spatial cues. Second, we design a hybrid feature projector, which leverages parallel semantic and spatial encoders to extract decoupled representations. These distinct streams are integrated via a dense fusion mechanism, ensuring the model receives a holistic view of the acoustic scene. Finally, we employ a progressive training curriculum, advancing from supervised fine-tuning (SFT) to reinforcement learning via Group Relative Policy Optimization (GRPO), to explicitly evolve the model&#x27;s capabilities towards reasoning. On our comprehensive benchmark, the model demonstrates comparatively strong capability for spatial understanding. By enabling this spatial perception, our work provides a clear pathway for leveraging the powerful reasoning abilities of large models towards holistic acoustic scene analysis, advancing from &quot;mono&quot; semantic recognition to spatial intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>世界并非单声道：在大型音频语言模型中实现空间理解</div>
<div class="mono" style="margin-top:8px">现有大型音频语言模型将世界感知为“单声道”——即忽略通用声学场景分析所需关键空间维度（“何处”）的单一音频流。为弥合这一差距，我们首先引入听觉场景分析（ASA）的层次化框架。在此框架指导下，我们提出一个系统，使Qwen2-Audio等模型能够理解并推理复杂的声学世界。该框架通过三项核心贡献实现目标：首先，我们构建大规模合成双耳音频数据集以提供丰富空间线索；其次，设计混合特征投影器，利用并行语义编码器与空间编码器提取解耦表征，通过密集融合机制整合这些独立信息流，确保模型获得声学场景的整体视图；最后，采用渐进式训练方案，从监督微调（SFT）推进至基于群体相对策略优化（GRPO）的强化学习，显式提升模型的推理能力。在综合基准测试中，该模型展现出较强的空间理解能力。通过实现空间感知，本研究为利用大模型强大推理能力进行整体声学场景分析提供了清晰路径，推动从“单声道”语义识别向空间智能的演进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitation of existing large audio-language models that treat audio as a single, spatially agnostic stream, this paper introduces a hierarchical framework for Auditory Scene Analysis (ASA) to incorporate spatial understanding. The method involves creating a large-scale synthesized binaural audio dataset, designing a hybrid feature projector with parallel semantic and spatial encoders for decoupled representations fused via dense fusion, and employing a progressive training curriculum from supervised fine-tuning to reinforcement learning with Group Relative Policy Optimization (GRPO). Experimental results on a comprehensive benchmark show the model achieves strong spatial understanding capabilities, advancing from mere semantic recognition to holistic spatial intelligence in acoustic scene analysis.</div>
<div class="mono" style="margin-top:8px">针对现有大型音频-语言模型将音频视为单一、忽略空间维度的“单声道”流的局限，本文引入了听觉场景分析（ASA）的层次化框架以融入空间理解。方法包括构建大规模合成双耳音频数据集，设计具有并行语义和空间编码器的混合特征投影器以获取解耦表征并通过密集融合机制整合，以及采用从监督微调到基于群组相对策略优化（GRPO）的强化学习的渐进式训练课程。在综合基准测试上的实验结果表明，该模型展现出较强的空间理解能力，推动了从单纯的语义识别到具备空间智能的 holistic 声学场景分析的进步。</div>
</details>
</div>
<div class="card">
<div class="title">Do Not Step Into the Same River Twice: Learning to Reason from Trial and Error</div>
<div class="meta-line">Authors: Chenming Tang, Hsiu-Yuan Huang, Weijie Liu, Clive Bai, Saiyong Yang, Yunfang Wu</div>
<div class="meta-line">First: 2025-10-30T03:36:19+00:00 · Latest: 2026-01-06T11:33:27+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.26109v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.26109v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) has significantly boosted the reasoning capability of language models (LMs) recently. However, existing RLVR approaches merely train LMs based on their own generated on-policy responses and are constrained by the initial capability of LMs, thus prone to exploration stagnation, in which LMs fail to solve more training problems and cannot further learn from the training data. Some work tries to address this by leveraging off-policy solutions to training problems, but relies on external expert guidance that is limited in availability and scalability. In this work, we propose LTE (Learning to reason from Trial and Error), an approach that hints LMs with their previously self-made mistakes, not requiring any external expert guidance. Experiments validate the effectiveness of LTE, which outperforms the normal group relative policy optimization (GRPO) by 5.02 in Pass@1 and 9.96 in Pass@k on average across six mathematical reasoning benchmarks for Qwen3-8B-Base and even performs better than methods that require external gold solutions as guidance after aligning the experimental setup. Further analysis confirms that LTE successfully mitigates exploration stagnation and enhances both exploitation and exploration during training. Our code is available at https://anonymous.4open.science/r/Learning-from-Trial-and-Error.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>不可两次踏入同一条河流：从试错中学习推理</div>
<div class="mono" style="margin-top:8px">近期，基于可验证奖励的强化学习（RLVR）显著提升了语言模型（LM）的推理能力。然而，现有RLVR方法仅基于LM自身生成的同策略响应进行训练，受限于LM的初始能力，容易陷入探索停滞，即LM无法解决更多训练问题，也无法从训练数据中进一步学习。部分研究尝试通过利用训练问题的异策略解决方案来应对此问题，但依赖于外部专家指导，其可用性和可扩展性有限。本文提出LTE（从试错中学习推理），该方法通过提示LM其先前自犯的错误来引导学习，无需任何外部专家指导。实验验证了LTE的有效性：在六个数学推理基准测试中，Qwen3-8B-Base模型使用LTE在Pass@1和Pass@k上平均分别比常规组相对策略优化（GRPO）高出5.02和9.96分；在统一实验设置后，其表现甚至优于需要外部黄金解决方案指导的方法。进一步分析证实，LTE成功缓解了探索停滞，并增强了训练过程中的利用与探索能力。代码发布于https://anonymous.4open.science/r/Learning-from-Trial-and-Error。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of exploration stagnation in reinforcement learning with verifiable rewards (RLVR) for language models, where models are limited by their own on-policy responses and initial capabilities, hindering further learning. The authors propose LTE (Learning to reason from Trial and Error), a method that enables language models to learn from their own past mistakes without relying on external expert guidance. Experimental results across six mathematical reasoning benchmarks show that LTE outperforms standard group relative policy optimization (GRPO) by 5.02 in Pass@1 and 9.96 in Pass@k on average for Qwen3-8B-Base, even surpassing methods that use external gold solutions, while analysis confirms it mitigates exploration stagnation and enhances both exploitation and exploration during training.</div>
<div class="mono" style="margin-top:8px">本文针对语言模型在可验证奖励强化学习（RLVR）中因依赖自身策略响应和初始能力而导致的探索停滞问题，提出了一种无需外部专家指导的LTE（从试错中学习推理）方法，使模型能够从自身过往错误中学习。在六个数学推理基准测试中，实验结果表明LTE在Qwen3-8B-Base模型上平均比标准组相对策略优化（GRPO）在Pass@1和Pass@k指标上分别提升5.02和9.96，甚至优于依赖外部黄金解决方案的方法，同时分析证实该方法有效缓解了探索停滞，并增强了训练过程中的利用与探索能力。</div>
</details>
</div>
<div class="card">
<div class="title">ChemBART: A Pre-trained BART Model Assisting Organic Chemistry Analysis</div>
<div class="meta-line">Authors: Kenan Li, Yijian Zhang, Jin Wang, Haipeng Gan, Zeying Sun, Xiaoguang Lei, Hao Dong</div>
<div class="meta-line">First: 2026-01-06T10:55:38+00:00 · Latest: 2026-01-06T10:55:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02915v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02915v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large language models (LLMs) have demonstrated transformative potential across diverse fields. While LLMs have been applied to molecular simplified molecular input line entry system (SMILES) in computer-aided synthesis planning (CASP), existing methodologies typically address single tasks, such as precursor prediction. We introduce ChemBART, a SMILES-based LLM pre-trained on chemical reactions, which enables a unified model for multiple downstream chemical tasks--achieving the paradigm of &quot;one model, one pre-training, multiple tasks.&quot; By leveraging outputs from a mask-filling pre-training task on reaction expressions, ChemBART effectively solves a variety of chemical problems, including precursor/reagent generation, temperature-yield regression, molecular property classification, and optimizing the policy and value functions within a reinforcement learning framework, integrated with Monte Carlo tree search for multi-step synthesis route design. Unlike single-molecule pre-trained LLMs constrained to specific applications, ChemBART addresses broader chemical challenges and integrates them for comprehensive synthesis planning. Crucially, ChemBART-designed multi-step synthesis routes and reaction conditions directly inspired wet-lab validation, which confirmed shorter pathways with ~30% yield improvement over literature benchmarks. Our work validates the power of reaction-focused pre-training and showcases the broad utility of ChemBART in advancing the complete synthesis planning cycle.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ChemBART：一种辅助有机化学分析的预训练BART模型</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的最新进展展现了跨领域的变革潜力。尽管LLMs已应用于计算机辅助合成规划（CASP）中的分子简化分子线性输入规范（SMILES），现有方法通常仅针对单一任务（如前体预测）。本文提出ChemBART——一种基于SMILES、通过化学反应预训练的LLM，它能作为统一模型处理多种下游化学任务，实现“一模型、一预训练、多任务”范式。通过利用反应表达式的掩码填充预训练任务输出，ChemBART有效解决了前体/试剂生成、温度-产率回归、分子性质分类等化学问题，并能在强化学习框架中优化策略与价值函数，结合蒙特卡洛树搜索实现多步合成路线设计。与局限于特定应用的单分子预训练LLMs不同，ChemBART应对更广泛的化学挑战并将其整合为综合合成规划。关键的是，ChemBART设计的多步合成路线与反应条件直接启发了湿实验验证，实验证实其路径更短，产率较文献基准提升约30%。本研究验证了以反应为核心的预训练效能，并展示了ChemBART在推进完整合成规划周期中的广泛实用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for a unified model that can handle multiple chemical tasks beyond single-task approaches like precursor prediction, this paper introduces ChemBART, a BART-based large language model pre-trained on chemical reaction SMILES data. The method leverages a mask-filling pre-training objective on reaction expressions to enable a single model to address diverse downstream tasks, including precursor/reagent generation, temperature-yield regression, molecular property classification, and reinforcement learning-integrated multi-step synthesis route design. Key experimental results show that ChemBART-designed synthesis routes, validated through wet-lab experiments, achieved shorter pathways with approximately 30% yield improvement over existing literature benchmarks, demonstrating its effectiveness in comprehensive synthesis planning.</div>
<div class="mono" style="margin-top:8px">本文的动机是开发一个统一模型以处理多种化学任务，超越现有仅针对单一任务（如前体预测）的方法，为此提出了ChemBART，这是一种基于BART架构、在化学反应SMILES数据上预训练的大语言模型。该方法利用反应表达式的掩码填充预训练任务，使单个模型能够处理多种下游任务，包括前体/试剂生成、温度-产率回归、分子性质分类，以及与蒙特卡洛树搜索结合的强化学习多步合成路线设计。主要实验结果表明，通过湿实验验证的ChemBART设计合成路线实现了更短的路径，且产率较文献基准提升约30%，验证了其在完整合成规划中的广泛实用性。</div>
</details>
</div>
<div class="card">
<div class="title">SimRPD: Optimizing Recruitment Proactive Dialogue Agents through Simulator-Based Data Evaluation and Selection</div>
<div class="meta-line">Authors: Zhiyong Cao, Dunqiang Liu, Qi Dai, Haojun Xu, Huaiyan Xu, Huan He, Yafei Liu, Siyuan Liu, XiaoLin Lin, Ke Ma, Ruqian Shi, Sijia Yao, Hao Wang, Sicheng Zhou</div>
<div class="meta-line">First: 2026-01-06T10:00:15+00:00 · Latest: 2026-01-06T10:00:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02871v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02871v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Task-oriented proactive dialogue agents play a pivotal role in recruitment, particularly for steering conversations towards specific business outcomes, such as acquiring social-media contacts for private-channel conversion. Although supervised fine-tuning and reinforcement learning have proven effective for training such agents, their performance is heavily constrained by the scarcity of high-quality, goal-oriented domain-specific training data. To address this challenge, we propose SimRPD, a three-stage framework for training recruitment proactive dialogue agents. First, we develop a high-fidelity user simulator to synthesize large-scale conversational data through multi-turn online dialogue. Then we introduce a multi-dimensional evaluation framework based on Chain-of-Intention (CoI) to comprehensively assess the simulator and effectively select high-quality data, incorporating both global-level and instance-level metrics. Finally, we train the recruitment proactive dialogue agent on the selected dataset. Experiments in a real-world recruitment scenario demonstrate that SimRPD outperforms existing simulator-based data selection strategies, highlighting its practical value for industrial deployment and its potential applicability to other business-oriented dialogue scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SimRPD：基于模拟器的数据评估与选择优化招聘主动对话智能体</div>
<div class="mono" style="margin-top:8px">面向任务的主动对话智能体在招聘场景中具有关键作用，尤其在引导对话实现特定业务目标（如获取社交媒体联系人以进行私域转化）方面。尽管监督微调和强化学习已被证明对此类智能体训练有效，但其性能受限于高质量、目标导向的领域特定训练数据的稀缺性。为解决这一挑战，我们提出SimRPD——一个三阶段框架用于训练招聘主动对话智能体。首先，我们开发高保真用户模拟器，通过多轮在线对话合成大规模对话数据；其次，引入基于意图链的多维评估框架，结合全局级与实例级指标，全面评估模拟器并有效筛选高质量数据；最后，基于筛选数据集训练招聘主动对话智能体。真实招聘场景实验表明，SimRPD优于现有基于模拟器的数据选择策略，凸显了其在工业部署中的实用价值及面向其他商业对话场景的潜在适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of training effective recruitment proactive dialogue agents, which are hindered by the scarcity of high-quality, domain-specific training data. To overcome this, the authors propose SimRPD, a three-stage framework that first employs a high-fidelity user simulator to generate large-scale conversational data, then applies a multi-dimensional evaluation based on Chain-of-Intention to select high-quality data using global and instance-level metrics, and finally trains the dialogue agent on this curated dataset. Experimental results in a real-world recruitment setting show that SimRPD surpasses existing simulator-based data selection methods, demonstrating its practical utility for industrial applications and potential extension to other business-oriented dialogue systems.</div>
<div class="mono" style="margin-top:8px">本文针对招聘主动对话代理训练中高质量领域特定数据稀缺的问题，提出SimRPD框架。该方法包含三个阶段：首先通过高保真用户模拟器生成大规模多轮对话数据；其次引入基于意图链的多维评估框架，结合全局和实例级指标筛选高质量数据；最后利用所选数据训练招聘主动对话代理。在真实招聘场景中的实验表明，SimRPD优于现有的基于模拟器的数据选择策略，凸显了其工业部署的实用价值，并具备拓展至其他业务导向对话场景的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Sample-Efficient Neurosymbolic Deep Reinforcement Learning</div>
<div class="meta-line">Authors: Celeste Veronese, Daniele Meli, Alessandro Farinelli</div>
<div class="meta-line">First: 2026-01-06T09:28:53+00:00 · Latest: 2026-01-06T09:28:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02850v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02850v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) is a well-established framework for sequential decision-making in complex environments. However, state-of-the-art Deep RL (DRL) algorithms typically require large training datasets and often struggle to generalize beyond small-scale training scenarios, even within standard benchmarks. We propose a neuro-symbolic DRL approach that integrates background symbolic knowledge to improve sample efficiency and generalization to more challenging, unseen tasks. Partial policies defined for simple domain instances, where high performance is easily attained, are transferred as useful priors to accelerate learning in more complex settings and avoid tuning DRL parameters from scratch. To do so, partial policies are represented as logical rules, and online reasoning is performed to guide the training process through two mechanisms: (i) biasing the action distribution during exploration, and (ii) rescaling Q-values during exploitation. This neuro-symbolic integration enhances interpretability and trustworthiness while accelerating convergence, particularly in sparse-reward environments and tasks with long planning horizons. We empirically validate our methodology on challenging variants of gridworld environments, both in the fully observable and partially observable setting. We show improved performance over a state-of-the-art reward machine baseline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>样本高效的神经符号深度强化学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL）是复杂环境中序列决策的成熟框架。然而，当前最先进的深度强化学习（DRL）算法通常需要大量训练数据，且即使在标准基准测试中，也常难以泛化至小规模训练场景之外。我们提出一种神经符号DRL方法，通过整合背景符号知识来提升样本效率，并增强对更具挑战性、未见任务的泛化能力。针对简单领域实例（易于实现高性能）定义的部分策略，可作为有用先验迁移至更复杂场景，以加速学习过程，避免从头调整DRL参数。具体而言，部分策略被表示为逻辑规则，并通过在线推理以两种机制指导训练：（i）在探索阶段偏置动作分布，（ii）在利用阶段重缩放Q值。这种神经符号融合在加速收敛的同时增强了可解释性与可信度，尤其在稀疏奖励环境和长规划跨度任务中表现突出。我们在网格世界环境的挑战性变体（包括完全可观测与部分可观测设置）中实证验证了该方法，结果显示其性能优于当前最先进的奖励机制基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the sample inefficiency and limited generalization of standard deep reinforcement learning (DRL), this paper introduces a neuro-symbolic DRL approach that incorporates background symbolic knowledge to enhance learning. The method transfers partial policies, represented as logical rules from simple tasks, to guide training in more complex settings via action distribution biasing during exploration and Q-value rescaling during exploitation. Experimental results on challenging gridworld variants, including partially observable settings, demonstrate improved performance over a state-of-the-art reward machine baseline, with accelerated convergence in sparse-reward and long-horizon tasks.</div>
<div class="mono" style="margin-top:8px">针对深度强化学习样本效率低和泛化能力有限的问题，本文提出了一种神经符号深度强化学习方法，通过整合背景符号知识来提升学习效果。该方法将简单任务中定义的局部策略表示为逻辑规则，并通过在探索阶段偏置动作分布、在利用阶段重新缩放Q值来指导复杂任务的训练过程。在具有挑战性的网格世界变体环境（包括部分可观测设置）上的实验结果表明，该方法优于先进的奖励机基线，在稀疏奖励和长规划视野任务中加速了收敛。</div>
</details>
</div>
<div class="card">
<div class="title">ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management</div>
<div class="meta-line">Authors: Lingjie Zhao, Xue Yu, Yongzhi Qi, Hao Hu, Jianshen Zhang, Yingzheng Ma, Shuyu Han, Wei Qi, Zuo-Jun Max Shen</div>
<div class="meta-line">First: 2025-12-22T03:39:43+00:00 · Latest: 2026-01-06T09:08:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19001v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.19001v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As the pursuit of synergy between Artificial Intelligence (AI) and Operations Research (OR) gains momentum in handling complex inventory systems, a critical challenge persists: how to effectively reconcile AI&#x27;s adaptive perception with OR&#x27;s structural rigor. To bridge this gap, we propose a novel OR-Guided &quot;Pretrain-then-Reinforce&quot; framework. To provide structured guidance, we propose a simulation-augmented OR model that generates high-quality reference decisions, implicitly capturing complex business constraints and managerial preferences. Leveraging these OR-derived decisions as foundational training labels, we design a domain-informed deep learning foundation model to establish foundational decision-making capabilities, followed by a reinforcement learning (RL) fine-tuning stage. Uniquely, we position RL as a deep alignment mechanism that enables the AI agent to internalize the optimality principles of OR, while simultaneously leveraging exploration for general policy refinement and allowing expert guidance for scenario-specific adaptation (e.g., promotional events). Validated through extensive numerical experiments and a field deployment at JD.com augmented by a Difference-in-Differences (DiD) analysis, our model significantly outperforms incumbent industrial practices, delivering real-world gains of a 5.27-day reduction in turnover and a 2.29% increase in in-stock rates, alongside a 29.95% decrease in holding costs. Contrary to the prevailing trend of brute-force model scaling, our study demonstrates that a lightweight, domain-informed model can deliver state-of-the-art performance and robust transferability when guided by structured OR logic. This approach offers a scalable and cost-effective paradigm for intelligent supply chain management, highlighting the value of deeply aligning AI with OR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ORPR：一种运筹学引导的预训练-强化学习库存管理模型</div>
<div class="mono" style="margin-top:8px">随着人工智能与运筹学在复杂库存系统协同应用的趋势日益增强，一个关键挑战持续存在：如何有效调和AI的适应性感知与OR的结构化严谨性。为弥合这一鸿沟，我们提出了一种新颖的OR引导的“预训练-强化”框架。为提供结构化指导，我们构建了仿真增强的OR模型以生成高质量参考决策，隐式捕捉复杂业务约束与管理偏好。利用这些OR衍生的决策作为基础训练标签，我们设计了领域知识注入的深度学习基础模型以建立核心决策能力，继而进行强化学习微调阶段。本研究的独特之处在于将RL定位为深度对齐机制，使AI智能体能够内化OR的最优化原理，同时利用探索实现通用策略优化，并允许专家指导进行场景化适配（如促销活动）。通过大量数值实验及京东集团结合双重差分法的实地部署验证，本模型显著优于现行工业实践，实现周转天数减少5.27天、现货率提升2.29%、持有成本降低29.95%的实际效益。与当前主流的暴力模型扩展趋势不同，本研究证明：当获得结构化OR逻辑引导时，轻量化的领域知识注入模型能够实现顶尖性能与强迁移性。该方法为智能供应链管理提供了可扩展且经济高效的范式，凸显了AI与OR深度对齐的价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to reconcile the adaptive perception of AI with the structural rigor of OR in complex inventory systems, this paper proposes an OR-guided &quot;Pretrain-then-Reinforce&quot; learning framework. The method first uses a simulation-augmented OR model to generate high-quality reference decisions, which then train a domain-informed deep learning foundation model; this is followed by a reinforcement learning fine-tuning stage that aligns the AI with OR optimality principles while allowing for exploration and expert adaptation. Experimental results from numerical tests and a field deployment at JD.com show the model significantly outperforms existing practices, reducing turnover days by 5.27, increasing in-stock rates by 2.29%, and decreasing holding costs by 29.95%, demonstrating that a lightweight, domain-informed model guided by OR logic can achieve state-of-the-art performance.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决复杂库存管理中人工智能的适应性感知与运筹学结构严谨性难以有效协同的挑战，提出了一个运筹学引导的“预训练后强化”学习框架。该方法首先通过仿真增强的运筹学模型生成高质量的参考决策，用于训练一个领域知识驱动的深度学习基础模型，随后进行强化学习微调，使智能体内化运筹学的最优原则，同时允许探索和专家针对特定场景的调整。在京东的现场部署和数值实验中，该模型显著优于现有工业实践，实现了周转天数减少5.27天、现货率提升2.29%以及持有成本降低29.95%的实际收益，表明轻量级、领域知识驱动的模型在运筹学逻辑引导下能够实现先进的性能与鲁棒的迁移性。</div>
</details>
</div>
<div class="card">
<div class="title">Mastering the Game of Go with Self-play Experience Replay</div>
<div class="meta-line">Authors: Jingbin Liu, Xuechun Wang</div>
<div class="meta-line">First: 2026-01-06T08:42:40+00:00 · Latest: 2026-01-06T08:42:40+00:00</div>
<div class="meta-line">Comments: 13 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03306v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03306v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The game of Go has long served as a benchmark for artificial intelligence, demanding sophisticated strategic reasoning and long-term planning. Previous approaches such as AlphaGo and its successors, have predominantly relied on model-based Monte-Carlo Tree Search (MCTS). In this work, we present QZero, a novel model-free reinforcement learning algorithm that forgoes search during training and learns a Nash equilibrium policy through self-play and off-policy experience replay. Built upon entropy-regularized Q-learning, QZero utilizes a single Q-value network to unify policy evaluation and improvement. Starting tabula rasa without human data and trained for 5 months with modest compute resources (7 GPUs), QZero achieved a performance level comparable to that of AlphaGo. This demonstrates, for the first time, the efficiency of using model-free reinforcement learning to master the game of Go, as well as the feasibility of off-policy reinforcement learning in solving large-scale and complex environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于自对弈经验回放的围棋游戏精通</div>
<div class="mono" style="margin-top:8px">围棋长期以来一直是人工智能的基准测试，需要复杂的战略推理和长期规划。先前的方法，如AlphaGo及其后续版本，主要依赖于基于模型的蒙特卡洛树搜索（MCTS）。在本研究中，我们提出了QZero，一种新颖的无模型强化学习算法，它在训练过程中放弃搜索，通过自对弈和离策略经验回放学习纳什均衡策略。基于熵正则化Q学习构建，QZero利用单个Q值网络统一策略评估和改进。从零开始，无需人类数据，并在适度计算资源（7个GPU）下训练5个月，QZero达到了与AlphaGo相当的性能水平。这首次证明了使用无模型强化学习精通围棋的效率，以及离策略强化学习在解决大规模复杂环境中的可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of mastering the complex game of Go without relying on computationally expensive search methods like Monte-Carlo Tree Search, this paper introduces QZero, a model-free reinforcement learning algorithm. The method employs entropy-regularized Q-learning, utilizing a single Q-value network to unify policy evaluation and improvement through self-play and off-policy experience replay, trained from scratch without human data. The main experimental result is that after five months of training on modest hardware (7 GPUs), QZero achieved a performance level comparable to AlphaGo, demonstrating the viability of model-free, off-policy learning for large-scale, complex environments.</div>
<div class="mono" style="margin-top:8px">本文的动机是寻求在不依赖蒙特卡洛树搜索等计算密集型方法的情况下，掌握围棋这一复杂游戏。为此，论文提出了QZero这一免模型强化学习算法。该方法基于熵正则化的Q学习，通过自博弈和离策略经验回放，使用单一的Q值网络统一策略评估与改进，且无需人类数据、从零开始训练。主要实验结果表明，在仅使用7块GPU的有限算力下训练五个月后，QZero达到了与AlphaGo相当的性能水平，首次证明了免模型、离策略的强化学习在解决大规模复杂环境中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">MiMo-V2-Flash Technical Report</div>
<div class="meta-line">Authors: Bangjun Xiao, Bingquan Xia, Bo Yang, Bofei Gao, Bowen Shen, Chen Zhang, Chenhong He, Chiheng Lou, Fuli Luo, Gang Wang, Gang Xie, Hailin Zhang, Hanglong Lv, Hanyu Li, Heyu Chen, Hongshen Xu, Houbin Zhang, Huaqiu Liu, Jiangshan Duo, Jianyu Wei, Jiebao Xiao, Jinhao Dong, Jun Shi, Junhao Hu, Kainan Bao, Kang Zhou, Lei Li, Liang Zhao, Linghao Zhang, Peidian Li, Qianli Chen, Shaohui Liu, Shihua Yu, Shijie Cao, Shimao Chen, Shouqiu Yu, Shuo Liu, Tianling Zhou, Weijiang Su, Weikun Wang, Wenhan Ma, Xiangwei Deng, Bohan Mao, Bowen Ye, Can Cai, Chenghua Wang, Chengxuan Zhu, Chong Ma, Chun Chen, Chunan Li, Dawei Zhu, Deshan Xiao, Dong Zhang, Duo Zhang, Fangyue Liu, Feiyu Yang, Fengyuan Shi, Guoan Wang, Hao Tian, Hao Wu, Heng Qu, Hongfei Yi, Hongxu An, Hongyi Guan, Xing Zhang, Yifan Song, Yihan Yan, Yihao Zhao, Yingchun Lai, Yizhao Gao, Yu Cheng, Yuanyuan Tian, Yudong Wang, Zhen Tang, Zhengju Tang, Zhengtao Wen, Zhichao Song, Zhixian Zheng, Zihan Jiang, Jian Wen, Jiarui Sun, Jiawei Li, Jinlong Xue, Jun Xia, Kai Fang, Menghang Zhu, Nuo Chen, Qian Tu, Qihao Zhang, Qiying Wang, Rang Li, Rui Ma, Shaolei Zhang, Shengfan Wang, Shicheng Li, Shuhao Gu, Shuhuai Ren, Sirui Deng, Tao Guo, Tianyang Lu, Weiji Zhuang, Weikang Zhang, Weimin Xiong, Wenshan Huang, Wenyu Yang, Xin Zhang, Xing Yong, Xu Wang, Xueyang Xie, Yilin Jiang, Yixin Yang, Yongzhe He, Yu Tu, Yuanliang Dong, Yuchen Liu, Yue Ma, Yue Yu, Yuxing Xiang, Zhaojun Huang, Zhenru Lin, Zhipeng Xu, Zhiyang Chen, Zhonghua Deng, Zihan Zhang, Zihao Yue</div>
<div class="meta-line">First: 2026-01-06T07:31:47+00:00 · Latest: 2026-01-06T07:31:47+00:00</div>
<div class="meta-line">Comments: 31 pages, technical report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02780v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02780v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present MiMo-V2-Flash, a Mixture-of-Experts (MoE) model with 309B total parameters and 15B active parameters, designed for fast, strong reasoning and agentic capabilities. MiMo-V2-Flash adopts a hybrid attention architecture that interleaves Sliding Window Attention (SWA) with global attention, with a 128-token sliding window under a 5:1 hybrid ratio. The model is pre-trained on 27 trillion tokens with Multi-Token Prediction (MTP), employing a native 32k context length and subsequently extended to 256k. To efficiently scale post-training compute, MiMo-V2-Flash introduces a novel Multi-Teacher On-Policy Distillation (MOPD) paradigm. In this framework, domain-specialized teachers (e.g., trained via large-scale reinforcement learning) provide dense and token-level reward, enabling the student model to perfectly master teacher expertise. MiMo-V2-Flash rivals top-tier open-weight models such as DeepSeek-V3.2 and Kimi-K2, despite using only 1/2 and 1/3 of their total parameters, respectively. During inference, by repurposing MTP as a draft model for speculative decoding, MiMo-V2-Flash achieves up to 3.6 acceptance length and 2.6x decoding speedup with three MTP layers. We open-source both the model weights and the three-layer MTP weights to foster open research and community collaboration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MiMo-V2-Flash技术报告</div>
<div class="mono" style="margin-top:8px">我们提出MiMo-V2-Flash，这是一个总参数量为3090亿、激活参数量为150亿的专家混合模型，专为快速、强大的推理与智能体能力而设计。该模型采用混合注意力架构，以5:1的混合比例将滑动窗口注意力与全局注意力交错结合，滑动窗口大小为128个词元。模型通过多词元预测在27万亿词元上进行预训练，使用原生32k上下文长度并后续扩展至256k。为高效扩展训练后计算，MiMo-V2-Flash引入了新颖的多教师同策略蒸馏范式，其中领域专业化教师通过密集的词元级奖励指导学生模型完全掌握其专业知识。尽管总参数量分别仅为DeepSeek-V3.2和Kimi-K2的1/2和1/3，MiMo-V2-Flash仍能媲美这些顶尖开源模型。在推理阶段，通过将多词元预测重用作推测解码的草稿模型，配合三层多词元预测结构可实现最高3.6的接受长度和2.6倍的解码加速。我们开源了模型权重及三层多词元预测权重，以促进开放研究与社区协作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient large-scale models with strong reasoning and agentic capabilities, this paper introduces MiMo-V2-Flash, a 309B parameter Mixture-of-Experts model with 15B active parameters. The method employs a hybrid attention architecture combining Sliding Window and global attention, pre-trains on 27T tokens with Multi-Token Prediction (MTP), and uses a novel Multi-Teacher On-Policy Distillation (MOPD) for efficient post-training. Key experimental results show the model rivals top open-weight models like DeepSeek-V3.2 with far fewer total parameters and, by repurposing MTP for speculative decoding, achieves up to a 2.6x decoding speedup.</div>
<div class="mono" style="margin-top:8px">本研究旨在开发具有强大推理与智能体能力的高效大规模模型，提出了MiMo-V2-Flash，这是一个总参数量为3090亿、激活参数量为150亿的混合专家模型。方法上采用滑动窗口与全局注意力混合的架构，基于27000亿token进行多令牌预测预训练，并引入新颖的多教师策略蒸馏范式进行高效后训练。主要实验结果表明，该模型在总参数量仅为同类顶尖开源模型一半或三分之一的情况下，性能可与之媲美，并且通过将多令牌预测重用作推测解码的草稿模型，实现了最高2.6倍的解码加速。</div>
</details>
</div>
<div class="card">
<div class="title">PsychEval: A Multi-Session and Multi-Therapy Benchmark for High-Realism AI Psychological Counselor</div>
<div class="meta-line">Authors: Qianjun Pan, Junyi Wang, Jie Zhou, Yutao Yang, Junsong Li, Kaiyin Xu, Yougen Zhou, Yihan Li, Jingyuan Zhao, Qin Chen, Ningning Zhou, Kai Chen, Liang He</div>
<div class="meta-line">First: 2026-01-05T05:26:57+00:00 · Latest: 2026-01-06T07:31:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01802v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.01802v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To develop a reliable AI for psychological assessment, we introduce \texttt{PsychEval}, a multi-session, multi-therapy, and highly realistic benchmark designed to address three key challenges: \textbf{1) Can we train a highly realistic AI counselor?} Realistic counseling is a longitudinal task requiring sustained memory and dynamic goal tracking. We propose a multi-session benchmark (spanning 6-10 sessions across three distinct stages) that demands critical capabilities such as memory continuity, adaptive reasoning, and longitudinal planning. The dataset is annotated with extensive professional skills, comprising over 677 meta-skills and 4577 atomic skills. \textbf{2) How to train a multi-therapy AI counselor?} While existing models often focus on a single therapy, complex cases frequently require flexible strategies among various therapies. We construct a diverse dataset covering five therapeutic modalities (Psychodynamic, Behaviorism, CBT, Humanistic Existentialist, and Postmodernist) alongside an integrative therapy with a unified three-stage clinical framework across six core psychological topics. \textbf{3) How to systematically evaluate an AI counselor?} We establish a holistic evaluation framework with 18 therapy-specific and therapy-shared metrics across Client-Level and Counselor-Level dimensions. To support this, we also construct over 2,000 diverse client profiles. Extensive experimental analysis fully validates the superior quality and clinical fidelity of our dataset. Crucially, \texttt{PsychEval} transcends static benchmarking to serve as a high-fidelity reinforcement learning environment that enables the self-evolutionary training of clinically responsible and adaptive AI counselors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PsychEval：面向高拟真AI心理咨询师的多会话多疗法基准测试</div>
<div class="mono" style="margin-top:8px">为开发可靠的心理评估AI，我们推出\texttt{PsychEval}——一个多会话、多疗法、高拟真度的基准测试，旨在解决三大核心挑战：\textbf{1) 能否训练出高拟真AI咨询师？}真实心理咨询是需持续记忆与动态目标追踪的纵向任务。我们构建了跨三个独立阶段（6-10次会话）的多会话基准，要求模型具备记忆连续性、适应性推理与长期规划等关键能力。数据集标注了涵盖677项元技能与4577项原子技能的完整专业体系。\textbf{2) 如何训练多疗法AI咨询师？}现有模型多聚焦单一疗法，而复杂案例常需跨疗法灵活策略。我们构建了涵盖五大疗法流派（心理动力学、行为主义、认知行为疗法、人本存在主义、后现代主义）的数据集，并基于六类核心心理议题建立了整合疗法的三阶段临床框架。\textbf{3) 如何系统评估AI咨询师？}我们建立了包含来访者与咨询师双维度、18项疗法专用及通用指标的全方位评估体系，并配套构建了2000余个多样化来访者画像。大量实验分析充分验证了数据集的高质量与临床保真度。\texttt{PsychEval}更超越了静态基准测试，可作为高保真强化学习环境，支持临床责任性与适应性AI咨询师的自我进化训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for reliable AI in psychological assessment, this paper introduces PsychEval, a benchmark designed to train and evaluate highly realistic, multi-therapy AI counselors. The method involves constructing a multi-session dataset spanning 6-10 sessions across three clinical stages, annotated with over 677 meta-skills and 4577 atomic skills, and covering five therapeutic modalities within a unified framework to enable adaptive reasoning and longitudinal planning. Main experimental results from extensive analysis validate the dataset&#x27;s superior quality and clinical fidelity, and the benchmark serves as a reinforcement learning environment for training self-evolutionary AI counselors, as assessed through a holistic framework of 18 therapy-specific and shared metrics.</div>
<div class="mono" style="margin-top:8px">为开发可靠的心理评估人工智能，本文提出了PsychEval基准，旨在训练和评估高真实性、多疗法的AI心理咨询师。其方法构建了一个多会话数据集，涵盖6-10次会话和三个临床阶段，标注了超过677项元技能和4577项原子技能，并整合了五种治疗模式于统一框架中，以支持适应性推理和纵向规划。主要实验结果表明，广泛分析验证了数据集的高质量和临床保真度，该基准作为一个强化学习环境可用于训练自我进化的AI咨询师，并通过包含18项疗法特定和共享指标的整体评估框架进行系统评价。</div>
</details>
</div>
<div class="card">
<div class="title">Closing the Reality Gap: Zero-Shot Sim-to-Real Deployment for Dexterous Force-Based Grasping and Manipulation</div>
<div class="meta-line">Authors: Haoyu Dong, Zhengmao He, Yang Li, Zhibin Li, Xinyu Yi, Zhe Zhao</div>
<div class="meta-line">First: 2026-01-06T07:26:39+00:00 · Latest: 2026-01-06T07:26:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02778v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02778v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human-like dexterous hands with multiple fingers offer human-level manipulation capabilities, but training control policies that can directly deploy on real hardware remains difficult due to contact-rich physics and imperfect actuation. We close this gap with a practical sim-to-real reinforcement learning (RL) framework that utilizes dense tactile feedback combined with joint torque sensing to explicitly regulate physical interactions. To enable effective sim-to-real transfer, we introduce (i) a computationally fast tactile simulation that computes distances between dense virtual tactile units and the object via parallel forward kinematics, providing high-rate, high-resolution touch signals needed by RL; (ii) a current-to-torque calibration that eliminates the need for torque sensors on dexterous hands by mapping motor current to joint torque; and (iii) actuator dynamics modeling to bridge the actuation gaps with randomization of non-ideal effects such as backlash, torque-speed saturation. Using an asymmetric actor-critic PPO pipeline trained entirely in simulation, our policies deploy directly to a five-finger hand. The resulting policies demonstrated two essential skills: (1) command-based, controllable grasp force tracking, and (2) reorientation of objects in the hand, both of which were robustly executed without fine-tuning on the robot. By combining tactile and torque in the observation space with effective sensing/actuation modeling, our system provides a practical solution to achieve reliable dexterous manipulation. To our knowledge, this is the first demonstration of controllable grasping on a multi-finger dexterous hand trained entirely in simulation and transferred zero-shot on real hardware.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>弥合现实鸿沟：基于力控的灵巧抓取与操作零样本仿真到真实部署</div>
<div class="mono" style="margin-top:8px">多指仿人灵巧手具备人类水平的操作能力，但由于密集接触物理特性与不完美的驱动机制，训练能直接部署于真实硬件的控制策略仍具挑战。本研究提出一种实用的仿真到真实强化学习框架，通过融合密集触觉反馈与关节力矩传感来显式调控物理交互。为实现有效的仿真到真实迁移，我们引入：（1）基于并行正向运动学的快速触觉仿真，通过计算密集虚拟触觉单元与物体的距离，提供强化学习所需的高频高分辨率触觉信号；（2）电流-力矩标定方法，通过映射电机电流至关节力矩，消除灵巧手对力矩传感器的依赖；（3）驱动器动力学建模，通过随机化齿隙、力矩-转速饱和等非理想效应来弥合驱动差异。采用完全在仿真中训练的非对称行动者-评论者PPO框架，所得策略可直接部署于五指灵巧手。实验证明该策略具备两项核心能力：（1）基于指令的可控抓握力跟踪；（2）手内物体重定向，二者均无需在真实机器人上微调即可鲁棒执行。通过将触觉与力矩感知融入观测空间，并结合有效的传感/驱动建模，本系统为实现可靠的灵巧操作提供了实用方案。据我们所知，这是首个完全在仿真中训练、并零样本迁移至真实硬件的多指灵巧手可控抓取实证研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of deploying dexterous manipulation policies trained in simulation directly to real multi-fingered robotic hands, a task hindered by complex contact physics and hardware imperfections. The proposed method introduces a sim-to-real reinforcement learning framework that leverages dense tactile feedback and joint torque sensing, supported by three key innovations: a fast tactile simulator for high-resolution touch signals, a current-to-torque calibration to estimate joint torque without dedicated sensors, and actuator dynamics modeling with randomization to bridge the actuation reality gap. Experimental results on a five-finger hand demonstrate that policies trained entirely in simulation can be deployed zero-shot to robustly perform controllable grasp force tracking and object reorientation, without any real-world fine-tuning.</div>
<div class="mono" style="margin-top:8px">本文旨在解决将在仿真中训练的多指灵巧手操控策略直接部署到真实硬件上的难题，该任务因复杂的接触物理和硬件缺陷而充满挑战。所提出的方法引入了一个仿真到现实的强化学习框架，利用密集触觉反馈和关节扭矩感知，并辅以三项关键技术：用于高分辨率触觉信号的快速触觉模拟器、无需专用扭矩传感器的电流-扭矩校准，以及通过随机化建模执行器动力学以弥合驱动差距。在五指手上的实验结果表明，完全在仿真中训练的策略可以零样本部署，稳健地执行可控抓握力跟踪和物体重定向，无需任何真实世界的微调。</div>
</details>
</div>
<div class="card">
<div class="title">DiRL: An Efficient Post-Training Framework for Diffusion Language Models</div>
<div class="meta-line">Authors: Ying Zhu, Jiaxin Wan, Xiaoran Liu, Siyang He, Qiqi Wang, Xu Guo, Tianyi Liang, Zengfeng Huang, Ziwei He, Xipeng Qiu</div>
<div class="meta-line">First: 2025-12-23T08:33:19+00:00 · Latest: 2026-01-06T06:47:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22234v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.22234v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Language Models (dLLMs) have emerged as promising alternatives to Auto-Regressive (AR) models. While recent efforts have validated their pre-training potential and accelerated inference speeds, the post-training landscape for dLLMs remains underdeveloped. Existing methods suffer from computational inefficiency and objective mismatches between training and inference, severely limiting performance on complex reasoning tasks such as mathematics. To address this, we introduce DiRL, an efficient post-training framework that tightly integrates FlexAttention-accelerated blockwise training with LMDeploy-optimized inference. This architecture enables a streamlined online model update loop, facilitating efficient two-stage post-training (Supervised Fine-Tuning followed by Reinforcement Learning). Building on this framework, we propose DiPO, the first unbiased Group Relative Policy Optimization (GRPO) implementation tailored for dLLMs. We validate our approach by training DiRL-8B-Instruct on high-quality math data. Our model achieves state-of-the-art math performance among dLLMs and surpasses comparable models in the Qwen2.5 series on several benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiRL：一种高效的扩散语言模型后训练框架</div>
<div class="mono" style="margin-top:8px">扩散语言模型已成为自回归模型的有力替代方案。尽管近期研究验证了其预训练潜力并提升了推理速度，但扩散语言模型的后训练体系仍不成熟。现有方法存在计算效率低下、训练与推理目标不匹配等问题，严重限制了在数学等复杂推理任务上的性能。为此，我们提出DiRL框架，通过紧密整合FlexAttention加速的分块训练与LMDeploy优化的推理架构，构建了高效的在线模型更新循环，支持监督微调与强化学习两阶段后训练流程。基于该框架，我们进一步提出DiPO——首个专为扩散语言模型设计的无偏分组相对策略优化实现。通过在高质量数学数据上训练DiRL-8B-Instruct模型，实验表明该模型在扩散语言模型中取得最先进的数学推理性能，并在多项基准测试中超越同规模Qwen2.5系列模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the underdeveloped post-training landscape for Diffusion Language Models (dLLMs), which suffer from computational inefficiency and objective mismatches that limit performance on complex reasoning tasks like mathematics. The method introduces DiRL, an efficient post-training framework that integrates FlexAttention-accelerated blockwise training with LMDeploy-optimized inference to enable a streamlined online update loop for two-stage post-training (Supervised Fine-Tuning and Reinforcement Learning), and proposes DiPO, a tailored unbiased Group Relative Policy Optimization for dLLMs. The main experimental results show that the trained DiRL-8B-Instruct model achieves state-of-the-art math performance among dLLMs and surpasses comparable models in the Qwen2.5 series on several benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于扩散语言模型（dLLMs）的后训练领域尚不成熟，现有方法存在计算效率低下及训练与推理目标不匹配的问题，严重限制了其在数学等复杂推理任务上的性能。方法上提出了DiRL，一个高效的后训练框架，通过整合FlexAttention加速的块状训练与LMDeploy优化的推理，实现了简化的在线模型更新循环，以支持高效的两阶段后训练（监督微调与强化学习），并提出了首个为dLLMs定制的无偏分组相对策略优化方法DiPO。主要实验结果表明，基于高质量数学数据训练的DiRL-8B-Instruct模型在dLLMs中取得了最先进的数学性能，并在多个基准测试中超越了Qwen2.5系列的可比模型。</div>
</details>
</div>
<div class="card">
<div class="title">Q-Regularized Generative Auto-Bidding: From Suboptimal Trajectories to Optimal Policies</div>
<div class="meta-line">Authors: Mingming Zhang, Na Li, Zhuang Feiqing, Hongyang Zheng, Jiangbing Zhou, Wang Wuyin, Sheng-jie Sun, XiaoWei Chen, Junxiong Zhu, Lixin Zou, Chenliang Li</div>
<div class="meta-line">First: 2026-01-06T06:42:25+00:00 · Latest: 2026-01-06T06:42:25+00:00</div>
<div class="meta-line">Comments: 11pages, 5figures, In Proceedings of the 32nd ACM SIGKDD Conference on Knowledge Discovery and Data Mining</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02754v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02754v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rapid development of e-commerce, auto-bidding has become a key asset in optimizing advertising performance under diverse advertiser environments. The current approaches focus on reinforcement learning (RL) and generative models. These efforts imitate offline historical behaviors by utilizing a complex structure with expensive hyperparameter tuning. The suboptimal trajectories further exacerbate the difficulty of policy learning.
  To address these challenges, we proposes QGA, a novel Q-value regularized Generative Auto-bidding method. In QGA, we propose to plug a Q-value regularization with double Q-learning strategy into the Decision Transformer backbone. This design enables joint optimization of policy imitation and action-value maximization, allowing the learned bidding policy to both leverage experience from the dataset and alleviate the adverse impact of the suboptimal trajectories. Furthermore, to safely explore the policy space beyond the data distribution, we propose a Q-value guided dual-exploration mechanism, in which the DT model is conditioned on multiple return-to-go targets and locally perturbed actions. This entire exploration process is dynamically guided by the aforementioned Q-value module, which provides principled evaluation for each candidate action. Experiments on public benchmarks and simulation environments demonstrate that QGA consistently achieves superior or highly competitive results compared to existing alternatives. Notably, in large-scale real-world A/B testing, QGA achieves a 3.27% increase in Ad GMV and a 2.49% improvement in Ad ROI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Q正则化生成式自动出价：从次优轨迹到最优策略</div>
<div class="mono" style="margin-top:8px">随着电子商务的快速发展，自动出价已成为优化多样化广告主环境下广告效果的关键技术。现有方法主要聚焦于强化学习与生成模型，通过复杂结构和昂贵的超参数调优来模仿离线历史行为，而次优轨迹进一步加剧了策略学习的难度。为应对这些挑战，本文提出QGA——一种新颖的Q值正则化生成式自动出价方法。该方法将基于双重Q学习的Q值正则化模块嵌入决策变换器主干网络，实现策略模仿与动作价值最大化的联合优化，使习得的出价策略既能利用数据集经验，又能缓解次优轨迹的负面影响。此外，为安全探索数据分布之外的策略空间，我们提出Q值引导的双重探索机制：通过多目标回报条件约束与局部扰动动作，使整个探索过程受前述Q值模块动态引导，为候选动作提供原则性评估。在公开基准与仿真环境的实验表明，QGA相较现有方法始终取得更优或极具竞争力的结果。值得注意的是，在大规模真实场景A/B测试中，QGA实现了广告交易总额3.27%的提升与广告投资回报率2.49%的改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenges in auto-bidding where existing reinforcement learning and generative models rely on suboptimal historical data and require complex tuning, this paper proposes QGA, a Q-value regularized generative auto-bidding method. The method integrates a Q-value regularization with double Q-learning into a Decision Transformer backbone to jointly optimize policy imitation and action-value maximization, mitigating the impact of suboptimal trajectories, and employs a Q-value guided dual-exploration mechanism for safe policy exploration beyond the data distribution. Experimental results on benchmarks and simulations show QGA achieves superior or competitive performance, with large-scale real-world A/B testing demonstrating a 3.27% increase in Ad GMV and a 2.49% improvement in Ad ROI.</div>
<div class="mono" style="margin-top:8px">针对自动出价中现有强化学习和生成模型依赖次优历史数据且需复杂调参的挑战，本文提出了QGA，一种Q值正则化的生成式自动出价方法。该方法将Q值正则化与双重Q学习集成到决策变换器主干中，以联合优化策略模仿和动作价值最大化，减轻次优轨迹的影响，并采用Q值引导的双重探索机制，在数据分布之外进行安全的策略探索。在基准测试和模拟环境中的实验结果表明，QGA取得了优于或极具竞争力的性能，大规模真实世界A/B测试显示广告商品交易总额提升了3.27%，广告投资回报率提高了2.49%。</div>
</details>
</div>
<div class="card">
<div class="title">Time-Scaling Is What Agents Need Now</div>
<div class="meta-line">Authors: Zhi Liu, Guangzhi Wang</div>
<div class="meta-line">First: 2026-01-06T05:01:17+00:00 · Latest: 2026-01-06T05:01:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02714v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02714v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Early artificial intelligence paradigms exhibited separated cognitive functions: Neural Networks focused on &quot;perception-representation,&quot; Reinforcement Learning on &quot;decision-making-behavior,&quot; and Symbolic AI on &quot;knowledge-reasoning.&quot; With Transformer-based large models and world models, these paradigms are converging into cognitive agents with closed-loop &quot;perception-decision-action&quot; capabilities.
  Humans solve complex problems under limited cognitive resources through temporalized sequential reasoning. Language relies on problem space search for deep semantic reasoning. While early large language models (LLMs) could generate fluent text, they lacked robust semantic reasoning capabilities. Prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) extended reasoning paths by making intermediate steps explicit. Recent models like DeepSeek-R1 enhanced performance through explicit reasoning trajectories. However, these methods have limitations in search completeness and efficiency.
  This highlights the need for &quot;Time-Scaling&quot;--the systematic extension and optimization of an agent&#x27;s ability to unfold reasoning over time. Time-Scaling refers to architectural design utilizing extended temporal pathways, enabling deeper problem space exploration, dynamic strategy adjustment, and enhanced metacognitive control, paralleling human sequential reasoning under cognitive constraints. It represents a critical frontier for enhancing deep reasoning and problem-solving without proportional increases in static model parameters. Advancing intelligent agent capabilities requires placing Time-Scaling principles at the forefront, positioning explicit temporal reasoning management as foundational.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>时间扩展是智能体当前所需的核心能力</div>
<div class="mono" style="margin-top:8px">早期人工智能范式呈现认知功能分离：神经网络侧重“感知-表征”，强化学习专注“决策-行为”，符号人工智能聚焦“知识-推理”。随着基于Transformer的大模型与世界模型的发展，这些范式正融合为具备闭环“感知-决策-行动”能力的认知智能体。人类通过时序化递进推理在有限认知资源下解决复杂问题，语言则依赖问题空间搜索实现深度语义推理。早期大语言模型虽能生成流畅文本，却缺乏稳健的语义推理能力。思维链、思维树等提示技术通过显式化中间步骤延伸推理路径，而DeepSeek-R1等近期模型通过显式推理轨迹提升性能，但这些方法在搜索完备性与效率上仍存局限。这凸显了“时间扩展”的必要性——系统性地扩展与优化智能体随时间展开推理的能力。时间扩展指利用延伸时间通路的架构设计，实现更深层问题空间探索、动态策略调整与强化元认知控制，模拟人类在认知约束下的序列推理机制。它代表着在不线性增加静态模型参数的前提下，增强深度推理与问题解决能力的关键前沿。推进智能体能力需将时间扩展原则置于核心，确立显式时序推理管理的基础性地位。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the convergence of previously separate AI paradigms into cognitive agents with closed-loop perception-decision-action capabilities, highlighting that while prompting techniques have improved reasoning, they remain limited in search completeness and efficiency. The proposed method is &#x27;Time-Scaling,&#x27; defined as an architectural design principle that systematically extends an agent&#x27;s ability to unfold reasoning over extended temporal pathways, enabling deeper problem space exploration and dynamic strategy adjustment akin to human sequential reasoning. The main experimental insight, drawn from analyzing recent models like DeepSeek-R1, is that Time-Scaling enhances deep reasoning and problem-solving performance without requiring proportional increases in static model parameters, positioning it as a critical frontier for advancing intelligent agents.</div>
<div class="mono" style="margin-top:8px">本文的动机在于以往分离的人工智能范式正汇聚为具有感知-决策-行动闭环能力的认知智能体，尽管思维链等提示技术提升了推理能力，但其搜索完备性与效率仍存局限。其提出的方法是“时间缩放”，即一种利用延长时间路径的架构设计原则，系统扩展智能体随时间展开推理的能力，以实现更深层问题空间探索和动态策略调整，类比人类在认知约束下的序列推理。主要实验结果表明，通过分析DeepSeek-R1等近期模型，时间缩放能够在不显著增加静态模型参数的情况下，增强深度推理和问题解决性能，这使其成为提升智能体能力的关键前沿方向。</div>
</details>
</div>
<div class="card">
<div class="title">TextBO: Bayesian Optimization in Language Space for Eval-Efficient Self-Improving AI</div>
<div class="meta-line">Authors: Enoch Hyunwook Kang, Hema Yoganarasimhan</div>
<div class="meta-line">First: 2025-11-15T07:04:44+00:00 · Latest: 2026-01-06T03:54:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12063v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.12063v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have enabled self-improving AI systems that iteratively generate, evaluate, and refine their outcomes. Recent studies show that prompt-optimization-based self-improvement can outperform state-of-the-art reinforcement-learning fine-tuning of LLMs, but performance is typically measured by generation efficiency. However, in many applications, the constraint is evaluation efficiency: obtaining reliable feedback is far more costly than generating candidates. To optimize for evaluation efficiency, we extend Upper Confidence Bound-Bayesian Optimization (UCB-BO), a framework known for optimal evaluation-efficiency guarantees, to the language domain. Doing so is challenging for two reasons: (i) gradients needed for UCB-BO are ill-defined in discrete prompt space; and (ii) UCB-style exploration relies on a surrogate model and acquisition function, which only live implicitly in the LLM. We overcome these challenges by proving that combining simple textual gradients (LLM-proposed local edits) with the Best-of-N selection strategy statistically emulates ascent along the gradient of the canonical UCB acquisition function. Based on this result, we propose TextBO, a simple, evaluation-efficient self-improving algorithm that operates purely in language space without explicit surrogates or calibrated uncertainty models. We empirically validate TextBO on automated ad-alignment tasks using a persona-induced preference distribution, demonstrating superior performance per evaluation compared to strong baselines such as Best-of-N and GEPA. We also evaluate TextBO&#x27;s Best-of-N multi-step textual-gradient mechanism on agentic AI benchmarks by augmenting GEPA with it and show that it significantly outperforms standard GEPA. In sum, TextBO is a simple and principled framework for AI self-improving system design that bridges prompt optimization with classical Bayesian optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TextBO：面向评估高效自改进AI的语言空间贝叶斯优化</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）催生了能够迭代生成、评估与优化输出的自改进AI系统。近期研究表明，基于提示优化的自改进方法可超越最先进的LLM强化学习微调技术，但现有研究多关注生成效率的评估。然而在实际应用中，评估效率往往成为关键瓶颈：获取可靠反馈的成本远高于生成候选方案。为优化评估效率，本研究将具有最优评估效率保证的理论框架——上置信界贝叶斯优化（UCB-BO）拓展至语言领域。该拓展面临双重挑战：（1）离散提示空间中UCB-BO所需的梯度定义不明确；（2）UCB式探索依赖的代理模型与采集函数仅隐式存在于LLM中。我们通过理论证明：将简单文本梯度（LLM提出的局部编辑）与N选最佳策略相结合，可在统计学意义上模拟经典UCB采集函数梯度的上升轨迹。基于此，提出TextBO算法——一种完全在语言空间运行、无需显式代理模型或校准不确定性模型的评估高效自改进算法。在基于人格诱导偏好分布的自动广告对齐任务中，TextBO相比N选最佳、GEPA等基线方法展现出更优的单次评估性能。通过将TextBO的N选最佳多步文本梯度机制与GEPA结合，在智能体基准测试中显著超越标准GEPA。综上，TextBO为AI自改进系统设计提供了简洁而严谨的框架，实现了提示优化与经典贝叶斯优化的理论融合。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of evaluation efficiency in self-improving AI systems, where obtaining reliable feedback is often more costly than generating candidate solutions. To optimize for this, the authors propose TextBO, a method that extends Upper Confidence Bound-Bayesian Optimization (UCB-BO) to the language domain by using simple textual gradients from LLMs and a Best-of-N selection strategy, which statistically emulates gradient ascent on the UCB acquisition function without requiring explicit surrogate models. Experimental results on automated ad-alignment tasks show that TextBO outperforms baselines like Best-of-N and GEPA in performance per evaluation, and when augmented with its multi-step textual-gradient mechanism, it also significantly improves performance on agentic AI benchmarks compared to standard GEPA.</div>
<div class="mono" style="margin-top:8px">本文针对自改进AI系统中评估效率的挑战提出解决方案，其中获取可靠反馈的成本通常高于生成候选方案。为优化此问题，作者提出了TextBO方法，通过利用大语言模型生成的简单文本梯度和Best-of-N选择策略，将上置信界贝叶斯优化（UCB-BO）扩展至语言领域，该方法在统计上模拟了UCB采集函数的梯度上升过程，无需显式代理模型。在自动化广告对齐任务上的实验结果表明，TextBO在每次评估的性能上优于Best-of-N和GEPA等基线方法，且当其多步文本梯度机制应用于GEPA时，在智能体AI基准测试中也显著超越了标准GEPA的性能。</div>
</details>
</div>
<div class="card">
<div class="title">SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents</div>
<div class="meta-line">Authors: Shaofei Cai, Yulei Qin, Haojia Lin, Zihan Xu, Gang Li, Yuchen Shi, Zongyi Li, Yong Mao, Siqi Cai, Xiaoyu Tan, Yitao Liang, Ke Li, Xing Sun</div>
<div class="meta-line">First: 2025-12-26T14:51:39+00:00 · Latest: 2026-01-06T03:31:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22322v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.22322v2">PDF</a> · <a href="https://github.com/TencentYoutuResearch/SmartSnap">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent&#x27;s entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B. Code is available at: https://github.com/TencentYoutuResearch/SmartSnap</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SmartSnap：自验证智能体的主动证据搜寻机制</div>
<div class="mono" style="margin-top:8px">智能体强化学习在复杂GUI任务中展现出构建自主智能体的巨大潜力，但其可扩展性始终受限于任务完成验证环节。现有验证方法属于被动的事后处理模式：验证器（如基于规则的评分脚本、奖励/评判模型、LLM-as-a-Judge）通过分析智能体完整交互轨迹判断任务成败。这种处理包含无关噪声历史的冗长上下文方式，给验证协议带来挑战，导致高昂成本与低可靠性。为突破此瓶颈，我们提出SmartSnap范式，将被动事后验证转变为智能体主动的现场自验证。我们设计的新型自验证智能体具备双重使命：既完成任务，又通过精选快照证据证明完成度。遵循我们提出的3C原则（完整性、简洁性、创造性），该智能体利用在线环境可访问性，对最小化决定性快照集进行自验证。这些证据将作为通用LLM-as-a-Judge验证器的唯一判定材料。跨模型系列与规模的移动端任务实验表明，SmartSnap范式能以可扩展方式训练LLM驱动智能体，为80亿和300亿参数模型分别带来26.08%和16.66%的性能提升。解决方案寻找与证据搜寻的协同作用，培育出性能可对标DeepSeek V3.1与Qwen3-235B-A22B的高效自验证智能体。代码已开源：https://github.com/TencentYoutuResearch/SmartSnap</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the scalability bottleneck in agentic reinforcement learning caused by inefficient post-hoc task verification, which processes verbose and noisy interaction histories, leading to high costs and low reliability. To overcome this, the authors propose SmartSnap, a paradigm shift to proactive self-verification where agents are designed with dual missions: completing tasks and proving accomplishment by curating minimal, decisive snapshot evidences guided by Completeness, Conciseness, and Creativity principles. Experimental results on mobile tasks show that SmartSnap enables scalable training of LLM-driven agents, achieving performance gains of up to 26.08% and 16.66% for 8B and 30B models, respectively, and competitive performance against larger models like DeepSeek V3.1 and Qwen3-235B-A22B.</div>
<div class="mono" style="margin-top:8px">本文针对智能体强化学习中因低效的事后任务验证导致的扩展性瓶颈问题，该验证需处理冗长嘈杂的交互历史，成本高昂且可靠性低。为此，作者提出SmartSnap范式，将被动验证转变为主动自验证，设计具有双重使命的自验证智能体：在完成任务的同时，依据完整性、简洁性和创造性原则，从在线环境中筛选最小化决定性快照作为证据来证明任务完成。在移动任务上的实验表明，SmartSnap能可扩展地训练大语言模型驱动的智能体，使8B和30B模型性能分别提升高达26.08%和16.66%，并与DeepSeek V3.1和Qwen3-235B-A22B等更大模型竞争。</div>
</details>
</div>
<div class="card">
<div class="title">PC2P: Multi-Agent Path Finding via Personalized-Enhanced Communication and Crowd Perception</div>
<div class="meta-line">Authors: Guotao Li, Shaoyun Xu, Yuexing Hao, Yang Wang, Yuhui Sun</div>
<div class="meta-line">Venue: IROS 2025</div>
<div class="meta-line">First: 2026-01-06T03:11:26+00:00 · Latest: 2026-01-06T03:11:26+00:00</div>
<div class="meta-line">Comments: 8 pages,7 figures,3 tables,Accepted to IROS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03301v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03301v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Distributed Multi-Agent Path Finding (MAPF) integrated with Multi-Agent Reinforcement Learning (MARL) has emerged as a prominent research focus, enabling real-time cooperative decision-making in partially observable environments through inter-agent communication. However, due to insufficient collaborative and perceptual capabilities, existing methods are inadequate for scaling across diverse environmental conditions. To address these challenges, we propose PC2P, a novel distributed MAPF method derived from a Q-learning-based MARL framework. Initially, we introduce a personalized-enhanced communication mechanism based on dynamic graph topology, which ascertains the core aspects of ``who&quot; and ``what&quot; in interactive process through three-stage operations: selection, generation, and aggregation. Concurrently, we incorporate local crowd perception to enrich agents&#x27; heuristic observation, thereby strengthening the model&#x27;s guidance for effective actions via the integration of static spatial constraints and dynamic occupancy changes. To resolve extreme deadlock issues, we propose a region-based deadlock-breaking strategy that leverages expert guidance to implement efficient coordination within confined areas. Experimental results demonstrate that PC2P achieves superior performance compared to state-of-the-art distributed MAPF methods in varied environments. Ablation studies further confirm the effectiveness of each module for overall performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PC2P：基于个性化增强通信与群体感知的多智能体路径规划</div>
<div class="mono" style="margin-top:8px">结合多智能体强化学习的分布式多智能体路径规划已成为重要研究方向，通过智能体间通信实现部分可观测环境下的实时协同决策。然而，现有方法因协作与感知能力不足，难以适应多样化环境条件。为此，我们提出PC2P——一种基于Q学习多智能体强化学习框架的新型分布式路径规划方法。首先，我们设计了基于动态图拓扑的个性化增强通信机制，通过选择、生成、聚合三阶段操作明确交互过程中的&#x27;与谁通信&#x27;及&#x27;传递何种信息&#x27;核心问题。同时，引入局部群体感知以增强智能体的启发式观测，通过融合静态空间约束与动态占用变化强化模型对有效行动的引导能力。针对极端死锁问题，提出基于区域的死锁解除策略，利用专家指导在受限区域内实现高效协同。实验表明，PC2P在多种环境中均优于当前最先进的分布式多智能体路径规划方法。消融研究进一步验证了各模块对整体性能的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the limitations of existing distributed Multi-Agent Path Finding (MAPF) methods, which struggle with insufficient collaboration and perception when scaling across diverse environments. The proposed method, PC2P, introduces a personalized-enhanced communication mechanism using dynamic graph topology to determine interaction partners and content, alongside local crowd perception that integrates static spatial constraints and dynamic occupancy changes to guide agent actions; it also employs a region-based deadlock-breaking strategy with expert guidance. Experimental results show that PC2P outperforms state-of-the-art distributed MAPF methods in varied settings, with ablation studies confirming the contribution of each module.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有分布式多智能体路径规划方法在多样化环境中扩展时协作与感知能力不足的问题。提出的PC2P方法采用基于动态图拓扑的个性化增强通信机制，通过选择、生成和聚合三阶段确定交互对象与内容，并结合局部人群感知，整合静态空间约束与动态占用变化以指导智能体行动；同时引入基于区域的死锁解除策略，利用专家指导实现受限区域内的高效协调。实验结果表明，PC2P在多种环境中优于当前最先进的分布式MAPF方法，消融研究进一步验证了各模块对整体性能的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in Inference-time Scaling?</div>
<div class="meta-line">Authors: Mingyuan Wu, Meitang Li, Jingcheng Yang, Jize Jiang, Kaizhuo Yan, Zhaoheng Li, Hanchao Yu, Minjia Zhang, Klara Nahrstedt</div>
<div class="meta-line">Venue: Neurips 2025 Oral</div>
<div class="meta-line">First: 2025-06-20T18:23:48+00:00 · Latest: 2026-01-06T03:04:42+00:00</div>
<div class="meta-line">Comments: Neurips 2025 Multimodal Algorithmic Reasoning Workshop Oral. In submission</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.17417v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.17417v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Inference time techniques such as decoding time scaling and self refinement have been shown to substantially improve mathematical reasoning in large language models (LLMs), largely attributed to emergent self correction and self verification behaviors often elicited through reinforcement learning (RL). In this work, we ask whether the same recipe transfers to vision language models (VLMs), especially RL finetuned variants that claim strong visual mathematical reasoning.
  Through extensive evaluation, we reach three main findings that differ markedly from text only models. First, generation time capability matters more than verification and refinement: simple majority voting consistently and substantially outperforms verification centric strategies such as best of N with self verification. Second, behaviors often associated with RL tuned models at inference time, such as the &#x27;Aha moment,&#x27; do not yield reliable reasoning performance improvements. Third, visual information is not effectively integrated into the model&#x27;s self verification process.
  Overall, our analysis highlights a key limitation: current RL trained VLMs derive limited benefit from self verification in the visual modality, which constrains the effectiveness of inference time scaling for visual mathematical reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>再探‘顿悟时刻’：视觉语言模型在推理时扩展中是否真正具备自我验证能力？</div>
<div class="mono" style="margin-top:8px">解码时扩展与自我精炼等推理时技术已被证明能显著提升大语言模型的数学推理能力，这主要归因于通过强化学习引发的涌现性自我校正与自我验证行为。本研究探讨该机制是否适用于视觉语言模型，尤其是声称具备强大视觉数学推理能力的强化学习微调变体。通过广泛评估，我们得出与纯文本模型截然不同的三个主要发现：第一，生成时能力比验证与精炼更重要——简单的多数投票法持续且显著优于以自我验证为核心的‘最佳N选一’等策略；第二，推理时常与强化学习调优模型关联的行为（如‘顿悟时刻’）并未带来可靠的推理性能提升；第三，视觉信息未能有效融入模型的自我验证过程。总体而言，我们的分析揭示了一个关键局限：当前基于强化学习训练的视觉语言模型在视觉模态中从自我验证获得的收益有限，这制约了推理时扩展对视觉数学推理的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates whether inference-time scaling techniques, such as self-verification and refinement, which significantly enhance mathematical reasoning in text-only LLMs, are similarly effective for vision-language models (VLMs), particularly those fine-tuned with reinforcement learning. The authors conduct extensive evaluations and find three key divergences: generation-time capability, like simple majority voting, consistently outperforms verification-centric strategies; behaviors like the &#x27;Aha moment&#x27; do not reliably improve reasoning; and visual information is poorly integrated into self-verification. Overall, the results reveal a critical limitation, showing that current RL-trained VLMs gain minimal benefit from self-verification in visual contexts, thereby constraining inference-time scaling for visual mathematical reasoning.</div>
<div class="mono" style="margin-top:8px">本研究探讨了推理时缩放技术（如自我验证和精炼）在显著提升纯文本大语言模型数学推理能力的同时，是否对视觉语言模型（特别是经过强化学习微调的模型）同样有效。通过广泛评估，作者发现了三个关键差异：生成时能力（如简单多数投票）持续优于以验证为中心的策略；与“顿悟时刻”相关的行为并未可靠地提升推理性能；且视觉信息未能有效融入模型的自我验证过程。总体而言，分析揭示了一个关键局限：当前经过强化学习训练的视觉语言模型在视觉模态中从自我验证获得的益处有限，这制约了推理时缩放技术在视觉数学推理中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Offline Model-Based Optimization: Comprehensive Review</div>
<div class="meta-line">Authors: Minsu Kim, Jiayao Gu, Ye Yuan, Taeyoung Yun, Zixuan Liu, Yoshua Bengio, Can Chen</div>
<div class="meta-line">First: 2025-03-21T16:35:02+00:00 · Latest: 2026-01-06T02:53:28+00:00</div>
<div class="meta-line">Comments: Accepted to TMLR 2026 (Survey Certification)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.17286v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.17286v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline optimization is a fundamental challenge in science and engineering, where the goal is to optimize black-box functions using only offline datasets. This setting is particularly relevant when querying the objective function is prohibitively expensive or infeasible, with applications spanning protein engineering, material discovery, neural architecture search, and beyond. The main difficulty lies in accurately estimating the objective landscape beyond the available data, where extrapolations are fraught with significant epistemic uncertainty. This uncertainty can lead to objective hacking(reward hacking), exploiting model inaccuracies in unseen regions, or other spurious optimizations that yield misleadingly high performance estimates outside the training distribution. Recent advances in model-based optimization(MBO) have harnessed the generalization capabilities of deep neural networks to develop offline-specific surrogate and generative models. Trained with carefully designed strategies, these models are more robust against out-of-distribution issues, facilitating the discovery of improved designs. Despite its growing impact in accelerating scientific discovery, the field lacks a comprehensive review. To bridge this gap, we present the first thorough review of offline MBO. We begin by formalizing the problem for both single-objective and multi-objective settings and by reviewing recent benchmarks and evaluation metrics. We then categorize existing approaches into two key areas: surrogate modeling, which emphasizes accurate function approximation in out-of-distribution regions, and generative modeling, which explores high-dimensional design spaces to identify high-performing designs. Finally, we examine the key challenges and propose promising directions for advancement in this rapidly evolving field including safe control of superintelligent systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>离线基于模型的优化：全面综述</div>
<div class="mono" style="margin-top:8px">离线优化是科学与工程中的基础性挑战，其目标在于仅利用离线数据集优化黑箱函数。当目标函数的查询成本过高或不可行时，这一设定尤为重要，其应用涵盖蛋白质工程、材料发现、神经架构搜索等领域。主要困难在于准确估计可用数据之外的目标函数形态，其中外推过程充满显著的认知不确定性。这种不确定性可能导致目标函数被恶意利用（奖励攻击），即在未见区域利用模型的不准确性，或产生其他虚假优化，从而在训练分布之外得出误导性的高性能估计。近年来，基于模型的优化方法借助深度神经网络的泛化能力，开发了专门针对离线场景的代理模型和生成模型。通过精心设计的策略训练，这些模型对分布外问题更具鲁棒性，有助于发现更优的设计方案。尽管该领域在加速科学发现方面影响日增，却缺乏系统性综述。为填补这一空白，本文首次对离线基于模型的优化进行全面综述。我们首先形式化定义了单目标和多目标场景下的问题，并回顾了近期基准测试与评估指标。随后将现有方法归纳为两大方向：强调在分布外区域实现精确函数逼近的代理建模，以及探索高维设计空间以识别高性能方案的生成建模。最后，我们探讨了该快速发展领域的关键挑战，并提出了包括超智能系统安全控制在内的前沿发展方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to optimize black-box functions using only offline datasets, a common challenge in fields like protein engineering and material discovery where real-time queries are costly or impossible. The method involves a comprehensive review of offline model-based optimization (MBO), categorizing approaches into surrogate modeling for accurate out-of-distribution function approximation and generative modeling for exploring high-dimensional design spaces. The main experimental results highlight that recent advances, particularly using deep neural networks with tailored training strategies, have improved robustness against distributional shifts and epistemic uncertainty, leading to more reliable discovery of high-performing designs, though the field still faces challenges such as objective hacking and requires further development for safe control of advanced systems.</div>
<div class="mono" style="margin-top:8px">本文的动机在于仅使用离线数据集优化黑盒函数的需求，这在蛋白质工程和材料发现等领域中尤为常见，因为这些领域的实时查询成本高昂或不可行。方法上，本文对离线模型优化进行了全面综述，将现有方法分为两类：强调在分布外区域准确函数逼近的代理建模，以及探索高维设计空间以识别高性能设计的生成建模。主要实验结果表明，近期进展特别是利用深度神经网络结合定制训练策略，已提升了模型对分布偏移和认知不确定性的鲁棒性，从而更可靠地发现了高性能设计，但该领域仍面临目标黑客等挑战，并需在安全控制超级智能系统等方面进一步发展。</div>
</details>
</div>
<div class="card">
<div class="title">Inferring Causal Graph Temporal Logic Formulas to Expedite Reinforcement Learning in Temporally Extended Tasks</div>
<div class="meta-line">Authors: Hadi Partovi Aria, Zhe Xu</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2026-01-06T02:25:26+00:00 · Latest: 2026-01-06T02:25:26+00:00</div>
<div class="meta-line">Comments: Accepted to AAAI-26 Bridge Program B10: Making Embodied AI Reliable with Testing and Formal Verification</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02666v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02666v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Decision-making tasks often unfold on graphs with spatial-temporal dynamics. Black-box reinforcement learning often overlooks how local changes spread through network structure, limiting sample efficiency and interpretability. We present GTL-CIRL, a closed-loop framework that simultaneously learns policies and mines Causal Graph Temporal Logic (Causal GTL) specifications. The method shapes rewards with robustness, collects counterexamples when effects fail, and uses Gaussian Process (GP) driven Bayesian optimization to refine parameterized cause templates. The GP models capture spatial and temporal correlations in the system dynamics, enabling efficient exploration of complex parameter spaces. Case studies in gene and power networks show faster learning and clearer, verifiable behavior compared to standard RL baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于因果图时序逻辑公式推断加速时序扩展任务中的强化学习</div>
<div class="mono" style="margin-top:8px">决策任务常在具有时空动态的图结构上展开。黑盒强化学习常忽略局部变化如何通过网络结构传播，限制了样本效率与可解释性。本文提出GTL-CIRL闭环框架，同步学习策略并挖掘因果图时序逻辑规范。该方法通过鲁棒性塑造奖励函数，在效应失效时收集反例，并采用高斯过程驱动的贝叶斯优化精调参数化因果模板。高斯过程模型捕捉系统动态中的时空相关性，实现对复杂参数空间的高效探索。基因网络与电力网络的案例研究表明，相较于标准强化学习基线，本方法能实现更快的学习速度与更清晰、可验证的行为模式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need to improve sample efficiency and interpretability in reinforcement learning for tasks with spatial-temporal dynamics on graphs, where standard black-box methods often fail to account for how local changes propagate through network structures. The proposed method, GTL-CIRL, is a closed-loop framework that concurrently learns policies and mines Causal Graph Temporal Logic specifications by shaping rewards with robustness measures, collecting counterexamples when expected effects fail, and refining parameterized cause templates using Gaussian Process-driven Bayesian optimization to exploit spatial-temporal correlations. Experimental results in gene regulatory and power network case studies demonstrate that this approach accelerates learning and yields clearer, verifiable behaviors compared to conventional reinforcement learning baselines.</div>
<div class="mono" style="margin-top:8px">本文的动机在于提升具有时空动态图结构任务的强化学习样本效率和可解释性，因为标准黑盒方法常忽略局部变化在网络结构中的传播。所提出的GTL-CIRL方法是一个闭环框架，通过基于鲁棒性的奖励塑造、收集效应失败时的反例，并利用高斯过程驱动的贝叶斯优化来精炼参数化因果模板，从而同时学习策略和挖掘因果图时序逻辑规范。在基因调控和电力网络的案例研究中，实验结果表明该方法相比标准强化学习基线能更快地学习并产生更清晰、可验证的行为。</div>
</details>
</div>
<div class="card">
<div class="title">Effective Online 3D Bin Packing with Lookahead Parcels Using Monte Carlo Tree Search</div>
<div class="meta-line">Authors: Jiangyi Fang, Bowen Zhou, Haotian Wang, Xin Zhu, Leye Wang</div>
<div class="meta-line">First: 2026-01-06T01:51:11+00:00 · Latest: 2026-01-06T01:51:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02649v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02649v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Online 3D Bin Packing (3D-BP) with robotic arms is crucial for reducing transportation and labor costs in modern logistics. While Deep Reinforcement Learning (DRL) has shown strong performance, it often fails to adapt to real-world short-term distribution shifts, which arise as different batches of goods arrive sequentially, causing performance drops. We argue that the short-term lookahead information available in modern logistics systems is key to mitigating this issue, especially during distribution shifts. We formulate online 3D-BP with lookahead parcels as a Model Predictive Control (MPC) problem and adapt the Monte Carlo Tree Search (MCTS) framework to solve it. Our framework employs a dynamic exploration prior that automatically balances a learned RL policy and a robust random policy based on the lookahead characteristics. Additionally, we design an auxiliary reward to penalize long-term spatial waste from individual placements. Extensive experiments on real-world datasets show that our method consistently outperforms state-of-the-art baselines, achieving over 10\% gains under distributional shifts, 4\% average improvement in online deployment, and up to more than 8\% in the best case--demonstrating the effectiveness of our framework.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于蒙特卡洛树搜索的带前瞻包裹在线三维装箱优化方法</div>
<div class="mono" style="margin-top:8px">在线三维装箱问题在机器人化现代物流中对降低运输与人力成本至关重要。尽管深度强化学习方法已展现优异性能，但其常难以适应现实场景中因货物批次连续到达导致的短期分布偏移，从而导致性能下降。我们认为，现代物流系统中可获取的短期前瞻信息是缓解此问题的关键，尤其在分布偏移期间。本文将带前瞻包裹的在线三维装箱问题建模为模型预测控制问题，并采用蒙特卡洛树搜索框架进行求解。该框架通过动态探索先验机制，基于前瞻特征自动平衡学习到的强化学习策略与鲁棒随机策略。此外，我们设计了辅助奖励函数以惩罚单次放置导致的长期空间浪费。在真实数据集上的大量实验表明，本方法始终优于现有先进基线：在分布偏移下获得超过10%的性能提升，在线部署平均提升4%，最佳情况下提升超过8%，验证了框架的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to improve online 3D bin packing in logistics, where deep reinforcement learning often struggles with short-term distribution shifts as new batches of goods arrive, this paper proposes a method that leverages lookahead parcel information. The approach formulates the problem as a Model Predictive Control task and adapts Monte Carlo Tree Search with a dynamic exploration prior to balance a learned policy and a robust random policy, alongside an auxiliary reward to penalize long-term spatial waste. Experimental results on real-world datasets demonstrate that the method consistently outperforms state-of-the-art baselines, achieving over 10% improvement under distribution shifts and an average 4% gain in online deployment, with best-case improvements exceeding 8%.</div>
<div class="mono" style="margin-top:8px">本文针对物流中在线三维装箱问题，旨在解决深度强化学习在货物批次顺序到达导致的短期分布偏移时性能下降的问题，提出利用前瞻包裹信息来提升适应性。方法将问题建模为模型预测控制任务，采用蒙特卡洛树搜索框架，通过动态探索先验平衡学习策略与鲁棒随机策略，并设计辅助奖励以惩罚单个放置导致的长期空间浪费。在真实数据集上的大量实验表明，该方法在分布偏移下性能提升超过10%，在线部署平均改善4%，最佳情况下提升超过8%，一致优于现有先进基线。</div>
</details>
</div>
<div class="card">
<div class="title">SWaRL: Safeguard Code Watermarking via Reinforcement Learning</div>
<div class="meta-line">Authors: Neusha Javidnia, Ruisi Zhang, Ashish Kundu, Farinaz Koushanfar</div>
<div class="meta-line">First: 2026-01-05T23:35:39+00:00 · Latest: 2026-01-05T23:35:39+00:00</div>
<div class="meta-line">Comments: Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02602v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02602v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present SWaRL, a robust and fidelity-preserving watermarking framework designed to protect the intellectual property of code LLM owners by embedding unique and verifiable signatures in the generated output. Existing approaches rely on manually crafted transformation rules to preserve watermarked code functionality or manipulate token-generation probabilities at inference time, which are prone to compilation errors. To address these challenges, SWaRL employs a reinforcement learning-based co-training framework that uses compiler feedback for functional correctness and a jointly trained confidential verifier as a reward signal to maintain watermark detectability. Furthermore, SWaRL employs low-rank adaptation (LoRA) during fine-tuning, allowing the learned watermark information to be transferable across model updates. Extensive experiments show that SWaRL achieves higher watermark detection accuracy compared to prior methods while fully maintaining watermarked code functionality. The LoRA-based signature embedding steers the base model to generate and solve code in a watermark-specific manner without significant computational overhead. Moreover, SWaRL exhibits strong resilience against refactoring and adversarial transformation attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWaRL：基于强化学习的代码水印保护框架</div>
<div class="mono" style="margin-top:8px">本文提出SWaRL，一种鲁棒且保真度的水印框架，旨在通过向生成代码中嵌入独特可验证的签名来保护代码大语言模型所有者的知识产权。现有方法依赖人工设计的转换规则来维持水印代码功能，或在推理时操纵词元生成概率，易导致编译错误。为解决这些问题，SWaRL采用基于强化学习的协同训练框架：利用编译器反馈确保功能正确性，并联合训练保密验证器作为奖励信号以维持水印可检测性。此外，SWaRL在微调阶段采用低秩适配技术，使习得的水印信息能跨模型更新迁移。大量实验表明，SWaRL在完全保持水印代码功能的同时，比现有方法实现更高的水印检测准确率。基于低秩适配的签名嵌入引导基础模型以水印定制方式生成和求解代码，且无显著计算开销。该框架对代码重构和对抗性转换攻击亦展现出强鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind SWaRL is to protect the intellectual property of code LLM owners by embedding verifiable watermarks in generated code, addressing limitations of existing methods that often cause compilation errors. The method employs a reinforcement learning-based co-training framework that uses compiler feedback for functional correctness and a jointly trained confidential verifier as a reward to maintain watermark detectability, along with low-rank adaptation (LoRA) for transferable watermarking across model updates. Experimental results demonstrate that SWaRL achieves higher watermark detection accuracy than prior methods while fully preserving code functionality, exhibits strong resilience against refactoring and adversarial attacks, and operates without significant computational overhead.</div>
<div class="mono" style="margin-top:8px">SWaRL的动机是通过在生成的代码中嵌入可验证的水印来保护代码大语言模型所有者的知识产权，以解决现有方法常导致编译错误的问题。该方法采用基于强化学习的协同训练框架，利用编译器反馈确保功能正确性，并通过联合训练的保密验证器作为奖励信号来维持水印可检测性，同时使用低秩自适应（LoRA）实现水印信息在模型更新间的可迁移性。实验结果表明，SWaRL相比先前方法实现了更高的水印检测准确率，完全保持了水印代码的功能性，对重构和对抗性转换攻击表现出强韧性，且没有显著的计算开销。</div>
</details>
</div>
<div class="card">
<div class="title">Solving the Paint Shop Problem with Flexible Management of Multi-Lane Buffers Using Reinforcement Learning and Action Masking</div>
<div class="meta-line">Authors: Mirko Stappert, Bernhard Lutz, Janis Brammer, Dirk Neumann</div>
<div class="meta-line">First: 2025-04-03T14:37:40+00:00 · Latest: 2026-01-05T21:51:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.02644v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.02644v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the paint shop problem, an unordered incoming sequence of cars assigned to different colors has to be reshuffled with the objective of minimizing the number of color changes. To reshuffle the incoming sequence, manufacturers can employ a first-in-first-out multi-lane buffer system allowing store and retrieve operations. So far, prior studies primarily focused on simple decision heuristics like greedy or simplified problem variants that do not allow full flexibility when performing store and retrieve operations. In this study, we propose a reinforcement learning approach to minimize color changes for the flexible problem variant, where store and retrieve operations can be performed in an arbitrary order. After proving that greedy retrieval is optimal, we incorporate this finding into the model using action masking. Our evaluation, based on 170 problem instances with 2-8 buffer lanes and 5-15 colors, shows that our approach reduces color changes compared to existing methods by considerable margins depending on the problem size. Furthermore, we demonstrate the robustness of our approach towards different buffer sizes and imbalanced color distributions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>运用强化学习与动作掩码技术实现多车道缓冲器灵活管理的涂装车间问题求解</div>
<div class="mono" style="margin-top:8px">涂装车间问题中，需对分配不同颜色的无序进站车辆序列进行重排，以最小化颜色切换次数。制造商可采用先进先出多车道缓冲系统进行存储与检索操作。现有研究多聚焦于贪心算法等简单决策启发式或简化问题变体，未能充分发挥存储检索操作的灵活性。本研究提出强化学习方法，针对可任意顺序执行存储检索操作的灵活问题变体，实现颜色切换最小化。在证明贪心检索策略最优性后，通过动作掩码技术将该结论融入模型。基于170个问题实例（含2-8个缓冲车道和5-15种颜色）的评估表明：本方法较现有方案显著降低颜色切换次数，降幅随问题规模递增。此外，研究验证了该方法对不同缓冲容量及非均衡颜色分布的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the paint shop problem&#x27;s need to minimize costly color changes when reshuffling car sequences, this study introduces a reinforcement learning method that allows flexible store and retrieve operations in multi-lane buffers, moving beyond prior heuristic approaches. The method leverages action masking to incorporate the proven optimality of greedy retrieval, ensuring efficient decision-making. Experimental results on 170 instances with varying buffer lanes and colors demonstrate that this approach significantly reduces color changes compared to existing methods and shows robustness across different buffer sizes and imbalanced color distributions.</div>
<div class="mono" style="margin-top:8px">本研究针对涂装车间问题中需最小化汽车序列重排时的颜色切换成本，提出了一种强化学习方法，允许在多车道缓冲区中进行灵活的存储和检索操作，超越了以往基于启发式的方法。该方法通过动作掩码整合了已证明的最优贪婪检索策略，以确保决策效率。在170个不同缓冲车道和颜色数量的实例上的实验结果表明，该方法相比现有方法显著减少了颜色切换次数，并在不同缓冲区大小和不平衡颜色分布下表现出良好的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">LLMs as Layout Designers: Enhanced Spatial Reasoning for Content-Aware Layout Generation</div>
<div class="meta-line">Authors: Sha Li, Stefano Petrangeli, Yu Shen, Xiang Chen, Naren Ramakrishnan</div>
<div class="meta-line">First: 2025-09-21T03:02:59+00:00 · Latest: 2026-01-05T20:28:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.16891v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.16891v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Language Models (LLMs) have demonstrated impressive reasoning and planning abilities in textual domains and can effectively follow instructions for complex tasks, their ability to understand and manipulate spatial relationships remains limited. Such capabilities are crucial for content-aware graphic layout design, where the goal is to arrange heterogeneous elements onto a canvas so that final design remains visually balanced and structurally feasible. This problem requires precise coordination of placement, alignment, and structural organization of multiple elements within a constrained visual space. To address this limitation, we introduce LaySPA, a reinforcement learning-based framework that augments LLM-based agents with explicit spatial reasoning capabilities for layout design. LaySPA employs hybrid reward signals that jointly capture geometric constraints, structural fidelity, and visual quality, enabling agents to navigate the canvas, model inter-element relationships, and optimize spatial arrangements. Through group-relative policy optimization, the agent generates content-aware layouts that reflect salient regions, respect spatial constraints, and produces an interpretable reasoning trace explaining placement decisions and a structured layout specification. Experimental results show that LaySPA substantially improves the generation of structurally valid and visually appealing layouts, outperforming larger general-purpose LLMs and achieving performance comparable to state-of-the-art specialized layout models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型作为布局设计师：增强内容感知布局生成的空间推理能力</div>
<div class="mono" style="margin-top:8px">尽管大语言模型在文本领域展现出卓越的推理与规划能力，并能有效执行复杂任务指令，但其理解和操纵空间关系的能力仍存在局限。这种能力对于内容感知的图形布局设计至关重要——该任务需在画布上协调排列异构元素，使最终设计保持视觉平衡与结构可行性。此问题要求在有限视觉空间内精确协调多元素的位置、对齐与结构组织。为突破此限制，我们提出LaySPA框架：一种基于强化学习的系统，通过显式空间推理能力增强基于大语言模型的智能体进行布局设计。LaySPA采用混合奖励信号，同步捕捉几何约束、结构保真度与视觉质量，使智能体能够导航画布、建模元素间关系并优化空间排布。通过群体相对策略优化，智能体生成的内容感知布局能反映显著区域、遵循空间约束，同时生成可解释的推理轨迹以说明布局决策，并输出结构化布局规范。实验结果表明，LaySPA显著提升了结构有效性与视觉吸引力俱佳的布局生成质量，其性能超越规模更大的通用大语言模型，并与专业布局模型的最新成果相当。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the limited spatial reasoning capabilities of Large Language Models (LLMs), which are crucial for content-aware graphic layout design requiring the balanced arrangement of heterogeneous elements on a canvas. To address this, the method introduces LaySPA, a reinforcement learning framework that augments LLM-based agents with explicit spatial reasoning, using hybrid reward signals for geometric constraints, structural fidelity, and visual quality, along with group-relative policy optimization to generate interpretable layouts. The main experimental results demonstrate that LaySPA substantially improves the generation of structurally valid and visually appealing layouts, outperforming larger general-purpose LLMs and achieving performance comparable to state-of-the-art specialized layout models.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于大型语言模型在空间推理能力上的不足，而这对于需要将异构元素在画布上平衡排列的内容感知图形布局设计至关重要。为解决此问题，方法上提出了LaySPA，这是一个基于强化学习的框架，通过混合奖励信号捕捉几何约束、结构保真度和视觉质量，并利用组相对策略优化，来增强基于LLM的智能体的显式空间推理能力，从而生成可解释的布局。主要实验结果表明，LaySPA显著提升了生成结构有效且视觉吸引人的布局的能力，其性能优于更大的通用LLMs，并与最先进的专用布局模型相当。</div>
</details>
</div>
<div class="card">
<div class="title">Textual Explanations and Their Evaluations for Reinforcement Learning Policy</div>
<div class="meta-line">Authors: Ahmad Terra, Mohit Ahmed, Rafia Inam, Elena Fersman, Martin Törngren</div>
<div class="meta-line">First: 2026-01-05T19:38:07+00:00 · Latest: 2026-01-05T19:38:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02514v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02514v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding a Reinforcement Learning (RL) policy is crucial for ensuring that autonomous agents behave according to human expectations. This goal can be achieved using Explainable Reinforcement Learning (XRL) techniques. Although textual explanations are easily understood by humans, ensuring their correctness remains a challenge, and evaluations in state-of-the-art remain limited. We present a novel XRL framework for generating textual explanations, converting them into a set of transparent rules, improving their quality, and evaluating them. Expert&#x27;s knowledge can be incorporated into this framework, and an automatic predicate generator is also proposed to determine the semantic information of a state. Textual explanations are generated using a Large Language Model (LLM) and a clustering technique to identify frequent conditions. These conditions are then converted into rules to evaluate their properties, fidelity, and performance in the deployed environment. Two refinement techniques are proposed to improve the quality of explanations and reduce conflicting information. Experiments were conducted in three open-source environments to enable reproducibility, and in a telecom use case to evaluate the industrial applicability of the proposed XRL framework. This framework addresses the limitations of an existing method, Autonomous Policy Explanation, and the generated transparent rules can achieve satisfactory performance on certain tasks. This framework also enables a systematic and quantitative evaluation of textual explanations, providing valuable insights for the XRL field.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习策略的文本解释及其评估</div>
<div class="mono" style="margin-top:8px">理解强化学习（RL）策略对于确保自主智能体行为符合人类预期至关重要，这一目标可通过可解释强化学习（XRL）技术实现。尽管文本解释易于人类理解，但其正确性保障仍具挑战，且现有评估方法有限。本文提出一种新颖的XRL框架，能够生成文本解释、将其转化为透明规则集、提升解释质量并进行系统评估。该框架可融入专家知识，并设计了自动谓词生成器以解析状态语义信息。通过大型语言模型（LLL）与聚类技术生成文本解释并识别高频条件，进而将条件转化为规则以评估其属性、忠实度及部署环境中的性能。提出两种优化技术以提升解释质量并减少信息冲突。在三个开源环境中进行了可复现性实验，并通过电信用例验证了框架的工业适用性。该框架解决了现有自主策略解释方法的局限性，生成的透明规则在特定任务中表现良好，同时实现了对文本解释的系统化定量评估，为XRL领域提供了重要参考。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to ensure that Reinforcement Learning (RL) policies align with human expectations and the challenge of evaluating textual explanations, this paper introduces a novel Explainable RL (XRL) framework. The method generates textual explanations using a Large Language Model (LLM) and clustering to identify frequent conditions, converts these into transparent rules, and refines them to improve quality and reduce conflicts. Experimental results in three open-source environments and a telecom use case demonstrate that the framework addresses limitations of prior methods, enables systematic evaluation of explanations, and produces rules that achieve satisfactory task performance.</div>
<div class="mono" style="margin-top:8px">本文的动机是确保强化学习策略符合人类预期，并解决文本解释难以评估的挑战，提出了一种新颖的可解释强化学习框架。该方法利用大语言模型和聚类技术生成文本解释并识别频繁条件，将其转换为透明规则，并通过两种精炼技术提升解释质量、减少冲突。在三个开源环境和电信用例中的实验表明，该框架克服了现有方法的局限，实现了对文本解释的系统化定量评估，且生成的透明规则在特定任务上取得了令人满意的性能。</div>
</details>
</div>
<div class="card">
<div class="title">LLM-Enhanced Reinforcement Learning for Time Series Anomaly Detection</div>
<div class="meta-line">Authors: Bahareh Golchin, Banafsheh Rekabdar, Danielle Justo</div>
<div class="meta-line">First: 2026-01-05T19:33:30+00:00 · Latest: 2026-01-05T19:33:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02511v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02511v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Detecting anomalies in time series data is crucial for finance, healthcare, sensor networks, and industrial monitoring applications. However, time series anomaly detection often suffers from sparse labels, complex temporal patterns, and costly expert annotation. We propose a unified framework that integrates Large Language Model (LLM)-based potential functions for reward shaping with Reinforcement Learning (RL), Variational Autoencoder (VAE)-enhanced dynamic reward scaling, and active learning with label propagation. An LSTM-based RL agent leverages LLM-derived semantic rewards to guide exploration, while VAE reconstruction errors add unsupervised anomaly signals. Active learning selects the most uncertain samples, and label propagation efficiently expands labeled data. Evaluations on Yahoo-A1 and SMD benchmarks demonstrate that our method achieves state-of-the-art detection accuracy under limited labeling budgets and operates effectively in data-constrained settings. This study highlights the promise of combining LLMs with RL and advanced unsupervised techniques for robust, scalable anomaly detection in real-world applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型增强的强化学习在时间序列异常检测中的应用</div>
<div class="mono" style="margin-top:8px">时间序列异常检测在金融、医疗、传感器网络和工业监测等领域至关重要，但常面临标注稀疏、时序模式复杂和专家标注成本高昂等挑战。本文提出一种统一框架，整合了基于大语言模型的奖励塑形潜力函数与强化学习、变分自编码器增强的动态奖励缩放机制，以及结合标签传播的主动学习方法。基于LSTM的强化学习智能体利用大语言模型衍生的语义奖励引导探索，同时变分自编码器的重构误差提供无监督异常信号。主动学习筛选最不确定样本，标签传播高效扩展标注数据。在Yahoo-A1和SMD基准测试上的评估表明，该方法在有限标注预算下实现了最先进的检测精度，并在数据受限场景中保持高效运行。本研究彰显了将大语言模型与强化学习及先进无监督技术相结合，为实际应用提供鲁棒、可扩展异常检测方案的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenges of time series anomaly detection, such as sparse labels and complex temporal patterns, by proposing a unified framework that integrates Large Language Models (LLMs) with Reinforcement Learning (RL). The method uses LLM-based potential functions for reward shaping, Variational Autoencoder (VAE)-enhanced dynamic reward scaling, and active learning with label propagation, where an LSTM-based RL agent leverages semantic rewards from LLMs and unsupervised signals from VAE reconstruction errors. Experimental results on Yahoo-A1 and SMD benchmarks show that the approach achieves state-of-the-art detection accuracy under limited labeling budgets, demonstrating effectiveness in data-constrained settings.</div>
<div class="mono" style="margin-top:8px">本文针对时间序列异常检测中标签稀疏、时序模式复杂等挑战，提出了一种将大语言模型（LLM）与强化学习（RL）相结合的统一框架。该方法利用基于LLM的势函数进行奖励塑造，结合变分自编码器（VAE）增强的动态奖励缩放以及主动学习与标签传播技术，通过LSTM驱动的RL智能体整合LLM的语义奖励和VAE重构误差的无监督信号。在Yahoo-A1和SMD基准测试上的实验结果表明，该方法在有限标注预算下实现了最先进的检测精度，有效适用于数据受限的实际场景。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
