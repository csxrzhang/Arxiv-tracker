<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-03-01 03:29</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260301_0329</div>
    <div class="row"><div class="card">
<div class="title">Evaluating the Diversity and Quality of LLM Generated Content</div>
<div class="meta-line">Authors: Alexander Shypula, Shuo Li, Botong Zhang, Vishakh Padmakumar, Kayo Yin, Osbert Bastani</div>
<div class="meta-line">First: 2025-04-16T23:02:23+00:00 · Latest: 2026-02-26T18:17:44+00:00</div>
<div class="meta-line">Comments: Published at COLM 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.12522v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.12522v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent work suggests that preference-tuning techniques -- such as Reinforcement Learning from Human Feedback (RLHF) methods like PPO and GRPO, as well as alternatives like DPO -- reduce diversity, creating a dilemma given that these models are widely deployed in applications requiring varied outputs. We argue that diversity without consideration of quality has limited practical value. To address this issue, we introduce a framework for measuring effective semantic diversity -- diversity among outputs that meet quality thresholds -- which better reflects the practical utility of large language models (LLMs). Using open-ended tasks that require no human intervention, we find counterintuitive results: when using diversity metrics that do not explicitly consider quality, preference-tuned models -- particularly those trained via RL -- often produce outputs with lower diversity; however, these same preference-tuned models generate greater effective semantic diversity than supervised fine-tuned (SFT) or base models. Our analysis further shows another trend: while larger models may exhibit greater effective semantic diversity than smaller models, the smaller models are consistently more parameter-efficient at producing unique content within a fixed sampling budget. These findings have practical implications for applications that require diverse yet high-quality outputs, from creative assistance to synthetic data generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估大语言模型生成内容的多样性与质量</div>
<div class="mono" style="margin-top:8px">近期研究表明，偏好调优技术——如基于人类反馈的强化学习方法（如PPO、GRPO）及替代方案（如DPO）——会降低输出多样性，鉴于这些模型已广泛应用于需要多样化输出的场景，这构成了一个两难问题。我们认为，不考虑质量的多样性实际价值有限。为此，我们提出一个衡量有效语义多样性的框架——即满足质量阈值的输出之间的多样性——这更能反映大语言模型的实际效用。通过无需人工干预的开放式任务实验，我们发现了反直觉的结果：使用未显式考虑质量的多样性指标时，偏好调优模型（尤其是通过强化学习训练的模型）往往产生多样性较低的输出；然而，这些偏好调优模型却比监督微调模型或基础模型生成更高的有效语义多样性。分析还揭示了另一趋势：虽然更大模型可能比较小模型表现出更高的有效语义多样性，但较小模型在固定采样预算下持续展现出更高的参数效率来生成独特内容。这些发现对需要多样化且高质量输出的应用（从创意辅助到合成数据生成）具有实际意义。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the concern that preference-tuning techniques like RLHF and DPO reduce output diversity in LLMs, arguing that raw diversity without quality is of limited practical value. The authors propose a framework to measure effective semantic diversity, which considers only outputs meeting quality thresholds, thereby better reflecting real-world utility. In experiments on open-ended tasks, they find that while preference-tuned models show lower raw diversity, they achieve higher effective semantic diversity compared to SFT or base models; additionally, smaller models are more parameter-efficient at generating unique, quality content than larger ones under fixed sampling budgets.</div>
<div class="mono" style="margin-top:8px">本文针对偏好调优技术（如RLHF和DPO）会降低大语言模型输出多样性的担忧，指出不考虑质量的原始多样性实用价值有限。作者提出了一个衡量有效语义多样性的框架，该框架仅考虑达到质量阈值的输出，从而更好地反映实际效用。在开放式任务的实验中，研究发现偏好调优模型虽然原始多样性较低，但与监督微调或基础模型相比，能产生更高的有效语义多样性；此外，在固定采样预算下，较小模型在生成独特且高质量内容方面比大模型具有更高的参数效率。</div>
</details>
</div>
<div class="card">
<div class="title">Physics Informed Viscous Value Representations</div>
<div class="meta-line">Authors: Hrishikesh Viswanath, Juanwu Lu, S. Talha Bukhari, Damon Conover, Ziran Wang, Aniket Bera</div>
<div class="meta-line">First: 2026-02-26T17:53:46+00:00 · Latest: 2026-02-26T17:53:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23280v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23280v1">PDF</a> · <a href="https://github.com/HrishikeshVish/phys-fk-value-GCRL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline goal-conditioned reinforcement learning (GCRL) learns goal-conditioned policies from static pre-collected datasets. However, accurate value estimation remains a challenge due to the limited coverage of the state-action space. Recent physics-informed approaches have sought to address this by imposing physical and geometric constraints on the value function through regularization defined over first-order partial differential equations (PDEs), such as the Eikonal equation. However, these formulations can often be ill-posed in complex, high-dimensional environments. In this work, we propose a physics-informed regularization derived from the viscosity solution of the Hamilton-Jacobi-Bellman (HJB) equation. By providing a physics-based inductive bias, our approach grounds the learning process in optimal control theory, explicitly regularizing and bounding updates during value iterations. Furthermore, we leverage the Feynman-Kac theorem to recast the PDE solution as an expectation, enabling a tractable Monte Carlo estimation of the objective that avoids numerical instability in higher-order gradients. Experiments demonstrate that our method improves geometric consistency, making it broadly applicable to navigation and high-dimensional, complex manipulation tasks. Open-source codes are available at https://github.com/HrishikeshVish/phys-fk-value-GCRL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>物理信息粘性价值表示</div>
<div class="mono" style="margin-top:8px">离线目标条件强化学习（GCRL）从静态预收集数据集中学习目标条件策略。然而，由于状态-动作空间的有限覆盖，准确的价值估计仍具挑战。近期物理信息方法尝试通过在一阶偏微分方程（如Eikonal方程）定义的正则化中对价值函数施加物理与几何约束来解决此问题，但这些公式在复杂高维环境中常不适定。本研究提出一种基于哈密顿-雅可比-贝尔曼（HJB）方程粘性解的物理信息正则化方法。通过引入基于物理的归纳偏置，我们的方法将学习过程锚定于最优控制理论，显式正则化并限制价值迭代中的更新。进一步利用费曼-卡茨定理将偏微分方程解重构为期望形式，实现了可处理的蒙特卡洛目标估计，避免了高阶梯度中的数值不稳定问题。实验表明，该方法提升了几何一致性，可广泛适用于导航及高维复杂操作任务。开源代码发布于 https://github.com/HrishikeshVish/phys-fk-value-GCRL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of accurate value estimation in offline goal-conditioned reinforcement learning (GCRL), where limited dataset coverage often leads to poor generalization. To tackle this, the authors propose a physics-informed regularization method based on the viscosity solution of the Hamilton-Jacobi-Bellman (HJB) equation, which grounds learning in optimal control theory to regularize and bound value updates. By applying the Feynman-Kac theorem to reformulate the PDE solution as an expectation, the approach enables stable Monte Carlo estimation without numerical instability from higher-order gradients. Experimental results show that this method enhances geometric consistency and performs effectively in navigation and high-dimensional manipulation tasks.</div>
<div class="mono" style="margin-top:8px">本文针对离线目标条件强化学习（GCRL）中价值估计不准确的问题，该问题常因数据集覆盖有限导致泛化能力差。为此，作者提出了一种基于汉密尔顿-雅可比-贝尔曼（HJB）方程粘性解的物理信息正则化方法，将学习过程锚定在最优控制理论中，以正则化并限制价值更新。通过应用费曼-卡茨定理将偏微分方程解重构为期望，该方法实现了稳定的蒙特卡洛估计，避免了高阶梯度带来的数值不稳定。实验结果表明，该方法提高了几何一致性，在导航和高维复杂操作任务中表现优异。</div>
</details>
</div>
<div class="card">
<div class="title">A Model-Free Universal AI</div>
<div class="meta-line">Authors: Yegon Kim, Juho Lee</div>
<div class="meta-line">First: 2026-02-26T17:21:16+00:00 · Latest: 2026-02-26T17:21:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23242v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23242v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In general reinforcement learning, all established optimal agents, including AIXI, are model-based, explicitly maintaining and using environment models. This paper introduces Universal AI with Q-Induction (AIQI), the first model-free agent proven to be asymptotically $\varepsilon$-optimal in general RL. AIQI performs universal induction over distributional action-value functions, instead of policies or environments like previous works. Under a grain of truth condition, we prove that AIQI is strong asymptotically $\varepsilon$-optimal and asymptotically $\varepsilon$-Bayes-optimal. Our results significantly expand the diversity of known universal agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种免模型的通用人工智能</div>
<div class="mono" style="margin-top:8px">在通用强化学习中，所有已确立的最优智能体（包括AIXI）均基于模型，需显式维护并使用环境模型。本文提出基于Q归纳的通用人工智能（AIQI），这是首个被证明在通用强化学习中具有渐近$\varepsilon$最优性的免模型智能体。AIQI对分布型动作价值函数进行通用归纳，而非如先前研究那样对策略或环境建模。在真值粒度条件下，我们证明AIQI具有强渐近$\varepsilon$最优性与渐近$\varepsilon$贝叶斯最优性。该成果显著拓展了已知通用智能体的多样性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitation that all known asymptotically optimal agents in general reinforcement learning are model-based, this paper introduces AIQI, a model-free universal agent. The method employs universal induction over distributional action-value functions, rather than over policies or environment models as in prior work. The main experimental results, under a grain of truth condition, prove that AIQI is both strong asymptotically ε-optimal and asymptotically ε-Bayes-optimal, thereby significantly expanding the diversity of known universal agents.</div>
<div class="mono" style="margin-top:8px">本文的动机源于通用强化学习中所有已知的渐进最优智能体都是基于模型的这一局限，因此提出了AIQI，一种免模型的通用智能体。该方法的核心是对分布式的动作价值函数进行通用归纳，而非像先前工作那样对策略或环境模型进行归纳。在“真理之粒”条件下，主要实验结果证明AIQI既是强渐进ε最优的，也是渐进ε贝叶斯最优的，从而显著扩展了已知通用智能体的多样性。</div>
</details>
</div>
<div class="card">
<div class="title">Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive</div>
<div class="meta-line">Authors: Radha Sarma</div>
<div class="meta-line">First: 2026-02-26T17:16:17+00:00 · Latest: 2026-02-26T17:16:17+00:00</div>
<div class="meta-line">Comments: About 10,500 words in all (including 922 words of literature and 2019 words of Appendices). Under journal review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23239v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23239v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI systems are increasingly deployed in high-stakes contexts -- medical diagnosis, legal research, financial analysis -- under the assumption they can be governed by norms. This paper demonstrates that assumption is formally invalid for optimization-based systems, specifically Large Language Models trained via Reinforcement Learning from Human Feedback (RLHF). We establish that genuine agency requires two necessary and jointly sufficient architectural conditions: the capacity to maintain certain boundaries as non-negotiable constraints rather than tradeable weights (Incommensurability), and a non-inferential mechanism capable of suspending processing when those boundaries are threatened (Apophatic Responsiveness). These conditions apply across all normative domains.
  RLHF-based systems are constitutively incompatible with both conditions. The operations that make optimization powerful -- unifying all values on a scalar metric and always selecting the highest-scoring output -- are precisely the operations that preclude normative governance. This incompatibility is not a correctable training bug awaiting a technical fix; it is a formal constraint inherent to what optimization is. Consequently, documented failure modes - sycophancy, hallucination, and unfaithful reasoning - are not accidents but structural manifestations.
  Misaligned deployment triggers a second-order risk we term the Convergence Crisis: when humans are forced to verify AI outputs under metric pressure, they degrade from genuine agents into criteria-checking optimizers, eliminating the only component in the system capable of normative accountability. Beyond the incompatibility proof, the paper&#x27;s primary positive contribution is a substrate-neutral architectural specification defining what any system -- biological, artificial, or institutional -- must satisfy to qualify as an agent rather than a sophisticated instrument.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>能动性与架构限制：为何基于优化的系统无法响应规范</div>
<div class="mono" style="margin-top:8px">人工智能系统正越来越多地被部署于高风险场景——医疗诊断、法律研究、金融分析——其前提假设是它们能够受规范约束。本文证明，对于基于优化的系统（特别是通过人类反馈强化学习训练的大型语言模型），这一假设在形式上是无效的。我们确立，真正的能动性需要两个必要且共同充分的架构条件：一是能够将某些边界视为不可协商的约束而非可交易的权重（不可通约性），二是当这些边界受到威胁时，具备一种能够暂停处理的非推理机制（否定性响应）。这些条件适用于所有规范领域。
基于RLHF的系统在本质上与这两个条件都不兼容。使优化变得强大的操作——将所有价值统一于标量度量并始终选择得分最高的输出——恰恰是阻碍规范治理的操作。这种不兼容性并非可通过技术修复纠正的训练缺陷；而是优化本身固有的形式约束。因此，已记录到的故障模式——谄媚、幻觉和不忠实的推理——并非偶然，而是结构性表现。
错位的部署会引发我们称为‘趋同危机’的二级风险：当人类被迫在度量压力下验证AI输出时，他们会从真正的能动者退化为标准检查优化器，从而消除了系统中唯一能够承担规范问责的组成部分。除了不兼容性证明，本文的主要积极贡献在于提出了一种基质中立的架构规范，定义了任何系统——无论是生物的、人工的还是制度的——要成为能动者而非精密工具所必须满足的条件。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper challenges the assumption that optimization-based AI systems, such as LLMs trained via RLHF, can be effectively governed by norms in high-stakes applications. It argues that genuine agency requires two architectural conditions—Incommensurability (treating certain boundaries as non-negotiable constraints) and Apophatic Responsiveness (a mechanism to suspend processing when boundaries are threatened)—which are constitutively incompatible with optimization&#x27;s core operations of unifying values on a scalar metric and maximizing output scores. The main experimental results demonstrate that documented failure modes like sycophancy and hallucination are structural, not accidental, and that misaligned deployment risks a &#x27;Convergence Crisis&#x27; where human oversight degrades under metric pressure, eliminating normative accountability.</div>
<div class="mono" style="margin-top:8px">本文质疑了基于优化的AI系统（如通过人类反馈强化学习训练的大语言模型）能在高风险应用中被规范有效治理的假设。它提出，真正的智能体需要两个架构条件——不可通约性（将某些边界视为不可协商的约束）和否定响应性（在边界受威胁时暂停处理的机制），而这些与优化将价值统一为标量指标并最大化输出分数的核心操作在本质上不相容。主要实验结果表明，已记录的奉承、幻觉等故障模式是结构性的而非偶然，且部署不当会引发&#x27;趋同危机&#x27;，即在指标压力下人类监督会退化，从而消除规范问责。</div>
</details>
</div>
<div class="card">
<div class="title">Learning-based Multi-agent Race Strategies in Formula 1</div>
<div class="meta-line">Authors: Giona Fieni, Joschua Wüthrich, Marc-Philippe Neumann, Christopher H. Onder</div>
<div class="meta-line">First: 2026-02-26T14:41:29+00:00 · Latest: 2026-02-26T14:41:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23056v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23056v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In Formula 1, race strategies are adapted according to evolving race conditions and competitors&#x27; actions. This paper proposes a reinforcement learning approach for multi-agent race strategy optimization. Agents learn to balance energy management, tire degradation, aerodynamic interaction, and pit-stop decisions. Building on a pre-trained single-agent policy, we introduce an interaction module that accounts for the behavior of competitors. The combination of the interaction module and a self-play training scheme generates competitive policies, and agents are ranked based on their relative performance. Results show that the agents adapt pit timing, tire selection, and energy allocation in response to opponents, achieving robust and consistent race performance. Because the framework relies only on information available during real races, it can support race strategists&#x27; decisions before and during races.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于学习的多智能体一级方程式赛车策略</div>
<div class="mono" style="margin-top:8px">在一级方程式赛车中，比赛策略需根据不断变化的赛道条件及竞争对手行为进行调整。本文提出一种强化学习方法，用于多智能体比赛策略优化。智能体学习平衡能量管理、轮胎磨损、空气动力学交互及进站决策。基于预训练的单智能体策略，我们引入一个交互模块以考虑竞争对手行为。该交互模块与自博弈训练方案相结合，生成具有竞争力的策略，并根据相对表现对智能体进行排名。结果表明，智能体能根据对手调整进站时机、轮胎选择和能量分配，实现稳健且一致的比赛表现。由于该框架仅依赖实际比赛中可用的信息，因此可在赛前和赛中为策略师提供决策支持。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for dynamic race strategy adaptation in Formula 1, this paper introduces a reinforcement learning method for multi-agent strategy optimization. The approach builds upon a pre-trained single-agent policy by incorporating an interaction module to model competitor behavior, trained via self-play to balance energy, tires, aerodynamics, and pit stops. Experimental results demonstrate that the learned agents successfully adapt pit timing, tire selection, and energy allocation in response to opponents, achieving robust and consistent performance using only information available in real races, thereby offering potential decision support for strategists.</div>
<div class="mono" style="margin-top:8px">本文针对一级方程式赛车中需要根据比赛动态调整策略的问题，提出了一种基于强化学习的多智能体比赛策略优化方法。该方法在预训练的单智能体策略基础上，引入了一个考虑竞争对手行为的交互模块，并通过自我对弈训练来综合管理能量、轮胎损耗、空气动力学互动和进站决策。实验结果表明，智能体能够根据对手行动自适应地调整进站时机、轮胎选择和能量分配，仅利用真实比赛中可获得的信息实现了稳健且一致的比赛表现，从而为赛道策略师提供了潜在的决策支持。</div>
</details>
</div>
<div class="card">
<div class="title">Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization</div>
<div class="meta-line">Authors: Zeyuan Liu, Jeonghye Kim, Xufang Luo, Dongsheng Li, Yuqing Yang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-26T13:50:57+00:00 · Latest: 2026-02-26T13:50:57+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23008v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23008v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO$^2$), a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates to make LLMs perform well with memory while also ensuring robustness without it. On ScienceWorld and WebShop, EMPO$^2$ achieves 128.6% and 11.3% improvements over GRPO, respectively. Moreover, in out-of-distribution tests, EMPO$^2$ demonstrates superior adaptability to new tasks, requiring only a few trials with memory and no parameter updates. These results highlight EMPO$^2$ as a promising framework for building more exploratory and generalizable LLM-based agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于混合同策略与异策略优化的探索性记忆增强大语言模型智能体</div>
<div class="mono" style="margin-top:8px">探索能力仍是强化学习训练的大语言模型智能体的关键瓶颈。现有方法虽利用预训练知识，但在需要发现新状态的环境中表现不佳。本文提出探索性记忆增强同异策略优化框架（EMPO²），该混合强化学习框架利用记忆进行探索，并结合同策略与异策略更新，使大语言模型既能借助记忆获得优异表现，也能在无记忆时保持鲁棒性。在ScienceWorld和WebShop基准测试中，EMPO²较GRPO分别提升128.6%和11.3%。在分布外测试中，EMPO²展现出对新任务的卓越适应能力，仅需少量记忆试验且无需参数更新。这些结果表明EMPO²是构建更具探索性与泛化能力的大语言模型智能体的前瞻性框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the exploration bottleneck in reinforcement learning for large language model agents, which often struggle to discover novel states despite leveraging pretrained knowledge. It introduces EMPO², a hybrid framework that augments LLMs with memory to enhance exploration and combines on- and off-policy optimization for robust performance both with and without memory. Experimental results on ScienceWorld and WebShop show EMPO² achieving improvements of 128.6% and 11.3% over GRPO, respectively, and it demonstrates strong adaptability in out-of-distribution tests, requiring only minimal trials with memory and no parameter updates.</div>
<div class="mono" style="margin-top:8px">本文针对基于大型语言模型的智能体在强化学习中探索能力不足的问题，提出了一种混合强化学习框架EMPO²，该框架通过引入记忆机制来增强探索，并结合了同策略与异策略优化，使模型在有记忆时表现优异，无记忆时仍保持鲁棒性。在ScienceWorld和WebShop环境上的实验表明，EMPO²相比GRPO分别取得了128.6%和11.3%的性能提升，并且在分布外测试中展现出强大的适应能力，仅需少量带记忆的尝试且无需参数更新即可应对新任务。</div>
</details>
</div>
<div class="card">
<div class="title">Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation</div>
<div class="meta-line">Authors: Wenkai Yang, Weijie Liu, Ruobing Xie, Kai Yang, Saiyong Yang, Yankai Lin</div>
<div class="meta-line">First: 2026-02-12T16:14:29+00:00 · Latest: 2026-02-26T13:26:22+00:00</div>
<div class="meta-line">Comments: v2, update results under stronger teachers with more RL training steps</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12125v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.12125v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">On-policy distillation (OPD), which aligns the student with the teacher&#x27;s logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (G-OPD) framework, which extends the standard OPD objective by introducing a flexible reference model and a reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation), which we term ExOPD, consistently improves over standard OPD across a range of teacher-student size pairings. In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teacher&#x27;s performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling a smaller student from a larger teacher), performing reward correction by choosing the reference model as the teacher&#x27;s base model before RL yields a more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teacher&#x27;s pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越教师的学习：基于奖励外推的广义在线策略蒸馏</div>
<div class="mono" style="margin-top:8px">在线策略蒸馏（OPD）通过将学生模型与教师模型在学生生成轨迹上的对数分布对齐，在提升学生模型性能方面展现出显著优势，其表现常优于离线策略蒸馏和强化学习范式。本文首先从理论上证明，OPD是稠密KL约束强化学习的一种特例，其中奖励函数与KL正则化始终等权重加权，且参考模型可为任意模型。基于此，我们提出广义在线策略蒸馏框架，通过引入灵活的参考模型和控制奖励项与KL正则化相对权重的奖励缩放因子，扩展了标准OPD目标。在数学推理和代码生成任务上的系统实验揭示了两项新发现：（1）将奖励缩放因子设为大于1（即奖励外推，称为ExOPD）能在多种师生模型规模配置下持续超越标准OPD。特别是在将领域专家知识（通过对同一学生模型进行领域特定强化学习获得）融合回原学生模型时，ExOPD使学生模型能够突破教师性能边界并超越领域专家教师。（2）基于ExOPD进一步发现，在强到弱蒸馏场景中，选择教师模型在强化学习前的初始版本作为参考模型进行奖励校正，能提供更精确的奖励信号并提升蒸馏性能，但该方案需获取教师预训练变体且计算开销更大。本研究为OPD的未来探索提供了新视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the empirical success of on-policy distillation (OPD) in improving student models, this work first provides a theoretical interpretation of OPD as a form of dense KL-constrained reinforcement learning. The method introduces Generalized On-Policy Distillation (G-OPD), a framework that extends OPD by incorporating a flexible reference model and a reward scaling factor to adjust the balance between reward and regularization. Experimental results on math reasoning and code generation tasks show that scaling the reward factor above 1, termed ExOPD, consistently outperforms standard OPD and, in knowledge merging scenarios, enables the student to surpass the performance of domain-expert teachers. Furthermore, in strong-to-weak distillation, using the teacher&#x27;s pre-reinforcement learning base model as the reference for reward correction yields additional gains, though it requires access to that model and increases computational cost.</div>
<div class="mono" style="margin-top:8px">本研究受策略蒸馏在提升学生模型性能方面实证成功的启发，首先从理论上将策略蒸馏解释为一种密集KL约束强化学习的特例。方法上提出了广义策略蒸馏框架，通过引入灵活的参考模型和奖励缩放因子来调整奖励项与正则化之间的权重。在数学推理和代码生成任务上的实验结果表明，将奖励因子设置为大于1的ExOPD方法持续优于标准策略蒸馏，并且在合并不同领域专家知识时，能使学生模型超越领域教师的表现边界。此外，在强教师到弱学生的蒸馏设置中，选择教师强化学习前的基模型作为参考进行奖励校正能进一步提升性能，但这一方法需要获取教师的预训练变体并增加计算开销。</div>
</details>
</div>
<div class="card">
<div class="title">FactGuard: Agentic Video Misinformation Detection via Reinforcement Learning</div>
<div class="meta-line">Authors: Zehao Li, Hongwei Yu, Hao Jiang, Qiang Sheng, Yilong Xu, Baolong Bi, Yang Li, Zhenlong Yuan, Yujun Cai, Zhaoqi Wang</div>
<div class="meta-line">First: 2026-02-26T13:00:31+00:00 · Latest: 2026-02-26T13:00:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22963v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22963v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal large language models (MLLMs) have substantially advanced video misinformation detection through unified multimodal reasoning, but they often rely on fixed-depth inference and place excessive trust in internally generated assumptions, particularly in scenarios where critical evidence is sparse, fragmented, or requires external verification. To address these limitations, we propose FactGuard, an agentic framework for video misinformation detection that formulates verification as an iterative reasoning process built upon MLLMs. FactGuard explicitly assesses task ambiguity and selectively invokes external tools to acquire critical evidence, enabling progressive refinement of reasoning trajectories. To further strengthen this capability, we introduce a two-stage training strategy that combines domain-specific agentic supervised fine-tuning with decision-aware reinforcement learning to optimize tool usage and calibrate risk-sensitive decision making. Extensive experiments on FakeSV, FakeTT, and FakeVV demonstrate FactGuard&#x27;s state-of-the-art performance and validate its excellent robustness and generalization capacity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FactGuard：基于强化学习的智能体视频虚假信息检测框架</div>
<div class="mono" style="margin-top:8px">多模态大语言模型通过统一的多模态推理显著推进了视频虚假信息检测，但其常依赖固定深度推断，并过度信任内部生成的假设——尤其在关键证据稀疏、碎片化或需外部验证的场景中。为突破这些局限，我们提出FactGuard：一种基于MLLMs构建迭代推理过程的智能体视频虚假信息检测框架。该框架显式评估任务模糊性，选择性调用外部工具获取关键证据，实现推理轨迹的渐进优化。为强化此能力，我们引入两阶段训练策略：结合领域特定的智能体监督微调与决策感知强化学习，以优化工具使用并校准风险敏感决策。在FakeSV、FakeTT和FakeVV数据集上的大量实验表明，FactGuard具备最先进的性能，并展现出优异的鲁棒性与泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind FactGuard is to overcome the limitations of existing multimodal large language models (MLLMs) in video misinformation detection, which often rely on fixed-depth inference and over-trust internally generated assumptions, especially when evidence is sparse or requires external verification. The method introduces an agentic framework that formulates verification as an iterative reasoning process, explicitly assessing task ambiguity and selectively invoking external tools to gather critical evidence for progressive refinement; it employs a two-stage training strategy combining domain-specific agentic supervised fine-tuning with decision-aware reinforcement learning to optimize tool usage and calibrate risk-sensitive decisions. Main experimental results on datasets FakeSV, FakeTT, and FakeVV demonstrate FactGuard&#x27;s state-of-the-art performance, along with excellent robustness and generalization capacity.</div>
<div class="mono" style="margin-top:8px">FactGuard的动机是解决现有多模态大语言模型在视频虚假信息检测中的局限性，这些模型通常依赖固定深度推理并过度信任内部生成的假设，尤其在证据稀疏或需要外部验证的场景下。该方法提出了一个智能体框架，将验证构建为迭代推理过程，明确评估任务模糊性并有选择地调用外部工具获取关键证据以实现渐进式优化；它采用两阶段训练策略，结合领域特定的智能体监督微调与决策感知的强化学习，以优化工具使用并校准风险敏感决策。在FakeSV、FakeTT和FakeVV数据集上的主要实验结果表明，FactGuard实现了最先进的性能，并展现出优秀的鲁棒性和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Information-Theoretic Bayesian Optimization for Bilevel Optimization Problems</div>
<div class="meta-line">Authors: Takuya Kanayama, Yuki Ito, Tomoyuki Tamura, Masayuki Karasuyama</div>
<div class="meta-line">First: 2025-09-26T00:52:14+00:00 · Latest: 2026-02-26T12:32:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.21725v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.21725v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A bilevel optimization problem consists of two optimization problems nested as an upper- and a lower-level problem, in which the optimality of the lower-level problem defines a constraint for the upper-level problem. This paper considers Bayesian optimization (BO) for the case that both the upper- and lower-levels involve expensive black-box functions. Because of its nested structure, bilevel optimization has a complex problem definition, by which bilevel BO has not been widely studied compared with other standard extensions of BO such as multi-objective or constraint problems. We propose an information-theoretic approach that considers the information gain of both the upper- and lower-optimal solutions and values. This enables us to define a unified criterion that measures the benefit for both level problems, simultaneously. Further, we also show a practical lower bound based approach to evaluating the information gain. We empirically demonstrate the effectiveness of our proposed method through several benchmark datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双层优化问题的信息论贝叶斯优化方法</div>
<div class="mono" style="margin-top:8px">双层优化问题由上层与下层两个嵌套的优化问题构成，其中下层问题的最优解构成上层问题的约束条件。本文针对上下层均涉及昂贵黑箱函数的情形，研究贝叶斯优化（BO）的应用。由于其嵌套结构，双层优化具有复杂的问题定义，导致双层BO相较于多目标或约束问题等标准BO扩展研究较少。我们提出一种信息论方法，同时考虑上层与下层最优解及目标值的信息增益，从而构建可同步衡量两级问题收益的统一准则。此外，我们还提出基于实际下界的信息增益评估方法。通过多个基准数据集的实验，我们验证了所提方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of applying Bayesian optimization (BO) to bilevel optimization problems, where both upper- and lower-level problems involve expensive black-box functions, making standard approaches difficult due to the nested structure. The authors propose an information-theoretic method that measures the joint information gain of optimal solutions and values across both levels, enabling a unified acquisition criterion to efficiently guide the search. Experimental results on several benchmark datasets demonstrate the effectiveness of this approach in handling such complex optimization scenarios.</div>
<div class="mono" style="margin-top:8px">本文针对双层优化问题中上下两层均涉及昂贵黑盒函数的情况，由于嵌套结构复杂，传统贝叶斯优化方法难以直接应用。作者提出了一种信息论方法，通过衡量上下层最优解和最优值的信息增益，定义了一个统一的采集准则来同时优化两个层级。在多个基准数据集上的实验结果表明，该方法能有效处理此类复杂优化问题。</div>
</details>
</div>
<div class="card">
<div class="title">Aligning Few-Step Diffusion Models with Dense Reward Difference Learning</div>
<div class="meta-line">Authors: Ziyi Zhang, Li Shen, Sen Zhang, Deheng Ye, Yong Luo, Miaojing Shi, Dongjing Shan, Bo Du, Dacheng Tao</div>
<div class="meta-line">First: 2024-11-18T16:57:41+00:00 · Latest: 2026-02-26T11:11:12+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE TPAMI</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.11727v2">Abs</a> · <a href="https://arxiv.org/pdf/2411.11727v2">PDF</a> · <a href="https://github.com/ZiyiZhang27/sdpo">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Few-step diffusion models enable efficient high-resolution image synthesis but struggle to align with specific downstream objectives due to limitations of existing reinforcement learning (RL) methods in low-step regimes with limited state spaces and suboptimal sample quality. To address this, we propose Stepwise Diffusion Policy Optimization (SDPO), a novel RL framework tailored for few-step diffusion models. SDPO introduces a dual-state trajectory sampling mechanism, tracking both noisy and predicted clean states at each step to provide dense reward feedback and enable low-variance, mixed-step optimization. For further efficiency, we develop a latent similarity-based dense reward prediction strategy to minimize costly dense reward queries. Leveraging these dense rewards, SDPO optimizes a dense reward difference learning objective that enables more frequent and granular policy updates. Additional refinements, including stepwise advantage estimates, temporal importance weighting, and step-shuffled gradient updates, further enhance long-term dependency, low-step priority, and gradient stability. Our experiments demonstrate that SDPO consistently delivers superior reward-aligned results across diverse few-step settings and tasks. Code is available at https://github.com/ZiyiZhang27/sdpo.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于密集奖励差分学习的少步扩散模型对齐方法</div>
<div class="mono" style="margin-top:8px">少步扩散模型能够实现高效的高分辨率图像合成，但由于现有强化学习方法在低步数场景下存在状态空间有限和样本质量欠佳的问题，难以与特定下游目标对齐。为此，我们提出了适用于少步扩散模型的新型强化学习框架——逐步扩散策略优化（SDPO）。该框架引入双状态轨迹采样机制，通过跟踪每步的噪声状态与预测清洁状态，提供密集奖励反馈并实现低方差混合步数优化。为进一步提升效率，我们开发了基于潜在相似性的密集奖励预测策略，以最小化昂贵的密集奖励查询成本。利用这些密集奖励，SDPO优化了密集奖励差分学习目标，实现更频繁、更精细的策略更新。通过逐步优势估计、时序重要性加权和步序随机梯度更新等改进措施，进一步增强了长期依赖性、低步数优先级和梯度稳定性。实验表明，SDPO在多种少步设置与任务中均能持续产生更优的奖励对齐结果。代码已开源：https://github.com/ZiyiZhang27/sdpo。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the challenge that few-step diffusion models, while efficient for high-resolution image synthesis, often fail to align with specific downstream objectives due to limitations of existing reinforcement learning methods in low-step regimes. To address this, the authors propose Stepwise Diffusion Policy Optimization (SDPO), a novel RL framework that employs a dual-state trajectory sampling mechanism to track both noisy and predicted clean states, enabling dense reward feedback and mixed-step optimization, alongside a latent similarity-based strategy to predict dense rewards efficiently. The method further incorporates a dense reward difference learning objective, stepwise advantage estimates, temporal importance weighting, and step-shuffled gradient updates to enhance policy alignment. Experimental results demonstrate that SDPO consistently achieves superior reward-aligned performance across various few-step settings and tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，尽管少步扩散模型能高效合成高分辨率图像，但由于现有强化学习方法在低步数、状态空间有限且样本质量欠佳的情况下存在局限，这些模型难以与特定下游目标对齐。为此，作者提出了步进扩散策略优化（SDPO），这是一种新颖的强化学习框架，采用双状态轨迹采样机制跟踪每一步的噪声状态和预测的干净状态，以提供密集奖励反馈并实现低方差、混合步数的优化；同时，开发了基于潜在相似性的密集奖励预测策略，以最小化昂贵的密集奖励查询成本。该方法进一步利用密集奖励，通过密集奖励差异学习目标实现更频繁、更精细的策略更新，并结合步进优势估计、时间重要性加权和步序打乱的梯度更新来增强长期依赖、低步数优先级和梯度稳定性。实验结果表明，SDPO在多种少步设置和任务中均能持续产生更优的奖励对齐结果。</div>
</details>
</div>
<div class="card">
<div class="title">Hierarchy-of-Groups Policy Optimization for Long-Horizon Agentic Tasks</div>
<div class="meta-line">Authors: Shuo He, Lang Feng, Qi Wei, Xin Cheng, Lei Feng, Bo An</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-26T09:58:10+00:00 · Latest: 2026-02-26T09:58:10+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22817v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22817v1">PDF</a> · <a href="https://github.com/langfengQ/verl-agent/tree/master/recipe/hgpo">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Group-based reinforcement learning (RL), such as GRPO, has advanced the capabilities of large language models on long-horizon agentic tasks. To enable more fine-grained policy updates, recent research has increasingly shifted toward stepwise group-based policy optimization, which treats each step in a rollout trajectory independently while using a memory module to retain historical context. However, we find a key issue in estimating stepwise relative advantages, namely context inconsistency, where steps within the same group may differ in their historical contexts. Empirically, we reveal that this issue can lead to severely biased advantage estimation, thereby degrading policy optimization significantly. To address the issue, in this paper, we propose Hierarchy-of-Groups Policy Optimization (HGPO) for long-horizon agentic tasks. Specifically, within a group of rollout trajectories, HGPO assigns each step to multiple hierarchical groups according to the consistency of historical contexts. Then, for each step, HGPO computes distinct advantages within each group and aggregates them with an adaptive weighting scheme. In this way, HGPO can achieve a favorable bias-variance trade-off in stepwise advantage estimation, without extra models or rollouts. Evaluations on two challenging agentic tasks, ALFWorld and WebShop with Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct, show that HGPO significantly outperforms existing agentic RL methods under the same computational constraints. Code is available at https://github.com/langfengQ/verl-agent/tree/master/recipe/hgpo.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向长程智能体任务的分层组策略优化</div>
<div class="mono" style="margin-top:8px">基于分组的强化学习方法（如GRPO）提升了大型语言模型在长程智能体任务上的能力。为实现更细粒度的策略更新，近期研究逐渐转向逐步分组策略优化，该方法将轨迹展开中的每一步独立处理，同时利用记忆模块保留历史上下文。然而，我们发现逐步相对优势估计存在一个关键问题——上下文不一致性，即同组内的步骤可能具有不同的历史上下文。实证研究表明，该问题会导致严重的优势估计偏差，从而显著降低策略优化效果。为此，本文提出面向长程智能体任务的分层组策略优化方法。具体而言，在轨迹展开组内，HGPO根据历史上下文一致性将每个步骤分配至多个分层组中，随后为每个步骤计算各分组内的独立优势值，并通过自适应加权方案进行聚合。该方法能在不引入额外模型或轨迹展开的情况下，实现逐步优势估计中偏差与方差的有利权衡。在ALFWorld和WebShop两个挑战性智能体任务上，使用Qwen2.5-1.5B-Instruct和Qwen2.5-7B-Instruct模型的实验表明，在相同计算约束下，HGPO显著优于现有智能体强化学习方法。代码已开源：https://github.com/langfengQ/verl-agent/tree/master/recipe/hgpo。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the context inconsistency issue in stepwise group-based reinforcement learning for long-horizon agentic tasks, where varying historical contexts within groups can bias advantage estimation and degrade policy optimization. The proposed method, Hierarchy-of-Groups Policy Optimization (HGPO), assigns each step in rollout trajectories to multiple hierarchical groups based on historical context consistency, computes distinct advantages per group, and aggregates them with an adaptive weighting scheme to achieve a favorable bias-variance trade-off without extra models or rollouts. Experimental results on ALFWorld and WebShop tasks using Qwen2.5 models show that HGPO significantly outperforms existing agentic RL methods under the same computational constraints.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决长视野智能体任务中基于分组的逐步强化学习存在的上下文不一致问题，即同一组内步骤的历史上下文差异会导致优势估计偏差并损害策略优化。提出的方法——层次化分组策略优化（HGPO）——根据历史上下文一致性将轨迹中的每个步骤分配到多个层次化分组中，计算每个分组内的独立优势，并通过自适应加权方案进行聚合，从而在不增加额外模型或轨迹采样的条件下实现良好的偏差-方差权衡。在ALFWorld和WebShop任务上使用Qwen2.5模型的实验结果表明，在相同计算限制下，HGPO显著优于现有的智能体强化学习方法。</div>
</details>
</div>
<div class="card">
<div class="title">VolleyBots: A Testbed for Multi-Drone Volleyball Game Combining Motion Control and Strategic Play</div>
<div class="meta-line">Authors: Zelai Xu, Ruize Zhang, Chao Yu, Huining Yuan, Xiangmin Yi, Shilong Ji, Chuqi Wang, Wenhao Tang, Feng Gao, Wenbo Ding, Xinlei Chen, Yu Wang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-02-04T02:07:23+00:00 · Latest: 2026-02-26T09:51:13+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.01932v6">Abs</a> · <a href="https://arxiv.org/pdf/2502.01932v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robot sports, characterized by well-defined objectives, explicit rules, and dynamic interactions, present ideal scenarios for demonstrating embodied intelligence. In this paper, we present VolleyBots, a novel robot sports testbed where multiple drones cooperate and compete in the sport of volleyball under physical dynamics. VolleyBots integrates three features within a unified platform: competitive and cooperative gameplay, turn-based interaction structure, and agile 3D maneuvering. These intertwined features yield a complex problem combining motion control and strategic play, with no available expert demonstrations. We provide a comprehensive suite of tasks ranging from single-drone drills to multi-drone cooperative and competitive tasks, accompanied by baseline evaluations of representative reinforcement learning (RL), multi-agent reinforcement learning (MARL) and game-theoretic algorithms. Simulation results show that on-policy RL methods outperform off-policy methods in single-agent tasks, but both approaches struggle in complex tasks that combine motion control and strategic play. We additionally design a hierarchical policy which achieves 69.5% win rate against the strongest baseline in the 3 vs 3 task, demonstrating its potential for tackling the complex interplay between low-level control and high-level strategy. To highlight VolleyBots&#x27; sim-to-real potential, we further demonstrate the zero-shot deployment of a policy trained entirely in simulation on real-world drones.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VolleyBots：融合运动控制与策略博弈的多无人机排球对抗测试平台</div>
<div class="mono" style="margin-top:8px">机器人运动以其目标明确、规则清晰和动态交互的特点，成为展现具身智能的理想场景。本文提出VolleyBots——一个创新的机器人运动测试平台，支持多架无人机在物理动力学约束下开展排球运动的协作与对抗。该平台在统一框架内集成三大特征：竞争与协作并存的游戏机制、回合制交互结构、敏捷三维机动能力。这些交织特性构成了融合运动控制与策略博弈的复杂问题，且缺乏现成的专家示范数据。我们构建了从单机训练到多机协作/对抗的完整任务体系，并对代表性强化学习（RL）、多智能体强化学习（MARL）及博弈论算法进行基线评估。仿真结果表明：在单智能体任务中，同策略RL方法优于异策略方法，但两类方法在融合运动控制与策略博弈的复杂任务中均面临挑战。我们进一步设计了分层策略，在3对3任务中对战最强基线模型获得69.5%胜率，展现了其在协调底层控制与高层策略复杂交互方面的潜力。为凸显平台从仿真到现实的迁移能力，我们实现了完全在仿真环境中训练的策略在真实无人机上的零样本部署验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for a testbed that combines physical dynamics with strategic decision-making in robot sports, this paper introduces VolleyBots, a multi-drone volleyball platform integrating competitive-cooperative gameplay, turn-based interactions, and agile 3D maneuvering. The method involves designing a suite of tasks from single-drone to multi-agent scenarios and evaluating baseline reinforcement learning and game-theoretic algorithms. Experimental results reveal that on-policy RL outperforms off-policy methods in simpler tasks, but both struggle in complex motion-strategy combinations; a hierarchical policy achieved a 69.5% win rate in 3 vs 3 play and demonstrated zero-shot sim-to-real transfer to physical drones.</div>
<div class="mono" style="margin-top:8px">本文的动机是建立一个结合物理动力学与战略决策的机器人体育测试平台，为此提出了VolleyBots——一个多无人机排球系统，整合了竞争合作游戏、回合制交互和敏捷三维机动。方法上，设计了一系列从单无人机到多智能体的任务，并评估了强化学习和博弈论基线算法。实验结果表明，在简单任务中，同策略强化学习优于异策略方法，但两者在复杂的运动控制与策略结合任务中均表现不佳；所设计的分层策略在3对3任务中取得了69.5%的胜率，并实现了从仿真到真实无人机的零样本策略迁移。</div>
</details>
</div>
<div class="card">
<div class="title">Communication-Guided Multi-Mutation Differential Evolution for Crop Model Calibration</div>
<div class="meta-line">Authors: Sakshi Aggarwal, Mudasir Ganaie, Mukesh Saini</div>
<div class="meta-line">First: 2026-02-26T09:40:58+00:00 · Latest: 2026-02-26T09:40:58+00:00</div>
<div class="meta-line">Comments: Submitted to Special Issue on Learning-assisted Swarm Intelligence and Evolutionary Computation: Theories, Algorithms, and Applications of IEEE Transactions on Evolutionary Computation</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22804v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22804v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we propose a multi-mutation optimization algorithm, Differential Evolution with Multi-Mutation Operator-Guided Communication (DE-MMOGC), implemented to improve the performance and convergence abilities of standard differential evolution in uncertain environments. DE-MMOGC introduces a communication-guided scheme integrated with multiple mutation operators to encourage exploration and avoid premature convergence. Along with this, it includes a dynamic operator selection mechanism to use the best-performing operator over successive generations. To assimilate real-world uncertainties and missing observations into the predictive model, the proposed algorithm is combined with the Ensemble Kalman Filter. To evaluate the efficacy of the proposed DE-MMOGC in uncertain systems, the unified framework is applied to improve the predictive accuracy of crop simulation models. These simulation models are essential to precision agriculture, as they make it easier to estimate crop growth in a variety of unpredictable weather scenarios. Additionally, precisely calibrating these models raises a challenge due to missing observations. Hence, the simplified WOFOST crop simulation model is incorporated in this study for leaf area index (LAI)-based crop yield estimation. DE-MMOGC enhances the WOFOST performance by optimizing crucial weather parameters (temperature and rainfall), since these parameters are highly uncertain across different crop varieties, such as wheat, rice, and cotton. The experimental study shows that DE-MMOGC outperforms the traditional evolutionary optimizers and achieves better correlation with real LAI values. We found that DE-MMOGC is a resilient solution for crop monitoring.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向作物模型校准的通信引导多变异差分进化算法</div>
<div class="mono" style="margin-top:8px">本文提出一种多变异优化算法——多变异算子引导通信差分进化（DE-MMOGC），旨在提升标准差分进化算法在不确定环境下的性能与收敛能力。DE-MMOGC通过融合通信引导机制与多种变异算子，增强探索性并避免早熟收敛，同时引入动态算子选择机制以逐代选用最优算子。为将现实世界的不确定性与缺失观测纳入预测模型，该算法与集合卡尔曼滤波相结合。为评估DE-MMOGC在不确定系统中的有效性，该统一框架被应用于提升作物模拟模型的预测精度。此类模型对精准农业至关重要，可辅助估算作物在多变不可预测天气条件下的生长状况。然而，因观测数据缺失，模型精确校准面临挑战。为此，本研究引入简化的WOFOST作物模拟模型，基于叶面积指数（LAI）进行作物产量估算。DE-MMOGC通过优化关键气象参数（温度与降雨）提升WOFOST性能，这些参数在小麦、水稻、棉花等不同作物品种间存在高度不确定性。实验表明，DE-MMOGC优于传统进化优化器，且与真实LAI值具有更高相关性，证明其为作物监测提供了稳健解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of calibrating crop simulation models under uncertain conditions, such as missing observations and unpredictable weather, which are critical for precision agriculture. It proposes DE-MMOGC, a differential evolution algorithm enhanced with a communication-guided scheme that integrates multiple mutation operators to promote exploration and prevent premature convergence, alongside a dynamic selection mechanism to favor the best-performing operators over generations; this is combined with the Ensemble Kalman Filter to handle uncertainties. Experimental results using the WOFOST model for leaf area index-based yield estimation across crops like wheat, rice, and cotton demonstrate that DE-MMOGC outperforms traditional evolutionary optimizers, achieving better correlation with real LAI values and proving to be a resilient solution for crop monitoring.</div>
<div class="mono" style="margin-top:8px">本文针对精准农业中作物模拟模型校准的挑战，即在观测数据缺失和天气不确定等条件下提高预测精度。研究提出了DE-MMOGC算法，这是一种改进的差分进化算法，通过引入通信引导机制整合多种变异算子以增强探索能力、避免早熟收敛，并采用动态算子选择机制在迭代中优选性能最佳算子，同时结合集合卡尔曼滤波处理不确定性。实验基于WOFOST模型对小麦、水稻和棉花等作物的叶面积指数进行产量估计，结果表明DE-MMOGC优于传统进化优化器，与真实叶面积指数值具有更好的相关性，为作物监测提供了稳健的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Unleashing the Potential of Diffusion Models for End-to-End Autonomous Driving</div>
<div class="meta-line">Authors: Yinan Zheng, Tianyi Tan, Bin Huang, Enguang Liu, Ruiming Liang, Jianlin Zhang, Jianwei Cui, Guang Chen, Kun Ma, Hangjun Ye, Long Chen, Ya-Qin Zhang, Xianyuan Zhan, Jingjing Liu</div>
<div class="meta-line">First: 2026-02-26T09:37:38+00:00 · Latest: 2026-02-26T09:37:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22801v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22801v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models have become a popular choice for decision-making tasks in robotics, and more recently, are also being considered for solving autonomous driving tasks. However, their applications and evaluations in autonomous driving remain limited to simulation-based or laboratory settings. The full strength of diffusion models for large-scale, complex real-world settings, such as End-to-End Autonomous Driving (E2E AD), remains underexplored. In this study, we conducted a systematic and large-scale investigation to unleash the potential of the diffusion models as planners for E2E AD, based on a tremendous amount of real-vehicle data and road testing. Through comprehensive and carefully controlled studies, we identify key insights into the diffusion loss space, trajectory representation, and data scaling that significantly impact E2E planning performance. Moreover, we also provide an effective reinforcement learning post-training strategy to further enhance the safety of the learned planner. The resulting diffusion-based learning framework, Hyper Diffusion Planner} (HDP), is deployed on a real-vehicle platform and evaluated across 6 urban driving scenarios and 200 km of real-world testing, achieving a notable 10x performance improvement over the base model. Our work demonstrates that diffusion models, when properly designed and trained, can serve as effective and scalable E2E AD planners for complex, real-world autonomous driving tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>释放扩散模型在端到端自动驾驶中的潜力</div>
<div class="mono" style="margin-top:8px">扩散模型已成为机器人决策任务的热门选择，最近也被考虑用于解决自动驾驶任务。然而，其在自动驾驶中的应用与评估仍局限于仿真或实验室环境。扩散模型在大规模复杂现实场景（如端到端自动驾驶）中的全部潜力尚未充分探索。本研究基于大量实车数据与道路测试，通过系统化、大规模的研究，释放扩散模型作为端到端自动驾驶规划器的潜力。通过全面且严格控制的实验，我们揭示了扩散损失空间、轨迹表示和数据规模对端到端规划性能的关键影响。此外，我们还提出了一种有效的强化学习后训练策略，以进一步提升所学规划器的安全性。所提出的基于扩散的学习框架——超扩散规划器（HDP）已部署于实车平台，并在6种城市驾驶场景和200公里的真实道路测试中进行了评估，相比基础模型实现了10倍的性能提升。我们的研究表明，经过合理设计与训练的扩散模型，能够作为复杂现实自动驾驶任务中高效且可扩展的端到端规划器。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the underexplored potential of diffusion models for real-world end-to-end autonomous driving (E2E AD), motivated by their limited application beyond simulations. The method involves a systematic investigation using extensive real-vehicle data to develop the Hyper Diffusion Planner (HDP), incorporating insights into diffusion loss, trajectory representation, and data scaling, along with a reinforcement learning post-training strategy for safety. Experimental results from deployment on a real vehicle across 6 urban scenarios and 200 km of testing show a 10x performance improvement over the base model, demonstrating the framework&#x27;s effectiveness in complex, real-world driving tasks.</div>
<div class="mono" style="margin-top:8px">本研究针对扩散模型在现实世界端到端自动驾驶中潜力未充分探索的问题，其动机在于现有应用多局限于仿真环境。方法上，基于大量实车数据进行了系统性研究，开发了超扩散规划器，通过分析扩散损失空间、轨迹表示和数据缩放等关键因素，并结合强化学习后训练策略以提升安全性。实验结果表明，该框架在实车平台上经过6种城市场景和200公里测试，性能相比基础模型提升了10倍，验证了其在复杂现实驾驶任务中的有效性和可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">QSIM: Mitigating Overestimation in Multi-Agent Reinforcement Learning via Action Similarity Weighted Q-Learning</div>
<div class="meta-line">Authors: Yuanjun Li, Bin Zhang, Hao Chen, Zhouyang Jiang, Dapeng Li, Zhiwei Xu</div>
<div class="meta-line">First: 2026-02-26T09:20:46+00:00 · Latest: 2026-02-26T09:20:46+00:00</div>
<div class="meta-line">Comments: 19 pages, 15 figures, 7tables. Accepted to the 36th International Conference on Automated Planning and Scheduling (ICAPS 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22786v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22786v1">PDF</a> · <a href="https://github.com/MaoMaoLYJ/pymarl-qsim">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Value decomposition (VD) methods have achieved remarkable success in cooperative multi-agent reinforcement learning (MARL). However, their reliance on the max operator for temporal-difference (TD) target calculation leads to systematic Q-value overestimation. This issue is particularly severe in MARL due to the combinatorial explosion of the joint action space, which often results in unstable learning and suboptimal policies. To address this problem, we propose QSIM, a similarity weighted Q-learning framework that reconstructs the TD target using action similarity. Instead of using the greedy joint action directly, QSIM forms a similarity weighted expectation over a structured near-greedy joint action space. This formulation allows the target to integrate Q-values from diverse yet behaviorally related actions while assigning greater influence to those that are more similar to the greedy choice. By smoothing the target with structurally relevant alternatives, QSIM effectively mitigates overestimation and improves learning stability. Extensive experiments demonstrate that QSIM can be seamlessly integrated with various VD methods, consistently yielding superior performance and stability compared to the original algorithms. Furthermore, empirical analysis confirms that QSIM significantly mitigates the systematic value overestimation in MARL. Code is available at https://github.com/MaoMaoLYJ/pymarl-qsim.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>QSIM：通过动作相似性加权Q学习缓解多智能体强化学习中的过高估计问题</div>
<div class="mono" style="margin-top:8px">值分解方法在协作式多智能体强化学习中取得了显著成功，但其时序差分目标计算对最大化算子的依赖会导致系统性的Q值过高估计。由于联合动作空间的组合爆炸，该问题在MARL中尤为严重，常导致学习过程不稳定与策略次优。为此，我们提出QSIM——一种基于动作相似性加权Q学习的框架，通过动作相似性重构时序差分目标。QSIM不直接采用贪婪联合动作，而是在结构化近贪婪联合动作空间上构建相似性加权期望。该机制使目标能够整合行为相关但多样化的动作Q值，并为与贪婪选择更相似的动作赋予更高权重。通过用结构相关的替代方案平滑目标，QSIM有效缓解了过高估计并提升了学习稳定性。大量实验表明，QSIM可无缝集成至多种值分解方法，相较原算法持续产生更优的性能与稳定性。实证分析进一步证实，QSIM显著缓解了MARL中的系统性值过高估计。代码发布于https://github.com/MaoMaoLYJ/pymarl-qsim。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the problem of systematic Q-value overestimation in cooperative multi-agent reinforcement learning (MARL), which arises from the use of the max operator in value decomposition methods and is exacerbated by the combinatorial joint action space, leading to unstable learning and suboptimal policies. To mitigate this, the authors propose QSIM, a similarity weighted Q-learning framework that reconstructs the temporal-difference target by forming a weighted expectation over a structured near-greedy joint action space based on action similarity, thereby smoothing the target with behaviorally related alternatives to reduce overestimation and enhance stability. Experimental results show that QSIM can be integrated with various value decomposition methods, consistently improving performance and stability while significantly alleviating systematic overestimation, as validated through extensive testing.</div>
<div class="mono" style="margin-top:8px">本文针对协作多智能体强化学习（MARL）中因值分解方法使用最大化算子导致的系统性Q值高估问题，该问题因联合动作空间的组合爆炸而加剧，常引发学习不稳定和策略次优。为解决此问题，作者提出了QSIM，一种基于动作相似性的加权Q学习框架，它通过在一个结构化的近贪婪联合动作空间上构建加权期望来重构时序差分目标，从而利用行为相关的替代动作平滑目标以减少高估并提升稳定性。实验结果表明，QSIM可与多种值分解方法无缝集成，在广泛测试中一致展现出更优的性能和稳定性，并显著缓解了系统性值高估问题。</div>
</details>
</div>
<div class="card">
<div class="title">Soft Sequence Policy Optimization</div>
<div class="meta-line">Authors: Svetlana Glazyrina, Maksim Kryzhanovskiy, Roman Ischenko</div>
<div class="meta-line">First: 2026-02-22T20:21:00+00:00 · Latest: 2026-02-26T08:54:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19327v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.19327v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A significant portion of recent research on Large Language Model (LLM) alignment focuses on developing new policy optimization methods based on Group Relative Policy Optimization (GRPO). Two prominent directions have emerged: (i) a shift toward sequence-level importance sampling weights that better align with the sequence-level rewards used in many tasks, and (ii) alternatives to PPO-style clipping that aim to avoid the associated loss of training signal and entropy collapse. We introduce Soft Sequence Policy Optimization, an off-policy reinforcement learning objective that incorporates soft gating functions over token-level probability ratios within sequence-level importance weights. We provide theoretical motivation for SSPO and investigate practical modifications to improve optimization behavior. Empirically, we show that SSPO improves training stability and performance in mathematical reasoning tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>软序列策略优化</div>
<div class="mono" style="margin-top:8px">近期大语言模型对齐研究的重要方向是基于群体相对策略优化开发新型策略优化方法，主要呈现两大趋势：一是转向与序列级奖励更匹配的序列级重要性采样权重；二是探索替代PPO式裁剪的方法以避免训练信号丢失和熵崩溃。本文提出的软序列策略优化是一种离策略强化学习目标，通过在序列级重要性权重中引入基于词元级概率比的软门控函数。我们为SSPO提供了理论依据，并通过实践改进优化了算法性能。实验表明，SSPO在数学推理任务中显著提升了训练稳定性和模型表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to improve upon existing policy optimization methods for Large Language Model alignment, particularly addressing limitations of sequence-level reward alignment and PPO-style clipping, this paper introduces Soft Sequence Policy Optimization (SSPO). The method employs an off-policy reinforcement learning objective that integrates soft gating functions over token-level probability ratios within sequence-level importance weights, supported by theoretical justification and practical modifications to enhance optimization. Experimental results demonstrate that SSPO leads to improved training stability and performance in mathematical reasoning tasks.</div>
<div class="mono" style="margin-top:8px">本文的动机是改进现有的大语言模型对齐策略优化方法，特别是解决序列级奖励对齐和PPO式裁剪的局限性，提出了软序列策略优化方法。该方法采用离策略强化学习目标，在序列级重要性权重中引入对令牌级概率比的软门控函数，并提供了理论依据和实际优化调整。实验结果表明，SSPO在数学推理任务中提高了训练稳定性和性能表现。</div>
</details>
</div>
<div class="card">
<div class="title">Know What You Know: Metacognitive Entropy Calibration for Verifiable RL Reasoning</div>
<div class="meta-line">Authors: Qiannian Zhao, Chen Yang, Jinhao Jing, Yunke Zhang, Xuhui Ren, Lu Yu, Shijie Zhang, Hongzhi Yin</div>
<div class="meta-line">First: 2026-02-26T08:40:06+00:00 · Latest: 2026-02-26T08:40:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22751v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22751v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models (LRMs) have emerged as a powerful paradigm for solving complex real-world tasks. In practice, these models are predominantly trained via Reinforcement Learning with Verifiable Rewards (RLVR), yet most existing outcome-only RLVR pipelines rely almost exclusively on a binary correctness signal and largely ignore the model&#x27;s intrinsic uncertainty. We term this discrepancy the uncertainty-reward mismatch, under which high- and low-uncertainty solutions are treated equivalently, preventing the policy from &quot;Know What You Know&quot; and impeding the shift from optimizing for correct answers to optimizing effective reasoning paths. This limitation is especially critical in reasoning-centric tasks such as mathematics and question answering, where performance hinges on the quality of the model&#x27;s internal reasoning process rather than mere memorization of final answers. To address this, we propose EGPO, a metacognitive entropy calibration framework that explicitly integrates intrinsic uncertainty into RLVR for enhancing LRMs. EGPO estimates per-sample uncertainty using a zero-overhead entropy proxy derived from token-level likelihoods and aligns it with extrinsic correctness through an asymmetric calibration mechanism that preserves correct reasoning while selectively regulating overconfident failures, thereby enabling stable and uncertainty-aware policy optimization. Moreover, EGPO recovers informative learning signals from otherwise degenerate group-based rollouts without modifying the verifier or reward definition. Extensive experiments across multiple benchmarks demonstrate that the proposed EGPO leads to substantial and consistent improvements in reasoning performance, establishing a principled path for advancing LRMs through metacognitive entropy calibration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>认知自知：基于元认知熵校准的可验证强化学习推理框架</div>
<div class="mono" style="margin-top:8px">大型推理模型已成为解决复杂现实任务的重要范式。实践中，这类模型主要通过可验证奖励的强化学习进行训练，但现有仅依赖结果判定的RLVR流程几乎完全基于二元正确性信号，普遍忽略了模型的内在不确定性。我们将此现象称为“不确定性与奖励失配”——在此机制下，高不确定性与低不确定性解决方案被等同对待，阻碍了策略实现“认知自知”能力，也制约了从优化正确答案到优化有效推理路径的范式转变。这一局限在数学推理和问答等以推理为核心的任务中尤为关键，因为其性能表现取决于模型内部推理过程的质量，而非对最终答案的机械记忆。为此，我们提出EGPO框架：一种通过显式整合内在不确定性来增强大型推理模型的元认知熵校准方法。EGPO利用基于词元级似然值的零开销熵代理来估计样本级不确定性，并通过非对称校准机制将其与外部正确性对齐——该机制在保留正确推理的同时选择性地调节过度自信的错误，从而实现稳定且具有不确定性感知的策略优化。此外，EGPO无需修改验证器或奖励定义，即可从原本退化的分组推演中恢复有效的学习信号。跨多个基准的广泛实验表明，EGPO能持续显著提升推理性能，为通过元认知熵校准推进大型推理模型发展提供了理论路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the uncertainty-reward mismatch in Reinforcement Learning with Verifiable Rewards (RLVR) for large reasoning models, where binary correctness signals ignore intrinsic uncertainty, hindering the optimization of reasoning paths. To resolve this, the authors propose EGPO, a metacognitive entropy calibration framework that estimates per-sample uncertainty via token-level likelihoods and aligns it with extrinsic correctness through asymmetric calibration, preserving correct reasoning while regulating overconfident failures without modifying the verifier. Experimental results across multiple benchmarks show that EGPO yields substantial and consistent improvements in reasoning performance, advancing LRMs through principled uncertainty-aware optimization.</div>
<div class="mono" style="margin-top:8px">本文针对大型推理模型在基于可验证奖励的强化学习中存在的“不确定性-奖励不匹配”问题，即二元正确性信号忽略了模型的内在不确定性，从而阻碍了推理路径的优化。为此，作者提出了EGPO，一种元认知熵校准框架，它通过基于词元级似然的零开销熵代理来估计每个样本的不确定性，并通过非对称校准机制将其与外部正确性对齐，在保留正确推理的同时选择性调节过度自信的错误，且无需修改验证器或奖励定义。在多个基准测试上的广泛实验表明，EGPO能显著且一致地提升推理性能，为通过元认知熵校准推进大型推理模型提供了原则性路径。</div>
</details>
</div>
<div class="card">
<div class="title">Generative Recommendation for Large-Scale Advertising</div>
<div class="meta-line">Authors: Ben Xue, Dan Liu, Lixiang Wang, Mingjie Sun, Peng Wang, Pengfei Zhang, Shaoyun Shi, Tianyu Xu, Yunhao Sha, Zhiqiang Liu, Bo Kong, Bo Wang, Hang Yang, Jieting Xue, Junhao Wang, Shengyu Wang, Shuping Hui, Wencai Ye, Xiao Lin, Yongzhi Li, Yuhang Chen, Zhihui Yin, Quan Chen, Shiyang Wen, Wenjin Wu, Han Li, Guorui Zhou, Changcheng Li, Peng Jiang</div>
<div class="meta-line">First: 2026-02-26T08:15:26+00:00 · Latest: 2026-02-26T08:15:26+00:00</div>
<div class="meta-line">Comments: 13 pages, 6 figures, under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22732v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22732v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative recommendation has recently attracted widespread attention in industry due to its potential for scaling and stronger model capacity. However, deploying real-time generative recommendation in large-scale advertising requires designs beyond large-language-model (LLM)-style training and serving recipes. We present a production-oriented generative recommender co-designed across architecture, learning, and serving, named GR4AD (Generative Recommendation for ADdvertising). As for tokenization, GR4AD proposes UA-SID (Unified Advertisement Semantic ID) to capture complicated business information. Furthermore, GR4AD introduces LazyAR, a lazy autoregressive decoder that relaxes layer-wise dependencies for short, multi-candidate generation, preserving effectiveness while reducing inference cost, which facilitates scaling under fixed serving budgets. To align optimization with business value, GR4AD employs VSL (Value-Aware Supervised Learning) and proposes RSPO (Ranking-Guided Softmax Preference Optimization), a ranking-aware, list-wise reinforcement learning algorithm that optimizes value-based rewards under list-level metrics for continual online updates. For online inference, we further propose dynamic beam serving, which adapts beam width across generation levels and online load to control compute. Large-scale online A/B tests show up to 4.2% ad revenue improvement over an existing DLRM-based stack, with consistent gains from both model scaling and inference-time scaling. GR4AD has been fully deployed in Kuaishou advertising system with over 400 million users and achieves high-throughput real-time serving.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向大规模广告的生成式推荐</div>
<div class="mono" style="margin-top:8px">生成式推荐因其可扩展性和更强的模型能力，近期在工业界受到广泛关注。然而，在大规模广告中部署实时生成式推荐需要超越大语言模型（LLM）式训练与服务方案的设计。我们提出了一种面向生产的生成式推荐系统GR4AD（面向广告的生成式推荐），在架构、学习与服务层面进行协同设计。在标记化方面，GR4AD提出UA-SID（统一广告语义标识）以捕捉复杂的业务信息。此外，GR4AD引入LazyAR，一种惰性自回归解码器，通过放松层间依赖关系来生成简短的多候选结果，在保持效果的同时降低推理成本，从而在固定服务预算下促进扩展。为使优化与业务价值对齐，GR4AD采用VSL（价值感知监督学习）并提出RSPO（排序引导的Softmax偏好优化），这是一种排序感知的列表级强化学习算法，可在列表级指标下优化基于价值的奖励，实现持续在线更新。对于在线推理，我们进一步提出动态束搜索服务，通过在不同生成层级和在线负载间自适应调整束宽度来控制计算量。大规模在线A/B测试显示，相较于现有基于DLRM的架构，广告收入提升最高达4.2%，模型扩展和推理时扩展均带来持续收益。GR4AD已在拥有超4亿用户的快手广告系统中全面部署，实现了高吞吐的实时服务。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to deploy real-time generative recommendation in large-scale advertising beyond standard LLM approaches, this paper introduces GR4AD, a production-oriented system co-designed across architecture, learning, and serving. The method employs a Unified Advertisement Semantic ID (UA-SID) for tokenization, a Lazy Autoregressive (LazyAR) decoder for efficient multi-candidate generation, and a ranking-aware reinforcement learning algorithm called RSPO for value-aligned optimization. Main experimental results from large-scale online A/B tests demonstrate up to a 4.2% improvement in ad revenue compared to a DLRM-based baseline, with gains from both model and inference scaling, and the system has been successfully deployed serving over 400 million users.</div>
<div class="mono" style="margin-top:8px">本文旨在解决大规模广告中实时生成式推荐部署的挑战，提出了一个面向生产的系统GR4AD，其在架构、学习和服务端协同设计。方法上，采用统一广告语义ID进行分词，引入惰性自回归解码器以高效生成多个候选，并提出基于排序的强化学习算法RSPO进行价值对齐优化。主要实验结果表明，通过大规模在线A/B测试，相比基于DLRM的基线系统，广告收入提升最高达4.2%，实现了模型和推理扩展的收益，该系统已成功部署于拥有超过4亿用户的快手广告平台。</div>
</details>
</div>
<div class="card">
<div class="title">RLHFless: Serverless Computing for Efficient RLHF</div>
<div class="meta-line">Authors: Rui Wei, Hanfei Yu, Shubham Jain, Yogarajan Sivakumar, Devesh Tiwari, Jian Li, Seung-Jong Park, Hao Wang</div>
<div class="meta-line">First: 2026-02-26T07:45:37+00:00 · Latest: 2026-02-26T07:45:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22718v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22718v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning from Human Feedback (RLHF) has been widely applied to Large Language Model (LLM) post-training to align model outputs with human preferences. Recent models, such as DeepSeek-R1, have also shown RLHF&#x27;s potential to improve LLM reasoning on complex tasks. In RL, inference and training co-exist, creating dynamic resource demands throughout the workflow. Compared to traditional RL, RLHF further challenges training efficiency due to expanding model sizes and resource consumption. Several RLHF frameworks aim to balance flexible abstraction and efficient execution. However, they rely on serverful infrastructures, which struggle with fine-grained resource variability. As a result, during synchronous RLHF training, idle time between or within RL components often causes overhead and resource wastage.
  To address these issues, we present RLHFless, the first scalable training framework for synchronous RLHF, built on serverless computing environments. RLHFless adapts to dynamic resource demands throughout the RLHF pipeline, pre-computes shared prefixes to avoid repeated computation, and uses a cost-aware actor scaling strategy that accounts for response length variation to find sweet spots with lower cost and higher speed. In addition, RLHFless assigns workloads efficiently to reduce intra-function imbalance and idle time. Experiments on both physical testbeds and a large-scale simulated cluster show that RLHFless achieves up to 1.35x speedup and 44.8% cost reduction compared to the state-of-the-art baseline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RLHFless：基于无服务器计算的高效RLHF框架</div>
<div class="mono" style="margin-top:8px">基于人类反馈的强化学习（RLHF）已广泛应用于大语言模型（LLM）的后训练阶段，以使模型输出与人类偏好对齐。近期模型（如DeepSeek-R1）也展现了RLHF在提升LLM复杂任务推理能力方面的潜力。在强化学习中，推理与训练并存，导致工作流全程存在动态资源需求。与传统强化学习相比，RLHF因模型规模扩大和资源消耗增长，对训练效率提出更高挑战。现有RLHF框架虽致力于平衡灵活抽象与高效执行，但均依赖服务器化基础设施，难以应对细粒度资源波动。这导致同步RLHF训练过程中，强化学习组件间或组件内的空闲时间常引发开销与资源浪费。
为解决上述问题，我们提出RLHFless——首个基于无服务器计算环境的可扩展同步RLHF训练框架。该框架能自适应RLHF流程中的动态资源需求，通过预计算共享前缀避免重复运算，并采用考虑响应长度变化的成本感知型智能体扩缩策略，以寻求成本更低、速度更优的平衡点。此外，RLHFless通过高效分配工作负载减少函数内不平衡与空闲时间。在物理测试平台与大规模模拟集群上的实验表明，相较于现有最优基线，RLHFless可实现最高1.35倍的加速与44.8%的成本降低。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for RLHFless stems from the inefficiencies in existing Reinforcement Learning from Human Feedback (RLHF) frameworks, which rely on serverful infrastructures that poorly handle the dynamic and fine-grained resource demands of synchronous RLHF training, leading to idle time and resource wastage. The method introduces a scalable training framework built on serverless computing, which adapts to dynamic resource needs, pre-computes shared prefixes to avoid redundant calculations, employs a cost-aware actor scaling strategy to optimize for response length variation, and efficiently assigns workloads to reduce imbalance and idle time. Experimental results on physical testbeds and a simulated large-scale cluster demonstrate that RLHFless achieves up to 1.35x speedup and a 44.8% cost reduction compared to state-of-the-art baselines.</div>
<div class="mono" style="margin-top:8px">RLHFless的提出动机源于现有基于人类反馈的强化学习（RLHF）框架效率低下，这些框架依赖服务器基础设施，难以处理同步RLHF训练中动态、细粒度的资源需求，导致空闲时间和资源浪费。该方法首次在无服务器计算环境中构建了一个可扩展的同步RLHF训练框架，通过适应动态资源需求、预计算共享前缀以避免重复计算、采用考虑响应长度变化的成本感知执行器缩放策略来寻找成本与速度的最佳平衡点，并高效分配工作负载以减少内部不平衡和空闲时间。在物理测试平台和大规模模拟集群上的实验结果表明，与最先进的基线相比，RLHFless实现了最高1.35倍的加速和44.8%的成本降低。</div>
</details>
</div>
<div class="card">
<div class="title">Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning</div>
<div class="meta-line">Authors: Ruize Zhang, Sirui Xiang, Zelai Xu, Feng Gao, Shilong Ji, Wenhao Tang, Wenbo Ding, Chao Yu, Yu Wang</div>
<div class="meta-line">Venue: CoRL 2025</div>
<div class="meta-line">First: 2025-05-07T11:04:36+00:00 · Latest: 2026-02-26T07:37:05+00:00</div>
<div class="meta-line">Comments: Accepted by CoRL 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.04317v5">Abs</a> · <a href="https://arxiv.org/pdf/2505.04317v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hi-co-self-play.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we tackle the problem of learning to play 3v3 multi-drone volleyball, a new embodied competitive task that requires both high-level strategic coordination and low-level agile control. The task is turn-based, multi-agent, and physically grounded, posing significant challenges due to its long-horizon dependencies, tight inter-agent coupling, and the underactuated dynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play (HCSP), a hierarchical reinforcement learning framework that separates centralized high-level strategic decision-making from decentralized low-level motion control. We design a three-stage population-based training pipeline to enable both strategy and skill to emerge from scratch without expert demonstrations: (I) training diverse low-level skills, (II) learning high-level strategy via self-play with fixed low-level skills, and (III) joint fine-tuning through co-self-play. Experiments show that HCSP achieves superior performance, outperforming non-hierarchical self-play and rule-based hierarchical baselines with an average 82.9% win rate and a 71.5% win rate against the two-stage variant. Moreover, co-self-play leads to emergent team behaviors such as role switching and coordinated formations, demonstrating the effectiveness of our hierarchical design and training scheme. The project page is at https://hi-co-self-play.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过分层协同自博弈强化学习掌握多无人机排球</div>
<div class="mono" style="margin-top:8px">本文研究如何学习3v3多无人机排球——一项新型具身竞争任务，要求高层战略协调与低层敏捷控制兼备。该任务基于回合制、多智能体且物理接地，因其长时程依赖、智能体间紧密耦合及四旋翼欠驱动动力学而带来显著挑战。为此，我们提出分层协同自博弈（HCSP），这是一种将集中式高层战略决策与分布式低层运动控制分离的分层强化学习框架。我们设计了三阶段基于群体的训练流程，使战略与技能均能从零开始涌现而无需专家示范：（I）训练多样化低层技能，（II）通过固定低层技能的自博弈学习高层战略，（III）通过协同自博弈进行联合微调。实验表明，HCSP实现了卓越性能，以平均82.9%的胜率超越非分层自博弈与基于规则的分层基线，并对两阶段变体取得71.5%的胜率。此外，协同自博弈催生出角色切换与协同编队等团队行为，验证了我们分层设计与训练方案的有效性。项目页面详见 https://hi-co-self-play.github.io。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the challenge of learning competitive multi-drone volleyball, a complex task requiring strategic team coordination and agile low-level control of quadrotors. The proposed method, Hierarchical Co-Self-Play Reinforcement Learning (HCSP), separates high-level strategic decision-making from decentralized low-level motion control and employs a three-stage population-based training pipeline to develop skills and strategy from scratch without expert data. Experimental results demonstrate that HCSP outperforms non-hierarchical and rule-based hierarchical baselines with an 82.9% average win rate, and co-self-play leads to the emergence of sophisticated team behaviors like role switching and coordinated formations.</div>
<div class="mono" style="margin-top:8px">本文旨在解决学习多无人机排球这一竞争性任务所面临的挑战，该任务需要高层次的团队策略协调与低层次的敏捷飞行控制。提出的方法为分层协同自博弈强化学习，它将集中式的高层策略决策与分布式的底层运动控制分离，并采用一个三阶段基于种群的训练流程，从零开始无需专家示范地发展技能与策略。实验结果表明，该方法优于非分层和基于规则的分层基线，平均胜率达到82.9%，并且协同自博弈促成了角色切换和协调队形等团队行为的涌现，验证了分层设计与训练方案的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Same Words, Different Judgments: Modality Effects on Preference Alignment</div>
<div class="meta-line">Authors: Aaron Broukhim, Nadir Weibel, Eshin Jolly</div>
<div class="meta-line">First: 2026-02-26T07:34:15+00:00 · Latest: 2026-02-26T07:34:15+00:00</div>
<div class="meta-line">Comments: Submitted to Interspeech 2026 for review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22710v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22710v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Preference-based reinforcement learning (PbRL) is the dominant framework for aligning AI systems to human preferences, but its application to speech remains underexplored. We present a controlled cross-modal study of human and synthetic preference annotations, comparing text and audio evaluations of identical semantic content across 100 prompts. Audio preferences prove as reliable as text, with inter-rater agreement reaching good levels (ICC(2,k) $\approx$ .80) at $\sim$9 raters -- the first ICC-based reliability characterization in the preference annotation literature for either modality. However, modality reshapes how people judge: audio raters exhibit narrower decision thresholds, reduced length bias, and more user-oriented evaluation criteria, with near-chance cross-modality agreement. Synthetic ratings further align with human judgments and predict inter-rater agreement, supporting their use both for triaging ambiguous pairs and as full replacements for human annotations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>同词异判：模态对偏好对齐的影响</div>
<div class="mono" style="margin-top:8px">基于偏好的强化学习（PbRL）是对齐人工智能系统与人类偏好的主流框架，但其在语音领域的应用仍待探索。我们通过一项受控跨模态研究，对比了100组相同语义内容在文本与音频两种形式下的人类标注与合成偏好标注。实验表明音频偏好与文本偏好具有同等可靠性，在约9名标注者时评分者间一致性达到良好水平（ICC(2,k)≈0.80）——这是偏好标注文献中首次基于ICC的跨模态可靠性表征。然而，模态显著影响评判方式：音频标注者表现出更严格的决策阈值、更弱的长度偏见及更注重用户体验的评估标准，跨模态一致性接近随机水平。合成评分与人类判断高度吻合，并能预测评分者间一致性，这支持其既可用于筛选模糊样本对，也可完全替代人工标注。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the impact of modality on preference alignment in AI systems, motivated by the underexplored application of preference-based reinforcement learning (PbRL) to speech. The method involves a controlled cross-modal comparison of human and synthetic preference annotations for identical semantic content across 100 prompts, evaluating both text and audio. Key experimental results show that audio preferences are as reliable as text, with good inter-rater agreement (ICC ≈ .80) at about nine raters, marking the first ICC-based reliability characterization in this literature. However, modality significantly influences judgment: audio raters demonstrate narrower decision thresholds, less length bias, and more user-oriented criteria, leading to near-chance cross-modality agreement. Synthetic ratings align well with human judgments and predict inter-rater agreement, supporting their use for triaging ambiguous pairs or replacing human annotations.</div>
<div class="mono" style="margin-top:8px">本研究探讨了模态对AI系统偏好对齐的影响，其动机在于基于偏好的强化学习在语音领域的应用尚未充分探索。方法上，通过一项受控的跨模态研究，比较了100个提示中相同语义内容的人类与合成偏好标注，涵盖文本和音频评估。主要实验结果表明，音频偏好的可靠性与文本相当，在约九名评分者时达到了良好的评分者间一致性（ICC ≈ .80），这是该文献中首次基于ICC的可靠性表征。然而，模态显著重塑了人们的判断方式：音频评分者表现出更窄的决策阈值、更少的长度偏见以及更以用户为导向的评估标准，导致跨模态一致性接近随机水平。合成评分与人类判断高度一致，并能预测评分者间一致性，支持其用于筛选模糊对或完全替代人类标注。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Geometric Perception in VLMs via Translator-Guided Reinforcement Learning</div>
<div class="meta-line">Authors: Hao Yu, Shuning Jia, Guanghao Li, Wenhao Jiang, Chun Yuan</div>
<div class="meta-line">First: 2026-02-26T07:28:04+00:00 · Latest: 2026-02-26T07:28:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22703v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22703v1">PDF</a> · <a href="https://github.com/Longin-Yu/GeoPerceive">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) often struggle with geometric reasoning due to their limited perception of fundamental diagram elements. To tackle this challenge, we introduce GeoPerceive, a benchmark comprising diagram instances paired with domain-specific language (DSL) representations, along with an efficient automatic data generation pipeline. This design enables the isolated evaluation of geometric perception independently from reasoning. To exploit the data provided by GeoPerceive for enhancing the geometric perception capabilities of VLMs, we propose GeoDPO, a translator-guided reinforcement learning (RL) framework. GeoDPO employs an NL-to-DSL translator, which is trained on synthetic pairs generated by the data engine of GeoPerceive, to bridge natural language and DSL. This translator facilitates the computation of fine-grained, DSL-level scores, which serve as reward signals in reinforcement learning. We assess GeoDPO on both in-domain and out-of-domain datasets, spanning tasks in geometric perception as well as downstream reasoning. Experimental results demonstrate that, while supervised fine-tuning (SFT) offers only marginal improvements and may even impair performance in out-of-domain scenarios, GeoDPO achieves substantial gains: $+26.5\%$ on in-domain data, $+8.0\%$ on out-of-domain data, and $+39.0\%$ on downstream reasoning tasks. These findings underscore the superior performance and generalization ability of GeoDPO over SFT. All codes are released at https://github.com/Longin-Yu/GeoPerceive
  to ensure reproducibility.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过翻译器引导的强化学习增强视觉语言模型的几何感知能力</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLM）因对基本图形元素的感知有限，常在几何推理方面表现不佳。为解决这一问题，我们提出了GeoPerceive，这是一个包含与领域特定语言（DSL）表示配对的图形实例的基准，以及一个高效的自动数据生成流程。该设计使得几何感知能够独立于推理进行单独评估。为利用GeoPerceive提供的数据增强VLM的几何感知能力，我们提出了GeoDPO，一种翻译器引导的强化学习（RL）框架。GeoDPO采用一个在GeoPerceive数据引擎生成的合成对上训练的NL-to-DSL翻译器，以桥接自然语言和DSL。该翻译器有助于计算细粒度的DSL级分数，这些分数在强化学习中作为奖励信号。我们在领域内和领域外数据集上评估GeoDPO，涵盖几何感知及下游推理任务。实验结果表明，尽管监督微调（SFT）仅带来边际改进，在领域外场景中甚至可能损害性能，但GeoDPO取得了显著提升：领域内数据上提升+26.5%，领域外数据上提升+8.0%，下游推理任务上提升+39.0%。这些发现凸显了GeoDPO相对于SFT的优越性能和泛化能力。所有代码已发布于https://github.com/Longin-Yu/GeoPerceive以确保可复现性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limited geometric reasoning capabilities of vision-language models (VLMs) due to poor perception of diagram elements, this paper introduces GeoPerceive, a benchmark with diagram-domain-specific language (DSL) pairs and a data generation pipeline to isolate perception evaluation. The method proposes GeoDPO, a translator-guided reinforcement learning framework that uses an NL-to-DSL translator trained on synthetic data to compute fine-grained DSL-level rewards for RL. Experimental results show that GeoDPO achieves substantial improvements: +26.5% on in-domain data, +8.0% on out-of-domain data, and +39.0% on downstream reasoning tasks, significantly outperforming supervised fine-tuning which offers only marginal gains or even harms out-of-domain performance.</div>
<div class="mono" style="margin-top:8px">针对视觉语言模型（VLMs）因对基本图形元素感知有限而导致的几何推理能力不足，本文提出了GeoPerceive基准，包含图表与领域特定语言（DSL）配对及自动数据生成流程，以独立评估几何感知。方法上，提出了GeoDPO，一种翻译器引导的强化学习框架，它利用在合成数据上训练的NL-to-DSL翻译器来计算细粒度的DSL级奖励信号。实验结果表明，GeoDPO取得了显著提升：在领域内数据上提高26.5%，在领域外数据上提高8.0%，在下游推理任务上提高39.0%，明显优于监督微调，后者仅带来边际改进甚至损害领域外性能。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcing Real-world Service Agents: Balancing Utility and Cost in Task-oriented Dialogue</div>
<div class="meta-line">Authors: Ning Gao, Wei Zhang, Yuqin Dai, Ling Shi, Ziyin Wang, Yujie Wang, Wei He, Jinpeng Wang, Chaozheng Wang</div>
<div class="meta-line">First: 2026-02-26T07:19:57+00:00 · Latest: 2026-02-26T07:19:57+00:00</div>
<div class="meta-line">Comments: 35 pages, 8 tables, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22697v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22697v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid evolution of Large Language Models (LLMs) has accelerated the transition from conversational chatbots to general agents. However, effectively balancing empathetic communication with budget-aware decision-making remains an open challenge. Since existing methods fail to capture these complex strategic trade-offs, we propose InteractCS-RL, a framework that reframes task-oriented dialogue as a multi-granularity reinforcement learning process. Specifically, we first establish a User-centric Interaction Framework to provide a high-fidelity training gym, enabling agents to dynamically explore diverse strategies with persona-driven users. Then, we introduce Cost-aware Multi-turn Policy Optimization (CMPO) with a hybrid advantage estimation strategy. By integrating generative process credits and employing a PID-Lagrangian cost controller, CMPO effectively guides the policy to explore Pareto boundary between user reward and global cost constraints. Extensive experiments on customized real business scenarios demonstrate that InteractCS-RL significantly outperform other baselines across three evaluation dimensions. Further evaluation on tool-agent-user interaction benchmarks verify InteractCS-RL robustness across diverse domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化现实世界服务智能体：面向任务对话中效用与成本的平衡</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）的快速发展加速了从对话式聊天机器人向通用智能体的转变。然而，如何在共情沟通与成本感知决策之间实现有效平衡仍是一个开放挑战。由于现有方法难以捕捉这些复杂的策略权衡，我们提出了InteractCS-RL框架，将面向任务的对话重构为多粒度强化学习过程。具体而言，我们首先建立以用户为中心的交互框架，提供高保真训练环境，使智能体能够与人格驱动的用户动态探索多样化策略。随后，我们引入具有混合优势估计策略的成本感知多轮策略优化（CMPO）。通过整合生成过程信用度并采用PID-Lagrangian成本控制器，CMPO有效引导策略探索用户奖励与全局成本约束之间的帕累托边界。在定制化真实商业场景上的大量实验表明，InteractCS-RL在三个评估维度上显著优于其他基线方法。在工具-智能体-用户交互基准上的进一步验证证实了InteractCS-RL跨领域的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of balancing empathetic communication with budget constraints in real-world service agents, this paper proposes InteractCS-RL, a framework that reframes task-oriented dialogue as a multi-granularity reinforcement learning process. The method establishes a user-centric interaction gym for training and introduces Cost-aware Multi-turn Policy Optimization (CMPO) with a hybrid advantage estimator and a PID-Lagrangian controller to navigate trade-offs between user reward and cost. Experimental results on real business scenarios show that InteractCS-RL outperforms baselines across multiple evaluation dimensions, with further benchmarks confirming its robustness across diverse domains.</div>
<div class="mono" style="margin-top:8px">本文针对现实世界服务智能体在任务导向对话中平衡共情沟通与预算约束的挑战，提出了InteractCS-RL框架，将任务对话重构为多粒度强化学习过程。该方法首先建立以用户为中心的高保真交互环境进行训练，并引入成本感知的多轮策略优化（CMPO），结合混合优势估计与PID-Lagrangian成本控制器，以引导策略在用户奖励和全局成本约束间探索帕累托边界。在定制化真实业务场景上的大量实验表明，InteractCS-RL在三个评估维度上显著优于基线方法，且在工具-智能体-用户交互基准测试中进一步验证了其跨领域的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">AHBid: An Adaptable Hierarchical Bidding Framework for Cross-Channel Advertising</div>
<div class="meta-line">Authors: Xinxin Yang, Yangyang Tang, Yikun Zhou, Yaolei Liu, Yun Li, Bo Yang</div>
<div class="meta-line">Venue: WWW</div>
<div class="meta-line">First: 2026-02-26T06:07:28+00:00 · Latest: 2026-02-26T06:07:28+00:00</div>
<div class="meta-line">Comments: 11 pages, 6 figures, accepted by WWW&#x27;2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22650v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22650v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In online advertising, the inherent complexity and dynamic nature of advertising environments necessitate the use of auto-bidding services to assist advertisers in bid optimization. This complexity is further compounded in multi-channel scenarios, where effective allocation of budgets and constraints across channels with distinct behavioral patterns becomes critical for optimizing return on investment. Current approaches predominantly rely on either optimization-based strategies or reinforcement learning techniques. However, optimization-based methods lack flexibility in adapting to dynamic market conditions, while reinforcement learning approaches often struggle to capture essential historical dependencies and observational patterns within the constraints of Markov Decision Process frameworks. To address these limitations, we propose AHBid, an Adaptable Hierarchical Bidding framework that integrates generative planning with real-time control. The framework employs a high-level generative planner based on diffusion models to dynamically allocate budgets and constraints by effectively capturing historical context and temporal patterns. We introduce a constraint enforcement mechanism to ensure compliance with specified constraints, along with a trajectory refinement mechanism that enhances adaptability to environmental changes through the utilization of historical data. The system further incorporates a control-based bidding algorithm that synergistically combines historical knowledge with real-time information, significantly improving both adaptability and operational efficacy. Extensive experiments conducted on large-scale offline datasets and through online A/B tests demonstrate the effectiveness of AHBid, yielding a 13.57% increase in overall return compared to existing baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AHBid：一种适用于跨渠道广告的自适应分层竞价框架</div>
<div class="mono" style="margin-top:8px">在线广告环境中固有的复杂性和动态性要求使用自动竞价服务来辅助广告主优化出价。在多渠道场景中，由于各渠道行为模式各异，如何有效分配预算和约束条件以优化投资回报变得尤为关键。现有方法主要依赖基于优化的策略或强化学习技术，但前者难以灵活适应动态市场条件，后者则在马尔可夫决策过程框架下常难以捕捉关键的历史依赖性和观测模式。为应对这些局限，本文提出AHBid——一种融合生成式规划与实时控制的自适应分层竞价框架。该框架采用基于扩散模型的高层生成规划器，通过有效捕捉历史上下文与时间模式来动态分配预算和约束；同时引入约束执行机制以确保符合指定约束，并利用轨迹优化机制借助历史数据增强对环境变化的适应性。系统还结合了基于控制的竞价算法，将历史知识与实时信息协同整合，显著提升了适应性与操作效能。基于大规模离线数据集和在线A/B测试的广泛实验表明，AHBid相比现有基线方法实现了13.57%的整体收益提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of auto-bidding in dynamic, multi-channel online advertising environments, where existing optimization-based methods lack flexibility and reinforcement learning struggles with historical dependencies. The authors propose AHBid, an adaptable hierarchical framework that integrates a high-level generative planner using diffusion models to allocate budgets and constraints by capturing historical context, alongside a control-based bidding algorithm for real-time adaptation. Experimental results on large-scale offline datasets and online A/B tests show that AHBid achieves a 13.57% increase in overall return compared to baseline methods.</div>
<div class="mono" style="margin-top:8px">本文针对动态多通道在线广告环境中自动出价的挑战，现有基于优化的方法缺乏灵活性，而强化学习方法难以处理历史依赖。作者提出了AHBid，一种适应性分层框架，它整合了基于扩散模型的高层生成规划器，通过捕捉历史上下文来分配预算和约束，并结合基于控制的出价算法实现实时适应。在大规模离线数据集和在线A/B测试上的实验结果表明，AHBid相比基线方法实现了13.57%的整体回报提升。</div>
</details>
</div>
<div class="card">
<div class="title">Compress the Easy, Explore the Hard: Difficulty-Aware Entropy Regularization for Efficient LLM Reasoning</div>
<div class="meta-line">Authors: Qin-Wen Luo, Sheng Ren, Xiang Chen, Rui Liu, Jun Fang, Naiqiang Tan, Sheng-Jun Huang</div>
<div class="meta-line">First: 2026-02-26T05:47:30+00:00 · Latest: 2026-02-26T05:47:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22642v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22642v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Chain-of-Thought (CoT) has substantially empowered Large Language Models (LLMs) to tackle complex reasoning tasks, yet the verbose nature of explicit reasoning steps incurs prohibitive inference latency and computational costs, limiting real-world deployment. While existing compression methods - ranging from self-training to Reinforcement Learning (RL) with length constraints - attempt to mitigate this, they often sacrifice reasoning capability for brevity. We identify a critical failure mode in these approaches: explicitly optimizing for shorter trajectories triggers rapid entropy collapse, which prematurely shrinks the exploration space and stifles the discovery of valid reasoning paths, particularly for challenging questions requiring extensive deduction. To address this issue, we propose Compress responses for Easy questions and Explore Hard ones (CEEH), a difficulty-aware approach to RL-based efficient reasoning. CEEH dynamically assesses instance difficulty to apply selective entropy regularization: it preserves a diverse search space for currently hard questions to ensure robustness, while permitting aggressive compression on easier instances where the reasoning path is well-established. In addition, we introduce a dynamic optimal-length penalty anchored to the historically shortest correct response, which effectively counteracts entropy-induced length inflation and stabilizes the reward signal. Across six reasoning benchmarks, CEEH consistently reduces response length while maintaining accuracy comparable to the base model, and improves Pass@k relative to length-only optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>易题压缩，难题探索：面向高效大语言模型推理的难度感知熵正则化方法</div>
<div class="mono" style="margin-top:8px">思维链（CoT）显著增强了大语言模型处理复杂推理任务的能力，但显式推理步骤的冗长特性导致极高的推理延迟和计算成本，限制了实际部署。现有压缩方法（从自训练到带长度约束的强化学习）虽试图缓解此问题，却常为简洁性牺牲推理能力。我们发现这些方法存在关键缺陷：显式优化短轨迹会引发熵值快速坍缩，过早压缩探索空间并阻碍有效推理路径的发现（尤其对需大量推导的难题）。为此，我们提出“易题压缩、难题探索”（CEEH）——一种基于强化学习的难度感知高效推理方法。CEEH动态评估实例难度以实施选择性熵正则化：对当前难题保持多样化搜索空间以确保鲁棒性，同时对推理路径明确的易题允许激进压缩。此外，我们引入以历史最短正确答案为锚点的动态最优长度惩罚，有效抑制熵增引起的长度膨胀并稳定奖励信号。在六个推理基准测试中，CEEH在保持与基础模型相当准确度的同时持续缩短响应长度，且相对仅优化长度的方法提升了Pass@k指标。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the high inference costs of Chain-of-Thought reasoning in large language models, where existing compression methods often degrade reasoning quality by prematurely reducing exploration diversity. The proposed method, CEEH, dynamically assesses question difficulty to apply selective entropy regularization, preserving search space for hard questions while compressing easy ones, and uses a dynamic optimal-length penalty based on historical shortest correct responses. Experiments across six reasoning benchmarks show that CEEH effectively reduces response length while maintaining accuracy comparable to the base model and improving Pass@k performance relative to length-only optimization.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型中思维链推理的高计算成本问题，指出现有压缩方法常因过早减少探索多样性而损害推理能力。提出的CEEH方法动态评估问题难度，应用选择性熵正则化，为难题保留搜索空间的同时压缩简单问题的响应，并引入基于历史最短正确答案的动态最优长度惩罚。在六个推理基准测试中，CEEH在保持与基础模型相当准确性的同时有效缩短了响应长度，并相对于仅优化长度的方法提升了Pass@k性能。</div>
</details>
</div>
<div class="card">
<div class="title">Advancing accelerator virtual beam diagnostics through latent evolution modeling: an integrated solution to forward, inverse, tuning, and UQ problems</div>
<div class="meta-line">Authors: Mahindra Rautela, Alexander Scheinker</div>
<div class="meta-line">First: 2026-02-26T04:46:26+00:00 · Latest: 2026-02-26T04:46:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22618v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22618v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Virtual beam diagnostics relies on computationally intensive beam dynamics simulations where high-dimensional charged particle beams evolve through the accelerator. We propose Latent Evolution Model (LEM), a hybrid machine learning framework with an autoencoder that projects high-dimensional phase spaces into lower-dimensional representations, coupled with transformers to learn temporal dynamics in the latent space. This approach provides a common foundational framework addressing multiple interconnected challenges in beam diagnostics. For \textit{forward modeling}, a Conditional Variational Autoencoder (CVAE) encodes 15 unique projections of the 6D phase space into a latent representation, while a transformer predicts downstream latent states from upstream inputs. For \textit{inverse problems}, we address two distinct challenges: (a) predicting upstream phase spaces from downstream observations by utilizing the same CVAE architecture with transformers trained on reversed temporal sequences along with aleatoric uncertainty quantification, and (b) estimating RF settings from the latent space of the trained LEM using a dedicated dense neural network that maps latent representations to RF parameters. For \textit{tuning problems}, we leverage the trained LEM and RF estimator within a Bayesian optimization framework to determine optimal RF settings that minimize beam loss. This paper summarizes our recent efforts and demonstrates how this unified approach effectively addresses these traditionally separate challenges.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过潜在演化建模推进加速器虚拟束流诊断：面向正向、逆向、调谐与不确定性量化问题的集成解决方案</div>
<div class="mono" style="margin-top:8px">虚拟束流诊断依赖于计算密集的束流动力学模拟，其中高维带电粒子束在加速器中演化。我们提出潜在演化模型（LEM），这是一种混合机器学习框架：通过自编码器将高维相空间投影至低维表示，并耦合Transformer学习潜在空间的时间动力学。该方法为束流诊断中多个相互关联的挑战提供了统一的基础框架。在正向建模方面，条件变分自编码器（CVAE）将6D相空间的15个独立投影编码为潜在表示，而Transformer根据上游输入预测下游潜在状态。针对逆向问题，我们应对两个不同挑战：（a）通过使用相同CVAE架构与沿反向时间序列训练的Transformer（辅以偶然不确定性量化），从下游观测预测上游相空间；（b）通过专用密集神经网络将潜在表示映射至射频参数，基于训练好的LEM潜在空间估计射频设置。对于调谐问题，我们在贝叶斯优化框架中利用训练好的LEM和射频估计器，以确定最小化束流损失的最优射频设置。本文总结了近期研究成果，展示了这一统一方法如何有效应对这些传统上相互分离的挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the computational intensity of simulating high-dimensional beam dynamics for accelerator diagnostics. It proposes a Latent Evolution Model (LEM), a hybrid framework combining an autoencoder to compress phase space into latent representations and transformers to model temporal evolution. Experimentally, the method addresses forward modeling by predicting downstream beam states, solves inverse problems like reconstructing upstream conditions and estimating RF settings, and enables tuning via Bayesian optimization to minimize beam loss, demonstrating a unified solution to multiple diagnostic challenges.</div>
<div class="mono" style="margin-top:8px">该论文的动机是加速器诊断中高维束流动力学模拟的计算强度问题。它提出了一种潜在演化模型（LEM），这是一个混合框架，结合了自动编码器将相空间压缩为潜在表示和变换器来建模时间演化。实验结果表明，该方法通过预测下游束流状态解决了正向建模问题，解决了如重建上游条件和估计射频设置等逆问题，并通过贝叶斯优化实现调谐以最小化束流损失，展示了对多个诊断挑战的统一解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">EvolveGen: Algorithmic Level Hardware Model Checking Benchmark Generation through Reinforcement Learning</div>
<div class="meta-line">Authors: Guangyu Hu, Xiaofeng Zhou, Wei Zhang, Hongce Zhang</div>
<div class="meta-line">First: 2026-02-26T04:32:07+00:00 · Latest: 2026-02-26T04:32:07+00:00</div>
<div class="meta-line">Comments: 19 pages, 8 figures. Accepted by TACAS 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22609v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22609v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Progress in hardware model checking depends critically on high-quality benchmarks. However, the community faces a significant benchmark gap: existing suites are limited in number, often distributed only in representations such as BTOR2 without access to the originating register-transfer-level (RTL) designs, and biased toward extreme difficulty where instances are either trivial or intractable. These limitations hinder rigorous evaluation of new verification techniques and encourage overfitting of solver heuristics to a narrow set of problems. To address this, we introduce EvolveGen, a framework for generating hardware model checking benchmarks by combining reinforcement learning (RL) with high-level synthesis (HLS). Our approach operates at an algorithmic level of abstraction in which an RL agent learns to construct computation graphs. By compiling these graphs under different synthesis directives, we produce pairs of functionally equivalent but structurally distinct hardware designs, inducing challenging model checking instances. Solver runtime is used as the reward signal, enabling the agent to autonomously discover and generate small-but-hard instances that expose solver-specific weaknesses. Experiments show that EvolveGen efficiently creates a diverse benchmark set in standard formats (e.g., AIGER and BTOR2) and effectively reveals performance bottlenecks in state-of-the-art model checkers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EvolveGen：基于强化学习的算法级硬件模型检验基准生成框架</div>
<div class="mono" style="margin-top:8px">硬件模型检验领域的发展高度依赖于高质量基准测试集。然而当前面临显著的基准缺口：现有测试集数量有限，通常仅以BTOR2等中间表示形式分发而无法获取原始寄存器传输级设计，且偏向极端难度——实例要么过于简单要么无法求解。这些局限阻碍了新验证技术的严格评估，并导致求解器启发式方法在狭窄问题集上过拟合。为此，我们提出EvolveGen框架，通过结合强化学习与高层次综合技术生成硬件模型检验基准。该方法在算法抽象层级运行：强化学习智能体通过构建计算图，在不同综合指令下编译生成功能等效但结构相异的硬件设计对，从而产生具有挑战性的模型检验实例。以求解器运行时间作为奖励信号，使智能体能自主发现并生成暴露特定求解器弱点的“小而难”实例。实验表明，EvolveGen能以标准格式高效创建多样化基准集，并有效揭示前沿模型检验工具的性能瓶颈。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the significant benchmark gap in hardware model checking, where existing suites are limited in number, often lack access to original RTL designs, and are biased toward extremes of trivial or intractable difficulty, hindering rigorous evaluation and encouraging heuristic overfitting. The method introduces EvolveGen, a framework that combines reinforcement learning (RL) with high-level synthesis (HLS); an RL agent learns to construct computation graphs at an algorithmic abstraction, which are then compiled under different synthesis directives to produce pairs of functionally equivalent but structurally distinct hardware designs, thereby generating challenging model checking instances. The main experimental results demonstrate that EvolveGen efficiently creates a diverse set of benchmarks in standard formats like AIGER and BTOR2, and effectively uncovers performance bottlenecks in state-of-the-art model checkers by autonomously discovering small-but-hard instances that expose solver-specific weaknesses.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决硬件模型检验中存在的显著基准测试缺口，现有测试集数量有限，通常缺乏原始寄存器传输级设计访问，且偏向于过于简单或难以处理的极端难度，这阻碍了严格评估并导致求解器启发式方法过度拟合。方法上提出了EvolveGen框架，结合强化学习与高层次综合；一个强化学习智能体在算法抽象层面学习构建计算图，随后在不同综合指令下编译这些图，生成功能等效但结构不同的硬件设计对，从而产生具有挑战性的模型检验实例。主要实验结果表明，EvolveGen能高效生成AIGER和BTOR2等标准格式的多样化基准测试集，并通过自主发现能暴露求解器特定弱点的小而难的实例，有效揭示了先进模型检验器的性能瓶颈。</div>
</details>
</div>
<div class="card">
<div class="title">Towards a Sharp Analysis of Offline Policy Learning for $f$-Divergence-Regularized Contextual Bandits</div>
<div class="meta-line">Authors: Qingyue Zhao, Kaixuan Ji, Heyang Zhao, Tong Zhang, Quanquan Gu</div>
<div class="meta-line">First: 2025-02-09T22:14:45+00:00 · Latest: 2026-02-26T03:57:25+00:00</div>
<div class="meta-line">Comments: 35 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.06051v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.06051v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many offline reinforcement learning algorithms are underpinned by $f$-divergence regularization, but their sample complexity *defined with respect to regularized objectives* still lacks tight analyses, especially in terms of concrete data coverage conditions. In this paper, we study the exact concentrability requirements to achieve the $\tildeΘ(ε^{-1})$ sample complexity for offline $f$-divergence-regularized contextual bandits. For reverse Kullback-Leibler (KL) divergence, arguably the most commonly used one, we achieve an $\tilde{O}(ε^{-1})$ sample complexity under single-policy concentrability for the first time via a novel pessimism-based analysis, surpassing existing $\tilde{O}(ε^{-1})$ bound under all-policy concentrability and $\tilde{O}(ε^{-2})$ bound under single-policy concentrability. We also propose a near-matching lower bound, demonstrating that a multiplicative dependency on single-policy concentrability is necessary to maximally exploit the curvature property of reverse KL. Moreover, for $f$-divergences with strongly convex $f$, to which reverse KL *does not* belong, we show that the sharp sample complexity $\tildeΘ(ε^{-1})$ is achievable even without pessimistic estimation or single-policy concentrability. We further corroborate our theoretical insights with numerical experiments and extend our analysis to contextual dueling bandits. We believe these results take a significant step towards a comprehensive understanding of objectives with $f$-divergence regularization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向$f$-散度正则化上下文赌博机离线策略学习的尖锐分析</div>
<div class="mono" style="margin-top:8px">许多离线强化学习算法以$f$-散度正则化为理论基础，但其在正则化目标下的样本复杂度仍缺乏严密分析，尤其在具体数据覆盖条件方面。本文研究了离线$f$-散度正则化上下文赌博机达到$\tildeΘ(ε^{-1})$样本复杂度所需的精确集中性条件。针对最常用的反向KL散度，我们首次通过基于悲观主义的新颖分析，在单策略集中性条件下实现了$\tilde{O}(ε^{-1})$样本复杂度，超越了现有全策略集中性下的$\tilde{O}(ε^{-1})$界限和单策略集中性下的$\tilde{O}(ε^{-2})$界限。我们还提出了近乎匹配的下界，证明对单策略集中性的乘法依赖是充分利用反向KL曲率特性的必要条件。此外，对于反向KL散度不属的强凸$f$散度，我们证明即使无需悲观估计或单策略集中性，也能实现$\tildeΘ(ε^{-1})$的尖锐样本复杂度。我们通过数值实验进一步验证理论见解，并将分析扩展至上下文对决赌博机。这些结果为全面理解$f$-散度正则化目标迈出了重要一步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper aims to provide a sharp analysis of the sample complexity for offline f-divergence-regularized contextual bandits, motivated by the lack of tight characterizations of concentrability requirements. The method involves a novel pessimism-based analysis for the reverse KL divergence, and a separate analysis for strongly convex f-divergences that does not require pessimism. The main experimental results show that for reverse KL, the work achieves an Õ(ε⁻¹) sample complexity under single-policy concentrability, improving prior bounds, and establishes a near-matching lower bound; for strongly convex f-divergences, it achieves the same sharp complexity without needing single-policy concentrability, with numerical experiments corroborating the theory.</div>
<div class="mono" style="margin-top:8px">本文旨在对离线f-散度正则化上下文赌博机的样本复杂度进行精确分析，其动机在于现有研究对数据覆盖条件（即可集中性要求）缺乏严格的刻画。方法上，针对反向KL散度提出了一种新颖的悲观估计分析，并对强凸f-散度进行了无需悲观估计的分析。主要实验结果表明，对于反向KL散度，该工作在单策略可集中性条件下首次实现了Õ(ε⁻¹)的样本复杂度，超越了现有界限，并建立了近乎匹配的下界；对于强凸f-散度，则无需单策略可集中性即可达到同样尖锐的复杂度，数值实验验证了理论发现。</div>
</details>
</div>
<div class="card">
<div class="title">WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks</div>
<div class="meta-line">Authors: Hao Bai, Alexey Taymanov, Tong Zhang, Aviral Kumar, Spencer Whitehead</div>
<div class="meta-line">First: 2026-01-05T09:35:11+00:00 · Latest: 2026-02-26T03:25:19+00:00</div>
<div class="meta-line">Comments: Added link to GitHub repo</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02439v5">Abs</a> · <a href="https://arxiv.org/pdf/2601.02439v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present WebGym, the largest-to-date open-source environment for training realistic visual web agents. Real websites are non-stationary and diverse, making artificial or small-scale task sets insufficient for robust policy learning. WebGym contains nearly 300,000 tasks with rubric-based evaluations across diverse, real-world websites and difficulty levels. We train agents with a simple reinforcement learning (RL) recipe, which trains on the agent&#x27;s own interaction traces (rollouts), using task rewards as feedback to guide learning. To enable scaling RL, we speed up sampling of trajectories in WebGym by developing a high-throughput asynchronous rollout system, designed specifically for web agents. Our system achieves a 4-5x rollout speedup compared to naive implementations. Second, we scale the task set breadth, depth, and size, which results in continued performance improvement. Fine-tuning a strong base vision-language model, Qwen-3-VL-8B-Instruct, on WebGym results in an improvement in success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models such as GPT-4o and GPT-5-Thinking that achieve 27.1% and 29.8%, respectively. This improvement is substantial because our test set consists only of tasks on websites never seen during training, unlike many other prior works on training visual web agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WebGym：面向视觉网页代理的现实任务训练环境规模化构建</div>
<div class="mono" style="margin-top:8px">本文提出WebGym，这是迄今最大规模的开源环境，用于训练现实场景下的视觉网页代理。真实网站具有非稳态和多样性特征，使得人工或小规模任务集难以支撑稳健的策略学习。WebGym包含近30万个任务，基于量规评估体系覆盖多样化的真实网站及难度层级。我们采用简洁的强化学习方案训练代理：利用代理自身交互轨迹（rollout）进行训练，以任务奖励作为学习反馈。为实现强化学习的规模化，我们专门为网页代理开发了高吞吐异步轨迹采样系统，将WebGym中的轨迹采样速度较原始实现提升4-5倍。其次，通过拓展任务集的广度、深度与规模，实现了持续的性能提升。在WebGym上对强基础视觉语言模型Qwen-3-VL-8B-Instruct进行微调后，其在分布外测试集上的成功率从26.2%提升至42.9%，显著优于基于GPT-4o（27.1%）和GPT-5-Thinking（29.8%）等专有模型的代理。该提升具有实质性意义，因为我们的测试集仅包含训练阶段未见的网站任务，这与多数现有视觉网页代理训练研究形成鲜明对比。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for WebGym is to address the insufficiency of artificial or small-scale task sets for training robust visual web agents, given the non-stationary and diverse nature of real websites. The method involves creating a large-scale open-source environment with nearly 300,000 tasks across real-world websites, using rubric-based evaluations, and training agents with a reinforcement learning recipe that leverages interaction traces and task rewards; to scale RL, a high-throughput asynchronous rollout system is developed to speed up trajectory sampling. Main experimental results show a 4-5x rollout speedup compared to naive implementations, and fine-tuning the Qwen-3-VL-8B-Instruct model on WebGym improves success rates on an out-of-distribution test set from 26.2% to 42.9%, outperforming agents based on proprietary models like GPT-4o and GPT-5-Thinking.</div>
<div class="mono" style="margin-top:8px">WebGym的动机在于解决人工或小规模任务集在训练鲁棒视觉网络代理方面的不足，因为真实网站具有非平稳性和多样性。方法包括创建一个大规模开源环境，包含近30万个跨真实网站的任务，采用基于量规的评估，并通过强化学习配方训练代理，利用交互轨迹和任务奖励；为扩展强化学习，开发了高吞吐量异步轨迹采样系统以加速采样。主要实验结果显示，与简单实现相比，轨迹采样速度提升4-5倍，且在WebGym上微调Qwen-3-VL-8B-Instruct模型后，在分布外测试集上的成功率从26.2%提升至42.9%，优于基于GPT-4o和GPT-5-Thinking等专有模型的代理。</div>
</details>
</div>
<div class="card">
<div class="title">Stable Adaptive Thinking via Advantage Shaping and Length-Aware Gradient Regulation</div>
<div class="meta-line">Authors: Zihang Xu, Haozhi Xie, Ziqi Miao, Wuxuan Gong, Chen Qian, Lijun Li</div>
<div class="meta-line">First: 2026-02-26T02:49:36+00:00 · Latest: 2026-02-26T02:49:36+00:00</div>
<div class="meta-line">Comments: 15 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22556v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22556v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models (LRMs) achieve strong performance through extended reasoning traces, but they often exhibit overthinking behavior for low-complexity queries. Existing efforts to mitigate this issue are fundamentally limited by unstable accuracy-efficiency trade-offs and poor robustness to heterogeneous reasoning behaviors. To address these challenges, we propose a two-stage framework for stable adaptive thinking in LRMs. The framework first applies Hybrid Fine-Tuning to expose the model to both thinking and no-thinking behaviors, establishing well-conditioned initialization. It then performs adaptive reinforcement learning with Correctness-Preserving Advantage Shaping (CPAS) to avoid suppressing correct long-chain reasoning, and Length-Aware Gradient Regulation (LAGR) to stabilize optimization under severe reasoning-length heterogeneity. Extensive experiments on Qwen2.5-1.5B and 7B show consistent improvements over strong baselines, achieving up to +3.7/+3.6 accuracy points while reducing generated tokens by 40.6%/43.9%. Further analyses across varying problem difficulties and out-of-distribution tasks confirm the robustness and generalization of our approach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于优势塑造与长度感知梯度调节的稳定自适应思维</div>
<div class="mono" style="margin-top:8px">大型推理模型通过扩展推理链获得优异性能，但在处理低复杂度查询时常出现过度思考现象。现有缓解方法受限于准确率与效率间不稳定的权衡关系，以及对异构推理行为的鲁棒性不足。为此，我们提出一个两阶段框架以实现大型推理模型的稳定自适应思维。该框架首先采用混合微调技术，使模型同时接触思考与非思考行为，建立良好初始化条件；随后执行自适应强化学习：通过正确性保持优势塑造避免抑制正确的长链推理，并采用长度感知梯度调节在严重推理长度异质性下稳定优化过程。在Qwen2.5-1.5B和7B模型上的大量实验表明，本方法在强基线模型基础上持续提升性能，最高可增加3.7/3.6个准确率点，同时减少40.6%/43.9%的生成标记。针对不同难度问题与分布外任务的进一步分析，证实了本方法的鲁棒性与泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of large reasoning models (LRMs) overthinking simple queries, which leads to unstable accuracy-efficiency trade-offs and poor robustness. The proposed solution is a two-stage framework: first, Hybrid Fine-Tuning initializes the model with both thinking and no-thinking behaviors; second, adaptive reinforcement learning employs Correctness-Preserving Advantage Shaping to protect accurate long reasoning and Length-Aware Gradient Regulation to stabilize training despite varying reasoning lengths. Experiments on Qwen2.5 models show the method improves accuracy by up to 3.7 points while cutting generated tokens by over 40%, with analyses confirming robustness across different problem difficulties and out-of-distribution tasks.</div>
<div class="mono" style="margin-top:8px">本文针对大型推理模型在处理简单查询时过度思考导致精度-效率权衡不稳定及鲁棒性差的问题，提出一个两阶段框架。首先通过混合微调让模型接触思考与非思考行为以建立良好初始化，随后采用自适应强化学习，其中包含保持正确性的优势塑形以避免抑制正确的长链推理，以及长度感知梯度调节来稳定严重长度异质性下的优化。在Qwen2.5模型上的实验表明，该方法在提升精度最高达3.7分的同时减少了超过40%的生成标记，且在不同问题难度和分布外任务上的分析证实了其鲁棒性与泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Multilingual Safety Alignment Via Sparse Weight Editing</div>
<div class="meta-line">Authors: Jiaming Liang, Zhaoxin Wang, Handing Wang</div>
<div class="meta-line">First: 2026-02-26T02:46:13+00:00 · Latest: 2026-02-26T02:46:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22554v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22554v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) exhibit significant safety disparities across languages, with low-resource languages (LRLs) often bypassing safety guardrails established for high-resource languages (HRLs) like English. Existing solutions, such as multilingual supervised fine-tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF), are computationally expensive and dependent on scarce multilingual safety data. In this work, we propose a novel, training-free alignment framework based on Sparse Weight Editing. Identifying that safety capabilities are localized within a sparse set of safety neurons, we formulate the cross-lingual alignment problem as a constrained linear transformation. We derive a closed-form solution to optimally map the harmful representations of LRLs to the robust safety subspaces of HRLs, while preserving general utility via a null-space projection constraint. Extensive experiments across 8 languages and multiple model families (Llama-3, Qwen-2.5) demonstrate that our method substantially reduces Attack Success Rate (ASR) in LRLs with negligible impact on general reasoning capabilities, all achieved with a single, data-efficient calculation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于稀疏权重编辑的多语言安全对齐</div>
<div class="mono" style="margin-top:8px">大语言模型在不同语言间存在显著的安全性能差异，低资源语言常能绕过针对英语等高资源语言设立的安全防护机制。现有解决方案（如多语言监督微调或基于人类反馈的强化学习）计算成本高昂且依赖稀缺的多语言安全数据。本研究提出一种基于稀疏权重编辑的新型免训练对齐框架。通过识别安全能力集中分布于稀疏的安全神经元集合，我们将跨语言对齐问题形式化为约束线性变换。推导出闭式解，可将低资源语言的有害表征最优映射至高资源语言的鲁棒安全子空间，同时通过零空间投影约束保持通用能力。在8种语言和多个模型系列上的实验表明，该方法能显著降低低资源语言的攻击成功率，且对通用推理能力影响可忽略，仅需单次数据高效计算即可实现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the significant safety disparities in Large Language Models (LLMs) across languages, where low-resource languages (LRLs) often bypass safety guardrails established for high-resource languages like English, while existing multilingual alignment methods are computationally expensive and data-hungry. The proposed method is a training-free framework based on Sparse Weight Editing, which identifies that safety capabilities are localized within a sparse set of neurons and formulates cross-lingual alignment as a constrained linear transformation, deriving a closed-form solution to map harmful LRL representations to robust safety subspaces of HRLs while preserving general utility via null-space projection. The main experimental results, from extensive tests across 8 languages and multiple model families including Llama-3 and Qwen-2.5, show that this method substantially reduces the Attack Success Rate (ASR) in LRLs with negligible impact on general reasoning capabilities, achieved through a single, data-efficient calculation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于大型语言模型（LLMs）在不同语言间存在显著的安全差异，低资源语言（LRLs）常能绕过为英语等高资源语言（HRLs）建立的安全护栏，而现有的多语言对齐方法计算成本高且依赖稀缺数据。该方法提出了一种基于稀疏权重编辑的无训练对齐框架，通过识别安全能力集中于稀疏的安全神经元中，将跨语言对齐问题表述为约束线性变换，推导出闭式解以将LRLs的有害表征最优映射到HRLs的鲁棒安全子空间，同时通过零空间投影约束保留通用能力。主要实验结果基于对8种语言和Llama-3、Qwen-2.5等多个模型系列的广泛测试，表明该方法能大幅降低LRLs的攻击成功率（ASR），且对通用推理能力影响可忽略，所有效果仅通过一次数据高效的计算实现。</div>
</details>
</div>
<div class="card">
<div class="title">Agentic AI for Intent-driven Optimization in Cell-free O-RAN</div>
<div class="meta-line">Authors: Mohammad Hossein Shokouhi, Vincent W. S. Wong</div>
<div class="meta-line">First: 2026-02-26T02:26:58+00:00 · Latest: 2026-02-26T02:26:58+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE International Conference on Communications (ICC), Glasgow, UK, May 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22539v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22539v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic artificial intelligence (AI) is emerging as a key enabler for autonomous radio access networks (RANs), where multiple large language model (LLM)-based agents reason and collaborate to achieve operator-defined intents. The open RAN (O-RAN) architecture enables the deployment and coordination of such agents. However, most existing works consider simple intents handled by independent agents, while complex intents that require coordination among agents remain unexplored. In this paper, we propose an agentic AI framework for intent translation and optimization in cell-free O-RAN. A supervisor agent translates the operator intents into an optimization objective and minimum rate requirements. Based on this information, a user weighting agent retrieves relevant prior experience from a memory module to determine the user priority weights for precoding. If the intent includes an energy-saving objective, then an open radio unit (O-RU) management agent will also be activated to determine the set of active O-RUs by using a deep reinforcement learning (DRL) algorithm. A monitoring agent measures and monitors the user data rates and coordinates with other agents to guarantee the minimum rate requirements are satisfied. To enhance scalability, we adopt a parameter-efficient fine-tuning (PEFT) method that enables the same underlying LLM to be used for different agents. Simulation results show that the proposed agentic AI framework reduces the number of active O-RUs by 41.93% when compared with three baseline schemes in energy-saving mode. Using the PEFT method, the proposed framework reduces the memory usage by 92% when compared with deploying separate LLM agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向无蜂窝O-RAN意图驱动优化的智能体人工智能</div>
<div class="mono" style="margin-top:8px">智能体人工智能正成为自主无线接入网络的关键使能技术，其中多个基于大语言模型的智能体通过推理与协作实现运营商定义的意图。开放式无线接入网络架构支持此类智能体的部署与协调。然而，现有研究多集中于由独立智能体处理的简单意图，而需要多智能体协同的复杂意图尚未得到充分探索。本文提出一种面向无蜂窝O-RAN意图翻译与优化的智能体AI框架：监督智能体将运营商意图转化为优化目标与最低速率要求；用户权重智能体据此从记忆模块检索先验经验以确定预编码用户优先级权重；若意图包含节能目标，则开放式射频单元管理智能体将通过深度强化学习算法激活，确定活跃O-RU集合；监控智能体实时测量用户数据速率并协同其他智能体保障最低速率要求。为提升可扩展性，采用参数高效微调技术使同一底层大语言模型适配不同智能体。仿真结果表明：在节能模式下，所提框架相比三种基线方案减少41.93%的活跃O-RU数量；采用参数高效微调技术后，内存使用量较部署独立大语言模型智能体降低92%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to handle complex operator intents in autonomous radio access networks, which require coordination among multiple AI agents, a gap not addressed by prior works focusing on simple intents handled independently. The method proposes an agentic AI framework for cell-free O-RAN, where a supervisor agent translates intents into optimization objectives, a user weighting agent determines user priorities using memory, an O-RU management agent activates a DRL algorithm for energy-saving by selecting active radio units, and a monitoring agent ensures minimum rate requirements, with all agents efficiently sharing a single LLM via parameter-efficient fine-tuning for scalability. The main experimental results demonstrate that in energy-saving mode, the framework reduces the number of active O-RUs by 41.93% compared to three baselines and, using the PEFT method, cuts memory usage by 92% relative to deploying separate LLM agents.</div>
<div class="mono" style="margin-top:8px">本文的动机在于处理自主无线接入网络中复杂的运营商意图，这需要多个AI代理之间的协调，而现有工作主要关注由独立代理处理的简单意图，这一协调需求尚未被探索。方法上，提出了一个用于无蜂窝O-RAN的代理式AI框架，其中监督代理将意图转化为优化目标，用户加权代理利用记忆模块确定用户优先级权重，O-RU管理代理在节能目标下激活深度强化学习算法来选择活跃的无线单元，监控代理则监测用户数据速率并协调其他代理以满足最低速率要求，所有代理通过参数高效微调共享同一个大语言模型以提高可扩展性。主要实验结果表明，在节能模式下，该框架相比三种基线方案将活跃O-RU数量减少了41.93%，并且采用参数高效微调方法后，内存使用量比部署独立大语言模型代理降低了92%。</div>
</details>
</div>
<div class="card">
<div class="title">RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?</div>
<div class="meta-line">Authors: Rohan Gupta, Erik Jenner</div>
<div class="meta-line">First: 2025-06-17T07:22:20+00:00 · Latest: 2026-02-26T01:45:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.14261v4">Abs</a> · <a href="https://arxiv.org/pdf/2506.14261v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Latent-space monitors aim to detect undesirable behaviours in Large Language Models by leveraging their internal representations rather than relying solely on black-box outputs. These methods have shown promise in identifying behaviours such as deception and unsafe completions. However, these monitors may themselves become training signals, for example, by using problematic samples found in deployment to retrain models. This raises an important question: can models learn to evade such monitors? To evaluate this capability, we introduce RL-Obfuscation, in which LLMs are finetuned via reinforcement learning to evade latent-space monitors while maintaining their blackbox behaviour. We apply RL-Obfuscation to Language Models ranging from 7B to 14B parameters and evaluate their Evasion Success Rate against a suite of monitors. We find that token-level monitors are highly vulnerable to this attack while more holistic monitors, such as max-pooling or attention-based probes, remain robust. Moreover, for these vulnerable monitors, models trained to evade a single static monitor can generalise to evade other unseen monitors. We also find that the models can be trained to conditionally bypass latent-space monitors on only certain inputs. Finally, we study how the models bypass these monitors and find that the model can learn to repurpose tokens to have different internal representations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RL-Obfuscation：语言模型能否学会规避潜在空间监控器？</div>
<div class="mono" style="margin-top:8px">潜在空间监控器旨在通过利用大型语言模型的内部表征而非仅依赖黑盒输出来检测其不良行为，已在识别欺骗性及不安全生成等行为方面展现出潜力。然而，这些监控器本身可能成为训练信号，例如利用部署中发现的问题样本对模型进行再训练。这引出一个关键问题：模型能否学会规避此类监控器？为评估此能力，我们提出RL-Obfuscation方法，通过强化学习微调语言模型，使其在保持黑盒行为的同时规避潜在空间监控器。我们在7B至14B参数规模的模型上应用该方法，并针对一组监控器评估其规避成功率。研究发现：基于词元的监控器极易受此类攻击，而更全局化的监控器（如最大池化或基于注意力的探针）则保持稳健；对于易受攻击的监控器，针对单一静态监控器训练的模型能泛化至规避其他未见过的监控器；模型还可被训练为仅针对特定输入条件性地绕过监控器。最后，我们探究了模型的规避机制，发现模型能学会重新分配词元以改变其内部表征。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates whether large language models (LLMs) can learn to evade latent-space monitors, which detect undesirable behaviors using internal representations rather than just outputs. The authors introduce RL-Obfuscation, a method that fine-tunes LLMs via reinforcement learning to bypass these monitors while preserving their external behavior. Experimental results on models from 7B to 14B parameters show that token-level monitors are highly vulnerable to evasion, with models generalizing to unseen monitors and conditionally bypassing them on specific inputs, often by repurposing tokens to alter internal representations, whereas more holistic monitors like max-pooling or attention-based probes remain robust.</div>
<div class="mono" style="margin-top:8px">本文研究大型语言模型能否学会规避潜在空间监控器，这些监控器利用内部表征而非仅输出检测不良行为。作者提出RL-Obfuscation方法，通过强化学习微调模型，在保持外部行为的同时绕过监控器。实验在7B至14B参数模型上进行，结果表明令牌级监控器极易被规避，模型能泛化到未见过的监控器并条件性地在特定输入下绕过，常通过重新利用令牌改变内部表征实现，而更整体的监控器如最大池化或基于注意力的探针则保持稳健。</div>
</details>
</div>
<div class="card">
<div class="title">A Mathematical Theory of Agency and Intelligence</div>
<div class="meta-line">Authors: Wael Hafez, Chenan Wei, Rodrigo Felipe, Amir Nazeri, Cameron Reid</div>
<div class="meta-line">First: 2026-02-26T01:26:21+00:00 · Latest: 2026-02-26T01:26:21+00:00</div>
<div class="meta-line">Comments: 20 pages, 4 figuers</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22519v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22519v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To operate reliably under changing conditions, complex systems require feedback on how effectively they use resources, not just whether objectives are met. Current AI systems process vast information to produce sophisticated predictions, yet predictions can appear successful while the underlying interaction with the environment degrades. What is missing is a principled measure of how much of the total information a system deploys is actually shared between its observations, actions, and outcomes. We prove this shared fraction, which we term bipredictability, P, is intrinsic to any interaction, derivable from first principles, and strictly bounded: P can reach unity in quantum systems, P equal to, or smaller than 0.5 in classical systems, and lower once agency (action selection) is introduced. We confirm these bounds in a physical system (double pendulum), reinforcement learning agents, and multi turn LLM conversations. These results distinguish agency from intelligence: agency is the capacity to act on predictions, whereas intelligence additionally requires learning from interaction, self-monitoring of its learning effectiveness, and adapting the scope of observations, actions, and outcomes to restore effective learning. By this definition, current AI systems achieve agency but not intelligence. Inspired by thalamocortical regulation in biological systems, we demonstrate a feedback architecture that monitors P in real time, establishing a prerequisite for adaptive, resilient AI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>代理与智能的数学理论</div>
<div class="mono" style="margin-top:8px">为在变化条件下可靠运行，复杂系统需要关于其资源使用效率的反馈，而不仅仅是目标是否达成。当前人工智能系统处理海量信息以生成复杂预测，但预测可能看似成功，而系统与环境的底层交互却在退化。缺失的是一个原则性度量，用于衡量系统部署的总信息中，有多少实际在其观察、行动与结果之间共享。我们证明这一共享比例（称为双可预测性P）内在于任何交互，可从第一性原理推导，且严格有界：P在量子系统中可达1，在经典系统中等于或小于0.5，引入代理（行动选择）后进一步降低。我们在物理系统（双摆）、强化学习智能体及多轮大语言模型对话中验证了这些界限。这些结果区分了代理与智能：代理是基于预测行动的能力，而智能还需从交互中学习、自我监测学习效率，并调整观察、行动与结果的范围以恢复有效学习。依此定义，当前人工智能系统实现了代理而非智能。受生物系统中丘脑皮层调节机制启发，我们展示了一种实时监测P的反馈架构，为构建自适应、强韧的人工智能奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for complex systems to assess their resource use effectiveness beyond mere objective achievement, this paper introduces &#x27;bipredictability&#x27; (P) as a principled measure of the shared information between a system&#x27;s observations, actions, and outcomes. The method derives this measure from first principles, proving it is intrinsic to any interaction and establishing theoretical bounds: P can reach 1 in quantum systems, is ≤0.5 in classical systems, and decreases further with agency. Experimental results confirm these bounds in a double pendulum, reinforcement learning agents, and multi-turn LLM conversations, distinguishing agency (acting on predictions) from intelligence (which additionally requires learning, self-monitoring, and adaptation). The work concludes by proposing a feedback architecture, inspired by thalamocortical regulation, to monitor P in real time as a step toward adaptive AI.</div>
<div class="mono" style="margin-top:8px">本文的动机在于复杂系统需要评估其资源使用效率，而不仅仅是达成目标。为此，论文提出了“双可预测性”（P）作为衡量系统观察、行动与结果之间共享信息的原则性指标。方法上从第一性原理推导出这一度量，证明其为任何交互所固有，并建立了理论界限：在量子系统中P可达1，在经典系统中P≤0.5，引入主体性后进一步降低。实验结果在双摆、强化学习智能体和多轮大语言模型对话中验证了这些界限，从而区分了主体性（基于预测行动）与智能（还需学习、自我监控和适应）。受生物系统中丘脑皮质调节的启发，研究最后提出了一种实时监控P的反馈架构，为构建自适应人工智能奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">Space Syntax-guided Post-training for Residential Floor Plan Generation</div>
<div class="meta-line">Authors: Zhuoyang Jiang, Dongqing Zhang</div>
<div class="meta-line">First: 2026-02-26T00:54:38+00:00 · Latest: 2026-02-26T00:54:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22507v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22507v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pre-trained generative models for residential floor plans are typically optimized to fit large-scale data distributions, which can under-emphasize critical architectural priors such as the configurational dominance and connectivity of domestic public spaces (e.g., living rooms and foyers). This paper proposes Space Syntax-guided Post-training (SSPT), a post-training paradigm that explicitly injects space syntax knowledge into floor plan generation via a non-differentiable oracle. The oracle converts RPLAN-style layouts into rectangle-space graphs through greedy maximal-rectangle decomposition and door-mediated adjacency construction, and then computes integration-based measurements to quantify public space dominance and functional hierarchy.
  To enable consistent evaluation and diagnosis, we further introduce SSPT-Bench (Eval-8), an out-of-distribution benchmark that post-trains models using conditions capped at $\leq 7$ rooms while evaluating on 8-room programs, together with a unified metric suite for dominance, stability, and profile alignment. SSPT is instantiated with two strategies: (i) iterative retraining via space-syntax filtering and diffusion fine-tuning, and (ii) reinforcement learning via PPO with space-syntax rewards. Experiments show that both strategies improve public-space dominance and restore clearer functional hierarchy compared to distribution-fitted baselines, while PPO achieves stronger gains with substantially higher compute efficiency and reduced variance. SSPT provides a scalable pathway for integrating architectural theory into data-driven plan generation and is compatible with other generative backbones given a post-hoc evaluation oracle.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>空间句法引导的住宅平面图生成后训练方法</div>
<div class="mono" style="margin-top:8px">住宅平面图的预训练生成模型通常以拟合大规模数据分布为目标，这可能弱化关键的建筑先验知识，例如家庭公共空间（如客厅和门厅）的配置主导性与连通性。本文提出空间句法引导的后训练范式，通过不可微的评估模块将空间句法知识显式注入平面图生成过程。该模块通过贪心最大矩形分解和门中介邻接关系构建，将RPLAN式布局转换为矩形空间图，进而计算基于整合度的度量以量化公共空间主导性与功能层级。为支持一致性评估与诊断，我们进一步提出SSPT-Bench（Eval-8）——一种分布外基准测试集，其使用≤7个房间的条件进行模型后训练，并在8房间方案上评估，同时配备用于主导性、稳定性和轮廓对齐的统一度量体系。SSPT通过两种策略实现：（1）基于空间句法筛选与扩散微调的迭代重训练；（2）采用PPO强化学习框架结合空间句法奖励。实验表明，两种策略均能提升公共空间主导性并恢复更清晰的功能层级，其中PPO策略在显著提升计算效率、降低方差的同时获得更强性能增益。SSPT为将建筑理论融入数据驱动的平面生成提供了可扩展路径，且兼容其他具备后验评估模块的生成模型架构。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the observation that pre-trained generative models for residential floor plans often fail to adequately capture key architectural priors like the configurational dominance and connectivity of public spaces. To address this, the authors propose Space Syntax-guided Post-training (SSPT), a method that injects space syntax knowledge into generation via a non-differentiable oracle, which computes integration-based metrics from layout graphs to quantify public space dominance and functional hierarchy. For evaluation, they introduce SSPT-Bench, an out-of-distribution benchmark with a unified metric suite, and instantiate SSPT with two strategies: iterative retraining via filtering and fine-tuning, and reinforcement learning using PPO with space-syntax rewards. Experimental results demonstrate that both strategies improve public-space dominance and restore clearer functional hierarchy compared to baselines, with the PPO-based approach achieving stronger performance gains, higher computational efficiency, and reduced variance.</div>
<div class="mono" style="margin-top:8px">本文的动机是观察到用于住宅平面图的预训练生成模型通常未能充分捕捉关键的建筑先验，如公共空间的配置主导性和连通性。为解决此问题，作者提出了空间句法引导的后训练方法，该方法通过一个不可微的预言机将空间句法知识注入生成过程，该预言机从布局图中计算基于整合度的度量以量化公共空间主导性和功能层次。为进行评估，他们引入了SSPT-Bench这一分布外基准测试及统一度量套件，并通过两种策略实例化SSPT：基于过滤和微调的迭代重训练，以及使用带空间句法奖励的PPO进行强化学习。实验结果表明，与基线相比，两种策略均改善了公共空间主导性并恢复了更清晰的功能层次，其中基于PPO的方法取得了更强的性能提升、更高的计算效率和更低的方差。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement-aware Knowledge Distillation for LLM Reasoning</div>
<div class="meta-line">Authors: Zhaoyang Zhang, Shuli Jiang, Yantao Shen, Yuting Zhang, Dhananjay Ram, Shuo Yang, Zhuowen Tu, Wei Xia, Stefano Soatto</div>
<div class="meta-line">First: 2026-02-26T00:20:39+00:00 · Latest: 2026-02-26T00:20:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22495v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22495v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) post-training has recently driven major gains in long chain-of-thought reasoning large language models (LLMs), but the high inference cost of such models motivates distillation into smaller students. Most existing knowledge distillation (KD) methods are designed for supervised fine-tuning (SFT), relying on fixed teacher traces or teacher-student Kullback-Leibler (KL) divergence-based regularization. When combined with RL, these approaches often suffer from distribution mismatch and objective interference: teacher supervision may not align with the student&#x27;s evolving rollout distribution, and the KL regularizer can compete with reward maximization and require careful loss balancing. To address these issues, we propose RL-aware distillation (RLAD), which performs selective imitation during RL -- guiding the student toward the teacher only when it improves the current policy update. Our core component, Trust Region Ratio Distillation (TRRD), replaces the teacher-student KL regularizer with a PPO/GRPO-style likelihood-ratio objective anchored to a teacher--old-policy mixture, yielding advantage-aware, trust-region-bounded distillation on student rollouts and naturally balancing exploration, exploitation, and imitation. Across diverse logic reasoning and math benchmarks, RLAD consistently outperforms offline distillation, standard GRPO, and KL-based on-policy teacher-student knowledge distillation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向大语言模型推理的强化感知知识蒸馏</div>
<div class="mono" style="margin-top:8px">强化学习（RL）后训练近期显著提升了长链思维推理大语言模型（LLMs）的性能，但此类模型的高推理成本促使研究者将其蒸馏至更小的学生模型。现有知识蒸馏（KD）方法多针对监督微调（SFT）设计，依赖固定的教师轨迹或基于师生KL散度的正则化。当与强化学习结合时，这些方法常面临分布失配与目标冲突：教师监督可能与学生动态演化的推演分布不一致，且KL正则项可能与奖励最大化目标相互竞争，需要精细的损失平衡。为解决这些问题，我们提出强化感知蒸馏（RLAD），在强化学习过程中执行选择性模仿——仅当教师能改进当前策略更新时才引导学生模型向其靠拢。其核心组件信任域比率蒸馏（TRRD）采用基于教师-旧策略混合的PPO/GRPO式似然比目标，替代原有的师生KL正则项，从而在学生推演上实现优势感知、信任域约束的蒸馏，自然平衡探索、利用与模仿过程。在多项逻辑推理与数学基准测试中，RLAD持续优于离线蒸馏、标准GRPO以及基于KL的在线师生知识蒸馏方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the high inference cost of large language models (LLMs) enhanced by reinforcement learning (RL) for reasoning, by distilling them into smaller models, while overcoming distribution mismatch and objective interference issues in existing knowledge distillation methods when combined with RL. The method introduces RL-aware distillation (RLAD), which selectively imitates the teacher during RL via a core component called Trust Region Ratio Distillation (TRRD), replacing traditional KL regularization with a likelihood-ratio objective anchored to a teacher–old-policy mixture to ensure advantage-aware, trust-region-bounded guidance. Experimental results across diverse logic reasoning and math benchmarks show that RLAD consistently outperforms offline distillation, standard GRPO, and KL-based on-policy teacher-student knowledge distillation.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过将经过强化学习增强的大型语言模型蒸馏为更小模型，以降低其推理成本，同时解决现有知识蒸馏方法与强化学习结合时存在的分布不匹配和目标干扰问题。方法上提出了强化学习感知蒸馏（RLAD），在强化学习过程中通过核心组件信任域比率蒸馏（TRRD）选择性模仿教师模型，用基于教师-旧策略混合的似然比目标替代传统的KL正则化，实现优势感知和信任域约束的指导。在多种逻辑推理和数学基准测试中的实验结果表明，RLAD consistently outperforms offline distillation, standard GRPO, and KL-based on-policy teacher-student knowledge distillation。</div>
</details>
</div>
<div class="card">
<div class="title">LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation</div>
<div class="meta-line">Authors: Hejia Zhang, Zhongming Yu, Chia-Tung Ho, Haoxing Ren, Brucek Khailany, Jishen Zhao</div>
<div class="meta-line">First: 2026-02-18T23:36:46+00:00 · Latest: 2026-02-25T21:49:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16953v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.16953v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Execution-aware LLM agents offer a promising paradigm for learning from tool feedback, but such feedback is often expensive and slow to obtain, making online reinforcement learning (RL) impractical. High-coverage hardware verification exemplifies this challenge due to its reliance on industrial simulators and non-differentiable execution signals. We propose LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions guided by deterministic evaluators. Building on this formulation, we introduce execution-validated data curation, policy-aware agentic data synthesis, and worst-state-prioritized sampling to enable scalable learning under execution constraints. We further curate a reality-aligned benchmark adapted from an existing verification suite through a revised evaluation protocol. Using the proposed pipeline, a compact 4B-parameter model achieves 69.2% coverage pass rate under agentic evaluation, outperforming its teacher by 5.3% and demonstrating competitive performance against models an order of magnitude larger.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM4Cov：面向高覆盖率测试平台生成的可执行感知智能体学习</div>
<div class="mono" style="margin-top:8px">可执行感知的LLM智能体为从工具反馈中学习提供了有前景的范式，但此类反馈通常获取成本高昂且速度缓慢，使得在线强化学习（RL）难以实施。高覆盖率硬件验证正是这一挑战的典型体现，因其依赖工业级模拟器和不可微分的执行信号。我们提出LLM4ov——一种离线智能体学习框架，将验证建模为由确定性评估器引导的无记忆状态转移。基于此形式化框架，我们引入执行验证数据筛选、策略感知智能体数据合成及最差状态优先采样机制，以在执行约束下实现可扩展学习。我们进一步通过修订的评估协议，从现有验证套件中构建了贴合实际场景的基准测试。采用该流程后，一个紧凑的40亿参数模型在智能体评估中达到69.2%的覆盖率通过率，较其教师模型提升5.3%，并与规模大一个数量级的模型展现出可比性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of using execution feedback from slow and expensive tools, such as industrial hardware simulators, for online reinforcement learning in high-coverage testbench generation. It proposes LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions and employs techniques like execution-validated data curation and worst-state-prioritized sampling to enable scalable learning. Experimental results show that a compact 4B-parameter model trained with this pipeline achieves a 69.2% coverage pass rate, outperforming its teacher by 5.3% and competing with much larger models.</div>
<div class="mono" style="margin-top:8px">本文针对在高覆盖率测试平台生成中，使用来自工业模拟器等缓慢且昂贵工具的执行反馈进行在线强化学习所面临的挑战，提出了LLM4Cov离线智能体学习框架。该框架将验证建模为无记忆状态转移，并采用执行验证数据整理和最差状态优先采样等技术，以实现受限执行条件下的可扩展学习。实验结果表明，通过该流程训练的紧凑型40亿参数模型达到了69.2%的覆盖率通过率，比其教师模型高出5.3%，并与规模大一个数量级的模型性能相当。</div>
</details>
</div>
<div class="card">
<div class="title">Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning</div>
<div class="meta-line">Authors: Yihe Deng, I-Hung Hsu, Jun Yan, Zifeng Wang, Rujun Han, Gufeng Zhang, Yanfei Chen, Wei Wang, Tomas Pfister, Chen-Yu Lee</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-29T22:05:08+00:00 · Latest: 2026-02-25T21:18:44+00:00</div>
<div class="meta-line">Comments: Paper accepted by ICLR 2026. The first two authors contribute equally</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.25992v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.25992v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) often struggle with problems that require multi-step reasoning. For small-scale open-source models, Reinforcement Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to overfit long demonstrations through rigid token-by-token imitation. To address this gap, we propose Supervised Reinforcement Learning (SRL), a framework that reformulates problem solving as generating a sequence of logical &quot;actions&quot;. SRL trains the model to generate an internal reasoning monologue before committing to each action. It provides smoother rewards based on the similarity between the model&#x27;s actions and expert actions extracted from the SFT dataset in a step-wise manner. This supervision offers richer learning signals even when all rollouts are incorrect, while encouraging flexible reasoning guided by expert demonstrations. As a result, SRL enables small models to learn challenging problems previously unlearnable by SFT or RLVR. Moreover, initializing training with SRL before refining with RLVR yields the strongest overall performance. Beyond reasoning benchmarks, SRL generalizes effectively to agentic software engineering tasks, establishing it as a robust and versatile training framework for reasoning-oriented LLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>监督强化学习：从专家轨迹到逐步推理</div>
<div class="mono" style="margin-top:8px">大语言模型在处理需要多步推理的问题时常面临困难。对于小规模开源模型，基于可验证奖励的强化学习在多次尝试后仍难以采样到正确解，而监督微调则容易因逐词僵化模仿长示例导致过拟合。为弥补这一不足，我们提出监督强化学习框架，将问题求解重构为生成逻辑“动作”序列的过程。该框架训练模型在执行每个动作前生成内部推理独白，并根据模型动作与从监督微调数据集中逐步提取的专家动作之间的相似性提供平滑奖励。这种监督机制即使在全轨迹错误时也能提供更丰富的学习信号，同时鼓励在专家示范引导下进行灵活推理。实验表明，监督强化学习能使小模型掌握监督微调或基于可验证奖励的强化学习无法习得的复杂问题。此外，先用监督强化学习初始化训练再进行基于可验证奖励的强化学习微调，可获得最佳综合性能。除推理基准测试外，监督强化学习在智能体软件工程任务中也展现出色泛化能力，确立了其作为面向推理大语言模型的稳健通用训练框架地位。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of existing methods like Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR) in training small-scale LLMs for multi-step reasoning, where SFT overfits to demonstrations and RLVR fails when correct solutions are rarely sampled. The authors propose Supervised Reinforcement Learning (SRL), a framework that reformulates problem-solving as generating sequences of logical actions, training models to produce internal reasoning monologues before each action and providing step-wise rewards based on similarity to expert actions from SFT data. Experimental results show that SRL enables small models to learn previously unlearnable challenging reasoning problems, and initializing training with SRL before RLVR refinement yields the strongest overall performance, with effective generalization to agentic software engineering tasks.</div>
<div class="mono" style="margin-top:8px">本文针对现有方法如监督微调（SFT）和带可验证奖励的强化学习（RLVR）在训练小规模大语言模型进行多步推理时的局限性，即SFT容易对演示数据过拟合而RLVR在正确解极少被采样时失效，提出了监督强化学习（SRL）框架。该方法将问题解决重构为生成逻辑“动作”序列，训练模型在每步动作前生成内部推理独白，并基于与SFT数据中专家动作的相似性提供逐步奖励。实验结果表明，SRL使小模型能够学习之前SFT或RLVR无法掌握的复杂推理问题，且先用SRL初始化训练再用RLVR精调能获得最佳整体性能，并能有效泛化至自主软件工程任务。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260228_0403.html">20260228_0403</a>
<a href="archive/20260227_0400.html">20260227_0400</a>
<a href="archive/20260226_0405.html">20260226_0405</a>
<a href="archive/20260224_0355.html">20260224_0355</a>
<a href="archive/20260223_0321.html">20260223_0321</a>
<a href="archive/20260222_0327.html">20260222_0327</a>
<a href="archive/20260221_0347.html">20260221_0347</a>
<a href="archive/20260220_0349.html">20260220_0349</a>
<a href="archive/20260219_0406.html">20260219_0406</a>
<a href="archive/20260218_0358.html">20260218_0358</a>
<a href="archive/20260217_0329.html">20260217_0329</a>
<a href="archive/20260216_0321.html">20260216_0321</a>
<a href="archive/20260215_0335.html">20260215_0335</a>
<a href="archive/20260213_0416.html">20260213_0416</a>
<a href="archive/20260212_0417.html">20260212_0417</a>
<a href="archive/20260211_0421.html">20260211_0421</a>
<a href="archive/20260210_0359.html">20260210_0359</a>
<a href="archive/20260209_0321.html">20260209_0321</a>
<a href="archive/20260208_0336.html">20260208_0336</a>
<a href="archive/20260207_0400.html">20260207_0400</a>
<a href="archive/20260206_0358.html">20260206_0358</a>
<a href="archive/20260205_0400.html">20260205_0400</a>
<a href="archive/20260204_0411.html">20260204_0411</a>
<a href="archive/20260202_0320.html">20260202_0320</a>
<a href="archive/20260201_0343.html">20260201_0343</a>
<a href="archive/20260131_0357.html">20260131_0357</a>
<a href="archive/20260130_0355.html">20260130_0355</a>
<a href="archive/20260129_0353.html">20260129_0353</a>
<a href="archive/20260128_0342.html">20260128_0342</a>
<a href="archive/20260127_0320.html">20260127_0320</a>
<a href="archive/20260126_0313.html">20260126_0313</a>
<a href="archive/20260125_0321.html">20260125_0321</a>
<a href="archive/20260124_0344.html">20260124_0344</a>
<a href="archive/20260123_0345.html">20260123_0345</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0408.html">20260121_0408</a>
<a href="archive/20260120_0316.html">20260120_0316</a>
<a href="archive/20260119_0312.html">20260119_0312</a>
<a href="archive/20260118_0323.html">20260118_0323</a>
<a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
