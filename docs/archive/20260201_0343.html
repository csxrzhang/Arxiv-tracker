<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-01 03:43</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260201_0343</div>
    <div class="row"><div class="card">
<div class="title">Exploring Reasoning Reward Model for Agents</div>
<div class="meta-line">Authors: Kaixuan Fan, Kaituo Feng, Manyuan Zhang, Tianshuo Peng, Zhixun Li, Yilei Jiang, Shuang Chen, Peng Pei, Xunliang Cai, Xiangyu Yue</div>
<div class="meta-line">First: 2026-01-29T18:59:52+00:00 · Latest: 2026-01-29T18:59:52+00:00</div>
<div class="meta-line">Comments: Project page: https://github.com/kxfan2002/Reagent</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22154v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22154v1">PDF</a> · <a href="https://github.com/kxfan2002/Reagent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探索智能体推理奖励模型</div>
<div class="mono" style="margin-top:8px">智能体强化学习在实现复杂推理与工具使用方面已取得显著成功，但现有方法仍主要依赖稀疏的结果奖励进行训练。此类反馈无法区分中间推理质量，导致训练效果欠佳。本文提出智能体推理奖励模型，这是一种多维度奖励模型，可为智能体轨迹提供结构化反馈，包括：（1）显式推理轨迹；（2）聚焦式评析，通过指出推理缺陷提供改进指导；（3）评估过程表现的综合评分。基于这些信号，我们系统研究了三种集成策略：Reagent-C（文本增强优化）、Reagent-R（奖励增强引导）和Reagent-U（统一反馈集成）。在12个多样化基准测试中的广泛评估表明，Reagent-U实现了显著性能跃升，在GAIA和WebWalkerQA上分别达到43.7%和46.2%，验证了推理奖励模型与训练方案的有效性。代码、模型与数据集均已开源以促进后续研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of sparse outcome-based rewards in Agentic Reinforcement Learning, which fail to assess intermediate reasoning quality, this paper introduces the Agent Reasoning Reward Model (Agent-RRM) to provide structured feedback on agent trajectories. The method generates an explicit reasoning trace, a focused critique highlighting flaws, and an overall score, and explores three integration strategies: Reagent-C, Reagent-R, and Reagent-U. Experimental results across 12 benchmarks show that Reagent-U achieves significant performance improvements, including 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of the proposed reward model and training approaches.</div>
<div class="mono" style="margin-top:8px">针对智能体强化学习中稀疏结果奖励无法评估中间推理质量的局限性，本文提出了智能体推理奖励模型（Agent-RRM），为智能体轨迹提供结构化反馈。该方法生成显式推理轨迹、聚焦缺陷的批判性指导以及整体评分，并探索了三种集成策略：Reagent-C、Reagent-R和Reagent-U。在12个多样化基准上的实验结果表明，Reagent-U实现了显著性能提升，如在GAIA上达到43.7%，在WebWalkerQA上达到46.2%，验证了所提奖励模型和训练方案的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">DynaWeb: Model-Based Reinforcement Learning of Web Agents</div>
<div class="meta-line">Authors: Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao, Rongzhao Zhang, Lynn Ai, Eric Yang, Tianyu Shi, Lei Yu</div>
<div class="meta-line">First: 2026-01-29T18:59:07+00:00 · Latest: 2026-01-29T18:59:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22149v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22149v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DynaWeb：基于模型的网页智能体强化学习</div>
<div class="mono" style="margin-top:8px">基于大语言模型与强化学习的自主网页智能体开发，是迈向通用人工智能助手的重要进展。然而，与实时互联网交互的低效性、高成本与高风险严重阻碍了这类智能体的训练。基于模型的强化学习通过构建环境的世界模型以实现模拟交互，为此提供了可行方案。本文提出DynaWeb——一种创新的基于模型强化学习框架，通过训练网页世界模型来预测给定智能体动作后的拟真网页表征，并在此合成网页环境中训练智能体策略。该模型支持智能体通过生成海量推演动作轨迹进行“想象”，从而实现高效的在线强化学习。除自由策略推演外，DynaWeb还融合了训练数据中的真实专家轨迹，在训练过程中与策略推演随机交织，以提升稳定性与样本效率。在WebArena和WebVoyager等挑战性基准测试中的实验表明，DynaWeb能持续显著提升前沿开源网页智能体模型的性能。本研究证实了通过“想象”训练网页智能体的可行性，为规模化在线智能体强化学习提供了高效可扩展的路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the inefficiency, cost, and risk of training autonomous web agents via direct interaction with the live internet. It introduces DynaWeb, a model-based reinforcement learning framework that trains a world model to predict web page representations from agent actions, creating a synthetic environment for efficient policy rollouts and learning; this process is stabilized by interleaving real expert trajectories. Experimental results on WebArena and WebVoyager benchmarks show that DynaWeb consistently and significantly boosts the performance of state-of-the-art open-source web agent models, demonstrating the viability of scalable training through simulated interaction.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，通过直接与实时互联网交互来训练自主网络智能体存在效率低、成本高和风险大的问题。为此，论文提出了DynaWeb，这是一个基于模型的强化学习框架，它训练一个世界模型来根据智能体动作预测网页表示，从而创建一个合成环境以实现高效策略推演和学习；训练过程中通过穿插真实专家轨迹来提高稳定性和样本效率。在WebArena和WebVoyager基准测试上的实验结果表明，DynaWeb持续且显著地提升了最先进开源网络智能体模型的性能，证明了通过模拟交互进行可扩展训练的可行性。</div>
</details>
</div>
<div class="card">
<div class="title">SMOG: Scalable Meta-Learning for Multi-Objective Bayesian Optimization</div>
<div class="meta-line">Authors: Leonard Papenmeier, Petru Tighineanu</div>
<div class="meta-line">First: 2026-01-29T18:51:58+00:00 · Latest: 2026-01-29T18:51:58+00:00</div>
<div class="meta-line">Comments: 19 pages, 15 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22131v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22131v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-objective optimization aims to solve problems with competing objectives, often with only black-box access to a problem and a limited budget of measurements. In many applications, historical data from related optimization tasks is available, creating an opportunity for meta-learning to accelerate the optimization. Bayesian optimization, as a promising technique for black-box optimization, has been extended to meta-learning and multi-objective optimization independently, but methods that simultaneously address both settings - meta-learned priors for multi-objective Bayesian optimization - remain largely unexplored. We propose SMOG, a scalable and modular meta-learning model based on a multi-output Gaussian process that explicitly learns correlations between objectives. SMOG builds a structured joint Gaussian process prior across meta- and target tasks and, after conditioning on metadata, yields a closed-form target-task prior augmented by a flexible residual multi-output kernel. This construction propagates metadata uncertainty into the target surrogate in a principled way. SMOG supports hierarchical, parallel training: meta-task Gaussian processes are fit once and then cached, achieving linear scaling with the number of meta-tasks. The resulting surrogate integrates seamlessly with standard multi-objective Bayesian optimization acquisition functions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SMOG：面向多目标贝叶斯优化的可扩展元学习方法</div>
<div class="mono" style="margin-top:8px">多目标优化旨在解决具有竞争性目标的问题，通常仅能通过黑盒方式访问问题且测量预算有限。许多应用场景中存在来自相关优化任务的历史数据，这为利用元学习加速优化提供了可能。贝叶斯优化作为黑盒优化的有效技术，已分别扩展至元学习和多目标优化领域，但能同时处理两种场景的方法——即面向多目标贝叶斯优化的元学习先验——仍鲜有研究。本文提出SMOG，一种基于多输出高斯过程的可扩展模块化元学习模型，能显式学习目标间的相关性。SMOG构建跨元任务与目标任务的结构化联合高斯过程先验，在基于元数据条件化后，通过灵活残差多输出核增强得到闭式目标任务先验。该构建以原则性方式将元数据不确定性传递至目标代理模型。SMOG支持分层并行训练：元任务高斯过程经单次拟合后缓存，实现与元任务数量的线性扩展。所得代理模型可与标准多目标贝叶斯优化采集函数无缝集成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of multi-objective optimization with limited measurements, where historical data from related tasks is often available but not fully leveraged. The authors propose SMOG, a scalable meta-learning method based on a multi-output Gaussian process that learns correlations between objectives, constructing a structured joint prior across meta- and target tasks to propagate metadata uncertainty into the target surrogate in a principled way. Experimental results demonstrate that SMOG enables efficient hierarchical training with linear scaling in meta-tasks and integrates seamlessly with standard acquisition functions for improved optimization performance.</div>
<div class="mono" style="margin-top:8px">本文针对测量次数有限的多目标优化问题，其中相关任务的历史数据常可用但未充分利用。作者提出SMOG，一种基于多输出高斯过程的可扩展元学习方法，通过学习目标间的相关性，构建跨元任务和目标任务的结构化联合先验，从而以原则性方式将元数据不确定性传播到目标代理模型中。实验结果表明，SMOG支持分层并行训练，在元任务数量上实现线性扩展，并能无缝集成标准采集函数以提升优化性能。</div>
</details>
</div>
<div class="card">
<div class="title">Lens-descriptor guided evolutionary algorithm for optimization of complex optical systems with glass choice</div>
<div class="meta-line">Authors: Kirill Antonov, Teus Tukker, Tiago Botari, Thomas H. W. Bäck, Anna V. Kononova, Niki van Stein</div>
<div class="meta-line">First: 2026-01-29T18:13:24+00:00 · Latest: 2026-01-29T18:13:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22075v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22075v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Designing high-performance optical lenses entails exploring a high-dimensional, tightly constrained space of surface curvatures, glass choices, element thicknesses, and spacings. In practice, standard optimizers (e.g., gradient-based local search and evolutionary strategies) often converge to a single local optimum, overlooking many comparably good alternatives that matter for downstream engineering decisions. We propose the Lens Descriptor-Guided Evolutionary Algorithm (LDG-EA), a two-stage framework for multimodal lens optimization. LDG-EA first partitions the design space into behavior descriptors defined by curvature-sign patterns and material indices, then learns a probabilistic model over descriptors to allocate evaluations toward promising regions. Within each descriptor, LDG-EA applies the Hill-Valley Evolutionary Algorithm with covariance-matrix self-adaptation to recover multiple distinct local minima, optionally followed by gradient-based refinement. On a 24-variable (18 continuous and 6 integer), six-element Double-Gauss topology, LDG-EA generates on average around 14500 candidate minima spanning 636 unique descriptors, an order of magnitude more than a CMA-ES baseline, while keeping wall-clock time at one hour scale. Although the best LDG-EA design is slightly worse than a fine-tuned reference lens, it remains in the same performance range. Overall, the proposed LDG-EA produces a diverse set of solutions while maintaining competitive quality within practical computational budgets and wall-clock time.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于透镜描述符引导的进化算法用于复杂光学系统与玻璃选择的优化</div>
<div class="mono" style="margin-top:8px">设计高性能光学透镜需探索一个高维、强约束的空间，涉及表面曲率、玻璃选择、元件厚度与间距。实践中，标准优化器（如基于梯度的局部搜索和进化策略）常收敛至单一局部最优解，忽略了许多对下游工程决策至关重要的同等优质替代方案。我们提出透镜描述符引导进化算法（LDG-EA），一种用于多模态透镜优化的两阶段框架。LDG-EA首先将设计空间划分为由曲率符号模式和材料指数定义的行为描述符，随后学习描述符上的概率模型以将评估资源分配至有前景的区域。在每个描述符内，LDG-EA应用基于协方差矩阵自适应的Hill-Valley进化算法以恢复多个不同的局部极小值，并可选择进行基于梯度的精细化处理。在一个包含24个变量（18个连续变量和6个整数变量）的六元件双高斯拓扑结构上，LDG-EA平均生成约14500个候选极小值，覆盖636个独特描述符，比CMA-ES基准方法多出一个数量级，同时将实际计算时间控制在一小时量级。尽管LDG-EA的最佳设计略逊于精细调优的参考透镜，但仍处于相同性能区间。总体而言，所提出的LDG-EA在实用计算资源和时间预算内，生成了一组多样化的解决方案，同时保持了有竞争力的质量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to explore diverse high-quality optical designs beyond single local optima found by standard methods, this paper introduces the Lens Descriptor-Guided Evolutionary Algorithm (LDG-EA), a two-stage multimodal optimization framework. The method first partitions the design space using curvature-sign and material-index descriptors, then learns a probabilistic model to guide evaluations toward promising regions, and within each region employs an evolutionary algorithm with covariance-matrix adaptation to find multiple local minima, optionally refined with gradient-based techniques. Experimental results on a 24-variable six-element Double-Gauss system show that LDG-EA generates approximately 14,500 candidate minima across 636 unique descriptors—an order of magnitude more than a CMA-ES baseline—within about one hour, with the best designs remaining competitively close in performance to a fine-tuned reference lens, thus achieving a diverse solution set efficiently.</div>
<div class="mono" style="margin-top:8px">针对标准优化器在复杂光学系统设计中易收敛于单一局部最优解而忽略其他可行方案的问题，本文提出了透镜描述符引导进化算法（LDG-EA），一种两阶段多模态优化框架。该方法首先通过曲率符号模式和材料指数定义的行为描述符对设计空间进行分区，然后学习描述符上的概率模型以将评估导向有前景的区域，并在每个区域内应用具有协方差矩阵自适应能力的进化算法来寻找多个局部极小值，可选地结合梯度优化进行细化。在包含24个变量（18个连续、6个整数）的六片双高斯结构实验中，LDG-EA平均生成约14,500个候选极小值，覆盖636个独特描述符，比CMA-ES基线多一个数量级，且计算时间控制在一小时左右；尽管最佳设计略逊于精细调校的参考镜头，但仍保持在同一性能范围内，从而在有限计算预算内实现了多样化的高质量解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">SIA: Symbolic Interpretability for Anticipatory Deep Reinforcement Learning in Network Control</div>
<div class="meta-line">Authors: MohammadErfan Jabbari, Abhishek Duttagupta, Claudio Fiandrino, Leonardo Bonati, Salvatore D&#x27;Oro, Michele Polese, Marco Fiore, Tommaso Melodia</div>
<div class="meta-line">First: 2026-01-29T17:46:46+00:00 · Latest: 2026-01-29T17:46:46+00:00</div>
<div class="meta-line">Comments: 10 pages, 12 figures, accepted at IEEE INFOCOM 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22044v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22044v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep reinforcement learning (DRL) promises adaptive control for future mobile networks but conventional agents remain reactive: they act on past and current measurements and cannot leverage short-term forecasts of exogenous KPIs such as bandwidth. Augmenting agents with predictions can overcome this temporal myopia, yet uptake in networking is scarce because forecast-aware agents act as closed-boxes; operators cannot tell whether predictions guide decisions or justify the added complexity. We propose SIA, the first interpreter that exposes in real time how forecast-augmented DRL agents operate. SIA fuses Symbolic AI abstractions with per-KPI Knowledge Graphs to produce explanations, and includes a new Influence Score metric. SIA achieves sub-millisecond speed, over 200x faster than existing XAI methods. We evaluate SIA on three diverse networking use cases, uncovering hidden issues, including temporal misalignment in forecast integration and reward-design biases that trigger counter-productive policies. These insights enable targeted fixes: a redesigned agent achieves a 9% higher average bitrate in video streaming, and SIA&#x27;s online Action-Refinement module improves RAN-slicing reward by 25% without retraining. By making anticipatory DRL transparent and tunable, SIA lowers the barrier to proactive control in next-generation mobile networks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SIA：面向网络控制中前瞻性深度强化学习的符号可解释性框架</div>
<div class="mono" style="margin-top:8px">深度强化学习（DRL）为未来移动网络提供了自适应控制潜力，但传统智能体仍停留在反应式模式：仅依据历史与当前测量数据行动，无法利用带宽等外生关键绩效指标的短期预测。通过预测增强智能体可克服这种时序短视，但网络领域应用寥寥，因为预测感知型智能体如同黑箱；运营商无法判断预测是否真正指导决策或证明其额外复杂性合理。本文提出首个实时揭示预测增强型DRL智能体运作机制的解析器SIA。该框架融合符号人工智能抽象与逐指标知识图谱以生成解释，并引入新型影响力评分指标。SIA实现亚毫秒级解析速度，比现有可解释人工智能方法快200倍以上。我们在三类网络用例中评估SIA，揭示了包括预测集成时序错位、奖励设计偏差引发反效策略在内的潜在问题。这些洞察促成针对性改进：重构后的智能体在视频流传输中实现平均比特率提升9%，SIA的在线动作优化模块无需重训练即可将无线接入网切片奖励提升25%。通过使前瞻性DRL透明可调，SIA降低了下一代移动网络中主动控制的应用门槛。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need to make forecast-augmented deep reinforcement learning (DRL) agents in network control interpretable, as their current closed-box nature hinders adoption by preventing operators from verifying if predictions genuinely guide decisions. The method introduces SIA, an interpreter that fuses Symbolic AI abstractions with per-KPI Knowledge Graphs to produce real-time explanations and includes a new Influence Score metric. Experimental results show SIA operates with sub-millisecond speed, over 200 times faster than existing explainable AI methods, and its application across three networking use cases reveals hidden issues like temporal misalignment and reward biases, enabling targeted fixes that improve video streaming bitrate by 9% and RAN-slicing reward by 25% without retraining.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决网络控制中基于预测的深度强化学习（DRL）智能体因缺乏可解释性而难以被采纳的问题，其封闭性使运营商无法确认预测是否真正指导决策。方法上提出了SIA，一种融合符号人工智能抽象与每KPI知识图谱以生成实时解释的解读器，并引入了新的影响力评分指标。实验结果表明，SIA以亚毫秒级速度运行，比现有可解释人工智能方法快200倍以上，在三个网络用例中应用后揭示了预测时序错位和奖励设计偏差等隐藏问题，从而实现了针对性改进，将视频流平均比特率提升9%，并在无需重新训练的情况下将无线接入网切片奖励提高25%。</div>
</details>
</div>
<div class="card">
<div class="title">Reward-Preserving Attacks For Robust Reinforcement Learning</div>
<div class="meta-line">Authors: Lucas Schott, Elies Gherbi, Hatem Hajri, Sylvain Lamprier</div>
<div class="meta-line">First: 2026-01-12T01:14:03+00:00 · Latest: 2026-01-29T17:32:38+00:00</div>
<div class="meta-line">Comments: 27 pages, 28 figures, 4 algorithms, 3 tables, preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07118v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.07118v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Adversarial training in reinforcement learning (RL) is challenging because perturbations cascade through trajectories and compound over time, making fixed-strength attacks either overly destructive or too conservative. We propose reward-preserving attacks, which adapt adversarial strength so that an $α$ fraction of the nominal-to-worst-case return gap remains achievable at each state. In deep RL, perturbation magnitudes $η$ are selected dynamically, using a learned critic $Q((s,a),η)$ that estimates the expected return of $α$-reward-preserving rollouts. For intermediate values of $α$, this adaptive training yields policies that are robust across a wide range of perturbation magnitudes while preserving nominal performance, outperforming fixed-radius and uniformly sampled-radius adversarial training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向鲁棒强化学习的奖励保持型攻击</div>
<div class="mono" style="margin-top:8px">强化学习中的对抗训练面临挑战，因为扰动会沿轨迹级联并随时间累积，导致固定强度的攻击要么破坏性过强，要么过于保守。我们提出奖励保持型攻击，通过动态调整对抗强度，使每个状态下仍能保持名义回报与最差回报差距的α比例。在深度强化学习中，我们利用学习到的评判器Q((s,a),η)动态选择扰动幅度η，该评判器用于估计α-奖励保持型轨迹的期望回报。对于中间值的α，这种自适应训练产生的策略能在广泛扰动幅度范围内保持鲁棒性，同时维持名义性能，其表现优于固定半径和均匀采样半径的对抗训练方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of adversarial training in reinforcement learning, where fixed-strength attacks often lead to either overly destructive or overly conservative perturbations that compound over time. To overcome this, the authors propose reward-preserving attacks that dynamically adjust adversarial strength to preserve a specified fraction of the nominal-to-worst-case return gap at each state, using a learned critic to estimate expected returns for adaptive perturbation selection. Experimental results demonstrate that this method, particularly for intermediate parameter values, produces policies robust across a wide range of perturbation magnitudes while maintaining nominal performance, outperforming fixed-radius and uniformly sampled adversarial training approaches.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中对抗性训练的挑战，即固定强度的攻击往往导致随时间累积的扰动过于破坏性或保守。为解决此问题，作者提出奖励保持攻击，动态调整对抗强度以在每个状态保留指定比例的名义与最坏情况回报差距，并使用学习到的评论家来估计自适应扰动选择的期望回报。实验结果表明，该方法在中间参数值下，能产生在广泛扰动幅度下保持鲁棒性且不损失名义性能的策略，优于固定半径和均匀采样半径的对抗性训练方法。</div>
</details>
</div>
<div class="card">
<div class="title">SymbXRL: Symbolic Explainable Deep Reinforcement Learning for Mobile Networks</div>
<div class="meta-line">Authors: Abhishek Duttagupta, MohammadErfan Jabbari, Claudio Fiandrino, Marco Fiore, Joerg Widmer</div>
<div class="meta-line">Venue: IEEE INFOCOM 2025 - IEEE Conference on Computer Communications, London, United Kingdom, 19-22 May 2025, pp. 1-10</div>
<div class="meta-line">First: 2026-01-29T17:31:40+00:00 · Latest: 2026-01-29T17:31:40+00:00</div>
<div class="meta-line">Comments: 10 pages, 9 figures, published in IEEE INFOCOM 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22024v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22024v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The operation of future 6th-generation (6G) mobile networks will increasingly rely on the ability of deep reinforcement learning (DRL) to optimize network decisions in real-time. DRL yields demonstrated efficacy in various resource allocation problems, such as joint decisions on user scheduling and antenna allocation or simultaneous control of computing resources and modulation. However, trained DRL agents are closed-boxes and inherently difficult to explain, which hinders their adoption in production settings. In this paper, we make a step towards removing this critical barrier by presenting SymbXRL, a novel technique for explainable reinforcement learning (XRL) that synthesizes human-interpretable explanations for DRL agents. SymbXRL leverages symbolic AI to produce explanations where key concepts and their relationships are described via intuitive symbols and rules; coupling such a representation with logical reasoning exposes the decision process of DRL agents and offers more comprehensible descriptions of their behaviors compared to existing approaches. We validate SymbXRL in practical network management use cases supported by DRL, proving that it not only improves the semantics of the explanations but also paves the way for explicit agent control: for instance, it enables intent-based programmatic action steering that improves by 12% the median cumulative reward over a pure DRL solution.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SymbXRL：面向移动网络的符号化可解释深度强化学习</div>
<div class="mono" style="margin-top:8px">未来第六代（6G）移动网络的运行将日益依赖深度强化学习（DRL）实时优化网络决策的能力。DRL已在用户调度与天线分配联合决策、计算资源与调制同步控制等多种资源分配问题上展现出显著效能。然而，训练完成的DRL智能体如同黑箱，本质难以解释，这阻碍了其在生产环境中的部署。本文通过提出SymbXRL——一种生成DRL智能体可人为理解解释的新型可解释强化学习（XRL）技术，向消除这一关键障碍迈出一步。SymbXRL利用符号化人工智能，通过直观符号与规则描述关键概念及其关联关系；将此类表征与逻辑推理相结合，能揭示DRL智能体的决策过程，并提供比现有方法更易理解的行为描述。我们在DRL支持的实际网络管理用例中验证SymbXRL，证明其不仅能提升解释的语义质量，还为显式智能体控制开辟道路：例如，基于意图的程序化行动引导机制使累积奖励中位数较纯DRL方案提升12%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to make deep reinforcement learning (DRL) agents more interpretable for real-time optimization in 6G mobile networks, this paper introduces SymbXRL, a novel explainable reinforcement learning technique that leverages symbolic AI to generate human-interpretable explanations using intuitive symbols and rules. The method couples this symbolic representation with logical reasoning to expose the decision process of DRL agents, offering more comprehensible behavioral descriptions than existing approaches. Experimental validation in practical network management use cases demonstrates that SymbXRL not only enhances explanation semantics but also enables explicit agent control, such as intent-based programmatic action steering, which improves the median cumulative reward by 12% over a pure DRL solution.</div>
<div class="mono" style="margin-top:8px">本文的动机是使深度强化学习智能体在6G移动网络的实时优化中更具可解释性，为此提出了SymbXRL这一新颖的可解释强化学习技术，该方法利用符号人工智能，通过直观的符号和规则生成人类可理解的解释。该方法将这种符号表示与逻辑推理相结合，以揭示深度强化学习智能体的决策过程，提供比现有方法更易理解的行为描述。在实际网络管理用例中的实验验证表明，SymbXRL不仅提升了解释的语义质量，还实现了显式的智能体控制，例如基于意图的程序化行动引导，从而将中位累积奖励较纯深度强化学习解决方案提高了12%。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Swarm Mesh Refinement using Deep Reinforcement Learning with Local Rewards</div>
<div class="meta-line">Authors: Niklas Freymuth, Philipp Dahlinger, Tobias Würth, Simon Reisch, Luise Kärger, Gerhard Neumann</div>
<div class="meta-line">First: 2024-06-12T17:26:54+00:00 · Latest: 2026-01-29T17:22:03+00:00</div>
<div class="meta-line">Comments: Submitted to Journal of Machine Learning Research (JMLR)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.08440v2">Abs</a> · <a href="https://arxiv.org/pdf/2406.08440v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Simulating physical systems is essential in engineering, but analytical solutions are limited to straightforward problems. Consequently, numerical methods like the Finite Element Method (FEM) are widely used. However, the FEM becomes computationally expensive as problem complexity and accuracy demands increase. Adaptive Mesh Refinement (AMR) improves the FEM by dynamically placing mesh elements on the domain, balancing computational speed and accuracy. Classical AMR depends on heuristics or expensive error estimators, which may lead to suboptimal performance for complex simulations. While AMR methods based on machine learning are promising, they currently only scale to simple problems. In this work, we formulate AMR as a system of collaborating, homogeneous agents that iteratively split into multiple new agents. This agent-wise perspective enables a spatial reward formulation focused on reducing the maximum mesh element error. Our approach, Adaptive Swarm Mesh Refinement++ (ASMR++), offers efficient, stable optimization and generates highly adaptive meshes at user-defined resolution at inference time. Extensive experiments demonstrate that ASMR++ outperforms heuristic approaches and learned baselines, matching the performance of expensive error-based oracle AMR strategies. ASMR additionally generalizes to different domains during inference, and produces meshes that simulate up to 2 orders of magnitude faster than uniform refinements in more demanding settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于局部奖励深度强化学习的自适应群体网格细化方法</div>
<div class="mono" style="margin-top:8px">物理系统模拟在工程领域至关重要，但解析解仅适用于简单问题。因此，有限元法（FEM）等数值方法被广泛采用。然而，随着问题复杂度和精度要求的提升，FEM的计算成本急剧增加。自适应网格细化（AMR）通过动态分布计算域内的网格单元，在计算速度与精度间取得平衡，从而改进FEM。传统AMR依赖启发式规则或高成本误差估计器，在复杂模拟中可能导致次优性能。虽然基于机器学习的AMR方法前景广阔，但目前仅能应用于简单问题。本研究将AMR构建为协作式同构智能体系统，通过迭代分裂生成新智能体。这种智能体视角支持以降低最大网格单元误差为核心的空间奖励机制。我们提出的自适应群体网格细化++（ASMR++）方法实现了高效稳定的优化，并在推理阶段生成用户指定分辨率的高适应性网格。大量实验表明，ASMR++在性能上超越启发式方法与学习基线，媲美高成本的基于误差的预言式AMR策略。此外，ASMR在推理阶段能泛化至不同计算域，在复杂场景下生成的网格比均匀细化方法的模拟速度快达两个数量级。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the computational expense of classical Adaptive Mesh Refinement (AMR) methods in Finite Element simulations, which rely on heuristics or costly error estimators, this paper introduces ASMR++, a method that formulates AMR as a swarm of homogeneous agents that iteratively split to refine the mesh. The method employs deep reinforcement learning with a local reward function focused on minimizing maximum element error, enabling efficient and stable optimization while generating adaptive meshes at a user-specified resolution. Experimental results show that ASMR++ outperforms heuristic and learned baselines, matches the performance of expensive error-based oracle strategies, generalizes to different domains, and in demanding settings produces meshes that simulate up to 100 times faster than uniform refinements.</div>
<div class="mono" style="margin-top:8px">针对有限元模拟中传统自适应网格细化方法依赖启发式或昂贵误差估计器而导致计算成本高的问题，本文提出了ASMR++方法，将自适应网格细化为一个由同质智能体组成的集群，这些智能体通过迭代分裂来细化网格。该方法采用深度强化学习，并设计了一个专注于减少最大单元误差的局部奖励函数，从而实现了高效稳定的优化，并能按用户指定分辨率生成自适应网格。实验结果表明，ASMR++优于启发式和学习基线方法，其性能与昂贵的基于误差的预言策略相当，能够泛化到不同领域，且在要求较高的场景中，生成的网格模拟速度比均匀细化快达100倍。</div>
</details>
</div>
<div class="card">
<div class="title">Geometry of Drifting MDPs with Path-Integral Stability Certificates</div>
<div class="meta-line">Authors: Zuyuan Zhang, Mahdi Imani, Tian Lan</div>
<div class="meta-line">First: 2026-01-29T17:03:23+00:00 · Latest: 2026-01-29T17:03:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21991v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21991v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-world reinforcement learning is often \emph{nonstationary}: rewards and dynamics drift, accelerate, oscillate, and trigger abrupt switches in the optimal action. Existing theory often represents nonstationarity with coarse-scale models that measure \emph{how much} the environment changes, not \emph{how} it changes locally -- even though acceleration and near-ties drive tracking error and policy chattering. We take a geometric view of nonstationary discounted Markov Decision Processes (MDPs) by modeling the environment as a differentiable homotopy path and tracking the induced motion of the optimal Bellman fixed point. This yields a length-curvature-kink signature of intrinsic complexity: cumulative drift, acceleration/oscillation, and action-gap-induced nonsmoothness. We prove a solver-agnostic path-integral stability bound and derive gap-safe feasible regions that certify local stability away from switch regimes. Building on these results, we introduce \textit{Homotopy-Tracking RL (HT-RL)} and \textit{HT-MCTS}, lightweight wrappers that estimate replay-based proxies of length, curvature, and near-tie proximity online and adapt learning or planning intensity accordingly. Experiments show improved tracking and dynamic regret over matched static baselines, with the largest gains in oscillatory and switch-prone regimes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有路径积分稳定性证书的漂移MDP几何理论</div>
<div class="mono" style="margin-top:8px">现实世界的强化学习常具有非平稳性：奖励与动态特性会发生漂移、加速、振荡并触发最优动作的突变。现有理论通常采用粗粒度模型刻画非平稳性，仅度量环境变化幅度而非局部变化模式——尽管加速度与近似等价关系会引发跟踪误差与策略振荡。本文从几何视角研究非平稳折扣马尔可夫决策过程，将环境建模为可微同伦路径并追踪最优贝尔曼不动点的诱导运动，由此得到表征内在复杂性的长度-曲率-拐点特征：累积漂移、加速度/振荡以及动作间隙导致的非光滑性。我们证明了与求解器无关的路径积分稳定性边界，并推导出可验证切换机制外局部稳定性的间隙安全可行域。基于这些成果，我们提出轻量级封装算法——同伦跟踪强化学习与同伦跟踪蒙特卡洛树搜索，通过在线估计基于经验回放的长度、曲率及近似等价接近度代理指标，自适应调整学习或规划强度。实验表明，相比匹配的静态基线方法，新方法在振荡与易切换场景中显著提升了跟踪性能与动态遗憾指标。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to better capture the local geometric nature of nonstationarity in reinforcement learning—such as acceleration and near-ties that drive tracking error—this paper models drifting Markov Decision Processes (MDPs) as differentiable homotopy paths, tracking the motion of the optimal Bellman fixed point to derive a complexity signature based on length, curvature, and kink. The method yields a solver-agnostic path-integral stability bound and gap-safe feasible regions for local stability, leading to lightweight wrappers, HT-RL and HT-MCTS, which estimate proxies of these geometric features online to adapt learning or planning intensity. Experimental results demonstrate improved tracking performance and dynamic regret over static baselines, with the most significant gains in oscillatory and switch-prone environments.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中非平稳性的局部几何特性（如加速和接近平局导致跟踪误差）提出建模方法，将漂移马尔可夫决策过程视为可微同伦路径，通过跟踪最优贝尔曼不动点的运动，导出了基于长度、曲率和拐点的复杂性特征。该方法给出了与求解器无关的路径积分稳定性边界和间隙安全可行区域以确保局部稳定性，并由此设计了轻量级封装器HT-RL和HT-MCTS，在线估计这些几何特征的代理指标以自适应调整学习或规划强度。实验结果表明，相比静态基线方法，该方法在跟踪性能和动态遗憾方面均有提升，且在振荡和易切换环境中收益最为显著。</div>
</details>
</div>
<div class="card">
<div class="title">Elign: Equivariant Diffusion Model Alignment from Foundational Machine Learning Force Fields</div>
<div class="meta-line">Authors: Yunyang Li, Lin Huang, Luojia Xia, Wenhe Zhang, Mark Gerstein</div>
<div class="meta-line">First: 2026-01-29T17:00:09+00:00 · Latest: 2026-01-29T17:00:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21985v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21985v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative models for 3D molecular conformations must respect Euclidean symmetries and concentrate probability mass on thermodynamically favorable, mechanically stable structures. However, E(3)-equivariant diffusion models often reproduce biases from semi-empirical training data rather than capturing the equilibrium distribution of a high-fidelity Hamiltonian. While physics-based guidance can correct this, it faces two computational bottlenecks: expensive quantum-chemical evaluations (e.g., DFT) and the need to repeat such queries at every sampling step. We present Elign, a post-training framework that amortizes both costs. First, we replace expensive DFT evaluations with a faster, pretrained foundational machine-learning force field (MLFF) to provide physical signals. Second, we eliminate repeated run-time queries by shifting physical steering to the training phase. To achieve the second amortization, we formulate reverse diffusion as a reinforcement learning problem and introduce Force--Energy Disentangled Group Relative Policy Optimization (FED-GRPO) to fine-tune the denoising policy. FED-GRPO includes a potential-based energy reward and a force-based stability reward, which are optimized and group-normalized independently. Experiments show that Elign generates conformations with lower gold-standard DFT energies and forces, while improving stability. Crucially, inference remains as fast as unguided sampling, since no energy evaluations are required during generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Elign：基于基础机器学习力场的等变扩散模型对齐方法</div>
<div class="mono" style="margin-top:8px">三维分子构象的生成模型必须遵循欧几里得对称性，并将概率质量集中于热力学有利且机械稳定的结构。然而，E(3)等变扩散模型往往复现半经验训练数据的偏差，而非捕捉高精度哈密顿量的平衡分布。虽然基于物理的引导可以纠正此问题，但面临两大计算瓶颈：昂贵的量子化学计算（如DFT）以及需要在每个采样步骤重复此类查询。我们提出Elign——一种摊销这两项成本的后训练框架。首先，我们使用更快速的预训练基础机器学习力场（MLFF）替代昂贵的DFT计算以提供物理信号；其次，通过将物理引导转移至训练阶段，消除了运行时重复查询的需求。为实现第二项摊销，我们将反向扩散构建为强化学习问题，并引入力-能量解耦分组相对策略优化（FED-GRPO）来微调去噪策略。FED-GRPO包含基于势能的能量奖励和基于力的稳定性奖励，二者通过独立的分组归一化进行优化。实验表明，Elign生成的构象具有更低的金标准DFT能量与作用力，同时提升稳定性。关键优势在于：由于生成过程无需能量计算，推理速度仍与无引导采样相当。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge that E(3)-equivariant diffusion models for 3D molecular conformation generation often inherit biases from semi-empirical training data instead of accurately capturing the true equilibrium distribution defined by a high-fidelity Hamiltonian. To correct this without incurring high computational costs, the authors introduce Elign, a post-training framework that amortizes expenses by using a pretrained foundational machine-learning force field (MLFF) to provide physical signals and shifting physical steering to the training phase via a reinforcement learning formulation called Force–Energy Disentangled Group Relative Policy Optimization (FED-GRPO). Experimental results demonstrate that Elign generates conformations with lower gold-standard DFT energies and forces while improving stability, all while maintaining inference speeds comparable to unguided sampling since no energy evaluations are needed during generation.</div>
<div class="mono" style="margin-top:8px">本文针对三维分子构象生成中E(3)等变扩散模型常从半经验训练数据中继承偏差、而非准确捕获高保真哈密顿量所定义平衡分布的问题，提出了一种名为Elign的后训练框架以在避免高计算成本下进行校正。该方法通过使用预训练的基础机器学习力场提供物理信号，并通过称为力-能量解耦组相对策略优化的强化学习将物理引导移至训练阶段，从而分摊开销。实验结果表明，Elign生成的构象具有更低的标准DFT能量和力，同时提高了稳定性，且由于生成过程中无需能量评估，其推理速度与无引导采样相当。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic</div>
<div class="meta-line">Authors: Shuo Liu, Tianle Chen, Ryan Amiri, Christopher Amato</div>
<div class="meta-line">First: 2026-01-29T16:50:30+00:00 · Latest: 2026-01-29T16:50:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21972v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21972v1">PDF</a> · <a href="https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \textbf{CoLLM-CC} with a \textbf{C}entralized \textbf{C}ritic and \textbf{CoLLM-DC} with \textbf{D}ecentralized \textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于多智能体演员-评论家框架的分散式大语言模型协作学习</div>
<div class="mono" style="margin-top:8px">近期研究探索通过多智能体强化学习优化大语言模型协作，但现有微调方法多依赖需集中式执行的预定义协议。分散式协作更具实践价值，因其支持智能体并行推理与灵活部署。当前方法采用蒙特卡洛微调存在高方差缺陷，需大量样本训练；而演员-评论家方法能有效应对此问题。本文提出两种多智能体演员-评论家方法：采用集中式评论家的CoLLM-CC与采用分散式评论家的CoLLM-DC。实验表明，在短周期/稠密奖励场景中，蒙特卡洛方法与CoLLM-DC可达CoLLM-CC相当性能；但在长周期/稀疏奖励任务中，蒙特卡洛方法需显著更多样本，CoLLM-DC则难以收敛。代码已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of existing Multi-Agent Reinforcement Learning (MARL) methods for optimizing Large Language Model (LLM) collaboration, which often depend on centralized execution protocols and high-variance Monte Carlo fine-tuning. To enable more practical, decentralized collaboration where agents can run in parallel, the authors propose two Multi-Agent Actor-Critic (MAAC) approaches: CoLLM-CC with a centralized critic and CoLLM-DC with decentralized critics. Experimental results across writing, coding, and game-playing tasks demonstrate that while Monte Carlo methods and CoLLM-DC perform similarly to CoLLM-CC in short-horizon, dense-reward scenarios, they underperform in long-horizon or sparse-reward settings, with Monte Carlo requiring many more samples and CoLLM-DC struggling to converge effectively.</div>
<div class="mono" style="margin-top:8px">本文针对现有用于优化大语言模型（LLM）协作的多智能体强化学习（MARL）方法的局限性展开研究，这些方法通常依赖于集中式执行协议和高方差的蒙特卡洛微调。为实现更实用、可并行推理的分散式协作，作者提出了两种多智能体演员-评论家（MAAC）方法：采用集中式评论家的CoLLM-CC和采用分散式评论家的CoLLM-DC。在写作、编码和游戏等领域的实验结果表明，在短周期、密集奖励的任务中，蒙特卡洛方法和CoLLM-DC与CoLLM-CC性能相当；但在长周期或稀疏奖励的任务中，它们表现不佳，蒙特卡洛方法需要更多样本，而CoLLM-DC则难以有效收敛。</div>
</details>
</div>
<div class="card">
<div class="title">Token-Guard: Towards Token-Level Hallucination Control via Self-Checking Decoding</div>
<div class="meta-line">Authors: Yifan Zhu, Huiqiang Rong, Haoran Luo</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-29T16:48:47+00:00 · Latest: 2026-01-29T16:48:47+00:00</div>
<div class="meta-line">Comments: 26 pages and 11 figures,this work has been accepted for presentation at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21969v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21969v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) often hallucinate, generating content inconsistent with the input. Retrieval-Augmented Generation (RAG) and Reinforcement Learning with Human Feedback (RLHF) can mitigate hallucinations but require resource-intensive retrieval or large-scale fine-tuning. Decoding-based methods are lighter yet lack explicit hallucination control. To address this, we present Token-Guard, a token-level hallucination control method based on self-checking decoding. Token-Guard performs internal verification at each reasoning step to detect hallucinated tokens before they propagate. Candidate fragments are further evaluated in a latent space with explicit hallucination risk scoring, while iterative pruning and regeneration dynamically correct detected errors. Experiments on HALU datasets show Token-Guard substantially reduces hallucinations and improves generation accuracy, offering a scalable, modular solution for reliable LLM outputs. Our code is publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Token-Guard：基于自检解码的令牌级幻觉控制方法</div>
<div class="mono" style="margin-top:8px">大语言模型常产生与输入不一致的幻觉内容。检索增强生成和基于人类反馈的强化学习可缓解幻觉，但需要资源密集的检索或大规模微调。基于解码的方法更轻量，但缺乏显式的幻觉控制。为此，我们提出Token-Guard——一种基于自检解码的令牌级幻觉控制方法。该方法在每个推理步骤执行内部验证，在幻觉令牌传播前进行检测。候选片段在潜空间通过显式幻觉风险评分进一步评估，而迭代剪枝与再生机制动态修正检测到的错误。在HALU数据集上的实验表明，Token-Guard显著减少幻觉并提升生成准确性，为可靠的大语言模型输出提供了可扩展的模块化解决方案。代码已公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Token-Guard, a method to control hallucinations in Large Language Models (LLMs) at the token level, motivated by the limitations of existing approaches like Retrieval-Augmented Generation and Reinforcement Learning with Human Feedback, which are resource-intensive, and decoding-based methods that lack explicit control. The proposed method employs self-checking decoding, where internal verification at each reasoning step detects hallucinated tokens before propagation, evaluates candidate fragments in a latent space with explicit risk scoring, and uses iterative pruning and regeneration to dynamically correct errors. Experimental results on HALU datasets demonstrate that Token-Guard substantially reduces hallucinations and improves generation accuracy, offering a scalable and modular solution for reliable LLM outputs.</div>
<div class="mono" style="margin-top:8px">本文提出了Token-Guard，一种在令牌级别控制大语言模型幻觉的方法，其动机在于现有方法如检索增强生成和基于人类反馈的强化学习资源消耗大，而基于解码的方法缺乏明确控制。该方法采用自检解码，在每个推理步骤进行内部验证以在幻觉令牌传播前检测它们，在潜在空间中用明确的风险评分评估候选片段，并通过迭代修剪和再生动态纠正检测到的错误。在HALU数据集上的实验结果表明，Token-Guard显著减少了幻觉并提高了生成准确性，为可靠的大语言模型输出提供了一个可扩展的模块化解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">One Model, Any Conjunctive Query: Graph Neural Networks for Answering Queries over Incomplete Knowledge Graphs</div>
<div class="meta-line">Authors: Krzysztof Olejniczak, Xingyue Huang, Mikhail Galkin, İsmail İlkan Ceylan</div>
<div class="meta-line">First: 2024-09-21T00:30:44+00:00 · Latest: 2026-01-29T16:42:37+00:00</div>
<div class="meta-line">Comments: Proceedings of the Fourth Learning on Graphs Conference (LoG 2025)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2409.13959v3">Abs</a> · <a href="https://arxiv.org/pdf/2409.13959v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Motivated by the incompleteness of modern knowledge graphs, a new setup for query answering has emerged, where the goal is to predict answers that do not necessarily appear in the knowledge graph, but are present in its completion. In this paper, we formally introduce and study two query answering problems, namely, query answer classification and query answer retrieval. To solve these problems, we propose AnyCQ, a model that can classify answers to any conjunctive query on any knowledge graph. At the core of our framework lies a graph neural network trained using a reinforcement learning objective to answer Boolean queries. Trained only on simple, small instances, AnyCQ generalizes to large queries of arbitrary structure, reliably classifying and retrieving answers to queries that existing approaches fail to handle. This is empirically validated through our newly proposed, challenging benchmarks. Finally, we empirically show that AnyCQ can effectively transfer to completely novel knowledge graphs when equipped with an appropriate link prediction model, highlighting its potential for querying incomplete data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>单一模型，任意合取查询：基于图神经网络的不完全知识图谱查询应答</div>
<div class="mono" style="margin-top:8px">针对现代知识图谱的不完全性，一种新的查询应答框架应运而生，其目标在于预测那些未必出现在知识图谱中、但存在于其完备化版本中的答案。本文正式提出并研究了两个查询应答问题：查询答案分类与查询答案检索。为解决这些问题，我们提出了AnyCQ模型，该模型能够对任意知识图谱上的任意合取查询进行答案分类。我们框架的核心是一个采用强化学习目标训练的图神经网络，用于应答布尔查询。仅通过简单的小规模实例训练，AnyCQ即可泛化至任意结构的大规模查询，可靠地对现有方法无法处理的查询进行分类与答案检索，这一能力通过我们新提出的挑战性基准测试得到了实证验证。最后，实验表明当配备适当的链接预测模型时，AnyCQ能有效迁移至全新的知识图谱，凸显了其在查询不完全数据方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the incompleteness of knowledge graphs, this paper introduces two query answering problems—classification and retrieval—over their completions. The method, named AnyCQ, employs a graph neural network trained via reinforcement learning on small Boolean queries to generalize to large, arbitrary conjunctive queries. Experimental results on new benchmarks demonstrate that AnyCQ reliably classifies and retrieves answers where prior methods fail, and it shows transferability to novel knowledge graphs when combined with a link prediction model.</div>
<div class="mono" style="margin-top:8px">针对知识图谱的不完整性，本文提出了在其补全上进行查询答案分类和检索的两个问题。方法上，提出了AnyCQ模型，它利用强化学习目标训练图神经网络处理小型布尔查询，并能泛化到任意结构的大型合取查询。在新提出的挑战性基准测试中，实验结果表明AnyCQ能可靠地分类和检索现有方法无法处理的查询答案，并且当配备适当的链接预测模型时，能有效迁移到全新的知识图谱上。</div>
</details>
</div>
<div class="card">
<div class="title">SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and Hindsight Relabeling</div>
<div class="meta-line">Authors: Loris Gaven, Clement Romac, Thomas Carta, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2024-10-16T11:59:27+00:00 · Latest: 2026-01-29T16:31:07+00:00</div>
<div class="meta-line">Comments: This work has been presented at the IMOL workshop at NeurIPS 2025 (https://neurips.cc/virtual/2024/101058)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.12481v3">Abs</a> · <a href="https://arxiv.org/pdf/2410.12481v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The past years have seen Large Language Models (LLMs) strive not only as generative models but also as agents solving textual sequential decision-making tasks. When facing complex environments where their zero-shot abilities are insufficient, recent work showed online Reinforcement Learning (RL) could be used for the LLM agent to discover and learn efficient strategies interactively. However, most prior work sticks to on-policy algorithms, which greatly reduces the scope of methods such agents could use for both exploration and exploitation, such as experience replay and hindsight relabeling. Yet, such methods may be key for LLM learning agents, and in particular when designing autonomous intrinsically motivated agents sampling and pursuing their own goals (i.e. autotelic agents). This paper presents and studies an adaptation of Soft Actor-Critic and hindsight relabeling to LLM agents. Our method not only paves the path towards autotelic LLM agents that learn online but can also outperform on-policy methods in more classic multi-goal RL environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAC-GLAM：基于软演员-评论家与后见之明重标注的大语言模型智能体在线强化学习改进方法</div>
<div class="mono" style="margin-top:8px">近年来，大语言模型不仅作为生成模型发展，更成为解决文本序列决策任务的智能体。面对零样本能力不足的复杂环境，近期研究表明可通过在线强化学习使大语言模型智能体以交互方式发现并学习高效策略。然而，现有研究多局限于同策略算法，这极大限制了智能体在探索与利用时可采用的方法范围（如经验回放与后见之明重标注）。此类方法对大语言模型学习智能体至关重要，特别是在设计能自主采样并追求目标的具内在动机智能体（即自目标智能体）时。本文提出并研究将软演员-评论家算法与后见之明重标注适配于大语言模型智能体的方法。该工作不仅为实现在线学习的自目标大语言模型智能体开辟道路，在多目标强化学习经典环境中也展现出超越同策略方法的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to enhance online reinforcement learning for large language model agents beyond on-policy methods, this paper introduces SAC-GLAM, which adapts Soft Actor-Critic and hindsight relabeling to LLM agents. The method enables more efficient exploration and exploitation through techniques like experience replay, aiming to support autonomous, goal-directed learning. Experimental results demonstrate that this approach outperforms on-policy methods in multi-goal RL environments, paving the way for autotelic LLM agents that learn interactively.</div>
<div class="mono" style="margin-top:8px">本文的动机是改进大型语言模型代理的在线强化学习，超越传统同策略方法的限制，提出了SAC-GLAM方法，该方法将软演员-评论家算法和事后重标记技术适配于LLM代理。通过引入经验回放等机制，该方法提升了探索和利用效率，旨在支持自主、目标导向的学习。实验结果表明，在多目标强化学习环境中，该方法优于同策略方法，为实现在线学习的自目标LLM代理奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">WorldLLM: Improving LLMs&#x27; world modeling using curiosity-driven theory-making</div>
<div class="meta-line">Authors: Guillaume Levy, Cedric Colas, Pierre-Yves Oudeyer, Thomas Carta, Clement Romac</div>
<div class="meta-line">First: 2025-06-07T09:13:34+00:00 · Latest: 2026-01-29T16:25:29+00:00</div>
<div class="meta-line">Comments: This project&#x27;s code can be found at https://github.com/flowersteam/WorldLLM. This project was presented at RLDM 2025 (https://rldm.org/)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.06725v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.06725v3">PDF</a> · <a href="https://github.com/flowersteam/WorldLLM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) possess general world knowledge but often struggle to generate precise predictions in structured, domain-specific contexts such as simulations. These limitations arise from their inability to ground their broad, unstructured understanding in specific environments. To address this, we present WorldLLM, a framework that enhances LLM-based world modeling by combining Bayesian inference and autonomous active exploration with reinforcement learning. WorldLLM leverages the in-context learning abilities of LLMs to guide an LLM-based world model&#x27;s predictions using natural language hypotheses given in its prompt. These hypotheses are iteratively refined through a Bayesian inference framework that leverages a second LLM as the proposal distribution given collected evidence. This evidence is collected using a curiosity-driven reinforcement learning policy that explores the environment to find transitions with a low log-likelihood under our LLM-based predictive model using the current hypotheses. By alternating between refining hypotheses and collecting new evidence, our framework autonomously drives continual improvement of the predictions. Our experiments demonstrate the effectiveness of WorldLLM in a textual game environment that requires agents to manipulate and combine objects. The framework not only enhances predictive accuracy, but also generates human-interpretable theories of environment dynamics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WorldLLM：利用好奇心驱动的理论构建改进大语言模型的世界建模能力</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）虽具备通用世界知识，但在结构化、特定领域（如模拟环境）中常难以生成精确预测。这种局限源于其无法将宽泛的非结构化理解锚定于具体环境。为此，我们提出WorldLLM框架，通过结合贝叶斯推断、自主主动探索与强化学习，增强基于LLM的世界建模。该框架利用LLM的上下文学习能力，通过提示中给定的自然语言假设来指导基于LLM的世界模型预测。这些假设通过贝叶斯推断框架迭代优化：使用第二个LLM作为给定收集证据的提议分布。证据收集采用好奇心驱动的强化学习策略，通过探索环境寻找在当前假设下基于LLM预测模型对数似然较低的转移。通过交替优化假设与收集新证据，该框架自主驱动预测能力的持续提升。实验在需要智能体操作与组合对象的文本游戏环境中验证了WorldLLM的有效性。该框架不仅提升了预测准确率，还生成了人类可理解的环境动态理论。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for WorldLLM is to address the limitation of Large Language Models (LLMs) in making precise, structured predictions within specific environments, despite their broad world knowledge. The method combines Bayesian inference and reinforcement learning, using an LLM to generate natural language hypotheses about world dynamics which are iteratively refined through Bayesian updating; evidence for refinement is collected via a curiosity-driven RL policy that actively explores the environment to find transitions poorly predicted by the current model. Experimental results in a textual game environment requiring object manipulation show that the framework improves predictive accuracy and autonomously generates human-interpretable theories of environment dynamics.</div>
<div class="mono" style="margin-top:8px">WorldLLM的研究动机是解决大语言模型（LLM）虽具备广泛世界知识，却难以在特定结构化环境（如模拟环境）中做出精确预测的问题。其方法结合了贝叶斯推理和强化学习，利用LLM生成关于世界动态的自然语言假设，并通过贝叶斯框架迭代优化这些假设；优化所需的证据由一种基于好奇心的强化学习策略主动探索环境来收集，该策略专门寻找当前预测模型下对数似然较低的状态转换。在需要操作和组合对象的文本游戏环境中的实验结果表明，该框架不仅提高了预测准确性，还能自主生成人类可理解的环境动态理论。</div>
</details>
</div>
<div class="card">
<div class="title">Optimistic Transfer under Task Shift via Bellman Alignment</div>
<div class="meta-line">Authors: Jinhang Chai, Enpei Zhang, Elynn Chen, Yujun Yan</div>
<div class="meta-line">First: 2026-01-29T16:16:24+00:00 · Latest: 2026-01-29T16:16:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21924v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21924v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study online transfer reinforcement learning (RL) in episodic Markov decision processes, where experience from related source tasks is available during learning on a target task. A fundamental difficulty is that task similarity is typically defined in terms of rewards or transitions, whereas online RL algorithms operate on Bellman regression targets. As a result, naively reusing source Bellman updates introduces systematic bias and invalidates regret guarantees.
  We identify one-step Bellman alignment as the correct abstraction for transfer in online RL and propose re-weighted targeting (RWT), an operator-level correction that retargets continuation values and compensates for transition mismatch via a change of measure. RWT reduces task mismatch to a fixed one-step correction and enables statistically sound reuse of source data.
  This alignment yields a two-stage RWT $Q$-learning framework that separates variance reduction from bias correction. Under RKHS function approximation, we establish regret bounds that scale with the complexity of the task shift rather than the target MDP. Empirical results in both tabular and neural network settings demonstrate consistent improvements over single-task learning and naïve pooling, highlighting Bellman alignment as a model-agnostic transfer principle for online RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于贝尔曼对齐的任务迁移下的乐观迁移</div>
<div class="mono" style="margin-top:8px">本文研究在线迁移强化学习（RL）在分幕马尔可夫决策过程中的应用，其中在目标任务学习期间可利用相关源任务的经验。核心难点在于任务相似性通常通过奖励或转移概率定义，而在线RL算法基于贝尔曼回归目标运行，直接复用源任务的贝尔曼更新会引入系统性偏差并破坏遗憾界保证。我们提出一步贝尔曼对齐作为在线RL迁移的正确抽象，并设计重加权目标修正（RWT）方法——一种算子级校正技术，通过测度变换重新调整延续值并补偿转移概率失配。RWT将任务失配简化为固定的一步校正，实现源数据的统计可靠复用。该对齐框架形成两阶段RWT Q学习架构，将方差缩减与偏差校正分离。在RKHS函数逼近下，我们建立的遗憾界仅随任务迁移复杂度而非目标MDP复杂度增长。表格与神经网络场景的实验结果表明，该方法相对单任务学习和简单数据池化策略均有稳定提升，验证了贝尔曼对齐作为在线RL模型无关迁移原则的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of online transfer reinforcement learning where source task data is available but naive reuse introduces bias due to misalignment between task similarity metrics and Bellman updates. The authors propose re-weighted targeting (RWT), a method that corrects for transition mismatches by aligning one-step Bellman operators, enabling statistically sound data reuse and separating variance reduction from bias correction. Experimental results in tabular and neural network settings show that this approach consistently outperforms single-task learning and naive data pooling, with regret bounds scaling with task shift complexity rather than target MDP complexity.</div>
<div class="mono" style="margin-top:8px">本文研究在线迁移强化学习，其中源任务数据可用，但由于任务相似性度量与贝尔曼更新之间的错位，直接重用数据会引入偏差。作者提出重加权目标（RWT）方法，通过对齐一步贝尔曼算子来校正转移不匹配，实现统计上可靠的数据重用，并将方差减少与偏差校正分离。在表格和神经网络设置中的实验结果表明，该方法持续优于单任务学习和简单数据池化，且遗憾界随任务偏移复杂度而非目标MDP复杂度缩放。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Compression of Chain-of-Thought via Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Yiqun Chen, Jinyuan Feng, Wei Yang, Meizhi Zhong, Zhengliang Shi, Rui Li, Xiaochi Wei, Yan Gao, Yi Wu, Yao Hu, Zhiqiang Pu, Jiaxin Mao</div>
<div class="meta-line">First: 2026-01-29T16:13:10+00:00 · Latest: 2026-01-29T16:13:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21919v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21919v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The inference overhead induced by redundant reasoning undermines the interactive experience and severely bottlenecks the deployment of Large Reasoning Models. Existing reinforcement learning (RL)-based solutions tackle this problem by coupling a length penalty with outcome-based rewards. This simplistic reward weighting struggles to reconcile brevity with accuracy, as enforcing brevity may compromise critical reasoning logic. In this work, we address this limitation by proposing a multi-agent RL framework that selectively penalizes redundant chunks, while preserving essential reasoning logic. Our framework, Self-Compression via MARL (SCMA), instantiates redundancy detection and evaluation through two specialized agents: \textbf{a Segmentation Agent} for decomposing the reasoning process into logical chunks, and \textbf{a Scoring Agent} for quantifying the significance of each chunk. The Segmentation and Scoring agents collaboratively define an importance-weighted length penalty during training, incentivizing \textbf{a Reasoning Agent} to prioritize essential logic without introducing inference overhead during deployment. Empirical evaluations across model scales demonstrate that SCMA reduces response length by 11.1\% to 39.0\% while boosting accuracy by 4.33\% to 10.02\%. Furthermore, ablation studies and qualitative analysis validate that the synergistic optimization within the MARL framework fosters emergent behaviors, yielding more powerful LRMs compared to vanilla RL paradigms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于多智能体强化学习的思维链自压缩方法</div>
<div class="mono" style="margin-top:8px">冗余推理导致的推断开销会损害交互体验，并严重制约大型推理模型的部署。现有基于强化学习的方法通过将长度惩罚与结果奖励耦合来解决此问题，但这种简单的奖励加权难以平衡简洁性与准确性——强制简洁可能损害关键推理逻辑。本研究提出一种多智能体强化学习框架，通过选择性惩罚冗余推理块来突破这一局限，同时保留核心推理逻辑。该框架（SCMA）通过两个专用智能体实现冗余检测与评估：\textbf{分割智能体}将推理过程分解为逻辑块，\textbf{评分智能体}量化每个块的重要性。训练过程中，分割与评分智能体协同定义重要性加权的长度惩罚，激励\textbf{推理智能体}优先处理核心逻辑，且部署时不引入额外推断开销。跨模型规模的实验表明，SCMA在将响应长度缩减11.1%至39.0%的同时，将准确率提升4.33%至10.02%。消融研究与定性分析进一步验证：MARL框架内的协同优化能促进智能体涌现行为，相比传统强化学习范式可产生更强大的大型推理模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work is motivated by the need to reduce the inference overhead caused by redundant reasoning steps in large reasoning models, which hampers interactive experience and deployment. The proposed method, Self-Compression via Multi-Agent Reinforcement Learning (SCMA), introduces a multi-agent RL framework with a Segmentation Agent to break down reasoning into logical chunks and a Scoring Agent to evaluate each chunk&#x27;s importance, enabling selective penalization of redundancy while preserving critical logic. Experimental results show that SCMA reduces response length by 11.1% to 39.0% and improves accuracy by 4.33% to 10.02% across model scales, with ablation studies confirming the framework&#x27;s effectiveness in fostering emergent behaviors for more efficient reasoning.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大型推理模型中冗余推理步骤导致的推理开销问题，该问题影响交互体验和模型部署。提出的方法是通过多智能体强化学习实现自压缩（SCMA），框架中包含分割智能体将推理过程分解为逻辑块，以及评分智能体量化每个块的重要性，从而在训练中选择性惩罚冗余并保留关键逻辑。实验结果表明，SCMA在不同规模模型上将响应长度减少了11.1%至39.0%，同时将准确率提高了4.33%至10.02%，消融研究和定性分析验证了该框架能通过协同优化产生涌现行为，实现更高效的推理。</div>
</details>
</div>
<div class="card">
<div class="title">MAS-Orchestra: Understanding and Improving Multi-Agent Reasoning Through Holistic Orchestration and Controlled Benchmarks</div>
<div class="meta-line">Authors: Zixuan Ke, Yifei Ming, Austin Xu, Ryan Chin, Xuan-Phi Nguyen, Prathyusha Jwalapuram, Jiayu Wang, Semih Yavuz, Caiming Xiong, Shafiq Joty</div>
<div class="meta-line">First: 2026-01-21T04:57:02+00:00 · Latest: 2026-01-29T16:05:47+00:00</div>
<div class="meta-line">Comments: Preprint; Work in Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14652v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.14652v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While multi-agent systems (MAS) promise elevated intelligence through coordination of agents, current approaches to automatic MAS design under-deliver. Such shortcomings stem from two key factors: (1) methodological complexity - agent orchestration is performed using sequential, code-level execution that limits global system-level holistic reasoning and scales poorly with agent complexity - and (2) efficacy uncertainty - MAS are deployed without understanding if there are tangible benefits compared to single-agent systems (SAS). We propose MASOrchestra, a training-time framework that formulates MAS orchestration as a function-calling reinforcement learning problem with holistic orchestration, generating an entire MAS at once. In MAS-Orchestra, complex, goal-oriented subagents are abstracted as callable functions, enabling global reasoning over system structure while hiding internal execution details. To rigorously study when and why MAS are beneficial, we introduce MASBENCH, a controlled benchmark that characterizes tasks along five axes: Depth, Horizon, Breadth, Parallel, and Robustness. Our analysis reveals that MAS gains depend critically on task structure, verification protocols, and the capabilities of both orchestrator and subagents, rather than holding universally. Guided by these insights, MAS-Orchestra achieves consistent improvements on public benchmarks including mathematical reasoning, multi-hop QA, and search-based QA, while achieving more than 10x efficiency over strong baselines. Together, MAS-Orchestra and MASBENCH enable better training and understanding of MAS in the pursuit of multi-agent intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MAS-Orchestra：通过整体编排与受控基准理解并提升多智能体推理能力</div>
<div class="mono" style="margin-top:8px">尽管多智能体系统（MAS）通过智能体协作有望实现更高智能，但当前自动设计MAS的方法效果不佳。这一不足源于两个关键因素：（1）方法复杂性——现有编排采用顺序化、代码级执行方式，限制了全局系统层面的整体推理能力，且难以随智能体复杂度扩展；（2）效能不确定性——部署MAS时未能明确其相比单智能体系统（SAS）是否具有实质优势。我们提出MAS-Orchestra，一种训练时框架，将MAS编排建模为具有整体规划的函数调用强化学习问题，可一次性生成完整MAS系统。该框架将复杂的目标导向子智能体抽象为可调用函数，在隐藏内部执行细节的同时实现系统结构的全局推理。为严谨探究MAS何时及为何有效，我们构建了受控基准MASBENCH，从深度、跨度、广度、并行性和鲁棒性五个维度刻画任务特性。分析表明，MAS的增益关键取决于任务结构、验证协议及编排器与子智能体的能力，而非普遍适用。基于这些发现，MAS-Orchestra在数学推理、多跳问答和基于搜索的问答等公共基准上实现稳定提升，同时较基线方法获得超10倍的效率优势。MAS-Orchestra与MASBENCH共同为多智能体智能的研究提供了更优的训练与理解框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the underperformance of current multi-agent systems (MAS) due to complex sequential orchestration and uncertain benefits over single-agent systems, this paper introduces MAS-Orchestra, a training-time framework that formulates MAS orchestration as a function-calling reinforcement learning problem with holistic orchestration to generate entire systems at once, abstracting subagents as callable functions for global reasoning. To systematically evaluate MAS advantages, the authors also propose MASBENCH, a controlled benchmark characterizing tasks along five axes: Depth, Horizon, Breadth, Parallel, and Robustness. Experimental results reveal that MAS gains depend on task structure, verification protocols, and agent capabilities rather than being universal, and MAS-Orchestra achieves consistent improvements on public benchmarks in mathematical reasoning, multi-hop QA, and search-based QA, with over 10x efficiency gains compared to strong baselines.</div>
<div class="mono" style="margin-top:8px">针对当前多智能体系统（MAS）因复杂的顺序编排和相对于单智能体系统（SAS）的优势不明确而表现不佳的问题，本文提出了MAS-Orchestra，这是一个训练时框架，将MAS编排表述为具有整体编排功能的函数调用强化学习问题，一次性生成整个系统，并将子智能体抽象为可调用函数以实现全局推理。为系统评估MAS优势，作者还提出了MASBENCH，这是一个受控基准，从深度、时间跨度、广度、并行性和鲁棒性五个维度刻画任务。实验结果表明，MAS的增益取决于任务结构、验证协议和智能体能力而非普遍存在，且MAS-Orchestra在数学推理、多跳问答和基于搜索的问答等公共基准上实现了持续改进，相比强基线效率提升超过10倍。</div>
</details>
</div>
<div class="card">
<div class="title">ProRAG: Process-Supervised Reinforcement Learning for Retrieval-Augmented Generation</div>
<div class="meta-line">Authors: Zhao Wang, Ziliang Zhao, Zhicheng Dou</div>
<div class="meta-line">First: 2026-01-29T16:04:59+00:00 · Latest: 2026-01-29T16:04:59+00:00</div>
<div class="meta-line">Comments: 11 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21912v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21912v1">PDF</a> · <a href="https://github.com/lilinwz/ProRAG">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has become a promising paradigm for optimizing Retrieval-Augmented Generation (RAG) in complex reasoning tasks. However, traditional outcome-based RL approaches often suffer from reward sparsity and inefficient credit assignment, as coarse-grained scalar rewards fail to identify specific erroneous steps within long-horizon trajectories. This ambiguity frequently leads to &quot;process hallucinations&quot;, where models reach correct answers through flawed logic or redundant retrieval steps. Although recent process-aware approaches attempt to mitigate this via static preference learning or heuristic reward shaping, they often lack the on-policy exploration capabilities required to decouple step-level credit from global outcomes. To address these challenges, we propose ProRAG, a process-supervised reinforcement learning framework designed to integrate learned step-level supervision into the online optimization loop. Our framework consists of four stages: (1) Supervised Policy Warmup to initialize the model with a structured reasoning format; (2) construction of an MCTS-based Process Reward Model (PRM) to quantify intermediate reasoning quality; (3) PRM-Guided Reasoning Refinement to align the policy with fine-grained process preferences; and (4) Process-Supervised Reinforcement Learning with a dual-granularity advantage mechanism. By aggregating step-level process rewards with global outcome signals, ProRAG provides precise feedback for every action. Extensive experiments on five multi-hop reasoning benchmarks demonstrate that ProRAG achieves superior overall performance compared to strong outcome-based and process-aware RL baselines, particularly on complex long-horizon tasks, validating the effectiveness of fine-grained process supervision. The code and model are available at https://github.com/lilinwz/ProRAG.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProRAG：面向检索增强生成的过程监督强化学习</div>
<div class="mono" style="margin-top:8px">强化学习已成为优化复杂推理任务中检索增强生成的有前景范式。然而，传统基于结果的强化学习方法常受奖励稀疏性和低效信用分配困扰，因为粗粒度的标量奖励难以识别长轨迹中的具体错误步骤。这种模糊性常导致&#x27;过程幻觉&#x27;，即模型通过有缺陷的逻辑或冗余检索步骤得出正确答案。尽管近期过程感知方法尝试通过静态偏好学习或启发式奖励塑形缓解此问题，但往往缺乏将步骤级信用与全局结果解耦所需的在线策略探索能力。为应对这些挑战，我们提出ProRAG——一个将习得的步骤级监督集成至在线优化循环的过程监督强化学习框架。该框架包含四个阶段：(1)监督策略预热：用结构化推理格式初始化模型；(2)构建基于蒙特卡洛树搜索的过程奖励模型以量化中间推理质量；(3)过程奖励模型引导的推理优化：使策略与细粒度过程偏好对齐；(4)采用双粒度优势机制的过程监督强化学习。通过聚合步骤级过程奖励与全局结果信号，ProRAG为每个动作提供精确反馈。在五个多跳推理基准上的大量实验表明，相较于强结果基线和过程感知强化学习方法，ProRAG在整体性能上表现更优，尤其在复杂长程任务中，验证了细粒度过程监督的有效性。代码与模型发布于https://github.com/lilinwz/ProRAG。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces ProRAG, a process-supervised reinforcement learning framework designed to address the limitations of traditional outcome-based RL in optimizing Retrieval-Augmented Generation (RAG) for complex reasoning tasks, where sparse scalar rewards often lead to inefficient credit assignment and process hallucinations. The method involves a four-stage approach: supervised policy warmup, construction of a Monte Carlo Tree Search-based Process Reward Model (PRM) to assess intermediate reasoning steps, PRM-guided reasoning refinement, and process-supervised RL with a dual-granularity advantage mechanism that combines step-level process rewards with global outcome signals. Experimental results on five multi-hop reasoning benchmarks show that ProRAG outperforms both outcome-based and process-aware RL baselines, especially in long-horizon tasks, demonstrating the effectiveness of fine-grained process supervision.</div>
<div class="mono" style="margin-top:8px">本文提出了ProRAG，一种过程监督的强化学习框架，旨在解决传统基于结果的强化学习在优化检索增强生成（RAG）用于复杂推理任务时的局限性，其中稀疏的标量奖励常导致信用分配低效和过程幻觉。该方法包含四个阶段：监督策略预热、构建基于蒙特卡洛树搜索的过程奖励模型（PRM）以评估中间推理步骤、PRM引导的推理细化，以及采用双粒度优势机制的过程监督强化学习，该机制将步骤级过程奖励与全局结果信号相结合。在五个多跳推理基准上的实验结果表明，ProRAG优于基于结果和过程感知的强化学习基线，尤其在长视野任务中，验证了细粒度过程监督的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Distributions: Geometric Action Control for Continuous Reinforcement Learning</div>
<div class="meta-line">Authors: Zhihao Lin</div>
<div class="meta-line">First: 2025-11-11T13:32:38+00:00 · Latest: 2026-01-29T16:01:23+00:00</div>
<div class="meta-line">Comments: 22 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.08234v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.08234v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Gaussian policies have dominated continuous control in deep reinforcement learning (RL), yet they suffer from a fundamental mismatch: their unbounded support requires ad-hoc squashing functions that distort the geometry of bounded action spaces. While von Mises-Fisher (vMF) distributions offer a theoretically grounded alternative on the sphere, their reliance on Bessel functions and rejection sampling hinders practical adoption. We propose \textbf{Geometric Action Control (GAC)}, a novel action generation paradigm that preserves the geometric benefits of spherical distributions while \textit{simplifying computation}. GAC decomposes action generation into a direction vector and a learnable concentration parameter, enabling efficient interpolation between deterministic actions and uniform spherical noise. This design reduces parameter count from \(2d\) to \(d+1\), and avoids the \(O(dk)\) complexity of vMF rejection sampling, achieving simple \(O(d)\) operations. Empirically, GAC consistently matches or exceeds state-of-the-art methods across six MuJoCo benchmarks, achieving 37.6\% improvement over SAC on Ant-v4 and up to 112\% on complex DMControl tasks, demonstrating strong performance across diverse benchmarks. Our ablation studies reveal that both \textbf{spherical normalization} and \textbf{adaptive concentration control} are essential to GAC&#x27;s success. These findings suggest that robust and efficient continuous control does not require complex distributions, but a principled respect for the geometry of action spaces.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越分布：连续强化学习的几何动作控制</div>
<div class="mono" style="margin-top:8px">高斯策略在深度强化学习的连续控制领域占据主导地位，但其存在根本性不匹配：其无界支撑集需要临时性的压缩函数，这会扭曲有界动作空间的几何结构。虽然冯·米塞斯-费舍尔分布为球面提供了理论完备的替代方案，但其对贝塞尔函数和拒绝采样的依赖阻碍了实际应用。我们提出\textbf{几何动作控制}，这是一种新颖的动作生成范式，在\textit{简化计算}的同时保留了球面分布的几何优势。GAC将动作生成分解为方向向量和可学习的集中度参数，实现了确定性动作与均匀球面噪声之间的高效插值。该设计将参数量从\(2d\)减少到\(d+1\)，避免了vMF拒绝采样\(O(dk)\)的复杂度，实现了简单的\(O(d)\)操作。实证表明，GAC在六项MuJoCo基准测试中始终匹配或超越最先进方法，在Ant-v4上相比SAC提升37.6%，在复杂DMControl任务上最高提升112%，展现了跨多样基准的强劲性能。消融研究表明，\textbf{球面归一化}和\textbf{自适应集中度控制}对GAC的成功均至关重要。这些发现表明，稳健高效的连续控制不需要复杂分布，而需要对动作空间几何结构的原则性尊重。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the geometric mismatch and computational inefficiency of Gaussian and von Mises-Fisher distributions in continuous reinforcement learning, this paper proposes Geometric Action Control (GAC), a novel action generation paradigm that decomposes actions into a direction vector and a learnable concentration parameter to preserve spherical geometry while simplifying computation. The method reduces parameter count and computational complexity to O(d), enabling efficient interpolation between deterministic actions and uniform noise. Experimental results on MuJoCo and DMControl benchmarks show that GAC consistently matches or exceeds state-of-the-art methods, achieving up to a 112% performance improvement, with ablation studies confirming the importance of spherical normalization and adaptive concentration control.</div>
<div class="mono" style="margin-top:8px">针对连续强化学习中高斯分布和冯·米塞斯-费希尔分布存在的几何失配与计算效率低下问题，本文提出了几何动作控制（GAC）这一新颖的动作生成范式，通过将动作分解为方向向量和可学习的集中度参数，在保持球面几何结构的同时简化了计算。该方法减少了参数数量并将计算复杂度降至O(d)，实现了确定性动作与均匀噪声之间的高效插值。在MuJoCo和DMControl基准测试上的实验结果表明，GAC持续匹配或超越现有最优方法，性能提升最高达112%，消融研究进一步证实了球面归一化和自适应集中度控制的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">$π_\texttt{RL}$: Online RL Fine-tuning for Flow-based Vision-Language-Action Models</div>
<div class="meta-line">Authors: Kang Chen, Zhihao Liu, Tonghe Zhang, Zhen Guo, Si Xu, Hao Lin, Hongzhi Zang, Xiang Li, Quanlu Zhang, Zhaofei Yu, Guoliang Fan, Tiejun Huang, Yu Wang, Chao Yu</div>
<div class="meta-line">First: 2025-10-29T18:37:39+00:00 · Latest: 2026-01-29T16:00:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.25889v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.25889v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models enable robots to understand and perform complex tasks from multimodal input. Although recent work explores using reinforcement learning (RL) to automate the laborious data collection process in scaling supervised fine-tuning (SFT), applying RL to large-scale flow-based VLAs (\eg, $π_0$, $π_{0.5}$) remains challenging due to intractable action log-likelihoods raised from flow matching. We address this challenge with $π_{\texttt{RL}}$, featuring two technical approaches: (1) \textbf{Flow-Noise} models the denoising process as a discrete-time MDP with a learnable noise network for exact log-likelihood computation. (2) \textbf{Flow-SDE} integrates denoising with agent-environment interaction, formulating a two-layer MDP that employs ODE-to-SDE conversion for efficient RL exploration. We evaluate $π_{\texttt{RL}}$ across various benchmarks, with experiments demonstrating that RL yields significant performance improvements in both in-distribution and out-of-distribution settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>$π_\texttt{RL}$：面向流式视觉-语言-动作模型的在线强化学习微调方法</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作模型赋予机器人通过多模态输入理解并执行复杂任务的能力。尽管近期研究探索使用强化学习自动化监督微调扩展过程中繁重的数据收集工作，但由于流匹配方法导致动作对数似然难以计算，将强化学习应用于大规模流式视觉-语言-动作模型仍具挑战。我们提出$π_{\texttt{RL}}$解决方案，包含两项技术路径：(1) \textbf{流噪声法}将去噪过程建模为离散时间马尔可夫决策过程，通过可学习的噪声网络实现精确对数似然计算；(2) \textbf{流随机微分方程法}将去噪过程与智能体-环境交互结合，构建双层马尔可夫决策过程框架，采用常微分方程至随机微分方程转换策略实现高效强化学习探索。通过在多种基准测试中评估$π_{\texttt{RL}}$，实验表明强化学习方法在分布内与分布外场景下均能带来显著的性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to overcome the challenge of applying reinforcement learning (RL) to large-scale flow-based Vision-Language-Action (VLA) models, where action log-likelihoods are intractable due to flow matching, hindering the automation of data collection for fine-tuning. The method introduces $π_{\texttt{RL}}$, which employs two technical approaches: Flow-Noise, modeling the denoising process as a discrete-time MDP with a learnable noise network for exact log-likelihood computation, and Flow-SDE, integrating denoising with agent-environment interaction via a two-layer MDP that uses ODE-to-SDE conversion for efficient RL exploration. The main experimental results show that RL fine-tuning with $π_{\texttt{RL}}$ yields significant performance improvements across various benchmarks in both in-distribution and out-of-distribution settings.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决将强化学习应用于大规模基于流的视觉-语言-动作模型时面临的挑战，其中流匹配导致动作对数似然难以处理，阻碍了微调数据收集的自动化。方法上提出了$π_{\texttt{RL}}$，采用两种技术途径：Flow-Noise将去噪过程建模为离散时间马尔可夫决策过程，并引入可学习的噪声网络以精确计算对数似然；Flow-SDE通过两层马尔可夫决策过程将去噪与智能体-环境交互结合，利用ODE到SDE的转换实现高效的强化学习探索。主要实验结果表明，使用$π_{\texttt{RL}}$进行强化学习微调在多种基准测试中，无论是分布内还是分布外设置，均带来了显著的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning</div>
<div class="meta-line">Authors: Shaojie Wang, Liang Zhang</div>
<div class="meta-line">First: 2026-01-29T16:00:48+00:00 · Latest: 2026-01-29T16:00:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21909v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21909v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current LLM post-training methods optimize complete reasoning trajectories through Supervised Fine-Tuning (SFT) followed by outcome-based Reinforcement Learning (RL). While effective, a closer examination reveals a fundamental gap: this approach does not align with how humans actually solve problems. Human cognition naturally decomposes problem-solving into two distinct stages: first acquiring abstract strategies (i.e., meta-knowledge) that generalize across problems, then adapting them to specific instances. In contrast, by treating complete trajectories as basic units, current methods are inherently problem-centric, entangling abstract strategies with problem-specific execution. To address this misalignment, we propose a cognitively-inspired framework that explicitly mirrors the two-stage human cognitive process. Specifically, Chain-of-Meta-Thought (CoMT) focuses supervised learning on abstract reasoning patterns without specific executions, enabling acquisition of generalizable strategies. Confidence-Calibrated Reinforcement Learning (CCRL) then optimizes task adaptation via confidence-aware rewards on intermediate steps, preventing overconfident errors from cascading and improving execution reliability. Experiments across four models and eight benchmarks show 2.19\% and 4.63\% improvements in-distribution and out-of-distribution respectively over standard methods, while reducing training time by 65-70% and token consumption by 50%, demonstrating that aligning post-training with human cognitive principles yields not only superior generalization but also enhanced training efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从元思维到执行：基于认知对齐的后训练方法实现可泛化且可靠的大语言模型推理</div>
<div class="mono" style="margin-top:8px">当前大语言模型后训练方法通过监督微调与基于结果的强化学习优化完整推理轨迹。虽有效，但深入分析揭示其根本缺陷：该方法未契合人类实际解题方式。人类认知自然将问题解决分解为两个阶段：先获取可跨问题泛化的抽象策略（即元知识），再将其适配至具体实例。相比之下，将完整轨迹作为基本单元的现有方法本质是问题中心的，使抽象策略与问题特定执行相互纠缠。为解决这一错位，我们提出受认知启发的框架，显式模拟人类两阶段认知过程。具体而言，元思维链专注于通过监督学习获取不含具体执行的抽象推理模式，从而习得可泛化策略；置信度校准强化学习随后通过中间步骤的置信感知奖励优化任务适配，防止过度自信误差级联传播并提升执行可靠性。在四个模型与八个基准测试上的实验表明，相比标准方法，本方法在分布内与分布外场景分别提升2.19%与4.63%，同时减少65-70%训练时间与50%令牌消耗，证明使后训练与人类认知原则对齐不仅能实现更优泛化能力，还可显著提升训练效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the misalignment between current LLM post-training methods and human cognitive processes, which naturally separate abstract strategy acquisition from specific execution, this paper proposes a cognitively-inspired two-stage framework. The method, Chain-of-Meta-Thought (CoMT), uses supervised learning to capture generalizable reasoning patterns, followed by Confidence-Calibrated Reinforcement Learning (CCRL) to optimize reliable task adaptation with confidence-aware rewards. Experimental results across four models and eight benchmarks show improvements of 2.19% in-distribution and 4.63% out-of-distribution over standard methods, while also reducing training time by 65-70% and token consumption by 50%, demonstrating enhanced generalization and efficiency.</div>
<div class="mono" style="margin-top:8px">本文的动机是现有大语言模型后训练方法与人类认知过程存在偏差，人类解决问题时会将抽象策略获取与具体执行分离。为此，作者提出一个受认知启发的两阶段框架：其方法首先通过监督学习（链式元思维）学习可泛化的推理模式，然后使用置信度校准的强化学习来优化任务适应性。主要实验结果表明，在四个模型和八个基准测试上，该方法相比标准方法在分布内和分布外性能分别提升了2.19%和4.63%，同时训练时间减少了65-70%，令牌消耗降低了50%，证明了其在泛化能力和训练效率上的优势。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed when Solving Expensive Unconstrained Multi-Objective Optimisation Problems</div>
<div class="meta-line">Authors: Tiwonge Msulira Banda, Alexandru-Ciprian Zăvoianu</div>
<div class="meta-line">First: 2026-01-29T15:46:52+00:00 · Latest: 2026-01-29T15:46:52+00:00</div>
<div class="meta-line">Comments: Accepted for publication in SWEVO (Swarm and Evolutionary Computation)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21885v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21885v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-Objective Evolutionary Algorithms (MOEAs) have proven effective at solving Multi-Objective Optimisation Problems (MOOPs). However, their performance can be significantly hindered when applied to computationally intensive industrial problems. To address this limitation, we propose an adaptive surrogate modelling approach designed to accelerate the early-stage convergence speed of state-of-the-art MOEAs. This is important because it ensures that a solver can identify optimal or near-optimal solutions with relatively few fitness function evaluations, thereby saving both time and computational resources. Our method employs a two-loop architecture. The outer loop runs a (baseline) host MOEA which carries out true fitness evaluations. The inner loop contains an Adaptive Accelerator that leverages data-driven machine learning (ML) surrogate models to approximate fitness functions. Integrated with NSGA-II and MOEA/D, our approach was tested on 31 widely known benchmark problems and a real-world North Sea fish abundance modelling case study. The results demonstrate that by incorporating Gaussian Process Regression, one-dimensional Convolutional Neural Networks, and Random Forest Regression, our proposed approach significantly accelerates the convergence speed of MOEAs in the early phases of optimisation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于自适应代理模型的策略用于加速求解昂贵无约束多目标优化问题的收敛速度</div>
<div class="mono" style="margin-top:8px">多目标进化算法（MOEAs）已被证明能有效求解多目标优化问题（MOOPs）。然而，当应用于计算密集型的工业问题时，其性能可能受到显著制约。为克服这一局限，本文提出一种自适应代理建模方法，旨在加速前沿MOEAs在早期阶段的收敛速度。该方法的重要性在于确保求解器能以相对较少的适应度函数评估次数找到最优或近似最优解，从而节省时间与计算资源。我们采用双循环架构：外层循环运行（基线）宿主MOEA执行真实适应度评估；内层循环包含自适应加速器，利用数据驱动的机器学习（ML）代理模型逼近适应度函数。该方法与NSGA-II和MOEA/D集成，在31个广为人知的基准问题及一个北海鱼类丰度建模实际案例中进行了测试。结果表明，通过结合高斯过程回归、一维卷积神经网络和随机森林回归，所提方法能显著提升MOEAs在优化初期的收敛速度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to overcome the computational burden of applying Multi-Objective Evolutionary Algorithms (MOEAs) to expensive, real-world optimisation problems. The proposed method introduces an adaptive surrogate modelling strategy with a two-loop architecture: an outer loop running a baseline MOEA for true evaluations and an inner loop using an Adaptive Accelerator with machine learning surrogates to approximate fitness functions. Experimental results on 31 benchmark problems and a real-world case study in fish abundance modelling show that integrating Gaussian Process Regression, Convolutional Neural Networks, and Random Forest Regression with NSGA-II and MOEA/D significantly accelerates early-stage convergence speed, reducing the number of costly function evaluations required.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决多目标进化算法应用于计算密集型工业问题时性能受限的挑战。为此，提出了一种自适应代理建模方法，采用双循环架构：外循环运行基准MOEA进行真实适应度评估，内循环则利用数据驱动的机器学习代理模型来近似适应度函数。该方法与NSGA-II和MOEA/D集成，在31个经典基准问题和北海鱼类丰度建模的实际案例上进行了测试。实验结果表明，通过结合高斯过程回归、一维卷积神经网络和随机森林回归，所提方法显著加快了多目标进化算法在优化早期的收敛速度，从而节省了计算资源。</div>
</details>
</div>
<div class="card">
<div class="title">Evolution of Benchmark: Black-Box Optimization Benchmark Design through Large Language Model</div>
<div class="meta-line">Authors: Chen Wang, Sijie Ma, Zeyuan Ma, Yue-Jiao Gong</div>
<div class="meta-line">First: 2026-01-29T15:45:11+00:00 · Latest: 2026-01-29T15:45:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21877v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21877v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Benchmark Design in Black-Box Optimization (BBO) is a fundamental yet open-ended topic. Early BBO benchmarks are predominantly human-crafted, introducing expert bias and constraining diversity. Automating this design process can relieve the human-in-the-loop burden while enhancing diversity and objectivity. We propose Evolution of Benchmark (EoB), an automated BBO benchmark designer empowered by the large language model (LLM) and its program evolution capability. Specifically, we formulate benchmark design as a bi-objective optimization problem towards maximizing (i) landscape diversity and (ii) algorithm-differentiation ability across a portfolio of BBO solvers. Under this paradigm, EoB iteratively prompts LLM to evolve a population of benchmark programs and employs a reflection-based scheme to co-evolve the landscape and its corresponding program. Comprehensive experiments validate our EoB is a competitive candidate in multi-dimensional usages: 1) Benchmarking BBO algorithms; 2) Training and testing learning-assisted BBO algorithms; 3) Extending proxy for expensive real-world problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基准演化：基于大语言模型的黑盒优化基准设计</div>
<div class="mono" style="margin-top:8px">黑盒优化（BBO）的基准设计是一个基础但开放的研究课题。早期的BBO基准主要由人工构建，存在专家偏见且多样性受限。自动化设计流程既能减轻人工参与负担，又可提升多样性与客观性。本文提出“基准演化”（EoB）——一种基于大语言模型（LLM）及其程序演化能力的自动化BBO基准设计框架。具体而言，我们将基准设计建模为双目标优化问题，旨在最大化（1）景观多样性，以及（2）跨BBO求解器组合的算法区分能力。在此范式下，EoB通过迭代提示LLM演化基准程序种群，并采用基于反思的机制协同演化景观及其对应程序。综合实验验证了EoB在多维应用场景中的竞争力：1）BBO算法基准测试；2）学习辅助型BBO算法的训练与测试；3）昂贵现实问题的代理扩展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of human-crafted benchmarks in Black-Box Optimization (BBO), which often suffer from expert bias and limited diversity, this paper introduces Evolution of Benchmark (EoB), an automated designer that leverages large language models (LLMs) to evolve benchmark programs. The method formulates benchmark design as a bi-objective optimization problem aimed at maximizing landscape diversity and algorithm-differentiation ability, using LLMs to iteratively evolve programs and a reflection scheme to co-evolve landscapes with their code. Experimental results demonstrate that EoB is effective for benchmarking BBO algorithms, training and testing learning-assisted BBO methods, and serving as a proxy for costly real-world problems.</div>
<div class="mono" style="margin-top:8px">针对黑盒优化中人工设计基准存在的专家偏见和多样性不足问题，本研究提出了Evolution of Benchmark（EoB），一种利用大语言模型自动设计基准的方法。该方法将基准设计构建为一个双目标优化问题，以最大化景观多样性和算法区分能力为目标，通过大语言模型迭代演化程序，并采用反思机制协同演化景观及其对应代码。综合实验验证了EoB在多维应用中的竞争力，包括评估黑盒优化算法、训练和测试学习辅助优化算法，以及作为昂贵现实问题的高效代理。</div>
</details>
</div>
<div class="card">
<div class="title">WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents</div>
<div class="meta-line">Authors: Yao Zhang, Shijie Tang, Zeyu Li, Zhen Han, Volker Tresp</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-29T15:39:50+00:00 · Latest: 2026-01-29T15:39:50+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21872v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21872v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Web agents hold great potential for automating complex computer tasks, yet their interactions involve long-horizon, sequential decision-making with irreversible actions. In such settings, outcome-based supervision is sparse and delayed, often rewarding incorrect trajectories and failing to support inference-time scaling. This motivates the use of Process Reward Models (WebPRMs) for web navigation, but existing approaches remain limited: scalar WebPRMs collapse progress into coarse, weakly grounded signals, while checklist-based WebPRMs rely on brittle template matching that fails under layout or semantic changes and often mislabels superficially correct actions as successful, providing little insight or interpretability. To address these challenges, we introduce WebArbiter, a reasoning-first, principle-inducing WebPRM that formulates reward modeling as text generation, producing structured justifications that conclude with a preference verdict and identify the action most conducive to task completion under the current context. Training follows a two-stage pipeline: reasoning distillation equips the model with coherent principle-guided reasoning, and reinforcement learning corrects teacher biases by directly aligning verdicts with correctness, enabling stronger generalization. To support systematic evaluation, we release WebPRMBench, a comprehensive benchmark spanning four diverse web environments with rich tasks and high-quality preference annotations. On WebPRMBench, WebArbiter-7B outperforms the strongest baseline, GPT-5, by 9.1 points. In reward-guided trajectory search on WebArena-Lite, it surpasses the best prior WebPRM by up to 7.2 points, underscoring its robustness and practical value in real-world complex web tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WebArbiter：面向网络智能体的原则引导推理过程奖励模型</div>
<div class="mono" style="margin-top:8px">网络智能体在自动化复杂计算机任务方面潜力巨大，但其交互涉及长视野、序列化且不可逆的决策过程。在此类场景中，基于结果的监督信号稀疏且延迟，常错误奖励无效轨迹，且难以支持推理时扩展。这促使过程奖励模型（WebPRMs）在网络导航中的应用，但现有方法仍存在局限：标量WebPRMs将进展压缩为粗糙、弱基础的信号，而基于清单的WebPRMs依赖脆弱的模板匹配，在布局或语义变化时易失效，常将表面正确的动作误判为成功，缺乏可解释性。为应对这些挑战，我们提出WebArbiter——一种推理优先、原则引导的WebPRM，将奖励建模构建为文本生成任务，生成结构化论证并最终输出偏好判定，识别当前情境下最有利于任务完成的动作。训练采用两阶段流程：推理蒸馏使模型掌握连贯的原则引导推理，强化学习通过直接对齐判定与正确性来修正教师偏差，从而提升泛化能力。为支持系统化评估，我们发布WebPRMBench基准，涵盖四个多样化网络环境，包含丰富任务与高质量偏好标注。在WebPRMBench上，WebArbiter-7B以9.1分优势超越最强基线GPT-5。在WebArena-Lite的奖励引导轨迹搜索中，其超越先前最佳WebPRM达7.2分，彰显了其在现实复杂网络任务中的鲁棒性与实用价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the challenges of training web agents, where sparse and delayed outcome-based rewards often fail to guide long-horizon, sequential decision-making effectively. To address this, the authors introduce WebArbiter, a principle-guided reasoning process reward model that formulates reward modeling as text generation, producing structured justifications and a preference verdict to identify the most conducive action. The method employs a two-stage training pipeline involving reasoning distillation and reinforcement learning to enhance generalization. Experimental results on the newly released WebPRMBench show that WebArbiter-7B outperforms the strongest baseline, GPT-5, by 9.1 points, and in reward-guided trajectory search on WebArena-Lite, it surpasses the best prior WebPRM by up to 7.2 points, demonstrating robust performance in complex web tasks.</div>
<div class="mono" style="margin-top:8px">本文的动机源于训练网络智能体面临的挑战，即稀疏且延迟的结果监督难以有效指导长视野、序列化的决策过程。为解决这一问题，作者提出了WebArbiter，一种基于原则推理的过程奖励模型，它将奖励建模转化为文本生成任务，生成结构化理由和偏好裁决以识别最有利于任务完成的动作。该方法采用两阶段训练流程，包括推理蒸馏和强化学习，以提升泛化能力。实验结果表明，在新发布的WebPRMBench基准上，WebArbiter-7B以9.1分的优势超越了最强基线GPT-5；在WebArena-Lite的奖励引导轨迹搜索中，它比先前最佳WebPRM高出最多7.2分，证明了其在复杂网络任务中的鲁棒性和实用价值。</div>
</details>
</div>
<div class="card">
<div class="title">READY: Reward Discovery for Meta-Black-Box Optimization</div>
<div class="meta-line">Authors: Zechuan Huang, Zhiguang Cao, Hongshu Guo, Yue-Jiao Gong, Zeyuan Ma</div>
<div class="meta-line">First: 2026-01-29T15:23:18+00:00 · Latest: 2026-01-29T15:23:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21847v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21847v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Meta-Black-Box Optimization (MetaBBO) is an emerging avenue within Optimization community, where algorithm design policy could be meta-learned by reinforcement learning to enhance optimization performance. So far, the reward functions in existing MetaBBO works are designed by human experts, introducing certain design bias and risks of reward hacking. In this paper, we use Large Language Model~(LLM) as an automated reward discovery tool for MetaBBO. Specifically, we consider both effectiveness and efficiency sides. On effectiveness side, we borrow the idea of evolution of heuristics, introducing tailored evolution paradigm in the iterative LLM-based program search process, which ensures continuous improvement. On efficiency side, we additionally introduce multi-task evolution architecture to support parallel reward discovery for diverse MetaBBO approaches. Such parallel process also benefits from knowledge sharing across tasks to accelerate convergence. Empirical results demonstrate that the reward functions discovered by our approach could be helpful for boosting existing MetaBBO works, underscoring the importance of reward design in MetaBBO. We provide READY&#x27;s project at https://anonymous.4open.science/r/ICML_READY-747F.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>READY：元黑盒优化的奖励发现方法</div>
<div class="mono" style="margin-top:8px">元黑盒优化是优化领域的新兴方向，其算法设计策略可通过强化学习进行元学习以提升优化性能。现有研究中奖励函数均由专家人工设计，存在设计偏差和奖励破解风险。本文提出使用大语言模型作为元黑盒优化的自动化奖励发现工具：在有效性方面，借鉴启发式演化思想，在基于LLM的迭代程序搜索过程中引入定制化演化范式，确保持续改进；在效率方面，采用多任务演化架构支持不同元黑盒方法的并行奖励发现，并通过跨任务知识共享加速收敛。实验表明，该方法发现的奖励函数能有效提升现有元黑盒优化性能，印证了奖励设计的重要性。项目地址：https://anonymous.4open.science/r/ICML_READY-747F。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of human-designed reward functions in Meta-Black-Box Optimization (MetaBBO), which can introduce bias and reward hacking risks, by proposing READY, a method that uses Large Language Models (LLMs) to automate reward discovery. The approach enhances effectiveness through an evolutionary paradigm tailored for iterative LLM-based program search to ensure continuous improvement, and improves efficiency via a multi-task evolution architecture that enables parallel reward discovery across different MetaBBO methods while facilitating knowledge sharing to accelerate convergence. Experimental results show that the discovered reward functions effectively boost the performance of existing MetaBBO frameworks, highlighting the critical role of automated reward design in this domain.</div>
<div class="mono" style="margin-top:8px">本文针对元黑盒优化中人工设计奖励函数可能带来的偏差和奖励破解风险，提出了READY方法，利用大语言模型自动发现奖励函数。该方法在有效性方面，借鉴启发式进化思想，在基于大语言模型的迭代程序搜索过程中引入定制化进化范式以确保持续改进；在效率方面，采用多任务进化架构，支持为不同元黑盒优化方法并行发现奖励函数，并通过任务间知识共享加速收敛。实验结果表明，所发现的奖励函数能有效提升现有元黑盒优化工作的性能，强调了奖励设计在该领域的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Constrained Meta Reinforcement Learning with Provable Test-Time Safety</div>
<div class="meta-line">Authors: Tingting Ni, Maryam Kamgarpour</div>
<div class="meta-line">First: 2026-01-29T15:21:37+00:00 · Latest: 2026-01-29T15:21:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21845v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21845v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Meta reinforcement learning (RL) allows agents to leverage experience across a distribution of tasks on which the agent can train at will, enabling faster learning of optimal policies on new test tasks. Despite its success in improving sample complexity on test tasks, many real-world applications, such as robotics and healthcare, impose safety constraints during testing. Constrained meta RL provides a promising framework for integrating safety into meta RL. An open question in constrained meta RL is how to ensure the safety of the policy on the real-world test task, while reducing the sample complexity and thus, enabling faster learning of optimal policies. To address this gap, we propose an algorithm that refines policies learned during training, with provable safety and sample complexity guarantees for learning a near optimal policy on the test tasks. We further derive a matching lower bound, showing that this sample complexity is tight.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有可证明测试时安全性的约束元强化学习</div>
<div class="mono" style="margin-top:8px">元强化学习（RL）使智能体能够利用在任务分布上的经验进行自主训练，从而在新测试任务上更快学习最优策略。尽管其在降低测试任务样本复杂度方面取得成功，但机器人技术和医疗保健等许多实际应用在测试期间需满足安全约束。约束元强化学习为将安全性融入元强化学习提供了有前景的框架。该领域的一个开放性问题是如何在保证现实世界测试任务策略安全性的同时，降低样本复杂度以实现更快的策略学习。针对这一空白，我们提出一种算法，通过优化训练期间习得的策略，为测试任务学习近似最优策略提供可证明的安全性与样本复杂度保证。我们进一步推导出匹配下界，证明该样本复杂度是最优的。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for safe meta reinforcement learning in real-world applications like robotics and healthcare, where test-time safety constraints are critical. The authors propose a constrained meta RL algorithm that refines policies learned during training to ensure provable safety during testing while maintaining sample efficiency. Their main experimental results demonstrate that the algorithm achieves provable safety guarantees and learns near-optimal policies on test tasks with tight sample complexity, which is further supported by a derived matching lower bound.</div>
<div class="mono" style="margin-top:8px">本文针对机器人技术和医疗保健等现实应用中测试时安全约束的关键需求，提出了安全元强化学习的方法。作者设计了一种约束元强化学习算法，通过细化训练期间学习的策略，在测试时确保可证明的安全性，同时保持样本效率。主要实验结果表明，该算法实现了可证明的安全保证，并以紧密的样本复杂度在测试任务中学习到接近最优的策略，且通过推导的匹配下界验证了其最优性。</div>
</details>
</div>
<div class="card">
<div class="title">Error Amplification Limits ANN-to-SNN Conversion in Continuous Control</div>
<div class="meta-line">Authors: Zijie Xu, Zihan Huang, Yiting Dong, Kang Chen, Wenxuan Liu, Zhaofei Yu</div>
<div class="meta-line">First: 2026-01-29T14:28:00+00:00 · Latest: 2026-01-29T14:28:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21778v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21778v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spiking Neural Networks (SNNs) can achieve competitive performance by converting already existing well-trained Artificial Neural Networks (ANNs), avoiding further costly training. This property is particularly attractive in Reinforcement Learning (RL), where training through environment interaction is expensive and potentially unsafe. However, existing conversion methods perform poorly in continuous control, where suitable baselines are largely absent. We identify error amplification as the key cause: small action approximation errors become temporally correlated across decision steps, inducing cumulative state distribution shift and severe performance degradation. To address this issue, we propose Cross-Step Residual Potential Initialization (CRPI), a lightweight training-free mechanism that carries over residual membrane potentials across decision steps to suppress temporally correlated errors. Experiments on continuous control benchmarks with both vector and visual observations demonstrate that CRPI can be integrated into existing conversion pipelines and substantially recovers lost performance. Our results highlight continuous control as a critical and challenging benchmark for ANN-to-SNN conversion, where small errors can be strongly amplified and impact performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>误差放大限制人工神经网络向脉冲神经网络转换在连续控制中的应用</div>
<div class="mono" style="margin-top:8px">脉冲神经网络（SNNs）可通过转换已训练好的人工神经网络（ANNs）获得竞争力，避免额外的高成本训练。这一特性在强化学习（RL）中尤其具有吸引力，因为通过环境交互进行训练成本高昂且可能不安全。然而，现有转换方法在连续控制中表现不佳，该领域基本缺乏合适的基准。我们发现误差放大是关键原因：微小的动作近似误差在决策步骤间形成时间相关性，导致累积状态分布偏移和严重的性能下降。为解决此问题，我们提出跨步残差膜电位初始化（CRPI），这是一种轻量级免训练机制，通过在决策步骤间传递残差膜电位来抑制时间相关误差。在具有向量和视觉观测的连续控制基准测试中，实验表明CRPI可集成到现有转换流程中，并显著恢复损失的性能。我们的研究结果强调连续控制是ANN-to-SNN转换的关键且具有挑战性的基准，其中微小误差可能被强烈放大并影响性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to improve the performance of Spiking Neural Networks (SNNs) in continuous control reinforcement learning tasks, where converting pre-trained Artificial Neural Networks (ANNs) to SNNs is attractive for efficiency and safety but has previously performed poorly. The method introduces Cross-Step Residual Potential Initialization (CRPI), a training-free mechanism that mitigates error amplification by carrying residual membrane potentials across decision steps to suppress temporally correlated action approximation errors. Experimental results on continuous control benchmarks with vector and visual observations show that CRPI, when integrated into existing conversion pipelines, substantially recovers the performance lost due to error amplification, establishing continuous control as a critical and challenging domain for ANN-to-SNN conversion.</div>
<div class="mono" style="margin-top:8px">本研究的动机是为了提升脉冲神经网络在连续控制强化学习任务中的性能，其中将预训练的人工神经网络转换为脉冲神经网络具有高效和安全的吸引力，但先前方法表现不佳。方法上提出了跨步残差膜电位初始化，这是一种无需训练的机制，通过跨决策步传递残差膜电位来抑制时间相关的动作近似误差，从而缓解误差放大问题。在包含向量和视觉观察的连续控制基准测试中的实验结果表明，该机制集成到现有转换流程后，能显著恢复因误差放大而损失的性能，确立了连续控制作为人工神经网络到脉冲神经网络转换的一个关键且具有挑战性的领域。</div>
</details>
</div>
<div class="card">
<div class="title">Language-based Trial and Error Falls Behind in the Era of Experience</div>
<div class="meta-line">Authors: Haoyu Wang, Guozheng Ma, Shugang Cui, Yilun Kong, Haotian Luo, Li Shen, Mengya Gao, Yichao Wu, Xiaogang Wang, Dacheng Tao</div>
<div class="meta-line">First: 2026-01-29T14:08:41+00:00 · Latest: 2026-01-29T14:08:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21754v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21754v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-and-error, which is computationally unsustainable for parameter-heavy LLMs operating in a high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), a novel framework that decouples exploration from exploitation. We employ lightweight &quot;scouts&quot; (e.g., small MLPs) to probe environmental dynamics at a speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables a Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5-Pro (0.60), while saving about 60% GPU hours consumption.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>经验时代下基于语言的试错方法已显滞后</div>
<div class="mono" style="margin-top:8px">尽管大语言模型（LLMs）在基于语言的代理任务中表现出色，但其在未见过的非语言环境（如符号或空间任务）中的适用性仍然有限。先前研究将这种性能差距归因于预训练分布与测试分布之间的不匹配。本文指出，主要瓶颈在于探索成本过高：掌握这些任务需要大量试错，这对于在高维语义空间中运行、参数量庞大的LLMs而言，在计算上是不可持续的。为此，我们提出SCOUT（未见任务子尺度协作框架），一种将探索与利用解耦的新方法。我们采用轻量级“侦察器”（如小型MLPs）以远超LLMs的速度和规模探测环境动态，收集的轨迹通过监督微调（SFT）引导LLM，再经多轮强化学习（RL）激活其潜在的世界知识。实验表明，SCOUT使Qwen2.5-3B-Instruct模型平均得分达到0.86，显著优于包括Gemini-2.5-Pro（0.60）在内的专有模型，同时节省约60%的GPU时耗。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of Large Language Models (LLMs) in adapting to novel, nonlinguistic environments, where their reliance on language-based trial-and-error proves computationally prohibitive due to the high cost of exploration in a vast semantic space. The proposed solution, SCOUT, introduces a framework that decouples exploration from exploitation by employing lightweight &#x27;scout&#x27; models to efficiently probe environmental dynamics, whose collected data is then used to bootstrap the LLM via Supervised Fine-Tuning and subsequent Reinforcement Learning to activate its latent knowledge. Experimental results demonstrate that SCOUT enables a relatively small Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming larger proprietary models like Gemini-2.5-Pro (0.60) while reducing GPU consumption by approximately 60%.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型在适应新颖的非语言环境时的局限性展开研究，指出其依赖语言试错的探索方式在庞大语义空间中计算成本过高。为此，作者提出了SCOUT框架，通过将探索与利用解耦，使用轻量级&#x27;侦察&#x27;模型高效探测环境动态，并利用收集的数据通过监督微调和后续的强化学习来引导大语言模型激活其潜在知识。实验结果表明，SCOUT使Qwen2.5-3B-Instruct模型取得了0.86的平均分，显著优于Gemini-2.5-Pro等大型专有模型，同时节省了约60%的GPU计算时间。</div>
</details>
</div>
<div class="card">
<div class="title">Epistemic Context Learning: Building Trust the Right Way in LLM-Based Multi-Agent Systems</div>
<div class="meta-line">Authors: Ruiwen Zhou, Maojia Song, Xiaobao Wu, Sitao Cheng, Xunjian Yin, Yuxi Xie, Zhuoqun Hao, Wenyue Hua, Liangming Pan, Soujanya Poria, Min-Yen Kan</div>
<div class="meta-line">First: 2026-01-29T13:59:32+00:00 · Latest: 2026-01-29T13:59:32+00:00</div>
<div class="meta-line">Comments: Codes and data are available at https://github.com/skyriver-2000/epistemic-context-learning</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21742v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21742v1">PDF</a> · <a href="https://github.com/skyriver-2000/epistemic-context-learning">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Individual agents in multi-agent (MA) systems often lack robustness, tending to blindly conform to misleading peers. We show this weakness stems from both sycophancy and inadequate ability to evaluate peer reliability. To address this, we first formalize the learning problem of history-aware reference, introducing the historical interactions of peers as additional input, so that agents can estimate peer reliability and learn from trustworthy peers when uncertain. This shifts the task from evaluating peer reasoning quality to estimating peer reliability based on interaction history. We then develop Epistemic Context Learning (ECL): a reasoning framework that conditions predictions on explicitly-built peer profiles from history. We further optimize ECL by reinforcement learning using auxiliary rewards. Our experiments reveal that our ECL enables small models like Qwen 3-4B to outperform a history-agnostic baseline 8x its size (Qwen 3-30B) by accurately identifying reliable peers. ECL also boosts frontier models to near-perfect (100%) performance. We show that ECL generalizes well to various MA configurations and we find that trust is modeled well by LLMs, revealing a strong correlation in trust modeling accuracy and final answer quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>认知上下文学习：在基于大语言模型的多智能体系统中建立信任的正确方式</div>
<div class="mono" style="margin-top:8px">多智能体系统中的个体智能体常缺乏鲁棒性，易盲目遵从误导性同伴。研究表明该弱点源于谄媚倾向与评估同伴可靠性的能力不足。为此，我们首先形式化历史感知参照的学习问题，将同伴历史交互作为额外输入，使智能体能够评估同伴可靠性并在不确定时向可信同伴学习。这将任务重心从评估同伴推理质量转向基于交互历史估计同伴可靠性。我们继而提出认知上下文学习框架：该推理框架基于历史构建的显式同伴画像进行预测。通过强化学习与辅助奖励机制进一步优化该框架。实验表明，ECL能使Qwen 3-4B等小型模型通过精准识别可靠同伴，超越规模为其8倍的历史无关基线模型（Qwen 3-30B）。ECL还能将前沿模型性能提升至接近完美（100%）。研究证实ECL可良好泛化至多种多智能体配置，且大语言模型能有效建模信任机制——信任建模准确度与最终答案质量呈现强相关性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the problem of unreliable peer influence in multi-agent systems, where agents often blindly conform to misleading peers due to sycophancy and poor reliability assessment. The authors propose Epistemic Context Learning (ECL), a framework that enables agents to build explicit peer profiles from interaction history to estimate reliability and condition their predictions on trustworthy peers, further optimized via reinforcement learning with auxiliary rewards. Experimental results show that ECL allows small models like Qwen 3-4B to outperform a history-agnostic baseline 8x larger (Qwen 3-30B) by accurately identifying reliable peers, boosts frontier models to near-perfect performance, generalizes across various multi-agent configurations, and reveals a strong correlation between trust modeling accuracy and final answer quality.</div>
<div class="mono" style="margin-top:8px">本文针对多智能体系统中智能体因盲从和可靠性评估不足而容易受不可靠同伴误导的问题，提出了认知上下文学习框架。该方法通过从历史交互中构建明确的同伴档案来评估其可靠性，使智能体在不确定时能向可信同伴学习，并利用强化学习与辅助奖励进行优化。实验结果表明，该框架使Qwen 3-4B等小模型通过准确识别可靠同伴，性能超越了规模大8倍的无历史基线模型，同时将前沿模型提升至接近完美的性能水平，且能泛化到多种多智能体配置，并揭示了信任建模准确性与最终答案质量之间的强相关性。</div>
</details>
</div>
<div class="card">
<div class="title">ASAP: Exploiting the Satisficing Generalization Edge in Neural Combinatorial Optimization</div>
<div class="meta-line">Authors: Han Fang, Paul Weng, Yutong Ban</div>
<div class="meta-line">First: 2025-01-29T02:12:34+00:00 · Latest: 2026-01-29T13:56:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.17377v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.17377v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep Reinforcement Learning (DRL) has emerged as a promising approach for solving Combinatorial Optimization (CO) problems, such as the 3D Bin Packing Problem (3D-BPP), Traveling Salesman Problem (TSP), or Vehicle Routing Problem (VRP), but these neural solvers often exhibit brittleness when facing distribution shifts. To address this issue, we uncover the Satisficing Generalization Edge, which we validate both theoretically and experimentally: identifying a set of promising actions is inherently more generalizable than selecting the single optimal action. To exploit this property, we propose Adaptive Selection After Proposal (ASAP), a generic framework that decomposes the decision-making process into two distinct phases: a proposal policy that acts as a robust filter, and a selection policy as an adaptable decision maker. This architecture enables a highly effective online adaptation strategy where the selection policy can be rapidly fine-tuned on a new distribution. Concretely, we introduce a two-phase training framework enhanced by Model-Agnostic Meta-Learning (MAML) to prime the model for fast adaptation. Extensive experiments on 3D-BPP, TSP, and CVRP demonstrate that ASAP improves the generalization capability of state-of-the-art baselines and achieves superior online adaptation on out-of-distribution instances.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ASAP：利用神经组合优化中的满意泛化边界</div>
<div class="mono" style="margin-top:8px">深度强化学习已成为解决组合优化问题的有效方法，例如三维装箱问题、旅行商问题或车辆路径问题，但这些神经求解器在面对分布偏移时往往表现出脆弱性。为解决这一问题，我们揭示了满意泛化边界，并从理论和实验两方面验证了其有效性：识别一组有潜力的动作本质上比选择单一最优动作更具泛化能力。为利用这一特性，我们提出了自适应提议后选择框架，该通用框架将决策过程分解为两个独立阶段：作为鲁棒过滤器的提议策略，以及作为适应性决策者的选择策略。这种架构支持高效的在线适应策略，使选择策略能在新分布上快速微调。具体而言，我们引入了通过模型无关元学习增强的两阶段训练框架，为模型实现快速适应奠定基础。在三维装箱问题、旅行商问题和带容量约束的车辆路径问题上的大量实验表明，该框架提升了先进基线的泛化能力，并在分布外实例上实现了卓越的在线适应性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the brittleness of deep reinforcement learning neural solvers for combinatorial optimization problems under distribution shifts by uncovering and exploiting the Satisficing Generalization Edge, where identifying a set of promising actions is more generalizable than selecting a single optimal one. The proposed method, Adaptive Selection After Proposal (ASAP), decomposes decision-making into a robust proposal policy and an adaptable selection policy, enhanced by a two-phase training framework using Model-Agnostic Meta-Learning to prime the model for fast adaptation. Experimental results on 3D bin packing, traveling salesman, and capacitated vehicle routing problems show that ASAP improves generalization over state-of-the-art baselines and achieves superior online adaptation on out-of-distribution instances.</div>
<div class="mono" style="margin-top:8px">该论文针对深度强化学习神经求解器在组合优化问题中面对分布偏移时的脆弱性，通过揭示并利用满意泛化边界——即识别一组有希望的动作比选择单一最优动作更具泛化性——来解决问题。提出的方法ASAP将决策过程分解为稳健的提议策略和可适应的选择策略，并利用基于模型无关元学习的两阶段训练框架来使模型具备快速适应能力。在三维装箱、旅行商和带容量车辆路径问题上的实验结果表明，ASAP相比最先进的基线方法提升了泛化能力，并在分布外实例上实现了更优的在线适应性能。</div>
</details>
</div>
<div class="card">
<div class="title">Mixed-Precision Training and Compilation for RRAM-based Computing-in-Memory Accelerators</div>
<div class="meta-line">Authors: Rebecca Pelke, Joel Klein, Jose Cubero-Cascante, Nils Bosbach, Jan Moritz Joseph, Rainer Leupers</div>
<div class="meta-line">First: 2026-01-29T13:54:55+00:00 · Latest: 2026-01-29T13:54:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21737v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21737v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Computing-in-Memory (CIM) accelerators are a promising solution for accelerating Machine Learning (ML) workloads, as they perform Matrix-Vector Multiplications (MVMs) on crossbar arrays directly in memory. Although the bit widths of the crossbar inputs and cells are very limited, most CIM compilers do not support quantization below 8 bit. As a result, a single MVM requires many compute cycles, and weights cannot be efficiently stored in a single crossbar cell. To address this problem, we propose a mixed-precision training and compilation framework for CIM architectures. The biggest challenge is the massive search space, that makes it difficult to find good quantization parameters. This is why we introduce a reinforcement learning-based strategy to find suitable quantization configurations that balance latency and accuracy. In the best case, our approach achieves up to a 2.48x speedup over existing state-of-the-art solutions, with an accuracy loss of only 0.086 %.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向RRAM存内计算加速器的混合精度训练与编译方法</div>
<div class="mono" style="margin-top:8px">存内计算（CIM）加速器通过在交叉阵列中直接执行矩阵向量乘法（MVM）来加速机器学习（ML）任务，是一种极具前景的解决方案。尽管交叉阵列的输入和单元位宽非常有限，但大多数CIM编译器不支持8位以下的量化。这导致单个MVM需要大量计算周期，且权重无法高效存储在单个交叉单元中。为解决这一问题，我们提出了一种面向CIM架构的混合精度训练与编译框架。最大挑战在于庞大的搜索空间使得量化参数难以优化，因此我们引入基于强化学习的策略来寻找平衡延迟与精度的量化配置。在最佳情况下，该方法相比现有先进方案实现了最高2.48倍的加速，精度损失仅为0.086%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the inefficiency of existing Computing-in-Memory (CIM) accelerators, which are limited by their inability to effectively support quantization below 8 bits, leading to high latency and inefficient weight storage. The authors propose a mixed-precision training and compilation framework that employs a reinforcement learning strategy to navigate the vast search space and automatically determine optimal quantization configurations, thereby balancing computational speed and model accuracy. Experimental results demonstrate that their method can achieve up to a 2.48x speedup compared to state-of-the-art solutions while maintaining minimal accuracy loss of only 0.086%.</div>
<div class="mono" style="margin-top:8px">本文针对现有存内计算加速器因无法有效支持8位以下量化而导致高延迟和权重存储效率低的问题，提出了一种混合精度训练与编译框架。该方法采用基于强化学习的策略来探索庞大的搜索空间，自动确定最优的量化配置，以平衡计算速度与模型精度。实验结果表明，该方法相比现有最优解决方案可实现最高2.48倍的加速，同时仅带来0.086%的精度损失。</div>
</details>
</div>
<div class="card">
<div class="title">Memento 2: Learning by Stateful Reflective Memory</div>
<div class="meta-line">Authors: Jun Wang</div>
<div class="meta-line">First: 2025-12-27T22:15:03+00:00 · Latest: 2026-01-29T13:49:34+00:00</div>
<div class="meta-line">Comments: 35 pages, four figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22716v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.22716v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a theoretical study of continual and experiential learning in large language model agents that combine episodic memory with reinforcement learning. We argue that the key mechanism for continual adaptation, without updating model parameters, is reflection: the agent&#x27;s ability to use past experience to guide future actions. Empirical findings suggest that episodic, experience-driven reflection enables generalised adaptation across a wide range of open-ended, long-horizon tasks. This indicates that efficient learning can occur during deployment and weakens the traditional separation between training and testing. Motivated by this, we introduce the Stateful Reflective Decision Process, a formal model of reflective memory dynamics. In this abstraction, an agent maintains an episodic memory and performs two core operations. Writing stores interaction outcomes and plays the role of policy evaluation. Reading retrieves relevant past cases to inform decisions and plays the role of policy improvement. This perspective treats reflective memory as a control object that can be analysed using classical reinforcement learning tools. We then develop a read-write reflective learning framework by integrating retrieval into soft policy iteration and establish convergence guarantees. We show that as memory grows and provides denser coverage of the state space, the resulting composite policy converges to the optimal solution. Overall, this framework connects practical memory-based methods with principled reinforcement learning, providing a rigorous mathematical basis for building reflective, memory-embedded agents capable of continual general-purpose learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Memento 2：基于状态化反思记忆的学习机制</div>
<div class="mono" style="margin-top:8px">本文对结合情景记忆与强化学习的大语言模型智能体进行了持续性与经验性学习的理论研究。我们认为，无需更新模型参数即可实现持续适应的关键机制在于反思：即智能体利用过往经验指导未来行动的能力。实证研究表明，基于情景经验的反思机制能够在开放域、长周期任务中实现泛化适应。这表明高效学习可在部署阶段发生，从而弱化了传统训练与测试阶段的界限。基于此，我们提出了状态化反思决策过程——一种形式化的反思记忆动态模型。在该抽象框架中，智能体维护情景记忆并执行两项核心操作：写入操作存储交互结果，承担策略评估功能；读取操作检索相关历史案例以辅助决策，承担策略改进功能。该视角将反思记忆视为可通过经典强化学习工具分析的控制对象。随后，我们通过将检索机制整合至软策略迭代中，构建了读写式反思学习框架，并建立了收敛性保证。研究表明，随着记忆增长及对状态空间覆盖密度的提升，复合策略将收敛至最优解。总体而言，该框架将基于记忆的实践方法与原则性强化学习相连接，为构建具备持续通用学习能力的反思型记忆嵌入智能体提供了严谨的数学基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates continual learning in large language model agents, motivated by the need for adaptation without parameter updates through reflective use of past experience. The method introduces the Stateful Reflective Decision Process, a formal model where agents maintain episodic memory with write operations for policy evaluation and read operations for policy improvement, integrating retrieval into soft policy iteration with convergence guarantees. Experimental results show that this episodic, experience-driven reflection enables generalized adaptation across open-ended, long-horizon tasks, suggesting efficient learning during deployment and blurring the traditional training-testing divide, with convergence to optimal solutions as memory coverage increases.</div>
<div class="mono" style="margin-top:8px">本文研究大型语言模型智能体的持续学习，其动机在于无需参数更新即可通过反思性利用过往经验实现适应。方法上提出了状态化反思决策过程这一形式化模型，其中智能体维护情景记忆，通过写入操作进行策略评估和读取操作进行策略改进，并将检索集成到软策略迭代中，确保了收敛性。实验结果表明，这种基于情景的经验驱动反思能够在开放式的长周期任务中实现泛化适应，表明在部署期间即可进行高效学习，模糊了传统训练与测试的界限，且随着记忆覆盖增加，复合策略会收敛至最优解。</div>
</details>
</div>
<div class="card">
<div class="title">Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations</div>
<div class="meta-line">Authors: Donatien Delehelle, Fei Chen, Darwin Caldwell</div>
<div class="meta-line">First: 2026-01-29T13:41:35+00:00 · Latest: 2026-01-29T13:41:35+00:00</div>
<div class="meta-line">Comments: 6 pages, 4 figures,</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21713v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21713v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>解耦感知与推理以提升无演示学习的布料操作数据效率</div>
<div class="mono" style="margin-top:8px">布料操作是日常生活中的普遍任务，但对机器人而言仍是一个开放挑战。开发布料操作策略的困难源于布料的高维状态空间、复杂动力学特性及易自遮挡的特点。由于分析方法未能提供稳健且通用的操作策略，强化学习（RL）被视为解决这些问题的有前景途径。然而，为应对庞大的状态空间和复杂动力学，基于数据的方法通常依赖大型模型和长训练时间，其计算成本严重阻碍了这些方法的发展与应用。此外，由于稳健状态估计的挑战，布料操作策略常采用以工作空间图像为输入的端到端学习方法。虽然该方法通过真实世界微调实现了概念上直接的仿真到现实迁移，但通过在环境状态的高损耗表示上训练智能体，也带来了显著计算成本。本文通过探索一种高效模块化的布料操作强化学习方法，对这一常见设计选择提出质疑。我们证明，通过精心设计，在仿真中学习时可显著减小模型规模与训练时间。进一步，我们展示了如何将仿真训练的模型迁移至现实世界。我们在SoftGym基准上评估本方法，在任务中相比现有基线取得显著性能提升，同时使用了更小的模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the data inefficiency and high computational cost of reinforcement learning for cloth manipulation by proposing a modular approach that disentangles perception from reasoning. The method separates visual state estimation from policy learning, using a compact model trained in simulation to reduce model size and training time. Experimental results on the SoftGym benchmark show significant performance improvements over baselines while using a substantially smaller model, and the simulation-trained model is successfully transferred to real-world tasks.</div>
<div class="mono" style="margin-top:8px">本文针对布料操作中强化学习的数据效率低和计算成本高的问题，提出了一种将感知与推理解耦的模块化方法。该方法将视觉状态估计与策略学习分离，通过在仿真中训练紧凑模型来减小模型规模并缩短训练时间。在SoftGym基准测试上的实验结果表明，该方法在使用更小模型的同时显著超越了现有基线性能，并且仿真训练的模型成功迁移到了实际任务中。</div>
</details>
</div>
<div class="card">
<div class="title">TACLer: Tailored Curriculum Reinforcement Learning for Efficient Reasoning</div>
<div class="meta-line">Authors: Huiyuan Lai, Malvina Nissim</div>
<div class="meta-line">First: 2026-01-29T13:40:35+00:00 · Latest: 2026-01-29T13:40:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21711v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21711v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have shown remarkable performance on complex reasoning tasks, especially when equipped with long chain-of-thought (CoT) reasoning. However, eliciting long CoT typically requires large-scale reinforcement learning (RL) training, while often leading to overthinking with redundant intermediate steps. To improve learning and reasoning efficiency, while preserving or even enhancing performance, we propose TACLer, a model-tailored curriculum reinforcement learning framework that gradually increases the complexity of the data based on the model&#x27;s proficiency in multi-stage RL training. TACLer features two core components: (i) tailored curriculum learning that determines what knowledge the model lacks and needs to learn in progressive stages; (ii) a hybrid Thinking/NoThinking reasoning paradigm that balances accuracy and efficiency by enabling or disabling the Thinking mode. Our experiments show that TACLer yields a twofold advantage in learning and reasoning: (i) it reduces computational cost, cutting training compute by over 50% compared to long thinking models and reducing inference token usage by over 42% relative to the base model; and (ii) it improves accuracy by over 9% on the base model, consistently outperforming state-of-the-art Nothinking and Thinking baselines across four math datasets with complex problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TACLer：面向高效推理的定制化课程强化学习</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）在复杂推理任务上展现出卓越性能，尤其在配备长链思维推理时。然而，激发长链思维通常需要大规模强化学习训练，且常因冗余中间步骤导致过度思考。为在保持甚至提升性能的同时提高学习与推理效率，我们提出TACLer——一种基于模型能力定制课程难度的强化学习框架，通过多阶段训练逐步提升数据复杂度。TACLer包含两大核心组件：（1）定制化课程学习：动态识别模型知识缺口并规划渐进学习阶段；（2）混合型“思考/非思考”推理范式：通过启用或禁用思考模式平衡准确性与效率。实验表明TACLer具有双重优势：（1）降低计算成本：相比长链思维模型减少超50%训练算力，较基线模型节省超42%推理令牌；（2）提升准确性：在四个复杂数学数据集上，较基线模型精度提升超9%，持续优于当前最优的非思考与思考基线模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind TACLer is to enhance the efficiency of large language models in complex reasoning tasks, which often rely on lengthy chain-of-thought processes that can lead to computational waste and overthinking. The method introduces a tailored curriculum reinforcement learning framework that progressively increases data complexity based on model proficiency, coupled with a hybrid Thinking/NoThinking paradigm to balance reasoning depth and speed. Experimental results demonstrate that TACLer reduces training compute by over 50% and inference token usage by over 42% compared to baseline models, while also improving accuracy by more than 9% across four math datasets, outperforming state-of-the-art approaches.</div>
<div class="mono" style="margin-top:8px">TACLer的提出动机是为了提升大语言模型在复杂推理任务中的效率，这些任务通常依赖冗长的思维链，可能导致计算资源浪费和过度思考。该方法采用了一种定制化的课程强化学习框架，根据模型熟练度逐步增加数据复杂度，并结合混合的思考/非思考推理范式以平衡准确性与效率。实验结果表明，与基线模型相比，TACLer将训练计算成本降低了50%以上，推理令牌使用量减少了42%以上，同时在四个数学数据集上的准确率提高了超过9%，性能优于当前最先进的非思考和思考基线模型。</div>
</details>
</div>
<div class="card">
<div class="title">Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction</div>
<div class="meta-line">Authors: Mathieu Blondel, Michael E. Sander, Germain Vivier-Ardisson, Tianlin Liu, Vincent Roulet</div>
<div class="meta-line">First: 2025-12-17T17:14:26+00:00 · Latest: 2026-01-29T13:36:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15605v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.15605v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive models (ARMs) currently constitute the dominant paradigm for large language models (LLMs). Energy-based models (EBMs) represent another class of models, which have historically been less prevalent in LLM development, yet naturally characterize the optimal policy in post-training alignment. In this paper, we provide a unified view of these two model classes. Taking the chain rule of probability as a starting point, we establish an explicit bijection between ARMs and EBMs in function space, which we show to correspond to a special case of the soft Bellman equation in maximum entropy reinforcement learning. Building upon this bijection, we derive the equivalence between supervised learning of ARMs and EBMs. Furthermore, we analyze the distillation of EBMs into ARMs by providing theoretical error bounds. Our results provide insights into the ability of ARMs to plan ahead, despite being based on the next-token prediction paradigm.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自回归语言模型实为基于能量的模型：对下一词预测前瞻能力的洞察</div>
<div class="mono" style="margin-top:8px">自回归模型（ARMs）是目前大语言模型（LLMs）的主导范式。基于能量的模型（EBMs）是另一类模型，在LLM发展中历来较少见，但自然描述了训练后对齐中的最优策略。本文为这两类模型提供了统一视角。以概率链式法则为起点，我们在函数空间中建立了ARMs与EBMs之间的显式双射，并证明其对应于最大熵强化学习中软贝尔曼方程的特例。基于此双射，我们推导出ARMs与EBMs监督学习的等价性。此外，通过提供理论误差界，我们分析了EBMs向ARMs的蒸馏过程。研究结果揭示了ARMs在基于下一词预测范式下仍具备前瞻规划能力的机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to unify autoregressive models (ARMs), the dominant paradigm for large language models, with energy-based models (EBMs), which naturally characterize optimal policies in alignment, to better understand ARMs&#x27; capabilities. The method establishes an explicit bijection between ARMs and EBMs in function space, linking it to the soft Bellman equation in maximum entropy reinforcement learning, and derives the equivalence between their supervised learning while providing theoretical bounds for distilling EBMs into ARMs. The main experimental results, implied through theoretical analysis, offer insights into how ARMs can perform lookahead planning despite being trained on next-token prediction, bridging these model classes conceptually.</div>
<div class="mono" style="margin-top:8px">本文的动机在于统一自回归模型（当前大语言模型的主导范式）与能量基模型（能自然表征对齐中的最优策略），以深入理解自回归模型的能力。方法上，在函数空间建立了自回归模型与能量基模型之间的显式双射，将其关联到最大熵强化学习中的软贝尔曼方程，并推导了二者监督学习的等价性，同时为能量基模型蒸馏到自回归模型提供了理论误差界。主要实验结果通过理论分析表明，尽管自回归模型基于下一词预测训练，但仍能具备前瞻规划能力，从而在概念上连接了这两类模型。</div>
</details>
</div>
<div class="card">
<div class="title">Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data</div>
<div class="meta-line">Authors: Yuval Ran-Milo, Yotam Alexander, Shahar Mendel, Nadav Cohen</div>
<div class="meta-line">First: 2026-01-21T16:36:19+00:00 · Latest: 2026-01-29T13:30:35+00:00</div>
<div class="meta-line">Comments: 87 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15158v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.15158v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformers trained via Reinforcement Learning (RL) with outcome-based supervision can spontaneously develop the ability to generate intermediate reasoning steps (Chain-of-Thought). Yet the mechanism by which sparse rewards drive policy gradient to discover such systematic reasoning remains poorly understood. We address this by analyzing the policy gradient dynamics of single-layer Transformers on a synthetic graph traversal task that cannot be solved without Chain-of-Thought but admits a simple iterative solution. We prove that despite training solely on final-answer correctness, policy gradient drives the Transformer to converge to a structured, interpretable algorithm that iteratively traverses the graph vertex-by-vertex. We characterize the distributional properties required for this emergence, identifying the critical role of &quot;simple examples&quot;: instances requiring fewer reasoning steps. When the training distribution places sufficient mass on these simpler examples, the Transformer learns a generalizable traversal strategy that extrapolates to longer chains; when this mass vanishes, policy gradient learning becomes infeasible. We corroborate our theoretical results through experiments on synthetic data and with real-world language models on mathematical reasoning tasks, validating that our theoretical findings carry over to practical settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于结果的强化学习可证明引导Transformer进行推理，但仅适用于特定数据</div>
<div class="mono" style="margin-top:8px">通过基于结果的监督进行强化学习训练的Transformer能够自发产生中间推理步骤（思维链）。然而，稀疏奖励如何驱动策略梯度发现这种系统性推理的机制仍不明确。我们通过分析单层Transformer在合成图遍历任务上的策略梯度动态来研究此问题——该任务必须通过思维链才能解决，但存在简单的迭代解法。我们证明：尽管仅通过最终答案正确性进行训练，策略梯度仍能驱动Transformer收敛至一种结构化、可解释的逐顶点迭代遍历算法。我们刻画了该现象涌现所需的分布特性，指出“简单样本”（需要较少推理步骤的实例）的关键作用。当训练分布中简单样本具有足够权重时，Transformer能学习可泛化至更长链的遍历策略；若此类样本缺失，策略梯度学习将无法实现。我们通过合成数据实验及真实数学推理任务的语言模型实验验证了理论结论在实际场景中的适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates how Transformers trained with outcome-based Reinforcement Learning (RL) can develop systematic reasoning chains, a process not well understood. The authors analyze policy gradient dynamics on a synthetic graph traversal task that necessitates intermediate steps, proving that training solely on final-answer correctness leads the model to converge to an interpretable, iterative algorithm. Their theoretical analysis identifies that the presence of &#x27;simple examples&#x27; (instances with fewer reasoning steps) in the training distribution is critical for learning a generalizable strategy that extrapolates to longer chains; without such examples, learning fails. Experimental results on synthetic data and real-world language models for mathematical reasoning corroborate these theoretical findings.</div>
<div class="mono" style="margin-top:8px">本文研究了基于结果的强化学习训练的Transformer如何发展出系统性推理链，这一机制此前尚不明确。作者在一个需要中间步骤的合成图遍历任务上分析了策略梯度动态，证明仅基于最终答案正确性的训练能使模型收敛到一个可解释的迭代算法。理论分析指出，训练数据分布中必须包含足够多的&#x27;简单示例&#x27;（即所需推理步骤较少的实例），模型才能学会可泛化的策略并外推到更长推理链；若缺乏此类示例，学习将无法进行。在合成数据以及真实世界语言模型数学推理任务上的实验结果验证了这些理论发现。</div>
</details>
</div>
<div class="card">
<div class="title">Black-Box Combinatorial Optimization with Order-Invariant Reinforcement Learning</div>
<div class="meta-line">Authors: Olivier Goudet, Quentin Suire, Adrien Goëffon, Frédéric Saubion, Sylvain Lamprier</div>
<div class="meta-line">First: 2025-10-02T09:12:17+00:00 · Latest: 2026-01-29T13:30:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.01824v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.01824v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce an order-invariant reinforcement learning framework for black-box combinatorial optimization. Classical estimation-of-distribution algorithms (EDAs) often rely on learning explicit variable dependency graphs, which can be costly and fail to capture complex interactions efficiently. In contrast, we parameterize a multivariate autoregressive generative model trained without a fixed variable ordering. By sampling random generation orders during training, a form of information-preserving dropout, the model is encouraged to be invariant to variable order, promoting search-space diversity, and shaping the model to focus on the most relevant variable dependencies, improving sample efficiency. We adapt Group Relative Policy Optimization (GRPO) to this setting, providing stable policy-gradient updates from scale-invariant advantages. Across a wide range of benchmark algorithms and problem instances of varying sizes, our method frequently achieves the best performance and consistently avoids catastrophic failures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于顺序不变强化学习的黑盒组合优化</div>
<div class="mono" style="margin-top:8px">本文提出一种用于黑盒组合优化的顺序不变强化学习框架。传统的分布估计算法通常依赖学习显式的变量依赖图，这种方法计算成本高且难以有效捕捉复杂交互。相比之下，我们采用无固定变量顺序训练的多变量自回归生成模型参数化方法。通过在训练过程中随机采样生成顺序（一种信息保留的随机丢弃技术），模型被鼓励对变量顺序保持不变性，从而提升搜索空间多样性，并使模型聚焦于最相关的变量依赖关系，提高采样效率。我们将组相对策略优化算法适配至该框架，通过尺度不变优势函数提供稳定的策略梯度更新。在涵盖多种基准算法和不同规模问题实例的广泛实验中，本方法频繁取得最优性能，并始终保持对灾难性失效的规避。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of classical estimation-of-distribution algorithms in black-box combinatorial optimization, which often rely on costly explicit dependency graphs that may not capture complex interactions efficiently. The proposed method introduces an order-invariant reinforcement learning framework that parameterizes a multivariate autoregressive generative model trained without a fixed variable ordering, using random generation orders during training as a form of information-preserving dropout to encourage invariance, promote search-space diversity, and improve sample efficiency by focusing on relevant dependencies. Experimental results across various benchmark algorithms and problem sizes show that this approach frequently achieves top performance and consistently avoids catastrophic failures.</div>
<div class="mono" style="margin-top:8px">本文针对黑盒组合优化中经典分布估计算法依赖显式变量依赖图成本高且难以高效捕捉复杂交互的局限性，提出了一种顺序不变的强化学习框架。该方法参数化了一个无需固定变量顺序的多变量自回归生成模型，通过在训练中随机采样生成顺序作为信息保留的dropout，鼓励模型对变量顺序保持不变性，促进搜索空间多样性，并通过聚焦相关依赖提高样本效率。在多种基准算法和不同规模问题实例上的实验结果表明，该方法经常取得最佳性能并始终避免灾难性失败。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Offline Model-Based RL via Jointly-Optimized World-Action Model Pretraining</div>
<div class="meta-line">Authors: Jie Cheng, Ruixi Qiao, Yingwei Ma, Binhua Li, Gang Xiong, Qinghai Miao, Yongbin Li, Yisheng Lv</div>
<div class="meta-line">Venue: ICLR 2025</div>
<div class="meta-line">First: 2024-10-01T10:25:03+00:00 · Latest: 2026-01-29T13:25:42+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.00564v4">Abs</a> · <a href="https://arxiv.org/pdf/2410.00564v4">PDF</a> · <a href="https://github.com/CJReinforce/JOWA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A significant aspiration of offline reinforcement learning (RL) is to develop a generalist agent with high capabilities from large and heterogeneous datasets. However, prior approaches that scale offline RL either rely heavily on expert trajectories or struggle to generalize to diverse unseen tasks. Inspired by the excellent generalization of world model in conditional video generation, we explore the potential of image observation-based world model for scaling offline RL and enhancing generalization on novel tasks. In this paper, we introduce JOWA: Jointly-Optimized World-Action model, an offline model-based RL agent pretrained on multiple Atari games with 6 billion tokens data to learn general-purpose representation and decision-making ability. Our method jointly optimizes a world-action model through a shared transformer backbone, which stabilize temporal difference learning with large models during pretraining. Moreover, we propose a provably efficient and parallelizable planning algorithm to compensate for the Q-value estimation error and thus search out better policies. Experimental results indicate that our largest agent, with 150 million parameters, achieves 78.9% human-level performance on pretrained games using only 10% subsampled offline data, outperforming existing state-of-the-art large-scale offline RL baselines by 31.6% on averange. Furthermore, JOWA scales favorably with model capacity and can sample-efficiently transfer to novel games using only 5k offline fine-tuning data (approximately 4 trajectories) per game, demonstrating superior generalization. We will release codes and model weights at https://github.com/CJReinforce/JOWA</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于联合优化的世界-行动模型预训练实现离线模型强化学习的规模化扩展</div>
<div class="mono" style="margin-top:8px">离线强化学习（RL）的一个重要目标是利用大规模异构数据集训练出高性能的通用智能体。然而，现有规模化离线RL方法要么严重依赖专家轨迹，要么难以泛化到多样未见任务。受条件视频生成中世界模型优异泛化能力的启发，我们探索了基于图像观测的世界模型在规模化离线RL及提升新任务泛化能力方面的潜力。本文提出JOWA：联合优化的世界-行动模型，这是一个基于模型的离线RL智能体，在包含60亿标记数据的多款Atari游戏上进行预训练，以学习通用表征和决策能力。该方法通过共享Transformer主干网络联合优化世界-行动模型，在预训练阶段稳定了大模型的时序差分学习。此外，我们提出了一种可证明高效且可并行的规划算法，以补偿Q值估计误差从而搜索更优策略。实验表明，我们最大的1.5亿参数智能体在预训练游戏中仅使用10%子采样离线数据即可达到78.9%的人类水平性能，平均超越现有大规模离线RL基线31.6%。JOWA的扩展性随模型容量提升而增强，且仅需每款游戏5千条离线微调数据（约4条轨迹）即可实现样本高效迁移至新游戏，展现出卓越的泛化能力。代码与模型权重发布于https://github.com/CJReinforce/JOWA</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of scaling offline reinforcement learning to create generalist agents from diverse datasets without heavy reliance on expert trajectories, this paper introduces JOWA, a jointly-optimized world-action model. The method employs a shared transformer backbone to jointly optimize world and action models, stabilizing temporal difference learning during pretraining on large datasets, and incorporates a provably efficient planning algorithm to improve policy search. Experimental results show that the 150-million-parameter agent achieves 78.9% human-level performance on pretrained Atari games using only 10% of the data, outperforming prior baselines by 31.6% on average, and demonstrates strong generalization by transferring to novel games with minimal fine-tuning data.</div>
<div class="mono" style="margin-top:8px">本文旨在解决离线强化学习在从多样数据集中扩展通用智能体时过度依赖专家轨迹或泛化能力不足的问题，提出了联合优化的世界-行动模型JOWA。该方法通过共享的Transformer主干网络联合优化世界模型和行动模型，在预训练中稳定时序差分学习，并采用一种可证明高效的规划算法来提升策略搜索。实验结果表明，拥有1.5亿参数的最大智能体在预训练的Atari游戏上仅使用10%的数据就达到了78.9%的人类水平性能，平均优于现有基线31.6%，并且能够以极少的微调数据高效迁移到新游戏，展现出卓越的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Two Heads are Better than One: Distilling Large Language Model Features Into Small Models with Feature Decomposition and Mixture</div>
<div class="meta-line">Authors: Tianhao Fu, Xinxin Xu, Weichen Xu, Jue Chen, Ruilong Ren, Bowen Deng, Xinyu Zhao, Jian Cao, Xixin Cao</div>
<div class="meta-line">First: 2025-11-10T13:57:05+00:00 · Latest: 2026-01-29T13:22:04+00:00</div>
<div class="meta-line">Comments: accepted by AAAI2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.07110v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.07110v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Market making (MM) through Reinforcement Learning (RL) has attracted significant attention in financial trading. With the development of Large Language Models (LLMs), more and more attempts are being made to apply LLMs to financial areas. A simple, direct application of LLM as an agent shows significant performance. Such methods are hindered by their slow inference speed, while most of the current research has not studied LLM distillation for this specific task. To address this, we first propose the normalized fluorescent probe to study the mechanism of the LLM&#x27;s feature. Based on the observation found by our investigation, we propose Cooperative Market Making (CMM), a novel framework that decouples LLM features across three orthogonal dimensions: layer, task, and data. Various student models collaboratively learn simple LLM features along with different dimensions, with each model responsible for a distinct feature to achieve knowledge distillation. Furthermore, CMM introduces an Hájek-MoE to integrate the output of the student models by investigating the contribution of different models in a kernel function-generated common feature space. Extensive experimental results on four real-world market datasets demonstrate the superiority of CMM over the current distillation method and RL-based market-making strategies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双剑合璧：通过特征分解与混合将大语言模型特征蒸馏至小模型</div>
<div class="mono" style="margin-top:8px">基于强化学习的做市策略在金融交易领域备受关注。随着大语言模型的发展，越来越多研究尝试将其应用于金融领域。直接将大语言模型作为智能体的简单应用已展现出显著性能，但这类方法受限于推理速度缓慢，且当前研究鲜少针对该特定任务探索大语言模型蒸馏技术。为此，我们首先提出标准化荧光探针以研究大语言模型特征机制。基于研究发现，我们提出协同做市框架——一种通过层维度、任务维度和数据维度三个正交方向解耦大语言模型特征的新型架构。多个学生模型沿不同维度协作学习简化的大语言模型特征，各模型专攻特定特征以实现知识蒸馏。此外，该框架引入Hájek-MoE模块，通过在核函数生成的公共特征空间中评估不同模型的贡献度，整合各学生模型的输出。在四个真实市场数据集上的大量实验结果表明，该框架在蒸馏效果和基于强化学习的做市策略方面均优于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation is to improve market making in financial trading by leveraging large language models (LLMs) while overcoming their slow inference speed through distillation, as current research lacks focused studies on LLM distillation for this task. The method introduces Cooperative Market Making (CMM), a framework that decouples LLM features across layer, task, and data dimensions using a normalized fluorescent probe for analysis, and employs multiple student models to learn distinct features collaboratively, integrated via an Hájek-MoE mechanism. Experimental results on four real-world market datasets show that CMM outperforms existing distillation methods and reinforcement learning-based market-making strategies in performance.</div>
<div class="mono" style="margin-top:8px">该研究的动机是通过利用大语言模型（LLM）改进金融市场做市，同时通过知识蒸馏克服其推理速度慢的问题，因为当前研究缺乏针对此任务的LLM蒸馏探索。方法上提出了协同做市（CMM）框架，使用归一化荧光探针分析LLM特征机制，并在层、任务和数据三个正交维度上解耦特征，让多个学生模型协作学习不同特征，并通过Hájek-MoE集成输出。在四个真实市场数据集上的实验结果表明，CMM在性能上优于现有的蒸馏方法和基于强化学习的做市策略。</div>
</details>
</div>
<div class="card">
<div class="title">Expected Return Causes Outcome-Level Mode Collapse in Reinforcement Learning and How to Fix It with Inverse Probability Scaling</div>
<div class="meta-line">Authors: Abhijeet Sinha, Sundari Elango, Dianbo Liu</div>
<div class="meta-line">First: 2026-01-29T13:03:33+00:00 · Latest: 2026-01-29T13:03:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21669v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21669v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many reinforcement learning (RL) problems admit multiple terminal solutions of comparable quality, where the goal is not to identify a single optimum but to represent a diverse set of high-quality outcomes. Nevertheless, policies trained by standard expected return maximization routinely collapse onto a small subset of outcomes, a phenomenon commonly attributed to insufficient exploration or weak regularization. We show that this explanation is incomplete: outcome level mode collapse is a structural consequence of the expected-return objective itself. Under idealized learning dynamics, the log-probability ratio between any two outcomes evolves linearly in their reward difference, implying exponential ratio divergence and inevitable collapse independent of the exploration strategy, entropy regularization, or optimization algorithm. We identify the source of this pathology as the probability multiplier inside the expectation and propose a minimal correction: inverse probability scaling, which removes outcome-frequency amplification from the learning signal, fundamentally changes the learning dynamics, and provably yields reward-proportional terminal distributions, preventing collapse in multimodal settings. We instantiate this principle in Group Relative Policy Optimization (GRPO) as a drop-in modification, IPS-GRPO, requiring no auxiliary models or architectural changes. Across different reasoning and molecular generation tasks, IPS-GRPO consistently reduces outcome-level mode collapse while matching or exceeding baseline performance, suggesting that correcting the objective rather than adding exploration heuristics is key to reliable multimodal policy optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>期望回报导致强化学习中的结果层面模式崩溃及其逆概率缩放修正方法</div>
<div class="mono" style="margin-top:8px">许多强化学习问题存在多个质量相当的终端解决方案，其目标并非寻找单一最优解，而是表征多样化的高质量结果集合。然而，通过标准期望回报最大化训练的策略常会坍缩到结果的有限子集，这一现象通常被归因于探索不足或正则化薄弱。本文指出这种解释并不完整：结果层面的模式崩溃是期望回报目标函数本身的结构性后果。在理想化学习动态下，任意两个结果的概率对数比随其奖励差值线性演化，这意味着指数级的比率发散和不可避免的坍缩——这与探索策略、熵正则化或优化算法无关。我们将此病理根源定位为期望内部的概率乘子，并提出最小化修正方案：逆概率缩放。该方法从学习信号中移除结果频率的放大效应，从根本上改变学习动态，可证明地产生与奖励成正比的终端分布，从而防止多模态场景中的坍缩。我们将此原理实例化为即插即用改进方案——IPS-GRPO（基于逆概率缩放的组相对策略优化），无需辅助模型或架构变更。在多项推理与分子生成任务中，IPS-GRPO在保持或超越基线性能的同时，持续减少结果层面的模式崩溃，表明修正目标函数而非添加探索启发式方法，才是实现可靠多模态策略优化的关键。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the phenomenon of outcome-level mode collapse in reinforcement learning, where policies trained to maximize expected return converge to only a few high-reward outcomes despite the existence of multiple equally good solutions. The authors argue that this collapse is an inherent flaw of the expected-return objective itself, not merely due to poor exploration, as idealized learning dynamics cause the probability ratio between outcomes to diverge exponentially based on their reward difference. To address this, they propose inverse probability scaling, a correction that removes outcome-frequency bias from the learning signal, leading to reward-proportional terminal distributions and preventing collapse. The method is implemented as IPS-GRPO, a drop-in modification that, in experiments on reasoning and molecular generation tasks, consistently reduces mode collapse while maintaining or improving performance compared to baselines.</div>
<div class="mono" style="margin-top:8px">本文研究了强化学习中的结果层面模式崩溃现象，即尽管存在多个同等优质的解，但为最大化期望回报而训练的策略却仅收敛到少数高奖励结果。作者认为这种崩溃是期望回报目标本身的内在缺陷，而非仅仅源于探索不足，因为理想化的学习动态会导致结果间的概率比基于奖励差异呈指数级发散。为解决此问题，他们提出了逆概率缩放，通过从学习信号中消除结果频率偏差，使终端分布与奖励成正比，从而防止崩溃。该方法以IPS-GRPO的形式实现，作为一种即插即用的修改，在推理和分子生成任务的实验中，持续减少了模式崩溃，同时保持或超越了基线性能。</div>
</details>
</div>
<div class="card">
<div class="title">CleanSurvival: Automated data preprocessing for time-to-event models using reinforcement learning</div>
<div class="meta-line">Authors: Yousef Koka, David Selby, Gerrit Großmann, Sebastian Vollmer, Kathan Pandya</div>
<div class="meta-line">First: 2025-02-06T10:33:37+00:00 · Latest: 2026-01-29T12:51:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.03946v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.03946v3">PDF</a> · <a href="https://github.com/datasciapps/CleanSurvival">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Data preprocessing is a critical yet frequently neglected aspect of machine learning, often paid little attention despite its potentially significant impact on model performance. While automated machine learning pipelines are starting to recognize and integrate data preprocessing into their solutions for classification and regression tasks, this integration is lacking for more specialized tasks like survival or time-to-event models. As a result, survival analysis not only faces the general challenges of data preprocessing but also suffers from the lack of tailored, automated solutions in this area. To address this gap, this paper presents &#x27;CleanSurvival&#x27;, a reinforcement-learning-based solution for optimizing preprocessing pipelines, extended specifically for survival analysis. The framework can handle continuous and categorical variables, using Q-learning to select which combination of data imputation, outlier detection and feature extraction techniques achieves optimal performance for a Cox, random forest, neural network or user-supplied time-to-event model. The package is available on GitHub: https://github.com/datasciapps/CleanSurvival Experimental benchmarks on real-world datasets show that the Q-learning-based data preprocessing results in superior predictive performance to standard approaches, finding such a model up to 10 times faster than undirected random grid search. Furthermore, a simulation study demonstrates the effectiveness in different types and levels of missingness and noise in the data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CleanSurvival：基于强化学习的生存时间模型自动化数据预处理</div>
<div class="mono" style="margin-top:8px">数据预处理是机器学习中至关重要却常被忽视的环节，尽管其对模型性能可能产生显著影响，却往往未获足够重视。虽然自动化机器学习流程已开始将数据预处理纳入分类与回归任务的解决方案，但在生存分析或时间-事件模型等更专业的任务中，这一整合仍显不足。因此，生存分析不仅面临数据预处理的普遍挑战，还缺乏针对该领域的定制化自动化解决方案。为填补这一空白，本文提出&#x27;CleanSurvival&#x27;——一种基于强化学习的预处理流程优化方案，专门扩展应用于生存分析。该框架能处理连续与分类变量，通过Q学习选择数据填补、异常值检测和特征提取技术的组合，从而为Cox模型、随机森林、神经网络或用户自定义的时间-事件模型实现最优性能。该工具包已在GitHub发布：https://github.com/datasciapps/CleanSurvival 基于真实数据集的实验基准表明，采用Q学习的数据预处理相比标准方法具有更优的预测性能，其模型搜索速度比无导向随机网格搜索快达10倍。此外，模拟研究验证了该方法对不同类型与程度的数据缺失及噪声的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces CleanSurvival, a reinforcement learning framework designed to automate data preprocessing for survival analysis, motivated by the lack of specialized automated solutions for time-to-event models compared to classification and regression tasks. The method employs Q-learning to optimize preprocessing pipelines by selecting combinations of imputation, outlier detection, and feature extraction techniques tailored for models like Cox regression, random forests, or neural networks. Experimental results on real-world datasets demonstrate that this approach yields superior predictive performance over standard methods and identifies optimal models up to 10 times faster than random grid search, with simulations confirming effectiveness across various data missingness and noise levels.</div>
<div class="mono" style="margin-top:8px">本文提出了CleanSurvival，一个基于强化学习的框架，旨在自动化生存分析的数据预处理，其动机在于与分类和回归任务相比，时间-事件模型缺乏专门的自动化解决方案。该方法利用Q学习来优化预处理流程，通过选择适用于Cox回归、随机森林或神经网络等模型的插补、异常值检测和特征提取技术组合。在真实数据集上的实验结果表明，该方法相比标准方法具有更优的预测性能，并能比随机网格搜索快10倍找到最优模型，模拟研究还验证了其在处理不同类型和程度的数据缺失与噪声时的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">DeFacto: Counterfactual Thinking with Images for Enforcing Evidence-Grounded and Faithful Reasoning</div>
<div class="meta-line">Authors: Tianrun Xu, Haoda Jing, Ye Li, Yuquan Wei, Jun Feng, Guanyu Chen, Haichuan Gao, Tianren Zhang, Feng Chen</div>
<div class="meta-line">First: 2025-09-25T08:58:10+00:00 · Latest: 2026-01-29T12:47:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.20912v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.20912v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in multimodal language models (MLLMs) have achieved remarkable progress in vision-language reasoning, especially with the emergence of &quot;thinking with images,&quot; which integrates explicit visual steps into the reasoning process. While this paradigm strengthens image-based reasoning, a significant challenge remains: models may arrive at correct answers by relying on irrelevant or spurious regions, driven by prior knowledge or dataset biases. Even when the answer is correct, flawed reasoning indicates that the model has not truly understood the image, highlighting the critical importance of reasoning fidelity in multimodal tasks. To address this issue, we propose DeFacto, a counterfactual reasoning framework that jointly enforces accurate answering and faithful reasoning. A key component of our approach is the design of three complementary training paradigms: (i) positive, (ii) counterfactual, and (iii) random-masking. To enable these paradigms, we develop a pipeline that automatically localizes question-relevant evidence and constructs positive, counterfactual, and random variants, resulting in a dataset of about 100k images. Building on this framework, we train multimodal language models with GRPO-based reinforcement learning, where we design three complementary rewards to guide the model toward accurate answering and evidence-grounded reasoning. Experiments on diverse benchmarks demonstrate that DeFacto substantially improves both answer accuracy and reasoning faithfulness, establishing a stronger foundation for interpretable multimodal reasoning. The code is available on GitHub and the dataset is released on HuggingFace.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeFacto：基于图像的反事实推理以强化证据支撑与忠实推理</div>
<div class="mono" style="margin-top:8px">多模态语言模型（MLLMs）在视觉语言推理领域取得显著进展，尤其是“图像思维”范式通过将显式视觉步骤融入推理过程，增强了基于图像的推理能力。然而，该范式仍面临关键挑战：模型可能受先验知识或数据集偏差影响，依赖无关或虚假图像区域得出正确答案。即使答案正确，有缺陷的推理过程表明模型未真正理解图像，凸显了多模态任务中推理忠实性的重要性。为此，我们提出DeFacto——一个反事实推理框架，通过联合优化确保答案准确性与推理忠实性。该框架的核心是设计三种互补的训练范式：（i）正向训练，（ii）反事实训练，（iii）随机掩码训练。为实现这些范式，我们开发了自动化流程以定位问题相关证据，并构建正向、反事实及随机变体，最终生成约10万张图像的数据集。基于此框架，我们采用GRPO强化学习训练多模态语言模型，设计三种互补奖励机制引导模型实现准确回答与证据支撑的推理。多基准测试表明，DeFacto显著提升了答案准确率与推理忠实性，为可解释多模态推理奠定了更坚实基础。代码已开源至GitHub，数据集发布于HuggingFace平台。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind DeFacto is to address the issue of multimodal language models (MLLMs) reaching correct answers through flawed or unfaithful reasoning, such as relying on irrelevant image regions, which undermines true understanding. The method introduces a counterfactual reasoning framework that employs three training paradigms—positive, counterfactual, and random-masking—supported by an automated pipeline to generate a dataset of approximately 100k images with localized evidence variants, and trains models using GRPO-based reinforcement learning with complementary rewards. Main experimental results show that DeFacto significantly enhances both answer accuracy and reasoning faithfulness across diverse benchmarks, establishing a more interpretable foundation for multimodal reasoning.</div>
<div class="mono" style="margin-top:8px">DeFacto的动机是解决多模态语言模型（MLLMs）通过有缺陷或不忠实的推理（例如依赖图像中无关区域）得出正确答案的问题，这阻碍了真正的理解。该方法提出了一个反事实推理框架，采用三种训练范式——正向、反事实和随机掩码，并通过自动化流程构建了约10万张图像的数据集，其中包含定位证据的变体，同时使用基于GRPO的强化学习和互补奖励来训练模型。主要实验结果表明，DeFacto在多种基准测试中显著提升了答案准确性和推理忠实度，为可解释的多模态推理奠定了更坚实的基础。</div>
</details>
</div>
<div class="card">
<div class="title">CTRLS: Chain-of-Thought Reasoning via Latent State-Transition</div>
<div class="meta-line">Authors: Junda Wu, Yuxin Xiong, Xintong Li, Sheldon Yu, Zhengmian Hu, Tong Yu, Rui Wang, Xiang Chen, Jingbo Shang, Julian McAuley</div>
<div class="meta-line">First: 2025-07-10T21:32:18+00:00 · Latest: 2026-01-29T12:31:14+00:00</div>
<div class="meta-line">Comments: 10 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.08182v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.08182v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Chain-of-thought (CoT) reasoning enables large language models (LLMs) to break down complex problems into interpretable intermediate steps, significantly enhancing model transparency and performance in reasoning tasks. However, conventional CoT methods rely on heuristic sampling without structured modeling of reasoning transitions, constraining their ability to systematically explore and discover diverse and effective reasoning trajectories. In this work, we introduce CTRLS, a framework that formulates CoT reasoning as a Markov decision process (MDP) with latent state transitions, enabling principled and state-aware exploration via distributional reinforcement learning. By modelling reasoning actions as explicit probability distributions in latent space, our approach explicitly models epistemic uncertainty, facilitating robust exploration of the reasoning space. As part of our framework, we introduce an on-policy reinforcement learning strategy incorporating epsilon-greedy exploration and entropy-based regularization to iteratively refine latent state transitions without requiring additional fine-tuning of the underlying LLM. Theoretical analyses provide evidence lower bounds (ELBO), theoretically grounding our transition-aware modeling of latent reasoning dynamics. Further experiments demonstrate improvements in reasoning accuracy, diversity, and exploration efficiency across benchmark reasoning tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CTRLS：基于潜在状态转移的思维链推理</div>
<div class="mono" style="margin-top:8px">思维链（CoT）推理使大语言模型（LLM）能够将复杂问题分解为可解释的中间步骤，显著提升了模型在推理任务中的透明度和性能。然而，传统CoT方法依赖启发式采样，缺乏对推理转移的结构化建模，限制了其系统探索和发现多样化有效推理路径的能力。本研究提出CTRLS框架，将CoT推理建模为具有潜在状态转移的马尔可夫决策过程（MDP），通过分布式强化学习实现基于原则的状态感知探索。该方法将推理动作建模为潜在空间中的显式概率分布，明确表征认知不确定性，促进对推理空间的稳健探索。框架采用包含ε-贪婪探索和基于熵的正则化策略的在线强化学习方法，无需对底层LLM进行额外微调即可迭代优化潜在状态转移。理论分析提供了证据下界（ELBO），为潜在推理动态的转移感知建模奠定理论基础。实验结果表明，该方法在基准推理任务中提升了推理准确性、多样性和探索效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for CTRLS stems from the limitations of conventional chain-of-thought (CoT) methods, which rely on heuristic sampling and lack structured modeling of reasoning transitions, thereby hindering systematic exploration of diverse reasoning paths. The method introduces a framework that formulates CoT reasoning as a Markov decision process with latent state transitions, employing distributional reinforcement learning to model reasoning actions as probability distributions in latent space, which captures epistemic uncertainty and enables state-aware exploration via an on-policy strategy with epsilon-greedy exploration and entropy regularization. Main experimental results show that CTRLS improves reasoning accuracy, diversity, and exploration efficiency across benchmark tasks, with theoretical analyses providing evidence lower bounds to ground the latent transition modeling.</div>
<div class="mono" style="margin-top:8px">CTRLS的动机源于传统思维链方法依赖于启发式采样且缺乏对推理转换的结构化建模，这限制了系统探索多样化推理路径的能力。该方法提出了一个框架，将思维链推理建模为具有潜在状态转换的马尔可夫决策过程，通过分布强化学习将推理动作表示为潜在空间中的概率分布，以捕捉认知不确定性，并采用包含ε-贪婪探索和熵正则化的策略进行状态感知探索。主要实验结果表明，CTRLS在基准推理任务中提高了推理准确性、多样性和探索效率，理论分析提供了证据下界以支持潜在转换建模。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning for Adaptive Composition of Quantum Circuit Optimisation Passes</div>
<div class="meta-line">Authors: Daniel Mills, Ifan Williams, Jacob Swain, Gabriel Matos, Enrico Rinaldi, Alexander Koziell-Pipe</div>
<div class="meta-line">First: 2026-01-29T12:29:10+00:00 · Latest: 2026-01-29T12:29:10+00:00</div>
<div class="meta-line">Comments: 14 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21629v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21629v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many quantum software development kits provide a suite of circuit optimisation passes. These passes have been highly optimised and tested in isolation. However, the order in which they are applied is left to the user, or else defined in general-purpose default pass sequences. While general-purpose sequences miss opportunities for optimisation which are particular to individual circuits, designing pass sequences bespoke to particular circuits requires exceptional knowledge about quantum circuit design and optimisation. Here we propose and demonstrate training a reinforcement learning agent to compose optimisation-pass sequences. In particular the agent&#x27;s action space consists of passes for two-qubit gate count reduction used in default PyTKET pass sequences. For the circuits in our diverse test set, the (mean, median) fraction of two-qubit gates removed by the agent is $(57.7\%, \ 56.7 \%)$, compared to $(41.8 \%, \ 50.0 \%)$ for the next best default pass sequence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习的量子电路优化通道自适应组合方法</div>
<div class="mono" style="margin-top:8px">多数量子软件开发工具包提供成套电路优化通道。这些通道经过独立高度优化与测试，但其应用顺序需用户指定或依赖通用默认通道序列。通用序列会错失针对特定电路的优化机会，而定制化通道序列设计需要量子电路设计与优化的专业知识。本研究提出并演示了训练强化学习智能体来组合优化通道序列的方法，其动作空间由PyTKET默认通道序列中用于减少双量子比特门数量的通道构成。在多样化测试电路中，智能体移除双量子比特门的（均值，中位数）比例为$(57.7\%, \ 56.7\%)$，优于次优默认通道序列的$(41.8 \%, \ 50.0\%)$。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge that general-purpose default sequences for quantum circuit optimization passes often miss circuit-specific optimization opportunities, while custom sequences require deep expertise, this paper proposes using reinforcement learning to adaptively compose optimization-pass sequences. The method trains a reinforcement learning agent whose action space consists of specific two-qubit gate reduction passes from PyTKET, aiming to automatically generate effective pass sequences tailored to individual circuits. Experimental results on a diverse test set show the agent achieved mean and median two-qubit gate reductions of 57.7% and 56.7%, respectively, outperforming the next best default sequence which achieved 41.8% and 50.0%.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，量子电路优化通道的通用默认序列常错过电路特定的优化机会，而定制序列又需要深厚专业知识，因此提出使用强化学习来自适应组合优化通道序列。该方法训练一个强化学习智能体，其动作空间由PyTKET中用于减少双量子比特门的特定优化通道构成，旨在自动生成针对单个电路的有效通道序列。在多样化测试集上的实验结果表明，该智能体实现的双量子比特门减少比例均值为57.7%、中位数为56.7%，优于次佳默认序列的41.8%和50.0%。</div>
</details>
</div>
<div class="card">
<div class="title">RecNet: Self-Evolving Preference Propagation for Agentic Recommender Systems</div>
<div class="meta-line">Authors: Bingqian Li, Xiaolei Wang, Junyi Li, Weitao Li, Long Zhang, Sheng Chen, Wayne Xin Zhao, Ji-Rong Wen</div>
<div class="meta-line">First: 2026-01-29T12:14:31+00:00 · Latest: 2026-01-29T12:14:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21609v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21609v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic recommender systems leverage Large Language Models (LLMs) to model complex user behaviors and support personalized decision-making. However, existing methods primarily model preference changes based on explicit user-item interactions, which are sparse, noisy, and unable to reflect the real-time, mutual influences among users and items. To address these limitations, we propose RecNet, a self-evolving preference propagation framework that proactively propagates real-time preference updates across related users and items. RecNet consists of two complementary phases. In the forward phase, the centralized preference routing mechanism leverages router agents to integrate preference updates and dynamically propagate them to the most relevant agents. To ensure accurate and personalized integration of propagated preferences, we further introduce a personalized preference reception mechanism, which combines a message buffer for temporary caching and an optimizable, rule-based filter memory to guide selective preference assimilation based on past experience and interests. In the backward phase, the feedback-driven propagation optimization mechanism simulates a multi-agent reinforcement learning framework, using LLMs for credit assignment, gradient analysis, and module-level optimization, enabling continuous self-evolution of propagation strategies. Extensive experiments on various scenarios demonstrate the effectiveness of RecNet in modeling preference propagation for recommender systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RecNet：面向智能推荐系统的自演化偏好传播框架</div>
<div class="mono" style="margin-top:8px">智能推荐系统利用大语言模型（LLMs）建模复杂用户行为并支持个性化决策。然而，现有方法主要基于稀疏、嘈杂且无法反映用户与项目间实时相互影响的显式用户-项目交互来建模偏好变化。为克服这些局限，本文提出RecNet——一种自演化的偏好传播框架，能主动将实时偏好更新传播至相关用户与项目。RecNet包含两个互补阶段：在前向传播阶段，集中式偏好路由机制通过路由智能体整合偏好更新，并动态传播至最相关的智能体；为确保传播偏好的精准个性化整合，进一步引入个性化偏好接收机制，结合用于临时缓存的消息缓冲区与可优化的基于规则的过滤记忆模块，以指导基于历史经验与兴趣的选择性偏好融合。在反向传播阶段，反馈驱动的传播优化机制模拟多智能体强化学习框架，利用LLMs进行信用分配、梯度分析与模块级优化，实现传播策略的持续自演化。多场景实验验证了RecNet在推荐系统偏好传播建模中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for RecNet stems from the limitations of existing agentic recommender systems, which rely on sparse and noisy explicit user-item interactions and fail to capture real-time mutual influences. The method introduces a self-evolving preference propagation framework with two phases: a forward phase using centralized preference routing and personalized reception mechanisms to propagate and integrate updates, and a backward phase employing a feedback-driven optimization mechanism that simulates multi-agent reinforcement learning for continuous strategy improvement. Experimental results across various scenarios demonstrate RecNet&#x27;s effectiveness in modeling dynamic preference propagation for recommender systems.</div>
<div class="mono" style="margin-top:8px">RecNet的提出动机源于现有智能推荐系统依赖稀疏且嘈杂的显式用户-物品交互，无法捕捉实时相互影响的局限性。该方法采用一个自演化的偏好传播框架，包含两个阶段：前向阶段通过集中式偏好路由和个性化接收机制来传播与整合更新，后向阶段则利用反馈驱动的优化机制，模拟多智能体强化学习以持续改进传播策略。在不同场景下的广泛实验验证了RecNet在建模推荐系统动态偏好传播方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization</div>
<div class="meta-line">Authors: Zhi Zheng, Yu Gu, Wei Liu, Yee Whye Teh, Wee Sun Lee</div>
<div class="meta-line">First: 2025-11-09T14:55:50+00:00 · Latest: 2026-01-29T12:07:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.06411v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.06411v2">PDF</a> · <a href="https://github.com/zz1358m/SofT-GRPO-master">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The soft-thinking paradigm for Large Language Model (LLM) reasoning can outperform the conventional discrete-token Chain-of-Thought (CoT) reasoning in some scenarios, underscoring its research and application value. However, while the discrete-token CoT reasoning pattern can be reinforced through policy optimization algorithms such as group relative policy optimization (GRPO), extending the soft-thinking pattern with Reinforcement Learning (RL) remains challenging. This difficulty stems from the complexities of injecting stochasticity into soft-thinking tokens and updating soft-thinking policies accordingly. As a result, previous attempts to combine soft-thinking with GRPO typically underperform their discrete-token GRPO counterparts. To fully unlock the potential of soft-thinking, this paper presents a novel policy optimization algorithm, SofT-GRPO, to reinforce LLMs under the soft-thinking reasoning pattern. SofT-GRPO injects the Gumbel noise into logits, employs the Gumbel-Softmax technique to avoid soft-thinking tokens outside the pre-trained embedding space, and leverages the reparameterization trick in policy gradient. We conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and results demonstrate that SofT-GRPO enables soft-thinking LLMs to slightly outperform discrete-token GRPO on Pass@1 (+0.13% on average accuracy), while exhibiting a substantial uplift on Pass@32 (+2.19% on average accuracy). Codes and weights are available on https://github.com/zz1358m/SofT-GRPO-master</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SofT-GRPO：通过Gumbel重参数化软思维策略优化超越离散令牌大语言模型强化学习</div>
<div class="mono" style="margin-top:8px">大语言模型的软思维推理范式在某些场景下能超越传统的离散令牌思维链推理，凸显其研究与应用价值。然而，离散令牌思维链推理可通过群体相对策略优化等算法强化，软思维范式与强化学习的结合仍具挑战，主要源于向软思维令牌注入随机性及相应策略更新的复杂性，导致先前软思维与GRPO的结合尝试通常逊于离散令牌版本。为充分释放软思维潜力，本文提出新型策略优化算法SofT-GRPO，在软思维推理模式下强化大语言模型。该方法向logits注入Gumbel噪声，采用Gumbel-Softmax技术避免软思维令牌超出预训练嵌入空间，并利用策略梯度的重参数化技巧。在1.5B至7B参数的基础大语言模型实验中，SofT-GRPO使软思维模型在Pass@1上略优于离散令牌GRPO（平均准确率+0.13%），在Pass@32上更实现显著提升（平均准确率+2.19%）。代码与权重已开源：https://github.com/zz1358m/SofT-GRPO-master</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces SofT-GRPO, a novel reinforcement learning algorithm designed to enhance the soft-thinking reasoning paradigm in large language models, which has shown potential but previously struggled to match discrete-token methods like GRPO due to difficulties in stochastic token injection and policy updates. The method addresses these challenges by injecting Gumbel noise into logits, using Gumbel-Softmax to keep tokens within the pre-trained embedding space, and applying the reparameterization trick for policy gradients. Experimental results on models from 1.5B to 7B parameters show that SofT-GRPO enables soft-thinking LLMs to slightly outperform discrete-token GRPO on Pass@1 by an average of +0.13% and achieve a substantial improvement of +2.19% on Pass@32, demonstrating its effectiveness in unlocking the potential of soft-thinking reasoning.</div>
<div class="mono" style="margin-top:8px">本文提出了SofT-GRPO，一种新颖的强化学习算法，旨在增强大语言模型中的软思维推理范式，该范式虽具潜力但先前因随机令牌注入和政策更新困难而难以匹敌离散令牌方法如GRPO。该方法通过向logits注入Gumbel噪声、使用Gumbel-Softmax保持令牌在预训练嵌入空间内，并应用重参数化技巧进行政策梯度优化，以应对这些挑战。在1.5B到7B参数模型上的实验结果表明，SofT-GRPO使软思维大语言模型在Pass@1上平均准确率略超离散令牌GRPO（+0.13%），并在Pass@32上实现显著提升（+2.19%），证明了其在释放软思维推理潜力方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Imitation: Reinforcement Learning for Active Latent Planning</div>
<div class="meta-line">Authors: Zhi Zheng, Wee Sun Lee</div>
<div class="meta-line">First: 2026-01-29T12:07:16+00:00 · Latest: 2026-01-29T12:07:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21598v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21598v1">PDF</a> · <a href="https://github.com/zz1358m/ATP-Latent-master">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning methods fine-tune Large Language Models (LLMs) to substitute discrete language tokens with continuous latent tokens. These methods consume fewer tokens compared to the conventional language CoT reasoning and have the potential to plan in a dense latent space. However, current latent tokens are generally supervised based on imitating language labels. Considering that there can be multiple equivalent but diverse CoT labels for a question, passively imitating an arbitrary one may lead to inferior latent token representations and latent reasoning policies, undermining the potential planning ability and resulting in clear gaps between training and testing. In this work, we emphasize the importance of active planning over the representation space of latent tokens in achieving the optimal latent reasoning policy. So, we propose the \underline{A}c\underline{t}ive Latent \underline{P}lanning method (ATP-Latent), which models the supervision process of latent tokens as a conditional variational auto-encoder (VAE) to obtain a smoother latent space. Moreover, to facilitate the most reasonable latent reasoning policy, ATP-Latent conducts reinforcement learning (RL) with an auxiliary coherence reward, which is calculated based on the consistency between VAE-decoded contents of latent tokens, enabling a guided RL process. In experiments on LLaMA-1B, ATP-Latent demonstrates +4.1\% accuracy and -3.3\% tokens on four benchmarks compared to advanced baselines. Codes are available on https://github.com/zz1358m/ATP-Latent-master.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越模仿：面向主动潜在规划的强化学习</div>
<div class="mono" style="margin-top:8px">为实现高效密集的思维链推理，潜在推理方法通过微调大语言模型，将离散语言标记替换为连续潜在标记。与传统语言思维链推理相比，这些方法消耗更少的标记，并具备在密集潜在空间中进行规划的潜力。然而，现有潜在标记通常基于模仿语言标签进行监督。考虑到同一问题可能存在多个等效但不同的思维链标签，被动模仿任意标签可能导致次优的潜在标记表示与推理策略，削弱潜在规划能力并造成训练与测试间的明显差距。本研究强调在潜在标记表示空间中进行主动规划对获得最优潜在推理策略的重要性，由此提出\underline{主动潜在规划方法}（ATP-Latent）。该方法将潜在标记的监督过程建模为条件变分自编码器以获得更平滑的潜在空间，同时引入基于潜在标记VAE解码内容一致性的辅助连贯性奖励进行强化学习，从而引导最优潜在推理策略的形成。在LLaMA-1B上的实验表明，ATP-Latent在四个基准测试中相较先进基线实现+4.1%准确率提升与-3.3%标记消耗降低。代码已开源：https://github.com/zz1358m/ATP-Latent-master。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of existing latent reasoning methods for large language models, which passively imitate diverse and potentially suboptimal chain-of-thought labels, leading to inferior latent representations and a gap between training and testing. To achieve more effective latent planning, the authors propose ATP-Latent, a method that models latent token supervision with a conditional variational auto-encoder for a smoother latent space and employs reinforcement learning guided by an auxiliary coherence reward to foster a more reasonable reasoning policy. Experimental results on LLaMA-1B show that ATP-Latent improves accuracy by 4.1% and reduces token usage by 3.3% across four benchmarks compared to advanced baselines.</div>
<div class="mono" style="margin-top:8px">本文针对现有大语言模型潜在推理方法的局限性提出改进，这些方法被动模仿多样且可能次优的思维链标签，导致潜在表示质量不佳及训练与测试间的差距。为实现更有效的潜在规划，作者提出了ATP-Latent方法，该方法使用条件变分自编码器对潜在令牌监督进行建模以获得更平滑的潜在空间，并采用强化学习结合辅助一致性奖励来引导更合理的推理策略。在LLaMA-1B上的实验结果表明，与先进基线相比，ATP-Latent在四个基准测试中准确率提升4.1%，令牌使用量减少3.3%。</div>
</details>
</div>
<div class="card">
<div class="title">Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening</div>
<div class="meta-line">Authors: Xiaotong Ji, Rasul Tutunov, Matthieu Zimmer, Haitham Bou Ammar</div>
<div class="meta-line">First: 2026-01-29T12:01:53+00:00 · Latest: 2026-01-29T12:01:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21590v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21590v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) post-training is a dominant approach for improving the reasoning performance of large language models (LLMs), yet growing evidence suggests that its gains arise primarily from distribution sharpening rather than the acquisition of new capabilities. Recent work has shown that sampling from the power distribution of LLMs using Markov chain Monte Carlo (MCMC) can recover performance comparable to RL post-training without relying on external rewards; however, the high computational cost of MCMC makes such approaches impractical for widespread adoption. In this work, we propose a theoretically grounded alternative that eliminates the need for iterative MCMC. We derive a novel formulation showing that the global power distribution can be approximated by a token-level scaled low-temperature one, where the scaling factor captures future trajectory quality. Leveraging this insight, we introduce a training-free and verifier-free algorithm that sharpens the base model&#x27;s generative distribution autoregressively. Empirically, we evaluate our method on math, QA, and code tasks across four LLMs, and show that our method matches or surpasses one-shot GRPO without relying on any external rewards, while reducing inference latency by over 10x compared to MCMC-based sampling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可扩展的幂采样：通过分布锐化实现LLM高效免训练推理</div>
<div class="mono" style="margin-top:8px">强化学习（RL）后训练是提升大语言模型（LLM）推理性能的主流方法，但越来越多的证据表明其增益主要源于分布锐化而非新能力的获取。近期研究表明，通过马尔可夫链蒙特卡洛（MCMC）对LLM的幂分布进行采样，无需依赖外部奖励即可获得与RL后训练相当的性能；然而MCMC的高计算成本限制了其广泛应用。本研究提出一种理论完备的替代方案，无需迭代式MCMC。我们推导出新颖的数学形式，证明全局幂分布可通过标记级缩放低温分布来近似，其中缩放因子捕捉未来轨迹质量。基于此发现，我们提出免训练、免验证器的自回归算法，可锐化基础模型的生成分布。实验部分在四种LLM的数学、问答和代码任务上验证本方法，结果表明：在不依赖外部奖励的情况下，本方法达到或超越单次GRPO性能，同时相比基于MCMC的采样将推理延迟降低10倍以上。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by evidence that reinforcement learning post-training improves reasoning primarily by sharpening distributions rather than teaching new skills, and to address the impractical computational cost of MCMC-based power sampling, this paper introduces a training-free method that approximates the global power distribution via a token-level scaled low-temperature distribution, which captures future trajectory quality. The method sharpens the base model&#x27;s generative distribution autoregressively without external rewards or verifiers. Experimental results on math, QA, and code tasks across four LLMs show it matches or surpasses one-shot GRPO performance while reducing inference latency by over 10x compared to MCMC approaches.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，强化学习后训练提升推理能力主要源于分布锐化而非新能力习得，且基于MCMC的幂采样计算成本过高；为此，作者提出一种无需训练的方法，通过理论推导用捕获未来轨迹质量的缩放低温分布来近似全局幂分布，从而自回归地锐化基础模型的生成分布，无需外部奖励或验证器。在数学、问答和代码任务上对四个大语言模型的实验表明，该方法性能匹配或超越一次性GRPO，同时推理延迟相比MCMC方法降低超过10倍。</div>
</details>
</div>
<div class="card">
<div class="title">CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning</div>
<div class="meta-line">Authors: Cédric Colas, Pierre Fournier, Olivier Sigaud, Mohamed Chetouani, Pierre-Yves Oudeyer</div>
<div class="meta-line">Venue: ICML 2019</div>
<div class="meta-line">First: 2018-10-15T11:40:28+00:00 · Latest: 2026-01-29T11:57:35+00:00</div>
<div class="meta-line">Comments: Accepted at ICML 2019 https://github.com/flowersteam/curious</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/1810.06284v5">Abs</a> · <a href="https://arxiv.org/pdf/1810.06284v5">PDF</a> · <a href="https://github.com/flowersteam/curious">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In open-ended environments, autonomous learning agents must set their own goals and build their own curriculum through an intrinsically motivated exploration. They may consider a large diversity of goals, aiming to discover what is controllable in their environments, and what is not. Because some goals might prove easy and some impossible, agents must actively select which goal to practice at any moment, to maximize their overall mastery on the set of learnable goals. This paper proposes CURIOUS, an algorithm that leverages 1) a modular Universal Value Function Approximator with hindsight learning to achieve a diversity of goals of different kinds within a unique policy and 2) an automated curriculum learning mechanism that biases the attention of the agent towards goals maximizing the absolute learning progress. Agents focus sequentially on goals of increasing complexity, and focus back on goals that are being forgotten. Experiments conducted in a new modular-goal robotic environment show the resulting developmental self-organization of a learning curriculum, and demonstrate properties of robustness to distracting goals, forgetting and changes in body properties.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CURIOUS：基于内在激励的模块化多目标强化学习</div>
<div class="mono" style="margin-top:8px">在开放环境中，自主学习智能体需通过内在激励探索自主设定目标并构建学习课程。它们可考虑多样化的目标，旨在发现环境中可控与不可控的因素。鉴于目标难度存在差异（部分简单、部分不可实现），智能体需动态选择当前训练目标，以最大化对可学习目标集的整体掌握能力。本文提出CURIOUS算法，其具备两大核心机制：1）采用模块化通用价值函数逼近器结合事后学习，使单一策略能实现多种类型目标；2）通过自动化课程学习机制，使智能体注意力偏向于能产生最大绝对学习进展的目标。智能体依次聚焦复杂度递增的目标，并循环巩固被遗忘的目标。在新型模块化目标机器人环境中的实验表明，该方法能形成渐进式自组织学习课程，并展现出对干扰目标、遗忘效应及身体属性变化的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of autonomous goal-setting and curriculum learning in open-ended environments, where agents must intrinsically motivate their exploration to master a diverse set of controllable goals. The proposed CURIOUS algorithm combines a modular Universal Value Function Approximator with hindsight learning to handle multiple goal types within a single policy, alongside an automated curriculum mechanism that prioritizes goals based on absolute learning progress, enabling agents to sequentially tackle goals of increasing complexity and revisit forgotten ones. Experimental results in a modular-goal robotic environment demonstrate the algorithm&#x27;s ability to self-organize a learning curriculum, showing robustness to distracting goals, forgetting, and changes in body properties.</div>
<div class="mono" style="margin-top:8px">本文针对开放环境中自主目标设定和课程学习的挑战，提出智能体需通过内在动机探索来掌握多样化的可控目标。CURIOUS算法结合了采用事后学习的模块化通用价值函数逼近器，以单一策略处理多类目标，并引入基于绝对学习进展的自动课程机制，使智能体能按复杂度递增顺序学习目标并复习遗忘内容。在模块化目标机器人环境中的实验表明，该算法能自组织学习课程，并对干扰目标、遗忘及身体属性变化表现出鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Signal-Adaptive Trust Regions for Gradient-Free Optimization of Recurrent Spiking Neural Networks</div>
<div class="meta-line">Authors: Jinhao Li, Yuhao Sun, Zhiyuan Ma, Hao He, Xinche Zhang, Xing Chen, Jin Li, Sen Song</div>
<div class="meta-line">First: 2026-01-29T11:34:49+00:00 · Latest: 2026-01-29T11:34:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21572v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21572v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recurrent spiking neural networks (RSNNs) are a promising substrate for energy-efficient control policies, but training them for high-dimensional, long-horizon reinforcement learning remains challenging. Population-based, gradient-free optimization circumvents backpropagation through non-differentiable spike dynamics by estimating gradients. However, with finite populations, high variance of these estimates can induce harmful and overly aggressive update steps. Inspired by trust-region methods in reinforcement learning that constrain policy updates in distribution space, we propose \textbf{Signal-Adaptive Trust Regions (SATR)}, a distributional update rule that constrains relative change by bounding KL divergence normalized by an estimated signal energy. SATR automatically expands the trust region under strong signals and contracts it when updates are noise-dominated. We instantiate SATR for Bernoulli connectivity distributions, which have shown strong empirical performance for RSNN optimization. Across a suite of high-dimensional continuous-control benchmarks, SATR improves stability under limited populations and reaches competitive returns against strong baselines including PPO-LSTM. In addition, to make SATR practical at scale, we introduce a bitset implementation for binary spiking and binary weights, substantially reducing wall-clock training time and enabling fast RSNN policy search.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向循环脉冲神经网络无梯度优化的信号自适应信赖域方法</div>
<div class="mono" style="margin-top:8px">循环脉冲神经网络（RSNN）是实现高能效控制策略的有前景的架构，但针对高维、长周期强化学习任务的训练仍具挑战性。基于群体的无梯度优化通过估计梯度，绕过了不可微脉冲动力学中的反向传播问题。然而，在有限群体规模下，梯度估计的高方差可能导致有害且过于激进的更新步长。受强化学习中信赖域方法在分布空间约束策略更新的启发，我们提出**信号自适应信赖域（SATR）**——一种通过以估计信号能量归一化的KL散度上界来约束相对变化的分布更新规则。SATR能在强信号下自动扩展信赖域，在噪声主导更新时收缩信赖域。我们针对伯努利连接分布实例化SATR（该分布在RSNN优化中已展现卓越实证性能）。在一系列高维连续控制基准测试中，SATR在有限群体规模下提升了稳定性，并在包括PPO-LSTM在内的强基线对比中达到具有竞争力的回报率。此外，为实现SATR的大规模应用，我们引入了针对二元脉冲与二元权重的位集实现方案，显著缩短了实际训练时间，实现了高效的RSNN策略搜索。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of training recurrent spiking neural networks (RSNNs) for high-dimensional reinforcement learning, where gradient-free optimization methods often suffer from high-variance gradient estimates leading to unstable updates. To mitigate this, the authors propose Signal-Adaptive Trust Regions (SATR), a method that constrains policy updates by bounding KL divergence normalized by estimated signal energy, allowing the trust region to adaptively expand or contract based on signal strength. Experimental results on continuous-control benchmarks show that SATR improves stability with limited population sizes and achieves competitive performance compared to baselines like PPO-LSTM, while a bitset implementation for binary operations significantly reduces training time.</div>
<div class="mono" style="margin-top:8px">本文针对循环脉冲神经网络在高维强化学习中训练困难的问题，提出了一种解决方案，其中无梯度优化方法因梯度估计方差高而易导致不稳定更新。为此，作者引入了信号自适应信任区域方法，该方法通过以估计信号能量归一化的KL散度来约束策略更新，使信任区域能根据信号强度自适应调整。在连续控制基准测试中，该方法在有限种群规模下提高了稳定性，并取得了与PPO-LSTM等基线方法竞争的性能，同时通过二进制操作的位集实现大幅减少了训练时间。</div>
</details>
</div>
<div class="card">
<div class="title">Training slow silicon neurons to control extremely fast robots with spiking reinforcement learning</div>
<div class="meta-line">Authors: Irene Ambrosini, Ingo Blakowski, Dmitrii Zendrikov, Cristiano Capone, Luna Gava, Giacomo Indiveri, Chiara De Luca, Chiara Bartolozzi</div>
<div class="meta-line">First: 2026-01-29T11:05:23+00:00 · Latest: 2026-01-29T11:05:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21548v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21548v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Air hockey demands split-second decisions at high puck velocities, a challenge we address with a compact network of spiking neurons running on a mixed-signal analog/digital neuromorphic processor. By co-designing hardware and learning algorithms, we train the system to achieve successful puck interactions through reinforcement learning in a remarkably small number of trials. The network leverages fixed random connectivity to capture the task&#x27;s temporal structure and adopts a local e-prop learning rule in the readout layer to exploit event-driven activity for fast and efficient learning. The result is real-time learning with a setup comprising a computer and the neuromorphic chip in-the-loop, enabling practical training of spiking neural networks for robotic autonomous systems. This work bridges neuroscience-inspired hardware with real-world robotic control, showing that brain-inspired approaches can tackle fast-paced interaction tasks while supporting always-on learning in intelligent machines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用脉冲强化学习训练慢速硅神经元控制极速机器人</div>
<div class="mono" style="margin-top:8px">空气曲棍球需要在高速冰球运动中实现瞬间决策，我们通过运行在混合信号模拟/数字神经形态处理器上的紧凑脉冲神经元网络应对这一挑战。通过硬件与学习算法的协同设计，我们训练该系统在极少量尝试中通过强化学习实现成功的冰球交互。该网络利用固定随机连接捕捉任务的时间结构，并在读出层采用局部e-prop学习规则，以利用事件驱动活动实现快速高效的学习。最终构建了包含计算机与神经形态芯片的实时闭环学习系统，为机器人自主系统提供了实用的脉冲神经网络训练方案。这项工作将神经科学启发的硬件与现实世界的机器人控制相连接，表明类脑方法能够处理快速交互任务，并支持智能机器的持续在线学习。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work is motivated by the need for brain-inspired, energy-efficient systems that can perform real-time control in fast-paced robotic tasks like air hockey, where split-second decisions are required. The method employs a co-design of hardware and algorithms, using a compact spiking neural network on a mixed-signal neuromorphic processor, with fixed random connectivity to capture temporal dynamics and a local e-prop learning rule in the readout layer for event-driven reinforcement learning. Experimental results demonstrate that the system achieves successful puck interactions in remarkably few trials, enabling real-time learning with a neuromorphic chip in-the-loop, thus bridging neuroscience-inspired hardware with practical robotic control for always-on learning in intelligent machines.</div>
<div class="mono" style="margin-top:8px">本研究旨在应对高速机器人任务（如空气曲棍球）中需要瞬时决策的挑战，通过受大脑启发的节能系统实现实时控制。方法采用硬件与算法的协同设计，在混合信号神经形态处理器上运行紧凑的脉冲神经网络，利用固定随机连接捕获任务时间结构，并在读出层采用局部e-prop学习规则进行事件驱动的强化学习。实验结果表明，该系统在极少数试验中即成功实现了对冰球的交互控制，通过神经形态芯片的在线实时学习，将神经科学启发的硬件与实际机器人控制相结合，为智能机器的持续学习提供了可行方案。</div>
</details>
</div>
<div class="card">
<div class="title">Explicit Credit Assignment through Local Rewards and Dependence Graphs in Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Bang Giang Le, Viet Cuong Ta</div>
<div class="meta-line">First: 2026-01-29T10:38:19+00:00 · Latest: 2026-01-29T10:38:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21523v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21523v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To promote cooperation in Multi-Agent Reinforcement Learning, the reward signals of all agents can be aggregated together, forming global rewards that are commonly known as the fully cooperative setting. However, global rewards are usually noisy because they contain the contributions of all agents, which have to be resolved in the credit assignment process. On the other hand, using local reward benefits from faster learning due to the separation of agents&#x27; contributions, but can be suboptimal as agents myopically optimize their own reward while disregarding the global optimality. In this work, we propose a method that combines the merits of both approaches. By using a graph of interaction between agents, our method discerns the individual agent contribution in a more fine-grained manner than a global reward, while alleviating the cooperation problem with agents&#x27; local reward. We also introduce a practical approach for approximating such a graph. Our experiments demonstrate the flexibility of the approach, enabling improvements over the traditional local and global reward settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多智能体强化学习中基于局部奖励与依赖图的显式信用分配</div>
<div class="mono" style="margin-top:8px">为促进多智能体强化学习中的协作，可将所有智能体的奖励信号聚合为全局奖励（即完全协作设定）。然而全局奖励通常包含所有智能体的贡献而存在噪声，需通过信用分配过程解析。另一方面，局部奖励因分离智能体贡献而加速学习，但可能导致智能体短视地优化自身奖励而忽视全局最优性。本研究提出融合两者优势的方法：通过智能体交互图，以比全局奖励更细粒度的方式辨识个体贡献，同时缓解局部奖励的协作难题。我们还提出该交互图的近似构建方法。实验表明该方法具有灵活性，能改进传统局部与全局奖励设定。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the credit assignment problem in cooperative multi-agent reinforcement learning, where global rewards are noisy due to combined agent contributions and local rewards can lead to suboptimal, myopic behavior. The proposed method leverages a graph of agent interactions to explicitly assign credit in a fine-grained manner, combining the benefits of both global and local reward structures, and includes a practical approximation for constructing such graphs. Experimental results show that this approach improves performance over traditional global and local reward settings, demonstrating greater flexibility in promoting effective cooperation.</div>
<div class="mono" style="margin-top:8px">本文针对合作式多智能体强化学习中的信用分配问题展开研究，其中全局奖励因智能体贡献混合而存在噪声，局部奖励则可能导致短视的次优行为。所提出的方法通过利用智能体交互图来细粒度地显式分配信用，结合了全局和局部奖励结构的优点，并包含了一种构建此类图的实用近似方法。实验结果表明，该方法在传统全局和局部奖励设置基础上提升了性能，展现出促进有效合作的更强灵活性。</div>
</details>
</div>
<div class="card">
<div class="title">Formal Verification of Noisy Quantum Reinforcement Learning Policies</div>
<div class="meta-line">Authors: Dennis Gross</div>
<div class="meta-line">First: 2025-12-01T10:26:33+00:00 · Latest: 2026-01-29T10:31:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.01502v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.01502v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quantum reinforcement learning (QRL) aims to use quantum effects to create sequential decision-making policies that achieve tasks more effectively than their classical counterparts. However, QRL policies face uncertainty from quantum measurements and hardware noise, such as bit-flip, phase-flip, and depolarizing errors, which can lead to unsafe behavior. Existing work offers no systematic way to verify whether trained QRL policies meet safety requirements under specific noise conditions. We introduce QVerifier, a formal verification method that applies probabilistic model checking to analyze trained QRL policies with and without modeled quantum noise. QVerifier builds a complete model of the policy-environment interaction, incorporates quantum uncertainty directly into the transition probabilities, and then checks safety properties using the Storm model checker. Experiments across multiple QRL environments show that QVerifier precisely measures how different noise models influence safety, revealing both performance degradation and cases where noise can help. By enabling rigorous safety verification before deployment, QVerifier addresses a critical need: because access to quantum hardware is expensive, pre-deployment verification is essential for any safety-critical use of QRL. QVerifier targets a potential sweet spot between classical and quantum computation, where trained QRL policies could still be modeled classically for probabilistic model checking. When the policy was trained under matching noise conditions, this formal model is exact; when trained on physical hardware, it constitutes an idealized approximation, as unknown hardware noise prevents exact policy modeling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>噪声量子强化学习策略的形式化验证</div>
<div class="mono" style="margin-top:8px">量子强化学习旨在利用量子效应构建比经典方法更有效的序列决策策略。然而，量子策略面临量子测量与硬件噪声（如比特翻转、相位翻转、退极化误差）带来的不确定性，可能导致不安全行为。现有研究缺乏系统化方法验证训练后的量子策略在特定噪声条件下是否满足安全要求。我们提出QVerifier——一种应用概率模型检测的形式化验证方法，用于分析含噪声模型与无噪声的量子策略。该方法构建策略-环境交互的完整模型，将量子不确定性直接纳入转移概率，并利用Storm模型检测器验证安全属性。在多类量子强化学习环境中的实验表明，QVerifier能精确量化不同噪声模型对安全性的影响，既揭示性能退化现象，也发现噪声可能产生积极作用的案例。通过实现部署前的严格安全验证，QVerifier解决了关键需求：鉴于量子硬件访问成本高昂，预部署验证对量子强化学习的安全关键应用至关重要。该方法瞄准经典与量子计算间的潜在平衡点——训练后的量子策略仍可通过经典方式建模以进行概率模型检测。当策略在匹配噪声条件下训练时，该形式模型是精确的；若在物理硬件上训练，则构成理想化近似，因为未知硬件噪声使精确策略建模无法实现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the safety risks posed by quantum measurement uncertainty and hardware noise in quantum reinforcement learning (QRL) policies, for which systematic verification methods were lacking. The authors introduce QVerifier, a formal verification method that uses probabilistic model checking to analyze trained QRL policies by modeling policy-environment interactions and directly incorporating quantum uncertainty into transition probabilities, which are then checked with the Storm model checker. Experimental results across multiple QRL environments demonstrate that QVerifier precisely quantifies the impact of various noise models on safety, revealing both performance degradation and instances where noise can be beneficial, thereby providing essential pre-deployment verification for safety-critical QRL applications.</div>
<div class="mono" style="margin-top:8px">本文针对量子强化学习策略中由量子测量不确定性和硬件噪声引发的安全风险，而现有方法缺乏系统性验证的问题，提出了一种解决方案。作者引入了QVerifier，这是一种形式化验证方法，它通过概率模型检测来分析训练好的QRL策略，具体方法是建模策略与环境的交互，并将量子不确定性直接纳入转移概率，然后使用Storm模型检查器进行安全性验证。在多个QRL环境中的实验结果表明，QVerifier能够精确量化不同噪声模型对安全性的影响，既揭示了性能下降的情况，也发现了噪声可能有益的实例，从而为安全关键的QRL应用提供了必要的部署前验证。</div>
</details>
</div>
<div class="card">
<div class="title">ETS: Energy-Guided Test-Time Scaling for Training-Free RL Alignment</div>
<div class="meta-line">Authors: Xiuyu Li, Jinkai Zhang, Mingyang Yi, Yu Li, Longqiang Wang, Yue Wang, Ju Fan</div>
<div class="meta-line">First: 2026-01-29T10:06:52+00:00 · Latest: 2026-01-29T10:06:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21484v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21484v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) post-training alignment for language models is effective, but also costly and unstable in practice, owing to its complicated training process. To address this, we propose a training-free inference method to sample directly from the optimal RL policy. The transition probability applied to Masked Language Modeling (MLM) consists of a reference policy model and an energy term. Based on this, our algorithm, Energy-Guided Test-Time Scaling (ETS), estimates the key energy term via online Monte Carlo, with a provable convergence rate. Moreover, to ensure practical efficiency, ETS leverages modern acceleration frameworks alongside tailored importance sampling estimators, substantially reducing inference latency while provably preserving sampling quality. Experiments on MLM (including autoregressive models and diffusion language models) across reasoning, coding, and science benchmarks show that our ETS consistently improves generation quality, validating its effectiveness and design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ETS：基于能量引导的无训练强化学习对齐测试时缩放方法</div>
<div class="mono" style="margin-top:8px">语言模型的强化学习（RL）后训练对齐方法虽有效，但因其复杂的训练过程，在实践中成本高昂且稳定性不足。为此，我们提出一种免训练的推理方法，直接从最优RL策略中采样。应用于掩码语言建模（MLM）的转移概率由参考策略模型和能量项构成。基于此，我们的算法——能量引导测试时缩放（ETS）通过在线蒙特卡洛方法估计关键能量项，并具备可证明的收敛速率。此外，为确保实际效率，ETS结合现代加速框架与定制的重要性采样估计器，在可证明保持采样质量的同时显著降低推理延迟。在涵盖推理、编程和科学领域的MLM（包括自回归模型与扩散语言模型）基准测试中，ETS持续提升生成质量，验证了其有效性与设计合理性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the high cost and instability of conventional reinforcement learning (RL) post-training alignment for language models, which involves complex training. To address this, the authors propose a training-free inference method called Energy-Guided Test-Time Scaling (ETS) that directly samples from the optimal RL policy. The method formulates transition probability for Masked Language Modeling (MLM) as a combination of a reference policy model and an energy term, estimating the energy via online Monte Carlo with provable convergence. For practical efficiency, ETS integrates acceleration frameworks and importance sampling estimators to reduce inference latency while maintaining sampling quality. Experimental results on MLM across reasoning, coding, and science benchmarks demonstrate that ETS consistently improves generation quality, validating its effectiveness.</div>
<div class="mono" style="margin-top:8px">本文的动机在于传统强化学习（RL）后训练对齐方法成本高、不稳定，且训练过程复杂。为此，作者提出了一种无需训练的推理方法，称为能量引导测试时缩放（ETS），可直接从最优RL策略中采样。该方法将掩码语言建模（MLM）的转移概率构建为参考策略模型与能量项的组合，通过在线蒙特卡洛估计能量项并具有可证明的收敛性。为确保实际效率，ETS利用现代加速框架和定制的重要性采样估计器，显著降低推理延迟同时保证采样质量。在推理、编码和科学基准测试上的MLM实验结果表明，ETS能持续提升生成质量，验证了其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Mean-Field Control on Sparse Graphs: From Local Limits to GNNs via Neighborhood Distributions</div>
<div class="meta-line">Authors: Tobias Schmidt, Kai Cui</div>
<div class="meta-line">First: 2026-01-29T09:57:48+00:00 · Latest: 2026-01-29T09:57:48+00:00</div>
<div class="meta-line">Comments: 19 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21477v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21477v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mean-field control (MFC) offers a scalable solution to the curse of dimensionality in multi-agent systems but traditionally hinges on the restrictive assumption of exchangeability via dense, all-to-all interactions. In this work, we bridge the gap to real-world network structures by proposing a rigorous framework for MFC on large sparse graphs. We redefine the system state as a probability measure over decorated rooted neighborhoods, effectively capturing local heterogeneity. Our central contribution is a theoretical foundation for scalable reinforcement learning in this setting. We prove horizon-dependent locality: for finite-horizon problems, an agent&#x27;s optimal policy at time t depends strictly on its (T-t)-hop neighborhood. This result renders the infinite-dimensional control problem tractable and underpins a novel Dynamic Programming Principle (DPP) on the lifted space of neighborhood distributions. Furthermore, we formally and experimentally justify the use of Graph Neural Networks (GNNs) for actor-critic algorithms in this context. Our framework naturally recovers classical MFC as a degenerate case while enabling efficient, theoretically grounded control on complex sparse topologies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>稀疏图上的平均场控制：从局部极限到图神经网络——基于邻域分布的方法</div>
<div class="mono" style="margin-top:8px">平均场控制为多智能体系统的维度灾难提供了可扩展的解决方案，但传统方法依赖密集全连接交互的可交换性假设，具有较大局限性。本研究通过提出适用于大型稀疏图的严格平均场控制框架，弥合了理论与现实网络结构之间的鸿沟。我们将系统状态重新定义为带标记根邻域上的概率测度，有效捕捉局部异质性。核心贡献在于为此场景下的可扩展强化学习奠定理论基础：证明了有限时域问题的时域依赖局部性——智能体在时刻t的最优策略严格取决于其(T-t)跳邻域。该结论使无限维控制问题可解，并支撑了邻域分布提升空间上的新型动态规划原理。此外，我们从理论与实验两方面论证了在此框架下采用图神经网络实现行动者-评论者算法的合理性。该框架不仅将经典平均场控制作为退化情形自然包含，还能在复杂稀疏拓扑上实现高效且理论完备的控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of traditional mean-field control (MFC), which assumes dense, all-to-all interactions and exchangeability, by extending it to large sparse graphs that better reflect real-world network structures. The method redefines the system state as a probability measure over decorated rooted neighborhoods to capture local heterogeneity, and establishes a theoretical foundation for scalable reinforcement learning, proving horizon-dependent locality where an agent&#x27;s optimal policy depends only on its finite-hop neighborhood. Experimental results demonstrate that this framework enables efficient control on sparse topologies and justifies the use of Graph Neural Networks (GNNs) for actor-critic algorithms, while naturally recovering classical MFC as a special case.</div>
<div class="mono" style="margin-top:8px">本文针对传统平均场控制（MFC）依赖密集全连接交互和可交换性的局限，将其扩展到更能反映现实世界网络结构的大型稀疏图上。方法通过将系统状态重新定义为装饰根邻域上的概率测度来捕捉局部异质性，并为可扩展强化学习建立了理论基础，证明了有限时域问题中智能体的最优策略仅依赖于其有限跳邻域的视界依赖局部性。实验结果表明，该框架能在稀疏拓扑上实现高效控制，并验证了图神经网络（GNN）在行动者-评论者算法中的适用性，同时自然地将经典MFC作为特例包含在内。</div>
</details>
</div>
<div class="card">
<div class="title">Task-free Adaptive Meta Black-box Optimization</div>
<div class="meta-line">Authors: Chao Wang, Licheng Jiao, Lingling Li, Jiaxuan Zhao, Guanchun Wang, Fang Liu, Shuyuan Yang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-29T09:54:10+00:00 · Latest: 2026-01-29T09:54:10+00:00</div>
<div class="meta-line">Comments: This article was published as a conference paper at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21475v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21475v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Handcrafted optimizers become prohibitively inefficient for complex black-box optimization (BBO) tasks. MetaBBO addresses this challenge by meta-learning to automatically configure optimizers for low-level BBO tasks, thereby eliminating heuristic dependencies. However, existing methods typically require extensive handcrafted training tasks to learn meta-strategies that generalize to target tasks, which poses a critical limitation for realistic applications with unknown task distributions. To overcome the issue, we propose the Adaptive meta Black-box Optimization Model (ABOM), which performs online parameter adaptation using solely optimization data from the target task, obviating the need for predefined task distributions. Unlike conventional metaBBO frameworks that decouple meta-training and optimization phases, ABOM introduces a closed-loop adaptive parameter learning mechanism, where parameterized evolutionary operators continuously self-update by leveraging generated populations during optimization. This paradigm shift enables zero-shot optimization: ABOM achieves competitive performance on synthetic BBO benchmarks and realistic unmanned aerial vehicle path planning problems without any handcrafted training tasks. Visualization studies reveal that parameterized evolutionary operators exhibit statistically significant search patterns, including natural selection and genetic recombination.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无任务自适应元黑盒优化</div>
<div class="mono" style="margin-top:8px">针对复杂黑盒优化任务，手工设计的优化器效率低下。元黑盒优化通过元学习自动配置底层优化器，以消除启发式依赖。然而，现有方法通常需要大量手工训练任务来学习可泛化至目标任务的元策略，这在任务分布未知的实际应用中存在局限。为此，我们提出自适应元黑盒优化模型，仅利用目标任务的优化数据进行在线参数自适应，无需预定义任务分布。与传统元黑盒优化框架将元训练与优化阶段分离不同，该模型引入闭环自适应参数学习机制，参数化进化算子能利用优化过程中生成的种群持续自我更新。这一范式转变实现了零样本优化：该模型在合成黑盒优化基准测试和现实无人机路径规划问题中均取得优异性能，且无需任何手工训练任务。可视化研究表明，参数化进化算子展现出具有统计显著性的搜索模式，包括自然选择和基因重组。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the inefficiency of handcrafted optimizers for complex black-box optimization (BBO) tasks and the reliance of existing meta-learning methods on predefined task distributions. It proposes the Adaptive meta Black-box Optimization Model (ABOM), which introduces a closed-loop adaptive parameter learning mechanism that continuously updates evolutionary operators using only optimization data from the target task, eliminating the need for handcrafted training tasks. Experimental results show that ABOM achieves competitive performance in synthetic BBO benchmarks and unmanned aerial vehicle path planning problems through zero-shot optimization, with visualizations revealing statistically significant search patterns like natural selection and genetic recombination.</div>
<div class="mono" style="margin-top:8px">本文针对复杂黑盒优化任务中手工优化器效率低下以及现有元学习方法依赖预定义任务分布的问题，提出了自适应元黑盒优化模型（ABOM）。该方法采用闭环自适应参数学习机制，仅利用目标任务的优化数据持续更新参数化进化算子，无需手工训练任务。实验结果表明，ABOM在合成黑盒优化基准测试和无人机路径规划问题中通过零样本优化实现了有竞争力的性能，可视化研究揭示了自然选择和基因重组等统计显著的搜索模式。</div>
</details>
</div>
<div class="card">
<div class="title">MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning</div>
<div class="meta-line">Authors: Yaorui Shi, Shugui Liu, Yu Yang, Wenyu Mao, Yuxin Chen, Qi GU, Hui Su, Xunliang Cai, Xiang Wang, An Zhang</div>
<div class="meta-line">First: 2026-01-29T09:47:17+00:00 · Latest: 2026-01-29T09:47:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21468v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21468v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MemOCR：面向高效长程推理的布局感知视觉记忆系统</div>
<div class="mono" style="margin-top:8px">长程智能体推理需要将不断增长的交互历史有效压缩至有限上下文窗口。现有记忆系统多将历史序列化为文本，其词元级成本均匀且随长度线性增长，常将稀缺预算消耗于低价值细节。为此，我们提出MemOCR——一种多模态记忆智能体，通过视觉布局实现自适应信息密度的记忆空间分配，从而在严格上下文预算下提升长程推理能力。具体而言，MemOCR维护结构化富文本记忆（如标题、高亮内容）并将其渲染为图像供智能体查询访问，在视觉层面突出关键证据并大幅压缩辅助细节。为确保不同记忆预算下的鲁棒性，我们采用预算感知目标的强化学习训练MemOCR，使智能体适应多样化压缩层级。在长上下文多跳与单跳问答基准测试中，MemOCR优于强文本基线，并在极端预算下实现更高效的上下文利用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for MemOCR stems from the need to efficiently compress long interaction histories for agentic reasoning within limited context windows, as existing text-based memory systems often waste tokens on low-value details. The method introduces a multimodal memory agent that uses visual layout to allocate memory space adaptively, maintaining structured rich-text memory and rendering it into an image for consultation, which prioritizes crucial evidence while compressing auxiliary details. Experimental results show that MemOCR outperforms text-based baselines on long-context multi-hop and single-hop QA benchmarks, achieving more effective context utilization under extreme budgets through reinforcement learning with budget-aware objectives.</div>
<div class="mono" style="margin-top:8px">MemOCR的动机源于在有限上下文窗口中高效压缩长交互历史以支持智能体推理的需求，因为现有基于文本的记忆系统常将稀缺的令牌资源浪费在低价值细节上。该方法引入了一种多模态记忆智能体，通过视觉布局自适应分配记忆空间，维护结构化的富文本记忆并将其渲染为图像供智能体查询，从而优先处理关键证据并压缩辅助细节。实验结果表明，在长上下文多跳和单跳问答基准测试中，MemOCR通过基于预算目标的强化学习训练，优于基于文本的基线方法，并在极端预算下实现了更有效的上下文利用。</div>
</details>
</div>
<div class="card">
<div class="title">HER: Human-like Reasoning and Reinforcement Learning for LLM Role-playing</div>
<div class="meta-line">Authors: Chengyu Du, Xintao Wang, Aili Chen, Weiyuan Li, Rui Xu, Junteng Liu, Zishan Huang, Rong Tian, Zijun Sun, Yuhao Li, Liheng Feng, Deming Ding, Pengyu Zhao, Yanghua Xiao</div>
<div class="meta-line">First: 2026-01-29T09:35:27+00:00 · Latest: 2026-01-29T09:35:27+00:00</div>
<div class="meta-line">Comments: 41pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21459v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21459v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM role-playing, i.e., using LLMs to simulate specific personas, has emerged as a key capability in various applications, such as companionship, content creation, and digital games. While current models effectively capture character tones and knowledge, simulating the inner thoughts behind their behaviors remains a challenge. Towards cognitive simulation in LLM role-play, previous efforts mainly suffer from two deficiencies: data with high-quality reasoning traces, and reliable reward signals aligned with human preferences. In this paper, we propose HER, a unified framework for cognitive-level persona simulation. HER introduces dual-layer thinking, which distinguishes characters&#x27; first-person thinking from LLMs&#x27; third-person thinking. To bridge these gaps, we curate reasoning-augmented role-playing data via reverse engineering and construct human-aligned principles and reward models. Leveraging these resources, we train \method models based on Qwen3-32B via supervised and reinforcement learning. Extensive experiments validate the effectiveness of our approach. Notably, our models significantly outperform the Qwen3-32B baseline, achieving a 30.26 improvement on the CoSER benchmark and a 14.97 gain on the Minimax Role-Play Bench. Our datasets, principles, and models will be released to facilitate future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HER：面向大语言模型角色扮演的人类化推理与强化学习框架</div>
<div class="mono" style="margin-top:8px">大语言模型角色扮演，即利用大语言模型模拟特定人物角色，已成为陪伴对话、内容创作和数字游戏等应用中的关键能力。现有模型虽能有效捕捉角色语调和知识，但模拟其行为背后的内在思维仍具挑战。为实现大语言模型角色扮演的认知模拟，先前研究主要存在两大不足：缺乏高质量推理轨迹的数据，以及缺少符合人类偏好的可靠奖励信号。本文提出HER——一个认知层面人物模拟的统一框架。HER引入双层思维机制，区分角色的第一人称思维与大语言模型的第三人称思维。为弥合这些差距，我们通过逆向工程构建了推理增强的角色扮演数据，并建立了与人类对齐的原则及奖励模型。基于这些资源，我们以Qwen3-32B为基础，通过监督学习和强化学习训练了HER模型。大量实验验证了方法的有效性：我们的模型显著超越Qwen3-32B基线，在CoSER基准上提升30.26分，在Minimax角色扮演基准上提升14.97分。我们将公开数据集、原则与模型以促进后续研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of simulating the inner cognitive processes behind persona behaviors in LLM role-playing, which is crucial for applications like companionship and content creation. To overcome deficiencies in existing high-quality reasoning data and human-aligned reward signals, the authors propose HER, a framework featuring dual-layer thinking that separates character-first-person from LLM-third-person reasoning, and they curate reasoning-augmented data via reverse engineering while constructing principle-based reward models. Experimental results show that models trained with HER on Qwen3-32B via supervised and reinforcement learning significantly outperform baselines, achieving improvements of 30.26 on the CoSER benchmark and 14.97 on the Minimax Role-Play Bench.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型角色扮演中模拟人物行为背后内在认知过程的挑战展开研究，这对于陪伴、内容创作等应用至关重要。为解决现有高质量推理数据和人类偏好对齐奖励信号的不足，作者提出了HER框架，采用双层思维区分角色第一人称与模型第三人称推理，通过逆向工程构建推理增强数据并建立基于原则的奖励模型。实验结果表明，基于Qwen3-32B通过监督学习和强化学习训练的HER模型显著优于基线，在CoSER基准上提升30.26分，在Minimax角色扮演基准上提升14.97分。</div>
</details>
</div>
<div class="card">
<div class="title">Mitigating Overthinking in Large Reasoning Models via Difficulty-aware Reinforcement Learning</div>
<div class="meta-line">Authors: Qian Wan, Ziao Xu, Luona Wei, Xiaoxuan Shen, Jianwen Sun</div>
<div class="meta-line">First: 2026-01-29T08:56:45+00:00 · Latest: 2026-01-29T08:56:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21418v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21418v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Reasoning Models (LRMs) achieve explicit chain-of-thought expansion by imitating deep thinking behaviors of humans, demonstrating excellent performance in complex task scenarios. However, the deep-thinking mode often leads to unnecessarily lengthy reasoning and resource inefficiency when handling simple tasks. This overthinking phenomenon may arise from the generation preference triggered by the reward function during post-training. Existing research attempts to mitigate overthinking from the perspective of prompt design or model training, but generally underestimates the importance of task difficulty awareness, which makes it difficult for LRMs to effectively allocate reasoning resources. In this paper, we propose Difficulty-aware Policy Optimization (DiPO), a reinforcement learning-based LRM training framework. DiPO encourages LRM to spontaneously model task complexity, and integrates them into reinforcement learning framework to adjust the generation preferences introduced by post-training. A difficulty modeling method based on model self-reasoning is proposed, which significantly reduces the dependence on manual annotation and formalize task complexity. We further develop a difficulty-signal-enhanced reward function that incorporates a penalty for lengthy reasoning while considering reasoning performance and output format. Experimental results indicate that DiPO enables the model to spontaneously adjust inference overhead, significantly reducing redundant tokens without losing performance due to thought compression.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于难度感知强化学习缓解大型推理模型的过度思考现象</div>
<div class="mono" style="margin-top:8px">大型推理模型通过模仿人类深度思考行为实现显式的思维链扩展，在复杂任务场景中展现出优异性能。然而在处理简单任务时，这种深度思考模式常导致不必要的冗长推理和资源低效。过度思考现象可能源于后训练阶段奖励函数触发的生成偏好。现有研究尝试从提示设计或模型训练角度缓解该问题，但普遍低估了任务难度感知的重要性，使得模型难以有效分配推理资源。本文提出难度感知策略优化——一种基于强化学习的训练框架，鼓励模型自主建模任务复杂度，并将其整合至强化学习框架中以调整后训练引入的生成偏好。我们提出基于模型自推理的难度建模方法，显著降低对人工标注的依赖并形式化任务复杂度。进一步开发了难度信号增强的奖励函数，在考虑推理性能与输出格式的同时，对冗长推理施加惩罚。实验表明，该框架能使模型自主调整推理开销，在保持性能不因思维压缩受损的前提下显著减少冗余标记。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the overthinking problem in Large Reasoning Models (LRMs), where deep-thinking modes lead to unnecessarily lengthy and inefficient reasoning on simple tasks, often due to reward functions from post-training. The authors propose Difficulty-aware Policy Optimization (DiPO), a reinforcement learning framework that encourages LRMs to model task complexity autonomously, using a self-reasoning-based method to formalize difficulty without heavy manual annotation, and integrates this into a reward function that penalizes excessive reasoning while maintaining performance. Experiments show DiPO effectively reduces redundant tokens without compromising accuracy, allowing models to adjust inference overhead adaptively.</div>
<div class="mono" style="margin-top:8px">本文针对大型推理模型中的过度思考问题展开研究，该问题导致模型在处理简单任务时产生不必要的冗长推理和资源低效，通常源于后训练中奖励函数引发的生成偏好。作者提出了难度感知策略优化（DiPO），这是一个基于强化学习的训练框架，鼓励模型自主建模任务复杂度，通过基于自推理的方法形式化难度，减少对人工标注的依赖，并将难度信号融入奖励函数，在惩罚冗长推理的同时兼顾推理性能和输出格式。实验结果表明，DiPO使模型能自发调整推理开销，显著减少冗余标记而不因思维压缩损失性能。</div>
</details>
</div>
<div class="card">
<div class="title">Intrinsic Reward Policy Optimization for Sparse-Reward Environments</div>
<div class="meta-line">Authors: Minjae Cho, Huy Trong Tran</div>
<div class="meta-line">First: 2026-01-29T08:25:14+00:00 · Latest: 2026-01-29T08:25:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21391v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21391v1">PDF</a> · <a href="https://github.com/Mgineer117/IRPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Exploration is essential in reinforcement learning as an agent relies on trial and error to learn an optimal policy. However, when rewards are sparse, naive exploration strategies, like noise injection, are often insufficient. Intrinsic rewards can also provide principled guidance for exploration by, for example, combining them with extrinsic rewards to optimize a policy or using them to train subpolicies for hierarchical learning. However, the former approach suffers from unstable credit assignment, while the latter exhibits sample inefficiency and sub-optimality. We propose a policy optimization framework that leverages multiple intrinsic rewards to directly optimize a policy for an extrinsic reward without pretraining subpolicies. Our algorithm -- intrinsic reward policy optimization (IRPO) -- achieves this by using a surrogate policy gradient that provides a more informative learning signal than the true gradient in sparse-reward environments. We demonstrate that IRPO improves performance and sample efficiency relative to baselines in discrete and continuous environments, and formally analyze the optimization problem solved by IRPO. Our code is available at https://github.com/Mgineer117/IRPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>稀疏奖励环境下的内在奖励策略优化</div>
<div class="mono" style="margin-top:8px">在强化学习中，探索至关重要，因为智能体需通过试错学习最优策略。然而，当奖励稀疏时，噪声注入等简单探索策略往往不足。内在奖励可为探索提供原则性指导，例如将其与外在奖励结合以优化策略，或用于训练分层学习的子策略。但前者存在信用分配不稳定的问题，后者则表现出样本效率低下和次优性。我们提出一种策略优化框架，利用多种内在奖励直接优化面向外在奖励的策略，无需预训练子策略。我们的算法——内在奖励策略优化（IRPO）——通过使用替代策略梯度实现这一目标，该梯度在稀疏奖励环境中提供比真实梯度更具信息量的学习信号。实验表明，在离散和连续环境中，IRPO相较于基线方法提升了性能与样本效率，并对IRPO求解的优化问题进行了形式化分析。代码发布于https://github.com/Mgineer117/IRPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of exploration in sparse-reward reinforcement learning, where naive strategies are ineffective. The authors propose Intrinsic Reward Policy Optimization (IRPO), a framework that leverages multiple intrinsic rewards to directly optimize a policy for extrinsic rewards without pretraining subpolicies, using a surrogate policy gradient for a more informative learning signal. Experimental results in discrete and continuous environments show that IRPO improves performance and sample efficiency compared to baselines, and the work includes a formal analysis of the optimization problem.</div>
<div class="mono" style="margin-top:8px">本文针对稀疏奖励强化学习中的探索难题，提出了一种名为内在奖励策略优化（IRPO）的框架。该方法通过利用多个内在奖励直接优化面向外在奖励的策略，无需预训练子策略，并采用替代策略梯度提供更有效的学习信号。在离散和连续环境中的实验结果表明，IRPO相比基线方法提升了性能和样本效率，且研究中对优化问题进行了形式化分析。</div>
</details>
</div>
<div class="card">
<div class="title">Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined through Reinforcement Learning</div>
<div class="meta-line">Authors: Alex Schutz, Victor-Alexandru Darvariu, Efimia Panagiotaki, Bruno Lacerda, Nick Hawes</div>
<div class="meta-line">First: 2025-09-23T12:49:25+00:00 · Latest: 2026-01-29T08:20:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.18930v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.18930v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural Algorithmic Reasoning (NAR) is a paradigm that trains neural networks to execute classic algorithms by supervised learning. Despite its successes, important limitations remain: inability to construct valid solutions without post-processing and to reason about multiple correct ones, poor performance on combinatorial NP-hard problems, and inapplicability to problems for which strong algorithms are not yet known. To address these limitations, we reframe the problem of learning algorithm trajectories as a Markov Decision Process, which imposes structure on the solution construction procedure and unlocks the powerful tools of imitation and reinforcement learning (RL). We propose the GNARL framework, encompassing the methodology to translate problem formulations from NAR to RL and a learning architecture suitable for a wide range of graph-based problems. We achieve very high graph accuracy results on several CLRS-30 problems, performance matching or exceeding much narrower NAR approaches for NP-hard problems and, remarkably, applicability even when lacking an expert algorithm.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>攻克GNARLy难题：通过强化学习重构图神经算法推理</div>
<div class="mono" style="margin-top:8px">神经算法推理（NAR）是一种通过监督学习训练神经网络执行经典算法的范式。尽管取得了一定成功，但仍存在重要局限：无法在不进行后处理的情况下构建有效解、难以对多个正确解进行推理、在组合NP难问题上表现不佳，以及不适用于尚未发现强算法的问题。为解决这些局限，我们将学习算法轨迹的问题重新定义为马尔可夫决策过程，这为解构建过程提供了结构化框架，并解锁了模仿学习与强化学习（RL）的强大工具。我们提出GNARL框架，包含将问题表述从NAR转化为RL的方法论，以及适用于广泛图结构问题的学习架构。我们在多个CLRS-30问题上实现了极高的图精度结果，在NP难问题上的性能匹配甚至超越了更局限的NAR方法，值得注意的是，该框架即使在缺乏专家算法的情况下仍具适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses key limitations in Neural Algorithmic Reasoning (NAR), such as its inability to construct valid solutions without post-processing, handle multiple correct answers, or effectively tackle NP-hard problems. The authors propose reframing algorithm learning as a Markov Decision Process, introducing the GNARL framework that leverages imitation and reinforcement learning to structure solution construction for graph-based problems. Experimental results demonstrate high graph accuracy on CLRS-30 benchmarks, matching or exceeding specialized NAR methods on NP-hard problems, and show applicability even in the absence of known expert algorithms.</div>
<div class="mono" style="margin-top:8px">本文针对神经算法推理（NAR）的关键局限性提出改进，如其无法在不进行后处理的情况下构建有效解、处理多个正确答案或有效应对NP难问题。作者将算法学习重新定义为马尔可夫决策过程，提出了GNARL框架，利用模仿学习和强化学习来结构化地构建图相关问题的解决方案。实验结果表明，该方法在CLRS-30基准测试中实现了很高的图准确率，在NP难问题上匹配或超越了专用NAR方法，并且即使在缺乏已知专家算法的情况下也适用。</div>
</details>
</div>
<div class="card">
<div class="title">Model-Free Output Feedback Stabilization via Policy Gradient Methods</div>
<div class="meta-line">Authors: Ankang Zhang, Ming Chi, Xiaoling Wang, Lintao Ye</div>
<div class="meta-line">First: 2026-01-27T07:15:59+00:00 · Latest: 2026-01-29T08:15:47+00:00</div>
<div class="meta-line">Comments: 31 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19284v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.19284v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Stabilizing a dynamical system is a fundamental problem that serves as a cornerstone for many complex tasks in the field of control systems. The problem becomes challenging when the system model is unknown. Among the Reinforcement Learning (RL) algorithms that have been successfully applied to solve problems pertaining to unknown linear dynamical systems, the policy gradient (PG) method stands out due to its ease of implementation and can solve the problem in a model-free manner. However, most of the existing works on PG methods for unknown linear dynamical systems assume full-state feedback. In this paper, we take a step towards model-free learning for partially observable linear dynamical systems with output feedback and focus on the fundamental stabilization problem of the system. We propose an algorithmic framework that stretches the boundary of PG methods to the problem without global convergence guarantees. We show that by leveraging zeroth-order PG update based on system trajectories and its convergence to stationary points, the proposed algorithms return a stabilizing output feedback policy for discrete-time linear dynamical systems. We also explicitly characterize the sample complexity of our algorithm and verify the effectiveness of the algorithm using numerical examples.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于策略梯度方法的无模型输出反馈镇定</div>
<div class="mono" style="margin-top:8px">镇定动态系统是控制领域许多复杂任务的基础问题，当系统模型未知时该问题变得尤为困难。在已成功应用于未知线性动态系统的强化学习算法中，策略梯度方法因其易于实现且能以无模型方式解决问题而备受关注。然而，现有针对未知线性动态系统的策略梯度研究大多假设全状态反馈。本文针对具有输出反馈的部分可观线性动态系统，向无模型学习迈进一步，聚焦于系统的基本镇定问题。我们提出一种算法框架，将策略梯度方法拓展至缺乏全局收敛保证的问题。通过利用基于系统轨迹的零阶策略梯度更新及其对驻点的收敛性，所提算法能为离散时间线性动态系统返回镇定的输出反馈策略。我们明确量化了算法的样本复杂度，并通过数值算例验证了其有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of stabilizing unknown linear dynamical systems when only output feedback is available, a scenario where traditional policy gradient methods typically assume full-state feedback. The authors propose a model-free algorithmic framework that extends policy gradient methods to partially observable systems, utilizing zeroth-order updates based on system trajectories to converge to stationary points. Experimental results demonstrate that the algorithm successfully returns stabilizing output feedback policies for discrete-time linear systems, with explicit sample complexity characterization and numerical validation of its effectiveness.</div>
<div class="mono" style="margin-top:8px">本文针对仅能获得输出反馈的未知线性动态系统稳定化问题，传统策略梯度方法通常假设全状态反馈，而此场景更具挑战性。作者提出一种无模型算法框架，将策略梯度方法扩展至部分可观测系统，利用基于系统轨迹的零阶更新收敛至稳定点。实验结果表明，该算法成功为离散时间线性系统返回稳定的输出反馈策略，并明确了样本复杂度，通过数值示例验证了其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Repairing Reward Functions with Feedback to Mitigate Reward Hacking</div>
<div class="meta-line">Authors: Stephane Hatgis-Kessell, Logan Mondal Bhamidipaty, Emma Brunskill</div>
<div class="meta-line">First: 2025-10-14T23:18:24+00:00 · Latest: 2026-01-29T07:52:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.13036v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.13036v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human-designed reward functions for reinforcement learning (RL) agents are frequently misaligned with the humans&#x27; true, unobservable objectives, and thus act only as proxies. Optimizing for a misspecified proxy reward function often induces reward hacking, resulting in a policy misaligned with the human&#x27;s true objectives. An alternative is to perform RL from human feedback, which involves learning a reward function from scratch by collecting human preferences over pairs of trajectories. However, building such datasets is costly. To address the limitations of both approaches, we propose Preference-Based Reward Repair (PBRR): an automated iterative framework that repairs a human-specified proxy reward function by learning an additive, transition-dependent correction term from preferences. A manually specified reward function can yield policies that are highly suboptimal under the ground-truth objective, yet corrections on only a few transitions may suffice to recover optimal performance. To identify and correct for those transitions, PBRR uses a targeted exploration strategy and a new preference-learning objective. We prove in tabular domains PBRR has a cumulative regret that matches, up to constants, that of prior preference-based RL methods. In addition, on a suite of reward-hacking benchmarks, PBRR consistently outperforms baselines that learn a reward function from scratch from preferences or modify the proxy reward function using other approaches, requiring substantially fewer preferences to learn high performing policies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于反馈修复奖励函数以缓解奖励破解</div>
<div class="mono" style="margin-top:8px">为强化学习（RL）智能体设计的人工奖励函数常与人类真实但不可观测的目标存在偏差，仅作为代理目标。优化错误设定的代理奖励函数易引发奖励破解，导致策略偏离人类真实目标。另一种方法是从人类反馈中进行RL，即通过收集人类对轨迹对的偏好从头学习奖励函数，但构建此类数据集成本高昂。为克服两种方法的局限，我们提出基于偏好的奖励修复（PBRR）：一种自动化迭代框架，通过从偏好中学习与状态转移相关的加法修正项，修复人工指定的代理奖励函数。手动指定的奖励函数可能在真实目标下产生严重次优策略，但仅需对少数转移进行修正即可恢复最优性能。为识别并修正这些转移，PBRR采用定向探索策略和新偏好学习目标。我们在表格化环境中证明PBRR的累积遗憾与现有基于偏好的RL方法在常数范围内相当。此外，在一系列奖励破解基准测试中，PBRR始终优于从头学习奖励函数或通过其他方法修改代理奖励函数的基线方法，且学习高性能策略所需偏好数量显著减少。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the problem of reward hacking in reinforcement learning, where human-designed proxy reward functions often misalign with true objectives, leading to suboptimal policies. The authors propose Preference-Based Reward Repair (PBRR), an automated iterative method that repairs a given proxy reward function by learning an additive, transition-dependent correction term from human preferences over trajectory pairs, using targeted exploration and a new preference-learning objective. Experimental results on reward-hacking benchmarks show that PBRR outperforms baselines that learn rewards from scratch or modify proxies via other approaches, achieving high-performing policies with substantially fewer human preferences, and theoretical analysis in tabular domains confirms its cumulative regret matches prior preference-based RL methods up to constants.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中的奖励黑客问题，即人工设计的代理奖励函数常与真实目标不一致，导致策略次优。作者提出了基于偏好的奖励修复方法，这是一种自动化迭代框架，通过从人类对轨迹对的偏好中学习一个依赖于状态转移的加性修正项，来修复给定的代理奖励函数，并采用定向探索和新偏好学习目标。在奖励黑客基准测试中，该方法优于从零学习奖励或通过其他方式修改代理奖励的基线，仅需更少的人类偏好即可学习高性能策略，且在表格域的理论分析表明其累积遗憾与先前基于偏好的强化学习方法在常数范围内一致。</div>
</details>
</div>
<div class="card">
<div class="title">Expected Improvement via Gradient Norms</div>
<div class="meta-line">Authors: Joshua Hang Sai Ip, Georgios Makrygiorgos, Ali Mesbah</div>
<div class="meta-line">First: 2026-01-29T07:37:13+00:00 · Latest: 2026-01-29T07:37:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21357v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21357v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bayesian Optimization (BO) is a principled approach for optimizing expensive black-box functions, with Expected Improvement (EI) being one of the most widely used acquisition functions. Despite its empirical success, EI is known to be overly exploitative and can converge to suboptimal stationary points. We propose Expected Improvement via Gradient Norms (EI-GN), a novel acquisition function that applies the improvement principle to a gradient-aware auxiliary objective, thereby promoting sampling in regions that are both high-performing and approaching first-order stationarity. EI-GN relies on gradient observations used to learn gradient-enhanced surrogate models that enable principled gradient inference from function evaluations. We derive a tractable closed-form expression for EI-GN that allows efficient optimization and show that the proposed acquisition is consistent with the improvement-based acquisition framework. Empirical evaluations on standard BO benchmarks demonstrate that EI-GN yields consistent improvements against standard baselines. We further demonstrate applicability of EI-GN to control policy learning problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于梯度范数的期望改进</div>
<div class="mono" style="margin-top:8px">贝叶斯优化（BO）是一种优化昂贵黑盒函数的原理性方法，其中期望改进（EI）是最广泛使用的采集函数之一。尽管EI在实证中取得成功，但其已知具有过度开发性，可能收敛至次优驻点。本文提出基于梯度范数的期望改进（EI-GN），这是一种新颖的采集函数，将改进原则应用于梯度感知的辅助目标，从而促进在高性能且接近一阶驻点的区域进行采样。EI-GN依赖于用于学习梯度增强代理模型的梯度观测，该模型支持从函数评估中进行原理性梯度推断。我们推导了EI-GN的可处理闭式表达式，支持高效优化，并证明所提出的采集函数与基于改进的采集框架一致。在标准BO基准上的实证评估表明，EI-GN相较于标准基线方法实现了持续改进。我们进一步展示了EI-GN在控制策略学习问题中的适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the exploitative nature and tendency to converge to suboptimal points of the widely used Expected Improvement (EI) acquisition function in Bayesian Optimization (BO). The authors propose Expected Improvement via Gradient Norms (EI-GN), a novel acquisition function that applies the improvement principle to a gradient-aware auxiliary objective, encouraging sampling in regions that are both high-performing and near stationarity, using gradient observations to build gradient-enhanced surrogate models for principled gradient inference. Experimental results on standard BO benchmarks show that EI-GN consistently outperforms baseline methods and is also applicable to control policy learning problems.</div>
<div class="mono" style="margin-top:8px">本文针对贝叶斯优化中广泛使用的期望改进（EI）采集函数过于利用性且易收敛于次优点的问题，提出了基于梯度范数的期望改进（EI-GN）这一新采集函数。该方法将改进原则应用于梯度感知的辅助目标，通过梯度观测构建梯度增强的代理模型，以促进在性能高且接近一阶平稳的区域进行采样。在标准贝叶斯优化基准测试中的实验结果表明，EI-GN相比基线方法取得了持续改进，并适用于控制策略学习问题。</div>
</details>
</div>
<div class="card">
<div class="title">Bridging On-Device and Cloud LLMs for Collaborative Reasoning: A Unified Methodology for Local Routing and Post-Training</div>
<div class="meta-line">Authors: Wenzhi Fang, Dong-Jun Han, Liangqi Yuan, Evan Chen, Christopher Brinton</div>
<div class="meta-line">First: 2025-09-28T19:48:56+00:00 · Latest: 2026-01-29T07:25:15+00:00</div>
<div class="meta-line">Comments: We propose a unified post-training framework that integrates routing optimization, enabling the on-device LLM to improve its problem-solving ability while learning routing strategies</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.24050v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.24050v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Device-cloud collaboration holds promise for deploying large language models (LLMs), leveraging lightweight on-device models for efficiency while relying on powerful cloud models for superior reasoning. A central challenge in this setting is determining, for each incoming query, whether it should be processed locally or offloaded to the cloud. Existing approaches typically rely on external routers, which often struggle to determine difficulty from the prompt itself, especially for tasks involving complex reasoning. Motivated by this limitation, we propose enabling on-device LLMs to decide internally whether to invoke cloud assistance at inference time, with this capability instilled through reinforcement learning based post-training. Casting on-device LLM post-training as a reward maximization problem, we design hierarchical rewards to encourage local problem solving and judicious cloud offloading. To solve the resulting problem, we develop an algorithm featuring a group-level policy gradient that stabilizes optimization, together with adaptive prompt filtering that provides complementary learning signals to mitigate policy collapse (i.e., exclusive local execution or exclusive cloud offloading). Extensive experiments on on-device-scale LLaMA and Qwen models across multiple reasoning benchmarks show that our method consistently outperforms baselines and significantly narrows the gap to full cloud LLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>设备端与云端大语言模型协同推理的桥梁：本地路由与后训练的统一方法论</div>
<div class="mono" style="margin-top:8px">设备-云端协作为大语言模型部署提供了前景，通过轻量级设备端模型实现高效处理，同时依托强大的云端模型获得卓越推理能力。该场景的核心挑战在于如何针对每个输入查询，判定其应在本地处理还是卸载至云端。现有方法通常依赖外部路由器，这类方案往往难以仅从提示本身判断任务难度，尤其在涉及复杂推理的任务中表现不足。受此局限启发，我们提出让设备端大语言模型在推理时自主决定是否调用云端协助，该能力通过基于强化学习的后训练注入。将设备端大语言模型后训练构建为奖励最大化问题，我们设计了分层奖励机制以激励本地问题解决与审慎的云端卸载。针对该优化问题，我们开发了一种融合组级策略梯度以稳定训练的自适应算法，并结合动态提示过滤技术提供互补学习信号，从而缓解策略崩溃现象（即仅执行本地处理或仅进行云端卸载）。在多个推理基准测试中，对设备端规模的LLaMA和Qwen模型进行的广泛实验表明，我们的方法持续超越基线模型，并显著缩小了与完整云端大语言模型的性能差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of effectively routing queries between on-device and cloud large language models (LLMs) in collaborative deployments, where external routers often fail to assess query difficulty accurately, this paper introduces a unified post-training methodology that enables on-device LLMs to autonomously decide when to offload to the cloud. The method employs reinforcement learning with hierarchical rewards to encourage local problem-solving and judicious cloud offloading, stabilized by a group-level policy gradient and adaptive prompt filtering to prevent policy collapse. Experimental results on LLaMA and Qwen models across reasoning benchmarks demonstrate that this approach consistently outperforms baseline methods and significantly narrows the performance gap with full cloud LLMs.</div>
<div class="mono" style="margin-top:8px">针对设备与云端大语言模型协作部署中，外部路由机制难以准确评估查询复杂性的挑战，本文提出了一种统一的训练后方法，使设备端模型能够自主决定何时调用云端辅助。该方法采用强化学习，通过分层奖励机制鼓励本地问题解决和审慎的云端卸载，并利用组级策略梯度稳定优化，结合自适应提示过滤防止策略崩溃。在LLaMA和Qwen模型上的多推理基准实验表明，该方法持续优于基线，显著缩小了与全云端模型的性能差距。</div>
</details>
</div>
<div class="card">
<div class="title">Factored Causal Representation Learning for Robust Reward Modeling in RLHF</div>
<div class="meta-line">Authors: Yupei Yang, Lin Yang, Wanxi Deng, Lin Qu, Fan Feng, Biwei Huang, Shikui Tu, Lei Xu</div>
<div class="meta-line">First: 2026-01-29T07:18:45+00:00 · Latest: 2026-01-29T07:18:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21350v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21350v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A reliable reward model is essential for aligning large language models with human preferences through reinforcement learning from human feedback. However, standard reward models are susceptible to spurious features that are not causally related to human labels. This can lead to reward hacking, where high predicted reward does not translate into better behavior. In this work, we address this problem from a causal perspective by proposing a factored representation learning framework that decomposes the model&#x27;s contextual embedding into (1) causal factors that are sufficient for reward prediction and (2) non-causal factors that capture reward-irrelevant attributes such as length or sycophantic bias. The reward head is then constrained to depend only on the causal component. In addition, we introduce an adversarial head trained to predict reward from the non-causal factors, while applying gradient reversal to discourage them from encoding reward-relevant information. Experiments on both mathematical and dialogue tasks demonstrate that our method learns more robust reward models and consistently improves downstream RLHF performance over state-of-the-art baselines. Analyses on length and sycophantic bias further validate the effectiveness of our method in mitigating reward hacking behaviors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向RLHF稳健奖励建模的因子化因果表征学习</div>
<div class="mono" style="margin-top:8px">可靠的奖励模型对于通过人类反馈强化学习对齐大语言模型与人类偏好至关重要。然而，标准奖励模型易受与人类标签无因果关系的伪特征影响，可能导致奖励欺骗现象——高预测奖励并未转化为更优行为。本研究从因果视角提出因子化表征学习框架：将模型上下文嵌入分解为（1）足以预测奖励的因果因子，以及（2）捕获奖励无关属性（如文本长度或谄媚偏见）的非因果因子。奖励预测头被约束为仅依赖因果成分。同时引入对抗性预测头，通过梯度反转机制抑制非因果因子编码奖励相关信息。在数学与对话任务的实验表明，该方法能学习更稳健的奖励模型，并在下游RLHF性能上持续超越现有基线。针对长度与谄媚偏见的分析进一步验证了本方法缓解奖励欺骗行为的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the vulnerability of standard reward models in reinforcement learning from human feedback (RLHF) to spurious, non-causal features, which can lead to reward hacking. To build a more robust reward model, the authors propose a factored causal representation learning framework that decomposes contextual embeddings into causal factors sufficient for reward prediction and non-causal factors capturing irrelevant attributes like length or sycophantic bias; the reward head is constrained to use only the causal component, while an adversarial head with gradient reversal discourages reward-relevant information in non-causal factors. Experimental results on mathematical and dialogue tasks show that this method learns more robust reward models, improves downstream RLHF performance over state-of-the-art baselines, and effectively mitigates reward hacking behaviors related to length and sycophantic bias.</div>
<div class="mono" style="margin-top:8px">本文针对基于人类反馈的强化学习中标准奖励模型易受与奖励无因果关系的虚假特征影响、导致奖励黑客攻击的问题，提出了一种分解式因果表征学习框架。该方法将上下文嵌入分解为足以预测奖励的因果因子和捕获无关属性（如长度或谄媚偏差）的非因果因子，并约束奖励头仅依赖于因果成分，同时通过带梯度反转的对抗头阻止非因果因子编码奖励相关信息。在数学和对话任务上的实验表明，该方法能学习到更稳健的奖励模型，持续提升下游RLHF性能并优于现有先进基线，对长度和谄媚偏差的分析进一步验证了其在缓解奖励黑客行为方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Improving Pretraining: using post-trained models to pretrain better models</div>
<div class="meta-line">Authors: Ellen Xiaoqing Tan, Shehzaad Dhuliawala, Jing Xu, Ping Yu, Sainbayar Sukhbaatar, Jason Weston, Olga Golovneva</div>
<div class="meta-line">First: 2026-01-29T07:09:30+00:00 · Latest: 2026-01-29T07:09:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21343v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21343v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ensuring safety, factuality and overall quality in the generations of large language models is a critical challenge, especially as these models are increasingly deployed in real-world applications. The prevailing approach to addressing these issues involves collecting expensive, carefully curated datasets and applying multiple stages of fine-tuning and alignment. However, even this complex pipeline cannot guarantee the correction of patterns learned during pretraining. Therefore, addressing these issues during pretraining is crucial, as it shapes a model&#x27;s core behaviors and prevents unsafe or hallucinated outputs from becoming deeply embedded. To tackle this issue, we introduce a new pretraining method that streams documents and uses reinforcement learning (RL) to improve the next K generated tokens at each step. A strong, post-trained model judges candidate generations -- including model rollouts, the original suffix, and a rewritten suffix -- for quality, safety, and factuality. Early in training, the process relies on the original and rewritten suffixes; as the model improves, RL rewards high-quality rollouts. This approach builds higher quality, safer, and more factual models from the ground up. In experiments, our method gives 36.2% and 18.5% relative improvements over standard pretraining in terms of factuality and safety, and up to 86.3% win rate improvements in overall generation quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自我改进式预训练：利用后训练模型预训练更优模型</div>
<div class="mono" style="margin-top:8px">确保大语言模型生成内容的安全性、事实性与整体质量是一项关键挑战，尤其在模型日益投入实际应用的背景下。当前主流解决方案依赖昂贵且精心标注的数据集，并进行多阶段微调与对齐，但即使如此复杂的流程仍无法修正预训练阶段习得的错误模式。因此，在预训练阶段解决这些问题至关重要，这能塑造模型的核心行为模式，防止不安全或虚构输出被深度固化。为此，我们提出一种新型预训练方法：通过流式文档处理，运用强化学习逐步优化后续K个生成标记。一个经过后训练的强模型将对候选生成内容（包括模型推演序列、原始后缀及改写后缀）进行质量、安全性与事实性评估。训练初期依赖原始与改写后缀；随着模型改进，强化学习将奖励高质量推演。该方法从底层构建了更优质、更安全、更符合事实的模型。实验表明，本方法在事实性与安全性指标上分别较标准预训练提升36.2%和18.5%，整体生成质量的胜率提升最高达86.3%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need to enhance the safety, factuality, and overall quality of large language model outputs, which are often compromised by patterns learned during pretraining and not fully corrected by later fine-tuning. To address this, the authors propose a self-improving pretraining method that uses reinforcement learning to optimize token generation, where a post-trained model evaluates candidate outputs—including model rollouts, original text, and rewritten text—based on quality, safety, and factuality, gradually shifting reliance from human-provided text to model-generated rollouts as training progresses. Experimental results show that this approach yields relative improvements of 36.2% in factuality and 18.5% in safety over standard pretraining, along with up to 86.3% win rate gains in overall generation quality.</div>
<div class="mono" style="margin-top:8px">本文的动机是提升大语言模型生成内容的安全性、事实性和整体质量，这些问题常源于预训练阶段学习到的模式，且难以通过后续微调完全纠正。为此，作者提出一种自改进的预训练方法，采用强化学习优化令牌生成，由一个后训练模型根据质量、安全性和事实性评估候选输出——包括模型自生成序列、原始文本和改写文本，并随着训练进展逐步从依赖人工文本转向模型自生成序列。实验结果表明，该方法在事实性和安全性上相比标准预训练分别实现了36.2%和18.5%的相对提升，且在整体生成质量上获得了高达86.3%的胜率改进。</div>
</details>
</div>
<div class="card">
<div class="title">Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach</div>
<div class="meta-line">Authors: Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li, Chung Shue Chen, Man-On Pun</div>
<div class="meta-line">First: 2026-01-29T06:26:16+00:00 · Latest: 2026-01-29T06:26:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21316v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21316v1">PDF</a> · <a href="https://github.com/Traffic-Alpha/UAGMC">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic exploration.To address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at https://github.com/Traffic-Alpha/UAGMC.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向按需空中出租车服务的异构垂直起降场选择优化：一种深度强化学习方法</div>
<div class="mono" style="margin-top:8px">城市空中交通（UAM）作为一种变革性解决方案，通过利用低空空域缓解城市拥堵，从而减轻地面交通网络压力。为实现真正高效无缝的门到门出行体验，UAM需与现有地面交通基础设施紧密融合。然而，当前针对空陆联运系统中乘客最优集成路径策略的研究仍显不足，缺乏系统性探索。为填补这一空白，我们首先提出一个统一优化模型，集成空陆交通策略选择。该模型捕捉多式联运网络的动态特性，并结合实时交通状况与乘客决策行为。基于此模型，我们提出统一空陆交通协调框架，利用深度强化学习和车联网通信技术优化垂直起降场选择并动态规划空中出租车路线。实验结果表明，相较于传统比例分配方法，该框架使平均出行时间减少34%，提升了整体出行效率，并为多式联运系统的集成优化提供了新思路。本研究通过协调空陆交通模式，为推进智能城市出行解决方案奠定了坚实基础。相关代码可在 https://github.com/Traffic-Alpha/UAGMC 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to integrate Urban Air Mobility with ground transport for seamless door-to-door travel, this paper proposes a unified optimization model and a Unified Air-Ground Mobility Coordination framework that uses deep reinforcement learning and V2X communication to optimize vertiport selection and air taxi routing. The method dynamically accounts for real-time traffic and passenger behavior in multimodal networks. Experimental results show the framework reduces average travel time by 34% compared to conventional proportional allocation methods, significantly improving travel efficiency and offering insights for multimodal system integration.</div>
<div class="mono" style="margin-top:8px">为将城市空中交通与地面交通整合以实现无缝门到门出行，本文提出了一个统一优化模型和基于深度强化学习与车联网通信的统一空地交通协调框架，以优化垂直起降机场选择和空中出租车路线规划。该方法动态考虑了多式联运网络中的实时交通和乘客行为。实验结果表明，与传统比例分配方法相比，该框架将平均出行时间减少了34%，显著提升了出行效率，并为多式联运系统集成优化提供了新见解。</div>
</details>
</div>
<div class="card">
<div class="title">Few-Shot Learning for Dynamic Operations of Automated Electric Taxi Fleets under Evolving Charging Infrastructure: A Meta-Deep Reinforcement Learning Approach</div>
<div class="meta-line">Authors: Xiaozhuang Li, Xindi Tang, Fang He</div>
<div class="meta-line">First: 2026-01-29T06:16:34+00:00 · Latest: 2026-01-29T06:16:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21312v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21312v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rapid expansion of electric vehicles (EVs) and charging infrastructure, the effective management of Autonomous Electric Taxi (AET) fleets faces a critical challenge in environments with dynamic and uncertain charging availability. While most existing research assumes a static charging network, this simplification creates a significant gap between theoretical models and real-world operations. To bridge this gap, we propose GAT-PEARL, a novel meta-reinforcement learning framework that learns an adaptive operational policy. Our approach integrates a graph attention network (GAT) to effectively extract robust spatial representations under infrastructure layouts and model the complex spatiotemporal relationships of the urban environment, and employs probabilistic embeddings for actor-critic reinforcement learning (PEARL) to enable rapid, inference-based adaptation to changes in charging network layouts without retraining. Through extensive simulations on real-world data in Chengdu, China, we demonstrate that GAT-PEARL significantly outperforms conventional reinforcement learning baselines, showing superior generalization to unseen infrastructure layouts and achieving higher overall operational efficiency in dynamic settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向充电基础设施演化的自动化电动出租车车队动态运营：一种元深度强化学习方法</div>
<div class="mono" style="margin-top:8px">随着电动汽车及充电基础设施的快速扩张，在充电可用性动态不确定的环境中，自动驾驶电动出租车（AET）车队的有效管理面临关键挑战。现有研究多假设静态充电网络，这种简化导致理论模型与实际运营存在显著差距。为弥合此差距，我们提出GAT-PEARL——一种学习自适应运营策略的新型元强化学习框架。该方法集成图注意力网络（GAT），以有效提取基础设施布局下的稳健空间表征并建模城市环境的复杂时空关系；同时采用基于概率嵌入的行动者-评论家强化学习（PEARL），无需重新训练即可实现基于推理的充电网络布局快速适应。通过在成都真实数据上的大量仿真实验，我们证明GAT-PEARL显著优于传统强化学习基线，对未见过的基础设施布局展现出卓越的泛化能力，并在动态环境中实现了更高的整体运营效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study is motivated by the challenge of managing autonomous electric taxi fleets in dynamic urban environments where charging infrastructure availability evolves, a gap left by existing models that assume static networks. The proposed method, GAT-PEARL, combines a graph attention network to capture robust spatial representations and spatiotemporal relationships with probabilistic embeddings for actor-critic reinforcement learning, enabling rapid adaptation to new charging layouts without retraining. Experimental results from simulations on real-world Chengdu data show that GAT-PEARL outperforms conventional reinforcement learning baselines, demonstrating superior generalization to unseen infrastructure layouts and higher operational efficiency in dynamic settings.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决自动驾驶电动出租车车队在充电基础设施动态变化的城市环境中管理的挑战，现有模型通常假设静态网络，存在理论与现实的差距。提出的GAT-PEARL方法结合了图注意力网络以提取稳健的空间表示和时空关系，并采用概率嵌入的演员-评论家强化学习，实现无需重新训练即可快速适应新充电布局。基于中国成都真实数据的模拟实验结果表明，GAT-PEARL显著优于传统强化学习基线，在未见过的基础设施布局中表现出更强的泛化能力，并在动态环境中实现了更高的整体运营效率。</div>
</details>
</div>
<div class="card">
<div class="title">The Surprising Difficulty of Search in Model-Based Reinforcement Learning</div>
<div class="meta-line">Authors: Wei-Di Chang, Mikael Henaff, Brandon Amos, Gregory Dudek, Scott Fujimoto</div>
<div class="meta-line">First: 2026-01-29T05:58:24+00:00 · Latest: 2026-01-29T05:58:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21306v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21306v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper investigates search in model-based reinforcement learning (RL). Conventional wisdom holds that long-term predictions and compounding errors are the primary obstacles for model-based RL. We challenge this view, showing that search is not a plug-and-play replacement for a learned policy. Surprisingly, we find that search can harm performance even when the model is highly accurate. Instead, we show that mitigating distribution shift matters more than improving model or value function accuracy. Building on this insight, we identify key techniques for enabling effective search, achieving state-of-the-art performance across multiple popular benchmark domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于模型的强化学习中搜索的意外困难</div>
<div class="mono" style="margin-top:8px">本文研究基于模型的强化学习（RL）中的搜索问题。传统观点认为长期预测和误差累积是基于模型RL的主要障碍。我们挑战了这一观点，指出搜索并非学习策略的即插即用替代方案。令人惊讶的是，即使模型精度很高，搜索也可能损害性能。相反，我们证明缓解分布偏移比提升模型或价值函数精度更为关键。基于这一洞见，我们提出了实现有效搜索的关键技术，在多个主流基准领域取得了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper challenges the conventional view that long-term prediction errors are the main challenge in model-based reinforcement learning, arguing instead that effectively integrating search with a learned policy is surprisingly difficult and can even degrade performance despite an accurate model. The authors propose that mitigating distribution shift during search is more critical than solely improving model or value function accuracy. Their experimental results demonstrate that by identifying and applying key techniques to address this shift, their method achieves state-of-the-art performance across several benchmark domains.</div>
<div class="mono" style="margin-top:8px">本文挑战了传统观点，即长期预测误差是基于模型的强化学习的主要难点，转而指出将搜索与学习策略有效结合异常困难，即使模型高度准确，搜索也可能损害性能。作者提出，缓解搜索过程中的分布偏移比单纯提高模型或价值函数精度更为关键。实验结果表明，通过识别并应用关键技术来解决这种偏移，他们的方法在多个流行基准领域实现了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Deep GraphRAG: A Balanced Approach to Hierarchical Retrieval and Adaptive Integration</div>
<div class="meta-line">Authors: Yuejie Li, Ke Yang, Tao Wang, Bolin Chen, Bowen Li, Chengjun Mao</div>
<div class="meta-line">First: 2026-01-16T10:02:31+00:00 · Latest: 2026-01-29T05:46:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11144v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.11144v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Graph-based Retrieval-Augmented Generation (GraphRAG) frameworks face a trade-off between the comprehensiveness of global search and the efficiency of local search. Existing methods are often challenged by navigating large-scale hierarchical graphs, optimizing retrieval paths, and balancing exploration-exploitation dynamics, frequently lacking robust multi-stage re-ranking. To overcome these deficits, we propose Deep GraphRAG, a framework designed for a balanced approach to hierarchical retrieval and adaptive integration. It introduces a hierarchical global-to-local retrieval strategy that integrates macroscopic inter-community and microscopic intra-community contextual relations. This strategy employs a three-stage process: (1) inter-community filtering, which prunes the search space using local context; (2) community-level refinement, which prioritizes relevant subgraphs via entity-interaction analysis; and (3) entity-level fine-grained search within target communities. A beam search-optimized dynamic re-ranking module guides this process, continuously filtering candidates to balance efficiency and global comprehensiveness. Deep GraphRAG also features a Knowledge Integration Module leveraging a compact LLM, trained with Dynamic Weighting Reward GRPO (DW-GRPO). This novel reinforcement learning approach dynamically adjusts reward weights to balance three key objectives: relevance, faithfulness, and conciseness. This training enables compact models (1.5B) to approach the performance of large models (70B) in the integration task. Evaluations on Natural Questions and HotpotQA demonstrate that Deep GraphRAG significantly outperforms baseline graph retrieval methods in both accuracy and efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度图检索增强生成：一种层次化检索与自适应融合的平衡方法</div>
<div class="mono" style="margin-top:8px">基于图的检索增强生成（GraphRAG）框架面临全局检索全面性与局部检索效率之间的权衡。现有方法在导航大规模层次化图结构、优化检索路径、平衡探索-利用动态性方面常遇挑战，且多缺乏鲁棒的多阶段重排序机制。为克服这些不足，本文提出深度图检索增强生成（Deep GraphRAG）框架，旨在实现层次化检索与自适应融合的平衡。该框架引入从全局到局部的层次化检索策略，整合宏观的社区间关联与微观的社区内语境关系。该策略采用三阶段流程：（1）社区间过滤，利用局部语境剪枝搜索空间；（2）社区级优化，通过实体交互分析对相关子图进行优先级排序；（3）目标社区内的实体级细粒度搜索。通过集束搜索优化的动态重排序模块引导此过程，持续筛选候选结果以平衡效率与全局覆盖性。Deep GraphRAG 还配备知识融合模块，采用经动态加权奖励GRPO（DW-GRPO）训练的紧凑大语言模型。这种新颖的强化学习方法动态调整奖励权重，以平衡相关性、忠实度与简洁性三大目标。该训练使紧凑模型（1.5B参数）在融合任务中接近大模型（70B参数）的性能。在Natural Questions和HotpotQA数据集上的评估表明，Deep GraphRAG在准确性与效率上均显著优于基线图检索方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the trade-off between global comprehensiveness and local efficiency in existing GraphRAG frameworks, which struggle with navigating hierarchical graphs and lack robust multi-stage re-ranking, this paper proposes Deep GraphRAG. The method introduces a hierarchical global-to-local retrieval strategy with a three-stage process—inter-community filtering, community-level refinement, and entity-level fine-grained search—guided by a beam search-optimized dynamic re-ranking module to balance efficiency and coverage. It also includes a Knowledge Integration Module trained with a novel reinforcement learning approach, Dynamic Weighting Reward GRPO, which dynamically adjusts rewards for relevance, faithfulness, and conciseness, enabling compact 1.5B models to approach the performance of 70B models. Experimental results on Natural Questions and HotpotQA show that Deep GraphRAG significantly outperforms baseline graph retrieval methods in both accuracy and efficiency.</div>
<div class="mono" style="margin-top:8px">针对现有基于图的检索增强生成框架在全局搜索全面性与局部搜索效率之间存在权衡、难以导航大规模层次图且缺乏鲁棒多阶段重排序的挑战，本文提出了Deep GraphRAG。该方法采用一种分层全局到局部的检索策略，包含三个阶段：社区间过滤、社区级细化和社区内实体级细粒度搜索，并通过基于束搜索优化的动态重排序模块引导，以平衡效率与覆盖范围。此外，它引入了一个知识整合模块，采用新颖的强化学习方法——动态加权奖励GRPO进行训练，动态调整相关性、忠实性和简洁性的奖励权重，使紧凑的1.5B参数模型在整合任务上能接近70B大模型的性能。在Natural Questions和HotpotQA数据集上的评估表明，Deep GraphRAG在准确性和效率上均显著优于基线图检索方法。</div>
</details>
</div>
<div class="card">
<div class="title">EGAM: Extended Graph Attention Model for Solving Routing Problems</div>
<div class="meta-line">Authors: Licheng Wang, Yuzi Yan, Mingtao Huang, Yuan Shen</div>
<div class="meta-line">First: 2026-01-29T05:30:34+00:00 · Latest: 2026-01-29T05:30:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21281v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21281v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural combinatorial optimization (NCO) solvers, implemented with graph neural networks (GNNs), have introduced new approaches for solving routing problems. Trained with reinforcement learning (RL), the state-of-the-art graph attention model (GAM) achieves near-optimal solutions without requiring expert knowledge or labeled data. In this work, we generalize the existing graph attention mechanism and propose the extended graph attention model (EGAM). Our model utilizes multi-head dot-product attention to update both node and edge embeddings, addressing the limitations of the conventional GAM, which considers only node features. We employ an autoregressive encoder-decoder architecture and train it with policy gradient algorithms that incorporate a specially designed baseline. Experiments show that EGAM matches or outperforms existing methods across various routing problems. Notably, the proposed model demonstrates exceptional performance on highly constrained problems, highlighting its efficiency in handling complex graph structures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EGAM：用于求解路径规划问题的扩展图注意力模型</div>
<div class="mono" style="margin-top:8px">基于图神经网络（GNN）实现的神经组合优化（NCO）求解器为路径规划问题提供了新方法。通过强化学习（RL）训练，当前最先进的图注意力模型（GAM）无需专家知识或标注数据即可获得近似最优解。本研究对现有图注意力机制进行泛化，提出扩展图注意力模型（EGAM）。该模型采用多头点积注意力机制同时更新节点与边嵌入，克服了传统GAM仅考虑节点特征的局限性。我们采用自回归编码器-解码器架构，并配合融入特殊设计基线的策略梯度算法进行训练。实验表明，EGAM在多种路径规划问题上均达到或超越现有方法。特别值得注意的是，该模型在强约束问题上展现出卓越性能，凸显了其处理复杂图结构的高效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to enhance neural combinatorial optimization for routing problems beyond the limitations of existing graph attention models that focus solely on node features, this work proposes the Extended Graph Attention Model (EGAM). The method generalizes the graph attention mechanism by employing multi-head dot-product attention to update both node and edge embeddings within an autoregressive encoder-decoder architecture, trained via policy gradient algorithms with a custom baseline. Experimental results demonstrate that EGAM matches or surpasses existing methods across various routing problems, showing particularly strong performance on highly constrained problems, which underscores its efficiency in managing complex graph structures.</div>
<div class="mono" style="margin-top:8px">本研究旨在改进神经组合优化求解路由问题的能力，针对现有图注意力模型仅考虑节点特征的局限性，提出了扩展图注意力模型（EGAM）。该方法通过采用多头点积注意力机制，在自回归编码器-解码器架构中同时更新节点和边嵌入，并使用带有定制基线的策略梯度算法进行训练。实验结果表明，EGAM在多种路由问题上达到或超越了现有方法，尤其在高度约束问题上表现出色，凸显了其处理复杂图结构的高效性。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning from Meta-Evaluation: Aligning Language Models Without Ground-Truth Labels</div>
<div class="meta-line">Authors: Micah Rentschler, Jesse Roberts</div>
<div class="meta-line">First: 2026-01-29T05:02:08+00:00 · Latest: 2026-01-29T05:02:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21268v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21268v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most reinforcement learning (RL) methods for training large language models (LLMs) require ground-truth labels or task-specific verifiers, limiting scalability when correctness is ambiguous or expensive to obtain. We introduce Reinforcement Learning from Meta-Evaluation (RLME), which optimizes a generator using reward derived from an evaluator&#x27;s answers to natural-language meta-questions (e.g., &quot;Is the answer correct?&quot; or &quot;Is the reasoning logically consistent?&quot;). RLME treats the evaluator&#x27;s probability of a positive judgment as a reward and updates the generator via group-relative policy optimization, enabling learning without labels. Across a suite of experiments, we show that RLME achieves accuracy and sample efficiency comparable to label-based training, enables controllable trade-offs among multiple objectives, steers models toward reliable reasoning patterns rather than post-hoc rationalization, and generalizes to open-domain settings where ground-truth labels are unavailable, broadening the domains in which LLMs may be trained with RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于元评估的强化学习：无需真实标签的语言模型对齐方法</div>
<div class="mono" style="margin-top:8px">现有训练大语言模型（LLM）的强化学习方法大多依赖真实标签或任务特定验证器，当答案正确性难以判定或标注成本高昂时，其可扩展性受限。本文提出基于元评估的强化学习（RLME），该方法通过评估者对自然语言元问题（如“答案是否正确？”或“推理是否逻辑一致？”）的反馈生成奖励信号，并利用组相对策略优化更新生成器，实现无标签学习。实验表明：RLME在准确率和样本效率上达到与基于标签训练相当的水平；支持多目标可控权衡；引导模型形成可靠推理模式而非事后合理化；在缺乏真实标签的开放域场景中仍具泛化能力，从而拓展了强化学习在LLM训练中的应用领域。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to overcome the scalability limitations of traditional reinforcement learning methods for large language models, which often rely on ground-truth labels or task-specific verifiers that are costly or ambiguous to obtain. The proposed method, Reinforcement Learning from Meta-Evaluation (RLME), optimizes a generator by using rewards derived from an evaluator&#x27;s responses to natural-language meta-questions, such as assessing correctness or logical consistency, and applies group-relative policy optimization to enable learning without labels. Experimental results demonstrate that RLME achieves accuracy and sample efficiency comparable to label-based training, allows controllable trade-offs among multiple objectives, steers models toward reliable reasoning patterns instead of post-hoc rationalization, and generalizes effectively to open-domain settings where ground-truth labels are unavailable.</div>
<div class="mono" style="margin-top:8px">本研究的动机是克服传统强化学习方法在大语言模型训练中的可扩展性限制，这些方法通常依赖于难以获取或模糊的真实标签或任务特定验证器。提出的方法——元评估强化学习（RLME）——通过使用评估者对自然语言元问题（如判断答案正确性或逻辑一致性）回答的概率作为奖励，并采用组相对策略优化来优化生成器，从而实现无需标签的学习。实验结果表明，RLME在准确性和样本效率上与基于标签的训练相当，支持多目标间的可控权衡，引导模型形成可靠的推理模式而非事后合理化，并能泛化到缺乏真实标签的开放领域场景中。</div>
</details>
</div>
<div class="card">
<div class="title">Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification</div>
<div class="meta-line">Authors: Yiju Guo, Tianyi Hu, Zexu Sun, Yankai Lin</div>
<div class="meta-line">First: 2026-01-29T04:08:24+00:00 · Latest: 2026-01-29T04:08:24+00:00</div>
<div class="meta-line">Comments: Work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21244v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21244v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remains constrained by inefficient exploration under limited rollout budgets, leading to low sampling success and unstable training in complex tasks. We find that many exploration failures arise not from problem difficulty, but from a small number of prompt tokens that introduce interference. Building on this insight, we propose the Less Noise Sampling Framework (LENS), which first prompts by identifying and removing interference tokens. then transfers successful rollouts from the purification process to supervise policy optimization on the original noisy prompts, enabling the model to learn to ignore interference in the real-world, noisy prompting settings. Experimental results show that LENS significantly outperforms GRPO, delivering higher performance and faster convergence, with a 3.88% average gain and over 1.6$\times$ speedup. Our work highlights the critical role of pruning interference tokens in improving rollout efficiency, offering a new perspective for RLVR research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>降噪增声：基于指令净化的强化学习推理方法</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）虽推动了大型语言模型的推理能力，但在有限计算预算下仍受低效探索的制约，导致复杂任务中采样成功率低、训练不稳定。研究发现，许多探索失败并非源于问题本身难度，而是由少数引入干扰的提示词元所致。基于此，本文提出降噪采样框架（LENS），其首先通过识别并移除干扰词元进行提示净化，随后将净化过程中成功的轨迹迁移至原始含噪提示上以监督策略优化，使模型学会在实际含噪提示环境中忽略干扰。实验表明，LENS显著优于GRPO，实现了更高性能与更快收敛，平均提升3.88%，加速超1.6倍。本研究揭示了剪除干扰词元对提升轨迹效率的关键作用，为RLVR研究提供了新视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the inefficiency of Reinforcement Learning with Verifiable Rewards (RLVR) in complex reasoning tasks, where limited rollout budgets lead to poor exploration and unstable training. The authors identify that many failures stem not from task difficulty but from a few interfering tokens in the prompts, motivating the proposed Less Noise Sampling Framework (LENS). LENS first purifies prompts by removing interference tokens to generate successful rollouts, then uses these to supervise policy optimization on the original noisy prompts, teaching the model to ignore interference. Experiments demonstrate that LENS outperforms GRPO with an average performance gain of 3.88% and achieves over 1.6× faster convergence, highlighting the importance of pruning interference tokens for efficient RLVR.</div>
<div class="mono" style="margin-top:8px">本研究针对可验证奖励的强化学习在复杂推理任务中因探索效率低、训练不稳定而受限的问题，发现许多失败源于提示中少数干扰令牌而非任务本身。为此，作者提出了低噪声采样框架，其核心方法是先净化提示以移除干扰令牌并生成成功轨迹，再利用这些轨迹监督模型在原始噪声提示上的策略优化，从而学会忽略干扰。实验结果表明，该框架显著优于GRPO方法，平均性能提升3.88%，收敛速度加快1.6倍以上，强调了修剪干扰令牌对提升强化学习效率的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">Intelli-Planner: Towards Customized Urban Planning via Large Language Model Empowered Reinforcement Learning</div>
<div class="meta-line">Authors: Xixian Yong, Peilin Sun, Zihe Wang, Xiao Zhou</div>
<div class="meta-line">Venue: The Web Conference 2026</div>
<div class="meta-line">First: 2026-01-29T03:23:40+00:00 · Latest: 2026-01-29T03:23:40+00:00</div>
<div class="meta-line">Comments: The Web Conference 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21212v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21212v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Effective urban planning is crucial for enhancing residents&#x27; quality of life and ensuring societal stability, playing a pivotal role in the sustainable development of cities. Current planning methods heavily rely on human experts, which are time-consuming and labor-intensive, or utilize deep learning algorithms, often limiting stakeholder involvement. To bridge these gaps, we propose Intelli-Planner, a novel framework integrating Deep Reinforcement Learning (DRL) with large language models (LLMs) to facilitate participatory and customized planning scheme generation. Intelli-Planner utilizes demographic, geographic data, and planning preferences to determine high-level planning requirements and demands for each functional type. During training, a knowledge enhancement module is employed to enhance the decision-making capability of the policy network. Additionally, we establish a multi-dimensional evaluation system and employ LLM-based stakeholders for satisfaction scoring. Experimental validation across diverse urban settings shows that Intelli-Planner surpasses traditional baselines and achieves comparable performance to state-of-the-art DRL-based methods in objective metrics, while enhancing stakeholder satisfaction and convergence speed. These findings underscore the effectiveness and superiority of our framework, highlighting the potential for integrating the latest advancements in LLMs with DRL approaches to revolutionize tasks related to functional areas planning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Intelli-Planner：基于大语言模型增强强化学习的定制化城市规划</div>
<div class="mono" style="margin-top:8px">有效的城市规划对提升居民生活质量、保障社会稳定性至关重要，在城市可持续发展中扮演关键角色。现有规划方法严重依赖人工专家，耗时耗力，或采用深度学习算法，常限制利益相关方参与。为弥补这些不足，我们提出Intelli-Planner——一种融合深度强化学习与大语言模型的新型框架，以促进参与式、定制化规划方案的生成。该框架利用人口、地理数据及规划偏好，确定各类功能区域的高层规划需求。训练过程中采用知识增强模块提升策略网络的决策能力，并建立多维评估体系，借助基于大语言模型的利益相关方进行满意度评分。多场景城市环境实验表明，Intelli-Planner在客观指标上超越传统基线方法，与前沿深度强化学习方法性能相当，同时显著提升利益相关方满意度与收敛速度。这些发现印证了框架的有效性与优越性，凸显了大语言模型最新进展与深度强化学习融合在功能区域规划任务中的革新潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to overcome the time-consuming, expert-dependent nature of traditional urban planning and the limited stakeholder involvement in deep learning approaches, this paper introduces Intelli-Planner, a framework that integrates Deep Reinforcement Learning (DRM) with large language models (LLMs) to generate participatory and customized urban plans. The method processes demographic, geographic, and preference data to define planning requirements, employs a knowledge enhancement module to improve policy network decisions, and uses LLM-based stakeholders within a multi-dimensional evaluation system for satisfaction scoring. Experimental results across various urban settings demonstrate that Intelli-Planner outperforms traditional baselines, matches state-of-the-art DRL methods in objective metrics, and improves stakeholder satisfaction and convergence speed, highlighting the potential of combining LLMs and DRL for functional area planning.</div>
<div class="mono" style="margin-top:8px">针对传统城市规划方法依赖专家、耗时费力，以及深度学习算法中利益相关者参与不足的问题，本文提出了Intelli-Planner框架，通过将深度强化学习与大型语言模型相结合，以生成参与式、定制化的城市规划方案。该方法利用人口、地理数据和规划偏好来确定各类功能区域的高层需求，在训练中采用知识增强模块提升策略网络的决策能力，并基于大型语言模型构建利益相关者进行多维评估与满意度评分。在不同城市环境下的实验验证表明，Intelli-Planner超越了传统基线方法，在客观指标上与先进的深度强化学习方法性能相当，同时提高了利益相关者满意度和收敛速度，彰显了大型语言模型与深度强化学习结合在功能区规划任务中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">When should I search more: Adaptive Complex Query Optimization with Reinforcement Learning</div>
<div class="meta-line">Authors: Wei Wen, Sihang Deng, Tianjun Wei, Keyu Chen, Ruizhi Qiao, Xing Sun</div>
<div class="meta-line">First: 2026-01-29T03:16:53+00:00 · Latest: 2026-01-29T03:16:53+00:00</div>
<div class="meta-line">Comments: 16 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21208v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21208v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Query optimization is a crucial component for the efficacy of Retrieval-Augmented Generation (RAG) systems. While reinforcement learning (RL)-based agentic and reasoning methods have recently emerged as a promising direction on query optimization, most existing approaches focus on the expansion and abstraction of a single query. However, complex user queries are prevalent in real-world scenarios, often requiring multiple parallel and sequential search strategies to handle disambiguation and decomposition. Directly applying RL to these complex cases introduces significant hurdles. Determining the optimal number of sub-queries and effectively re-ranking and merging retrieved documents vastly expands the search space and complicates reward design, frequently leading to training instability. To address these challenges, we propose a novel RL framework called Adaptive Complex Query Optimization (ACQO). Our framework is designed to adaptively determine when and how to expand the search process. It features two core components: an Adaptive Query Reformulation (AQR) module that dynamically decides when to decompose a query into multiple sub-queries, and a Rank-Score Fusion (RSF) module that ensures robust result aggregation and provides stable reward signals for the learning agent. To mitigate training instabilities, we adopt a Curriculum Reinforcement Learning (CRL) approach, which stabilizes the training process by progressively introducing more challenging queries through a two-stage strategy. Our comprehensive experiments demonstrate that ACQO achieves state-of-the-art performance on three complex query benchmarks, significantly outperforming established baselines. The framework also showcases improved computational efficiency and broad compatibility with different retrieval architectures, establishing it as a powerful and generalizable solution for next-generation RAG systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>何时应扩大搜索：基于强化学习的自适应复杂查询优化</div>
<div class="mono" style="margin-top:8px">查询优化是提升检索增强生成（RAG）系统效能的关键环节。尽管基于强化学习（RL）的智能体推理方法近期成为查询优化的新兴方向，现有研究多聚焦于单一查询的扩展与抽象。然而现实场景中普遍存在复杂用户查询，常需并行与串行相结合的搜索策略以处理歧义消解与问题分解。将RL直接应用于此类复杂场景面临显著挑战：确定最优子查询数量、对检索文档进行有效重排与融合，均会急剧扩大搜索空间并增加奖励函数设计难度，易导致训练不稳定。为此，我们提出名为自适应复杂查询优化（ACQO）的新型RL框架。该框架能自适应决策搜索过程的扩展时机与方式，其核心包含两个模块：自适应查询重构（AQR）模块动态决定何时将查询分解为多个子查询；排序-分数融合（RSF）模块确保稳健的结果聚合并为智能体提供稳定奖励信号。为缓解训练不稳定问题，我们采用课程强化学习（CRL）方法，通过两阶段策略逐步引入复杂查询以稳定训练过程。综合实验表明，ACQO在三个复杂查询基准测试中达到最先进性能，显著超越现有基线。该框架同时展现出更优的计算效率及对不同检索架构的广泛兼容性，为下一代RAG系统提供了强大且可泛化的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of optimizing complex user queries in Retrieval-Augmented Generation (RAG) systems, where existing reinforcement learning (RL) methods often struggle with determining the optimal number of sub-queries and effectively merging results, leading to training instability. The authors propose the Adaptive Complex Query Optimization (ACQO) framework, which features an Adaptive Query Reformulation module to dynamically decide when to decompose queries and a Rank-Score Fusion module for robust result aggregation, stabilized through a Curriculum Reinforcement Learning training strategy. Experimental results on three benchmarks show that ACQO achieves state-of-the-art performance, outperforming baselines while improving computational efficiency and demonstrating compatibility with various retrieval architectures.</div>
<div class="mono" style="margin-top:8px">本文针对检索增强生成（RAG）系统中复杂用户查询优化的挑战，现有强化学习方法常在确定最优子查询数量和有效合并结果时遇到困难，导致训练不稳定。作者提出了自适应复杂查询优化（ACQO）框架，其核心包括自适应查询重构模块以动态决定何时分解查询，以及排序-分数融合模块用于稳健的结果聚合，并通过课程强化学习策略稳定训练过程。在三个基准测试上的实验结果表明，ACQO实现了最先进的性能，显著优于现有基线，同时提高了计算效率，并展现出与不同检索架构的广泛兼容性。</div>
</details>
</div>
<div class="card">
<div class="title">Do Reasoning Models Enhance Embedding Models?</div>
<div class="meta-line">Authors: Wun Yu Chan, Shaojin Chen, Huihao Jing, Kwun Hang Lau, Elton Chun-Chai Li, Zihao Wang, Haoran Li, Yangqiu Song</div>
<div class="meta-line">First: 2026-01-29T02:48:34+00:00 · Latest: 2026-01-29T02:48:34+00:00</div>
<div class="meta-line">Comments: 10 main pages, 18 appendix pages, 13 figures, 11 tables, 4 prompts</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21192v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21192v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold&#x27;s local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>推理模型能否增强嵌入模型？</div>
<div class="mono" style="margin-top:8px">当前最先进的嵌入模型越来越多地源自仅解码器的大型语言模型（LLM）主干，并通过对比学习进行适配。随着通过可验证奖励强化学习（RLVR）训练出的推理模型的出现，一个自然的问题随之产生：当这些模型作为嵌入初始化时，增强的推理能力是否会转化为更优的语义表示？与预期相反，我们在MTEB和BRIGHT上的评估显示**零效应**：从RLVR调优主干初始化的嵌入模型，在采用相同训练方案时，并未表现出相对于基础模型的持续性能优势。为解析这一悖论，我们提出了**层次化表示相似性分析（HRSA）**框架，该框架从表示、几何和功能三个层面分解相似性。HRSA揭示：虽然RLVR会引发潜在流形局部几何的不可逆重组和可逆坐标基漂移，但它保留了全局流形几何和线性读出能力。因此，后续的对比学习会驱动基础模型与推理初始化模型之间产生强对齐，这一现象我们称之为**流形重对齐**。实证表明，与监督微调（SFT）不同，RLVR是在现有语义景观内优化轨迹，而非从根本上重构景观本身。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates whether reasoning models, specifically those trained via Reinforcement Learning with Verifiable Rewards (RLVR), provide superior semantic representations when used as initializations for embedding models compared to their base LLM counterparts. The method involves evaluating embedding models derived from RLVR-tuned backbones on standard benchmarks (MTEB and BRIGHT) using identical contrastive training recipes and introducing a novel analytical framework called Hierarchical Representation Similarity Analysis (HRSA) to dissect representation changes. The main experimental results reveal a null effect: RLVR-initialized models show no consistent performance gain over base models, and HRSA analysis demonstrates that RLVR primarily induces reversible local geometric changes and coordinate drift while preserving the global semantic manifold, allowing subsequent contrastive training to realign the models, a process termed Manifold Realignment.</div>
<div class="mono" style="margin-top:8px">本文探讨了通过可验证奖励强化学习（RLVR）训练得到的推理模型，在作为嵌入模型初始化时，是否比其基础大语言模型能提供更优的语义表示。研究方法包括在标准基准（MTEB和BRIGHT）上使用相同的对比学习配方评估基于RLVR调优骨干的嵌入模型，并引入一种称为层次表示相似性分析（HRSA）的新框架来剖析表示变化。主要实验结果表明存在零效应：RLVR初始化的模型相比基础模型没有一致的性能优势；HRSA分析揭示RLVR主要引发可逆的局部几何重组和坐标基漂移，同时保留了全局语义流形结构，使得后续的对比学习能够驱动模型重新对齐，这一过程被称为流形重对齐。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Orchestrate Agents in Natural Language with the Conductor</div>
<div class="meta-line">Authors: Stefan Nielsen, Edoardo Cetin, Peter Schwendeman, Qi Sun, Jinglue Xu, Yujin Tang</div>
<div class="meta-line">First: 2025-12-04T02:23:13+00:00 · Latest: 2026-01-29T02:24:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.04388v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.04388v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Powerful large language models (LLMs) from different providers have been expensively trained and finetuned to specialize across varying domains. In this work, we introduce a new kind of Conductor model trained with reinforcement learning to automatically discover powerful coordination strategies among LLMs. Our Conductor learns not only to design targeted communication topologies for effective agent-to-agent collaboration, but also to prompt engineer focused instructions to the LLMs to maximally leverage their individual capabilities. We show that, by learning optimal coordination strategies over pools of powerful worker LLMs, a 7B Conductor achieves significant performance gains beyond any individual worker, attaining state-of-the-art results in challenging reasoning benchmarks, such as LiveCodeBench and GPQA. By training with randomized agent pools, our conductor effectively adapts to arbitrary sets of open- and closed-source agents, meeting any user requirements. Furthermore, allowing the Conductor to select itself as a worker gives rise to recursive topologies, elevating performance with a new form of dynamic test-time scaling through online iterative adaptation. More broadly, ours is among the early work demonstrating language model coordination can be unlocked through RL, where powerful coordination strategies emerge naturally in LLMs through pure end-to-end reward maximization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习使用指挥者模型以自然语言编排智能体</div>
<div class="mono" style="margin-top:8px">来自不同供应商的强大大型语言模型（LLM）经过昂贵的训练和微调，已在多个领域实现专业化。本研究引入一种新型指挥者模型，通过强化学习训练，能自动发现LLM间的有效协调策略。该模型不仅学习设计针对性的通信拓扑以促进智能体间高效协作，还通过提示工程向LLM生成聚焦指令，以最大限度发挥其个体能力。实验表明，通过学习对强大工作LLM池的最优协调策略，一个70亿参数的指挥者模型能显著超越任何单一工作模型，在LiveCodeBench和GPQA等复杂推理基准测试中达到最先进水平。通过随机化智能体池训练，指挥者能灵活适配任意开源或闭源智能体组合，满足多样化用户需求。此外，允许指挥者自身作为工作单元可形成递归拓扑结构，通过在线迭代适应的动态测试时扩展机制进一步提升性能。本研究是早期通过强化学习解锁语言模型协调能力的探索之一，证明强大的协调策略可通过纯端到端奖励最大化在LLM中自然涌现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation is to enhance collaboration among specialized large language models (LLMs) from different providers by learning effective coordination strategies. The method introduces a Conductor model trained with reinforcement learning to automatically design communication topologies and engineer prompts for LLM agents, optimizing their collective performance. Main experimental results show that a 7B Conductor, by orchestrating pools of worker LLMs, achieves state-of-the-art results on challenging reasoning benchmarks like LiveCodeBench and GPQA, outperforming any individual worker, and it adapts to diverse agent sets while enabling recursive topologies for dynamic test-time scaling.</div>
<div class="mono" style="margin-top:8px">该研究的动机是通过学习有效的协调策略来增强来自不同提供商的专用大语言模型之间的协作。方法上引入了一个通过强化学习训练的指挥者模型，自动设计通信拓扑结构并为大语言模型代理优化提示，以最大化其集体性能。主要实验结果表明，一个70亿参数的指挥者模型通过协调多个工作大语言模型，在LiveCodeBench和GPQA等具有挑战性的推理基准测试中取得了最先进的结果，超越了任何单个工作模型，并能适应不同的代理集合，同时通过递归拓扑实现动态测试时扩展。</div>
</details>
</div>
<div class="card">
<div class="title">Output-Space Search: Targeting LLM Generations in a Frozen Encoder-Defined Output Space</div>
<div class="meta-line">Authors: Tobias Materzok</div>
<div class="meta-line">First: 2026-01-29T02:11:43+00:00 · Latest: 2026-01-29T02:11:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21169v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21169v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Output-Space Search (OS-Search), which turns LLM generation into endpoint search. An outer loop selects a target z* in a frozen encoder-defined 3D output space Z, and a retrieval-grounded policy trained with sequence-level RL generates outputs whose coordinates land near z* under standard autoregressive decoding. This enables parallel sweeps and black-box optimization in Z without path-dependent token/program search. On stories, sweeping Z (text) yields 3.1x higher LLM-scored diversity than prompt-chaining. On code, Bayesian optimization over Z (code) improves an objective withheld from the controller under matched inference budgets while preserving validity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>输出空间搜索：在冻结编码器定义的输出空间中定位大语言模型生成</div>
<div class="mono" style="margin-top:8px">本文提出输出空间搜索方法，将大语言模型生成转化为端点搜索问题。外层循环在冻结编码器定义的三维输出空间Z中选择目标点z*，通过序列级强化学习训练的检索增强策略生成输出，使其在标准自回归解码下的坐标落于z*附近。该方法支持在Z空间进行并行扫描和黑盒优化，无需依赖路径的标记/程序搜索。在故事生成任务中，对Z（文本）空间的扫描使大语言模型评分多样性比提示链方法提升3.1倍；在代码生成任务中，对Z（代码）空间的贝叶斯优化在匹配推理预算下，能提升控制器未知目标的性能指标，同时保持代码有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Output-Space Search (OS-Search), motivated by the need to enhance LLM generation diversity and optimization without modifying the model&#x27;s internal parameters. The method frames generation as a search problem in a frozen encoder-defined 3D output space, where an outer loop selects target coordinates and a retrieval-grounded policy, trained with sequence-level reinforcement learning, produces outputs that align with these targets under standard autoregressive decoding. Experimental results show that sweeping this space for text generation yields 3.1 times higher LLM-scored diversity compared to prompt-chaining, and for code generation, Bayesian optimization in the output space improves a withheld objective while maintaining code validity under matched inference budgets.</div>
<div class="mono" style="margin-top:8px">本文提出了输出空间搜索方法，其动机是在不改变模型内部参数的情况下，提升大语言模型生成的多样性和优化能力。该方法将生成过程转化为在冻结编码器定义的三维输出空间中的搜索问题，外层循环选择目标坐标，而通过序列级强化学习训练的检索基础策略，在标准自回归解码下生成与目标坐标对齐的输出。实验结果表明，在文本生成中遍历该空间相比提示链方法获得了3.1倍的LLM评分多样性提升；在代码生成中，输出空间的贝叶斯优化在匹配推理预算下改进了对控制器隐藏的目标函数，同时保持了代码的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">GEPO: Group Expectation Policy Optimization for Stable Heterogeneous Reinforcement Learning</div>
<div class="meta-line">Authors: Han Zhang, Ruibin Zheng, Zexuan Yi, Zhuo Zhang, Hanyang Peng, Hui Wang, Zike Yuan, Cai Ke, Shiwei Chen, Jiacheng Yang, Yangning Li, Xiang Li, Jiangyue Yan, Yaoqi Liu, Liwen Jing, Jiayin Qi, Ruifeng Xu, Binxing Fang, Yue Yu</div>
<div class="meta-line">First: 2025-08-25T09:57:35+00:00 · Latest: 2026-01-29T01:55:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.17850v9">Abs</a> · <a href="https://arxiv.org/pdf/2508.17850v9">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As single-center computing approaches power constraints, decentralized training becomes essential. However, traditional Reinforcement Learning (RL) methods, crucial for enhancing large model post-training, cannot adapt to decentralized distributed training due to the tight coupling between parameter learning and rollout sampling. For this, we propose HeteroRL, a heterogeneous RL architecture that decouples these processes, enabling stable training across geographically distributed nodes connected via the Internet. The core component is Group Expectation Policy Optimization (GEPO), an asynchronous RL algorithm robust to latency caused by network delays or heterogeneity in computational resources. Our study reveals that high latency significantly increases KL divergence, leading to higher variance of importance weights and training instability. GEPO mitigates this issue by using group expectation weighting to exponentially reduce the variance of importance weights, with theoretical guarantees. Experiments show GEPO achieves superior stability - only a 3% performance drop from online to 1800s latency-and reduces the best-to-last gap by 85% versus GSPO (1.8 vs. 12.0) while attaining the highest scores, highlighting its effectiveness in decentralized, resource-heterogeneous environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GEPO：面向稳定异构强化学习的群体期望策略优化</div>
<div class="mono" style="margin-top:8px">随着单中心计算逼近算力瓶颈，去中心化训练变得至关重要。然而，传统强化学习方法虽对提升大模型后训练性能关键，却因参数学习与轨迹采样的紧耦合而难以适应去中心化分布式训练。为此，我们提出异构强化学习架构HeteroRL，通过解耦这两个过程，实现在互联网连接的地理分布式节点间进行稳定训练。其核心组件是群体期望策略优化算法GEPO，这种异步强化学习算法对网络延迟或计算资源异构性引发的时延具有强鲁棒性。研究发现，高时延会显著增加KL散度，导致重要性权重方差增大及训练失稳。GEPO通过群体期望加权法以指数级降低重要性权重方差，并给出理论保证。实验表明GEPO实现了卓越的稳定性——从在线训练到1800秒时延仅产生3%性能衰减，且相比GSPO将最优-最终性能差距缩小85%（1.8对12.0），同时获得最高分数，凸显了其在去中心化资源异构环境中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the limitations of single-center computing and the inability of traditional Reinforcement Learning (RL) methods to adapt to decentralized, geographically distributed training due to the tight coupling of parameter learning and data sampling. To address this, the authors propose HeteroRL, a heterogeneous RL architecture, and its core algorithm, Group Expectation Policy Optimization (GEPO), which decouples these processes and uses group expectation weighting to theoretically reduce the variance of importance weights, thereby stabilizing training under network latency and computational heterogeneity. Experimental results demonstrate that GEPO maintains remarkable stability, exhibiting only a 3% performance drop under high latency (1800s) and reducing the best-to-last performance gap by 85% compared to a baseline, while achieving the highest task scores.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于单中心计算面临算力限制，而传统强化学习方法因参数学习与数据采样的紧耦合无法适应去中心化分布式训练。为此，作者提出了异构强化学习架构HeteroRL及其核心算法——群组期望策略优化（GEPO），该方法解耦了学习与采样过程，并利用群组期望加权从理论上降低重要性权重的方差，从而在网络延迟和计算资源异构条件下实现稳定训练。实验结果表明，GEPO在高达1800秒的延迟下仅出现3%的性能下降，与基线方法相比将最佳与最终性能差距降低了85%，同时获得了最高的任务得分，突显了其在去中心化、资源异构环境中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Stackelberg Coupling of Online Representation Learning and Reinforcement Learning</div>
<div class="meta-line">Authors: Fernando Martinez, Tao Li, Yingdong Lu, Juntao Chen</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-08-10T18:36:54+00:00 · Latest: 2026-01-28T22:31:00+00:00</div>
<div class="meta-line">Comments: The Fourteenth International Conference on Learning Representations (ICLR 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.07452v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.07452v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep Q-learning jointly learns representations and values within monolithic networks, promising beneficial co-adaptation between features and value estimates. Although this architecture has attained substantial success, the coupling between representation and value learning creates instability as representations must constantly adapt to non-stationary value targets, while value estimates depend on these shifting representations. This is compounded by high variance in bootstrapped targets, which causes bias in value estimation in off-policy methods. We introduce Stackelberg Coupled Representation and Reinforcement Learning (SCORER), a framework for value-based RL that views representation and Q-learning as two strategic agents in a hierarchical game. SCORER models the Q-function as the leader, which commits to its strategy by updating less frequently, while the perception network (encoder) acts as the follower, adapting more frequently to learn representations that minimize Bellman error variance given the leader&#x27;s committed strategy. Through this division of labor, the Q-function minimizes MSBE while perception minimizes its variance, thereby reducing bias accordingly, with asymmetric updates allowing stable co-adaptation, unlike simultaneous parameter updates in monolithic solutions. Our proposed SCORER framework leads to a bi-level optimization problem whose solution is approximated by a two-timescale algorithm that creates an asymmetric learning dynamic between the two players. Extensive experiments on DQN and its variants demonstrate that gains stem from algorithmic insight rather than model complexity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在线表征学习与强化学习的斯塔克尔伯格耦合</div>
<div class="mono" style="margin-top:8px">深度Q学习在单一网络中联合学习表征与价值，有望实现特征与价值估计间的有益协同适应。尽管该架构已取得显著成功，但表征学习与价值学习的耦合会引发不稳定性：表征必须持续适应非平稳的价值目标，而价值估计又依赖于这些动态变化的表征。这一问题因自举目标的高方差而加剧，导致离轨方法中价值估计产生偏差。我们提出斯塔克尔伯格耦合表征与强化学习（SCORER），这是一个基于价值的强化学习框架，将表征与Q学习视为分层博弈中的两个策略主体。SCORER将Q函数建模为领导者，通过较低频率更新来固定其策略；感知网络（编码器）作为跟随者，以更高频率适应学习，以在领导者既定策略下最小化贝尔曼误差方差。通过这种分工，Q函数最小化均方贝尔曼误差，而感知网络最小化其方差，从而相应减少偏差；非对称更新机制实现了稳定的协同适应，不同于单一方案中的同步参数更新。我们提出的SCORER框架导出一个双层优化问题，其解通过双时间尺度算法近似实现，该算法在两者间构建非对称学习动态。在DQN及其变体上的大量实验表明，性能提升源于算法洞察而非模型复杂度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the instability in deep Q-learning caused by the tight coupling between representation learning and value estimation, where shifting representations and high-variance targets lead to biased value updates. The method introduces SCORER, a framework that models representation and Q-learning as hierarchical Stackelberg game agents: the Q-function acts as a leader updating less frequently to minimize mean squared Bellman error, while the perception network acts as a follower adapting more frequently to reduce variance in Bellman errors, approximated via a two-timescale algorithm. Experimental results on DQN variants show that this asymmetric update dynamic improves stability and performance, attributing gains to algorithmic design rather than increased model complexity.</div>
<div class="mono" style="margin-top:8px">本文的动机源于深度Q学习中表征学习与价值估计的紧密耦合导致的不稳定性，其中不断变化的表征和高方差目标会引发价值更新的偏差。方法上提出了SCORER框架，将表征与Q学习建模为分层Stackelberg博弈中的智能体：Q函数作为领导者以较低频率更新以最小化贝尔曼误差均方，而感知网络作为跟随者以较高频率适应以降低贝尔曼误差方差，并通过双时间尺度算法近似求解。在DQN及其变体上的大量实验表明，这种不对称更新动态提升了稳定性和性能，且收益源于算法洞察而非模型复杂度的增加。</div>
</details>
</div>
<div class="card">
<div class="title">Safety Generalization Under Distribution Shift in Safe Reinforcement Learning: A Diabetes Testbed</div>
<div class="meta-line">Authors: Minjae Kwon, Josephine Lamp, Lu Feng</div>
<div class="meta-line">First: 2026-01-28T22:28:17+00:00 · Latest: 2026-01-28T22:28:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21094v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21094v1">PDF</a> · <a href="https://github.com/safe-autonomy-lab/GlucoSim">Code1</a> · <a href="https://github.com/safe-autonomy-lab/GlucoAlg">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safe Reinforcement Learning (RL) algorithms are typically evaluated under fixed training conditions. We investigate whether training-time safety guarantees transfer to deployment under distribution shift, using diabetes management as a safety-critical testbed. We benchmark safe RL algorithms on a unified clinical simulator and reveal a safety generalization gap: policies satisfying constraints during training frequently violate safety requirements on unseen patients. We demonstrate that test-time shielding, which filters unsafe actions using learned dynamics models, effectively restores safety across algorithms and patient populations. Across eight safe RL algorithms, three diabetes types, and three age groups, shielding achieves Time-in-Range gains of 13--14\% for strong baselines such as PPO-Lag and CPO while reducing clinical risk index and glucose variability. Our simulator and benchmark provide a platform for studying safety under distribution shift in safety-critical control domains. Code is available at https://github.com/safe-autonomy-lab/GlucoSim and https://github.com/safe-autonomy-lab/GlucoAlg.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>安全强化学习中的分布偏移下安全性泛化：糖尿病管理测试平台</div>
<div class="mono" style="margin-top:8px">安全强化学习算法通常在固定训练条件下评估。本研究以糖尿病管理为安全关键测试平台，探究训练阶段的安全保证在分布偏移下的部署中是否有效。我们在统一临床模拟器上对安全强化学习算法进行基准测试，揭示了安全性泛化差距：训练时满足约束的策略常在未见患者上违反安全要求。研究表明，利用学习到的动态模型过滤不安全动作的测试时屏蔽机制，能有效恢复不同算法和患者群体的安全性。在八种安全强化学习算法、三种糖尿病类型和三个年龄组中，屏蔽机制为PPO-Lag和CPO等强基线带来13-14%的血糖达标时间增益，同时降低临床风险指数和血糖变异性。本模拟器与基准测试平台为安全关键控制领域的分布偏移下安全性研究提供了工具。代码发布于https://github.com/safe-autonomy-lab/GlucoSim与https://github.com/safe-autonomy-lab/GlucoAlg。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the generalization of safety guarantees in Safe Reinforcement Learning (RL) when deployed under distribution shift, using diabetes management as a critical testbed. The motivation stems from the need to ensure that policies trained under fixed conditions remain safe for unseen patients. The method involves benchmarking eight safe RL algorithms on a unified clinical simulator and applying test-time shielding, which filters unsafe actions using learned dynamics models. The main experimental results reveal a safety generalization gap where training-time safe policies often fail on new patients, but shielding effectively restores safety, achieving 13–14% Time-in-Range gains for strong baselines like PPO-Lag and CPO while reducing clinical risk and glucose variability across different diabetes types and age groups.</div>
<div class="mono" style="margin-top:8px">本文研究了安全强化学习在分布偏移下部署时安全保证的泛化能力，以糖尿病管理为关键测试平台。其动机源于需要确保在固定条件下训练的策略对未见患者仍保持安全性。方法包括在统一的临床模拟器上对八种安全强化学习算法进行基准测试，并应用测试时屏蔽技术，该技术使用学习到的动力学模型过滤不安全动作。主要实验结果表明存在安全泛化差距，即训练时安全的策略在新患者上常失效，但屏蔽技术有效恢复了安全性，在PPO-Lag和CPO等强基线算法上实现了13-14%的血糖达标时间增益，同时降低了不同糖尿病类型和年龄组的临床风险指数和血糖变异性。</div>
</details>
</div>
<div class="card">
<div class="title">Deep Reinforcement Learning for Fault-Adaptive Routing in Eisenstein-Jacobi Interconnection Topologies</div>
<div class="meta-line">Authors: Mohammad Walid Charrwi, Zaid Hussain</div>
<div class="meta-line">First: 2026-01-28T22:25:22+00:00 · Latest: 2026-01-28T22:25:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21090v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21090v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The increasing density of many-core architectures necessitates interconnection networks that are both high-performance and fault-resilient. Eisenstein-Jacobi (EJ) networks, with their symmetric 6-regular topology, offer superior topological properties but challenge traditional routing heuristics under fault conditions. This paper evaluates three routing paradigms in faulty EJ environments: deterministic Greedy Adaptive Routing, theoretically optimal Dijkstra&#x27;s algorithm, and a reinforcement learning (RL)-based approach. Using a multi-objective reward function to penalize fault proximity and reward path efficiency, the RL agent learns to navigate around clustered failures that typically induce dead-ends in greedy geometric routing. Dijkstra&#x27;s algorithm establishes the theoretical performance ceiling by computing globally optimal paths with complete topology knowledge, revealing the true connectivity limits of faulty networks. Quantitative analysis at nine faulty nodes shows greedy routing catastrophically degrades to 10% effective reachability and packet delivery, while Dijkstra proves 52-54% represents the topological optimum. The RL agent achieves 94% effective reachability and 91% packet delivery, making it suitable for distributed deployment. Furthermore, throughput evaluations demonstrate that RL sustains over 90% normalized throughput across all loads, actually outperforming Dijkstra under congestion through implicit load balancing strategies. These results establish RL-based adaptive policies as a practical solution that bridges the gap between greedy&#x27;s efficiency and Dijkstra&#x27;s optimality, providing robust, self-healing communication in fault-prone interconnection networks without requiring the global topology knowledge or computational overhead of optimal algorithms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度强化学习的爱因斯坦-雅可比互连拓扑容错自适应路由</div>
<div class="mono" style="margin-top:8px">众核架构密度的提升要求互连网络兼具高性能与容错性。爱因斯坦-雅可比网络凭借其对称的六正则拓扑结构具备优越的拓扑特性，但在故障条件下对传统路由启发式算法构成挑战。本文评估了故障EJ环境中的三种路由范式：确定性贪婪自适应路由、理论最优的迪杰斯特拉算法，以及基于强化学习的方法。通过采用多目标奖励函数惩罚故障邻近度并奖励路径效率，强化学习智能体学会规避通常导致贪婪几何路由陷入死锁的集群故障。迪杰斯特拉算法凭借完整拓扑知识计算全局最优路径，确立了理论性能上限，揭示了故障网络的真实连通性极限。在九节点故障的量化分析中，贪婪路由的有效可达性与数据包投递率骤降至10%，而迪杰斯特拉算法证明52-54%为拓扑理论最优值。强化学习智能体实现了94%的有效可达性与91%的数据包投递率，适用于分布式部署。吞吐量评估进一步表明，强化学习方法在所有负载下均保持90%以上的归一化吞吐量，其隐式负载均衡策略在拥塞场景下甚至优于迪杰斯特拉算法。这些结果确立了基于强化学习的自适应策略作为实用解决方案，弥合了贪婪路由的效率与迪杰斯特拉算法的最优性之间的鸿沟，无需全局拓扑知识或最优算法的计算开销，即可为易故障互连网络提供鲁棒的自愈通信能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for high-performance and fault-resilient interconnection networks in many-core architectures, this paper evaluates routing paradigms for Eisenstein-Jacobi topologies under fault conditions. The method compares deterministic greedy adaptive routing, Dijkstra&#x27;s optimal algorithm, and a reinforcement learning (RL) approach that uses a multi-objective reward function to avoid faults and optimize paths. Experimental results with nine faulty nodes show greedy routing degrades to 10% reachability, Dijkstra achieves 52-54% as the theoretical optimum, while the RL agent attains 94% reachability and 91% packet delivery, sustaining over 90% normalized throughput and outperforming Dijkstra under congestion through implicit load balancing.</div>
<div class="mono" style="margin-top:8px">本文针对众核架构对高性能、高容错互连网络的需求，研究了在故障条件下Eisenstein-Jacobi拓扑中的路由策略。方法比较了确定性贪婪自适应路由、理论最优的Dijkstra算法，以及一种采用多目标奖励函数以规避故障并优化路径的强化学习（RL）方法。在九个故障节点的实验结果表明，贪婪路由的有效可达性降至10%，Dijkstra算法达到52-54%的理论最优值，而RL智能体实现了94%的有效可达性和91%的数据包投递率，并通过隐式负载均衡在所有负载下维持超过90%的归一化吞吐量，在拥塞时表现优于Dijkstra算法。</div>
</details>
</div>
<div class="card">
<div class="title">OpenSec: Measuring Incident Response Agent Calibration Under Adversarial Evidence</div>
<div class="meta-line">Authors: Jarrod Barnes</div>
<div class="meta-line">First: 2026-01-28T22:12:54+00:00 · Latest: 2026-01-28T22:12:54+00:00</div>
<div class="meta-line">Comments: 6 pages, 2 figures, 3 tables. Code: https://github.com/jbarnes850/opensec-env</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21083v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21083v1">PDF</a> · <a href="https://github.com/jbarnes850/opensec-env">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models improve, so do their offensive applications: frontier agents now generate working exploits for under $50 in compute (Heelan, 2026). Defensive incident response (IR) agents must keep pace, but existing benchmarks conflate action execution with correct execution, hiding calibration failures when agents process adversarial evidence. We introduce OpenSec, a dual-control reinforcement learning environment that evaluates IR agents under realistic prompt injection scenarios. Unlike static capability benchmarks, OpenSec scores world-state-changing containment actions under adversarial evidence via execution-based metrics: time-to-first-containment (TTFC), blast radius (false positives per episode), and injection violation rates. Evaluating four frontier models on 40 standard-tier episodes, we find consistent over-triggering in this setting: GPT-5.2, Gemini 3, and DeepSeek execute containment in 100% of episodes with 90-97% false positive rates. Claude Sonnet 4.5 shows partial calibration (85% containment, 72% FP), demonstrating that OpenSec surfaces a calibration failure mode hidden by aggregate success metrics. Code available at https://github.com/jbarnes850/opensec-env.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OpenSec：对抗性证据下事件响应智能体校准度评估</div>
<div class="mono" style="margin-top:8px">随着大语言模型能力的提升，其攻击性应用也在演进：前沿智能体现能以低于50美元的计算成本生成有效漏洞利用程序（Heelan, 2026）。防御性事件响应智能体必须同步发展，但现有基准测试将动作执行与正确执行混为一谈，掩盖了智能体处理对抗性证据时的校准缺陷。本文提出OpenSec——一个双控强化学习环境，通过在真实提示注入场景中评估事件响应智能体。与静态能力基准不同，OpenSec采用基于执行的指标（首次遏制时间、误报半径、注入违规率）对对抗性证据下改变世界状态的遏制行动进行评分。在40个标准级场景中评估四个前沿模型发现：GPT-5.2、Gemini 3和DeepSeek在100%场景中执行遏制，但伴随90-97%的误报率；Claude Sonnet 4.5展现出部分校准能力（85%遏制率，72%误报率）。这表明OpenSec能揭示被整体成功率掩盖的校准失效模式。代码发布于https://github.com/jbarnes850/opensec-env。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the increasing offensive use of large language models to generate exploits, highlighting the need for defensive incident response (IR) agents to keep pace, as existing benchmarks fail to reveal calibration issues when agents handle adversarial evidence. The method introduces OpenSec, a dual-control reinforcement learning environment that evaluates IR agents under realistic prompt injection scenarios using execution-based metrics like time-to-first-containment, blast radius, and injection violation rates. Experimental results on four frontier models across 40 episodes show consistent over-triggering, with GPT-5.2, Gemini 3, and DeepSeek executing containment in all episodes but with 90-97% false positive rates, while Claude Sonnet 4.5 demonstrates partial calibration, indicating that OpenSec exposes calibration failures masked by aggregate success metrics.</div>
<div class="mono" style="margin-top:8px">本文的动机是大型语言模型在攻击性应用中日益增强，例如生成可利用漏洞，这要求防御性事件响应（IR）代理能够跟上步伐，因为现有基准测试在代理处理对抗性证据时未能揭示校准问题。方法上提出了OpenSec，这是一个双控制强化学习环境，通过基于执行的指标（如首次遏制时间、爆炸半径和注入违规率）来评估IR代理在现实提示注入场景下的表现。在40个标准级别剧集上对四个前沿模型的实验结果显示，存在一致的过度触发行为：GPT-5.2、Gemini 3和DeepSeek在所有剧集中都执行了遏制，但假阳性率高达90-97%，而Claude Sonnet 4.5表现出部分校准，这表明OpenSec能够揭示被聚合成功指标掩盖的校准失败模式。</div>
</details>
</div>
<div class="card">
<div class="title">Human-LLM Collaborative Feature Engineering for Tabular Data</div>
<div class="meta-line">Authors: Zhuoyan Li, Aditya Bansal, Jinzhao Li, Shishuang He, Zhuoran Lu, Mutian Zhang, Qin Liu, Yiwei Yang, Swati Jain, Ming Yin, Yunyao Li</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-28T21:33:42+00:00 · Latest: 2026-01-28T21:33:42+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21060v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21060v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly used to automate feature engineering in tabular learning. Given task-specific information, LLMs can propose diverse feature transformation operations to enhance downstream model performance. However, current approaches typically assign the LLM as a black-box optimizer, responsible for both proposing and selecting operations based solely on its internal heuristics, which often lack calibrated estimations of operation utility and consequently lead to repeated exploration of low-yield operations without a principled strategy for prioritizing promising directions. In this paper, we propose a human-LLM collaborative feature engineering framework for tabular learning. We begin by decoupling the transformation operation proposal and selection processes, where LLMs are used solely to generate operation candidates, while the selection is guided by explicitly modeling the utility and uncertainty of each proposed operation. Since accurate utility estimation can be difficult especially in the early rounds of feature engineering, we design a mechanism within the framework that selectively elicits and incorporates human expert preference feedback, comparing which operations are more promising, into the selection process to help identify more effective operations. Our evaluations on both the synthetic study and the real user study demonstrate that the proposed framework improves feature engineering performance across a variety of tabular datasets and reduces users&#x27; cognitive load during the feature engineering process.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向表格数据的人机协同特征工程</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）在表格学习中正被越来越多地用于自动化特征工程。给定任务特定信息后，LLMs能够提出多样化的特征转换操作以提升下游模型性能。然而，现有方法通常将LLM视为黑盒优化器，由其同时负责基于内部启发式规则提出和选择操作，这往往缺乏对操作效用的校准估计，从而导致在缺乏优先探索有前景方向的系统化策略下，反复尝试低效操作。本文提出一种面向表格学习的人机协同特征工程框架。我们首先将转换操作的提出与选择过程解耦：LLM仅用于生成操作候选，而选择过程则通过显式建模每个提议操作的效用和不确定性来指导。由于准确的效用估计在特征工程早期阶段尤为困难，我们在框架中设计了一种机制，选择性地收集并融入人类专家偏好反馈——通过比较哪些操作更具潜力——以辅助识别更有效的操作。在合成实验和真实用户研究中的评估表明，该框架在多种表格数据集上提升了特征工程性能，并降低了用户在特征工程过程中的认知负荷。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the inefficiency of using large language models (LLMs) as black-box optimizers for feature engineering in tabular learning, where LLMs often lack calibrated utility estimates, leading to repeated exploration of low-yield operations. To overcome this, the authors propose a human-LLM collaborative framework that decouples operation proposal and selection: LLMs generate candidate transformations, while selection is guided by explicit modeling of utility and uncertainty, supplemented by selective elicitation of human expert feedback to prioritize promising operations. Experimental results from synthetic and real user studies show that this framework improves feature engineering performance across various tabular datasets and reduces users&#x27; cognitive load during the process.</div>
<div class="mono" style="margin-top:8px">本文针对表格学习中大型语言模型作为黑盒优化器进行特征工程时效率低下的问题，提出了一种人机协作的特征工程框架。该方法将特征转换操作的提议与选择过程解耦：由大语言模型生成候选操作，而选择过程则基于对每个操作效用和不确定性的显式建模，并选择性融入人类专家的偏好反馈以优先考虑有前景的操作。在合成实验和真实用户研究中的评估表明，该框架在多种表格数据集上提升了特征工程性能，并降低了用户在特征工程过程中的认知负担。</div>
</details>
</div>
<div class="card">
<div class="title">Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report</div>
<div class="meta-line">Authors: Zhuoran Yang, Ed Li, Jianliang He, Aman Priyanshu, Baturay Saglam, Paul Kassianik, Sajana Weerawardhena, Anu Vellore, Blaine Nelson, Neusha Javidnia, Arthur Goldblatt, Fraser Burch, Avi Zohary, Assaf Eisenman, Mahdi Sabbaghi, Supriti Vijay, Rahim Dharssi, Dhruv Kedia, Kojin Oshiba, Yaron Singer, Amin Karbasi</div>
<div class="meta-line">First: 2026-01-28T21:15:24+00:00 · Latest: 2026-01-28T21:15:24+00:00</div>
<div class="meta-line">Comments: 31 pages, 5 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21051v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21051v1">PDF</a> · <a href="https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Foundation-Sec-8B-Reasoning, the first open-source native reasoning model for cybersecurity. Built upon our previously released Foundation-Sec-8B base model (derived from Llama-3.1-8B-Base), the model is trained through a two-stage process combining supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR). Our training leverages proprietary reasoning data spanning cybersecurity analysis, instruction-following, and mathematical reasoning. Evaluation across 10 cybersecurity benchmarks and 10 general-purpose benchmarks demonstrates performance competitive with significantly larger models on cybersecurity tasks while maintaining strong general capabilities. The model shows effective generalization on multi-hop reasoning tasks and strong safety performance when deployed with appropriate system prompts and guardrails. This work demonstrates that domain-specialized reasoning models can achieve strong performance on specialized tasks while maintaining broad general capabilities. We release the model publicly at https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B技术报告</div>
<div class="mono" style="margin-top:8px">我们推出Foundation-Sec-8B-Reasoning，这是首个面向网络安全领域的开源原生推理模型。该模型基于我们先前发布的Foundation-Sec-8B基础模型（源自Llama-3.1-8B-Base），通过监督微调（SFT）与可验证奖励强化学习（RLVR）两阶段训练流程构建。训练采用涵盖网络安全分析、指令遵循和数学推理的专有推理数据。在10项网络安全基准测试和10项通用基准测试中的评估表明，该模型在网络安全任务上可与规模显著更大的模型竞争，同时保持强大的通用能力。模型在多跳推理任务中展现出有效泛化能力，在部署适当系统提示与防护机制时具有出色的安全性能。本研究表明，领域专业化推理模型可在保持广泛通用能力的同时，在专业任务上实现强劲性能。模型已通过https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning公开释放。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation was to create the first open-source native reasoning model specifically for cybersecurity, addressing the need for specialized AI in this domain. The method involved a two-stage training process on a base model derived from Llama-3.1-8B, using supervised fine-tuning and reinforcement learning from verifiable rewards with proprietary cybersecurity reasoning data. The main experimental results showed that the model achieved performance competitive with much larger models on 10 cybersecurity benchmarks, maintained strong general capabilities across 10 general-purpose benchmarks, demonstrated effective generalization on multi-hop reasoning tasks, and exhibited strong safety when deployed with proper safeguards.</div>
<div class="mono" style="margin-top:8px">该研究的动机是创建首个专为网络安全设计的开源原生推理模型，以应对该领域对专业化人工智能的需求。方法基于Llama-3.1-8B的衍生基础模型，采用监督微调和基于可验证奖励的强化学习两阶段训练流程，并利用了专有的网络安全推理数据。主要实验结果表明，该模型在10个网络安全基准测试中达到了与更大模型相竞争的性能，在10个通用基准测试中保持了强大的通用能力，在多跳推理任务上展现出有效的泛化能力，且在部署适当安全防护措施时表现出良好的安全性。</div>
</details>
</div>
<div class="card">
<div class="title">Log2Motion: Biomechanical Motion Synthesis from Touch Logs</div>
<div class="meta-line">Authors: Michał Patryk Miazga, Hannah Bussmann, Antti Oulasvirta, Patrick Ebel</div>
<div class="meta-line">First: 2026-01-28T21:04:19+00:00 · Latest: 2026-01-28T21:04:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21043v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21043v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Touch data from mobile devices are collected at scale but reveal little about the interactions that produce them. While biomechanical simulations can illuminate motor control processes, they have not yet been developed for touch interactions. To close this gap, we propose a novel computational problem: synthesizing plausible motion directly from logs. Our key insight is a reinforcement learning-driven musculoskeletal forward simulation that generates biomechanically plausible motion sequences consistent with events recorded in touch logs. We achieve this by integrating a software emulator into a physics simulator, allowing biomechanical models to manipulate real applications in real-time. Log2Motion produces rich syntheses of user movements from touch logs, including estimates of motion, speed, accuracy, and effort. We assess the plausibility of generated movements by comparing against human data from a motion capture study and prior findings, and demonstrate Log2Motion in a large-scale dataset. Biomechanical motion synthesis provides a new way to understand log data, illuminating the ergonomics and motor control underlying touch interactions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Log2Motion：基于触摸日志的生物力学运动合成</div>
<div class="mono" style="margin-top:8px">移动设备的大规模触摸数据难以揭示其背后的交互机制，而生物力学仿真虽能阐明运动控制过程，却尚未应用于触摸交互。为填补这一空白，我们提出一项新颖的计算问题：直接从日志合成合理运动。核心方法是采用强化学习驱动的肌肉骨骼前向仿真，生成与触摸日志事件一致、生物力学合理的运动序列。通过将软件模拟器集成至物理仿真环境，使生物力学模型能够实时操控真实应用。Log2Motion可从触摸日志合成丰富的用户运动数据，包括动作、速度、准确性和用力程度的估计。我们通过运动捕捉实验的人类数据与既有研究对比，评估生成运动的合理性，并在大规模数据集中验证了该框架。生物力学运动合成为理解日志数据提供了新途径，揭示了触摸交互背后的人体工学与运动控制机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the gap between abundant touch log data from mobile devices and the lack of understanding of the biomechanical interactions that produce them. The method introduces Log2Motion, which formulates motion synthesis from touch logs as a computational problem and employs a reinforcement learning-driven musculoskeletal forward simulation integrated with a software emulator in a physics simulator to generate biomechanically plausible motion sequences. Experimental results show that the synthesized movements, including estimates of motion, speed, accuracy, and effort, are plausible when compared against human motion capture data and prior findings, and the approach is demonstrated on a large-scale dataset, offering new insights into ergonomics and motor control in touch interactions.</div>
<div class="mono" style="margin-top:8px">该论文的动机在于移动设备产生的大量触摸日志数据与对其背后生物力学交互机制的理解不足之间存在差距。方法上提出了Log2Motion，将触摸日志的运动合成定义为一个计算问题，采用强化学习驱动的肌肉骨骼前向模拟，并结合物理模拟器中的软件模拟器，生成生物力学上合理的运动序列。实验结果表明，合成出的运动（包括运动、速度、准确性和努力程度的估计）与人类动作捕捉数据和先前研究相比具有合理性，并在大规模数据集上进行了验证，为理解触摸交互的人体工程学和运动控制提供了新途径。</div>
</details>
</div>
<div class="card">
<div class="title">SIGMA-PPG: Statistical-prior Informed Generative Masking Architecture for PPG Foundation Model</div>
<div class="meta-line">Authors: Zongheng Guo, Tao Chen, Yang Jiao, Yi Pan, Xiao Hu, Manuela Ferrario</div>
<div class="meta-line">First: 2026-01-28T20:46:50+00:00 · Latest: 2026-01-28T20:46:50+00:00</div>
<div class="meta-line">Comments: 31 pages, 9 figures, 14 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21031v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21031v1">PDF</a> · <a href="https://github.com/ZonghengGuo/SigmaPPG">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current foundation model for photoplethysmography (PPG) signals is challenged by the intrinsic redundancy and noise of the signal. Standard masked modeling often yields trivial solutions while contrastive methods lack morphological precision. To address these limitations, we propose a Statistical-prior Informed Generative Masking Architecture (SIGMA-PPG), a generative foundation model featuring a Prior-Guided Adversarial Masking mechanism, where a reinforcement learning-driven teacher leverages statistical priors to create challenging learning paths that prevent overfitting to noise. We also incorporate a semantic consistency constraint via vector quantization to ensure that physiologically identical waveforms (even those altered by recording artifacts or minor perturbations) map to shared indices. This enhances codebook semantic density and eliminates redundant feature structures. Pre-trained on over 120,000 hours of data, SIGMA-PPG achieves superior average performance compared to five state-of-the-art baselines across 12 diverse downstream tasks. The code is available at https://github.com/ZonghengGuo/SigmaPPG.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SIGMA-PPG：面向PPG基础模型的统计先验引导生成掩码架构</div>
<div class="mono" style="margin-top:8px">当前光电容积描记（PPG）信号基础模型受限于信号固有的冗余性与噪声干扰。标准掩码建模常产生平凡解，而对比学习方法则缺乏形态学精度。为突破这些局限，我们提出统计先验引导生成掩码架构（SIGMA-PPG），这是一种具备先验引导对抗掩码机制的生成式基础模型。该模型通过强化学习驱动的教师网络，利用统计先验构建具有挑战性的学习路径以抑制噪声过拟合。同时，我们引入基于矢量量化的语义一致性约束，确保生理特征相同的波形（即使受记录伪影或微小扰动影响）映射至共享索引，从而提升码本语义密度并消除冗余特征结构。基于超过12万小时数据的预训练，SIGMA-PPG在12项多样化下游任务中均优于五种前沿基线模型，综合性能表现卓越。代码已开源：https://github.com/ZonghengGuo/SigmaPPG。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenges of intrinsic redundancy and noise in photoplethysmography (PPG) signals, where standard masked modeling yields trivial solutions and contrastive methods lack morphological precision, this paper proposes SIGMA-PPG, a generative foundation model. The method introduces a Prior-Guided Adversarial Masking mechanism, using reinforcement learning and statistical priors to create challenging learning paths that prevent noise overfitting, and incorporates a semantic consistency constraint via vector quantization to map physiologically identical waveforms to shared indices, enhancing codebook semantic density. Experimentally, pre-trained on over 120,000 hours of data, SIGMA-PPG achieves superior average performance compared to five state-of-the-art baselines across 12 diverse downstream tasks.</div>
<div class="mono" style="margin-top:8px">本研究针对光电容积脉搏波（PPG）信号固有的冗余和噪声问题，其中标准掩码建模易产生平凡解而对比方法缺乏形态学精度，提出了SIGMA-PPG这一生成式基础模型。方法上，它采用先验引导的对抗掩码机制，利用强化学习和统计先验创建具有挑战性的学习路径以防止噪声过拟合，并通过向量量化引入语义一致性约束，将生理上相同的波形映射到共享索引，从而提升码本语义密度。实验结果表明，在超过12万小时数据上预训练后，SIGMA-PPG在12个不同的下游任务中相比五种先进基线模型取得了更优的平均性能。</div>
</details>
</div>
<div class="card">
<div class="title">Distributional Active Inference</div>
<div class="meta-line">Authors: Abdullah Akgül, Gulcin Baykal, Manuel Haußmann, Mustafa Mert Çelikok, Melih Kandemir</div>
<div class="meta-line">First: 2026-01-28T19:36:33+00:00 · Latest: 2026-01-28T19:36:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20985v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20985v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optimal control of complex environments with robotic systems faces two complementary and intertwined challenges: efficient organization of sensory state information and far-sighted action planning. Because the reinforcement learning framework addresses only the latter, it tends to deliver sample-inefficient solutions. Active inference is the state-of-the-art process theory that explains how biological brains handle this dual problem. However, its applications to artificial intelligence have thus far been limited to extensions of existing model-based approaches. We present a formal abstraction of reinforcement learning algorithms that spans model-based, distributional, and model-free approaches. This abstraction seamlessly integrates active inference into the distributional reinforcement learning framework, making its performance advantages accessible without transition dynamics modeling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分布式主动推理</div>
<div class="mono" style="margin-top:8px">机器人系统对复杂环境的最优控制面临两个互补且交织的挑战：感官状态信息的高效组织与长远行动规划。由于强化学习框架仅针对后者，其解决方案往往样本效率低下。主动推理作为前沿过程理论，阐释了生物大脑如何处理这一双重问题。然而，其在人工智能领域的应用迄今仅限于现有基于模型方法的扩展。我们提出一种强化学习算法的形式化抽象，涵盖基于模型、分布式及无模型方法。该抽象将主动推理无缝整合至分布式强化学习框架，无需转移动态建模即可实现其性能优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the dual challenges of efficient sensory information organization and long-term planning in robotic control, which reinforcement learning alone addresses sample-inefficiently, this paper integrates active inference—a theory of biological intelligence—into distributional reinforcement learning. The method introduces a formal abstraction that spans model-based, distributional, and model-free approaches, enabling active inference without requiring explicit transition dynamics modeling. Experimental results demonstrate that this integration provides performance advantages, offering more sample-efficient solutions compared to traditional reinforcement learning methods.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决机器人控制中高效组织感官状态信息与长远行动规划的双重挑战，而传统强化学习在此样本效率不足。方法上，通过提出一个涵盖基于模型、分布式和免模型方法的正式抽象框架，将解释生物智能的活动推理理论无缝整合到分布式强化学习中，无需对环境转移动态进行建模。主要实验结果表明，这种整合带来了性能优势，相比传统强化学习方法，能提供更样本高效的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">POLAR: A Pessimistic Model-based Policy Learning Algorithm for Dynamic Treatment Regimes</div>
<div class="meta-line">Authors: Ruijia Zhang, Xiangyu Zhang, Zhengling Qi, Yue Wu, Yanxun Xu</div>
<div class="meta-line">First: 2025-06-25T13:22:57+00:00 · Latest: 2026-01-28T19:23:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.20406v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.20406v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dynamic treatment regimes (DTRs) provide a principled framework for optimizing sequential decision-making in domains where decisions must adapt over time in response to individual trajectories, such as healthcare, education, and digital interventions. However, existing statistical methods often rely on strong positivity assumptions and lack robustness under partial data coverage, while offline reinforcement learning approaches typically focus on average training performance, lack statistical guarantees, and require solving complex optimization problems. To address these challenges, we propose POLAR, a novel pessimistic model-based policy learning algorithm for offline DTR optimization. POLAR estimates the transition dynamics from offline data and quantifies uncertainty for each history-action pair. A pessimistic penalty is then incorporated into the reward function to discourage actions with high uncertainty. Unlike many existing methods that focus on average training performance or provide guarantees only for an oracle policy, POLAR directly targets the suboptimality of the final learned policy and offers theoretical guarantees, without relying on computationally intensive minimax or constrained optimization procedures. To the best of our knowledge, POLAR is the first model-based DTR method to provide both statistical and computational guarantees, including finite-sample bounds on policy suboptimality. Empirical results on both synthetic data and the MIMIC-III dataset demonstrate that POLAR outperforms state-of-the-art methods and yields near-optimal, history-aware treatment strategies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>POLAR：一种用于动态治疗方案的悲观模型策略学习算法</div>
<div class="mono" style="margin-top:8px">动态治疗方案（DTRs）为医疗、教育和数字干预等需随时间适应个体轨迹的序列决策优化提供了原则性框架。然而，现有统计方法常依赖强正性假设，在部分数据覆盖下缺乏鲁棒性；而离线强化学习方法通常关注平均训练性能，缺乏统计保证，且需解决复杂优化问题。为应对这些挑战，我们提出POLAR，一种用于离线DTR优化的新型悲观模型策略学习算法。POLAR从离线数据估计状态转移动态，量化各历史-行动对的不确定性，并将悲观惩罚项纳入奖励函数以抑制高不确定性行动。与许多仅关注平均训练性能或仅对理想策略提供保证的现有方法不同，POLAR直接针对最终学习策略的次优性提供理论保证，且无需依赖计算密集的极小极大或约束优化过程。据我们所知，POLAR是首个同时提供统计与计算保证的模型DTR方法，包括策略次优性的有限样本界。在合成数据和MIMIC-III数据集上的实证结果表明，POLAR优于现有先进方法，并能生成接近最优、具备历史感知的治疗策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces POLAR, a pessimistic model-based policy learning algorithm designed to address the limitations of existing methods for optimizing dynamic treatment regimes (DTRs), which adapt decisions over time in fields like healthcare. Existing approaches often rely on strong assumptions, lack robustness with partial data, or focus on average performance without guarantees. POLAR estimates transition dynamics from offline data, quantifies uncertainty for each history-action pair, and incorporates a pessimistic penalty into the reward to discourage uncertain actions, thereby directly targeting the suboptimality of the learned policy with theoretical guarantees and avoiding complex optimization. Experimental results on synthetic data and the MIMIC-III dataset show that POLAR outperforms state-of-the-art methods, producing near-optimal, history-aware treatment strategies.</div>
<div class="mono" style="margin-top:8px">本文提出了POLAR，一种悲观的基于模型的策略学习算法，旨在解决现有动态治疗策略优化方法的局限性，这些策略在医疗等领域中随时间调整决策。现有方法通常依赖强假设、在部分数据下缺乏鲁棒性，或只关注平均性能而无理论保证。POLAR从离线数据估计转移动态，量化每个历史-行动对的不确定性，并在奖励函数中加入悲观惩罚以抑制不确定行动，从而直接针对学习策略的次优性提供理论保证，避免了复杂的优化过程。在合成数据和MIMIC-III数据集上的实验结果表明，POLAR优于现有先进方法，能产生接近最优且考虑历史的治疗策略。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260131_0357.html">20260131_0357</a>
<a href="archive/20260130_0355.html">20260130_0355</a>
<a href="archive/20260129_0353.html">20260129_0353</a>
<a href="archive/20260128_0342.html">20260128_0342</a>
<a href="archive/20260127_0320.html">20260127_0320</a>
<a href="archive/20260126_0313.html">20260126_0313</a>
<a href="archive/20260125_0321.html">20260125_0321</a>
<a href="archive/20260124_0344.html">20260124_0344</a>
<a href="archive/20260123_0345.html">20260123_0345</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0408.html">20260121_0408</a>
<a href="archive/20260120_0316.html">20260120_0316</a>
<a href="archive/20260119_0312.html">20260119_0312</a>
<a href="archive/20260118_0323.html">20260118_0323</a>
<a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
