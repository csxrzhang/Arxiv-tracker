<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-28 03:42</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260128_0342</div>
    <div class="row"><div class="card">
<div class="title">Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes</div>
<div class="meta-line">Authors: Amrith Setlur, Zijian Wang, Andrew Cohen, Paria Rashidinejad, Sang Michael Xie</div>
<div class="meta-line">First: 2026-01-26T18:57:00+00:00 · Latest: 2026-01-26T18:57:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18795v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18795v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. PrefixRL boosts the learning signal on hard problems by modulating the difficulty of the problem through the off-policy prefix length. We prove that the PrefixRL objective is not only consistent with the standard RL objective but also more sample efficient. Empirically, we discover back-generalization: training only on prefixed problems generalizes to out-of-distribution unprefixed performance, with learned strategies often differing from those in the prefix. In our experiments, we source the off-policy traces by rejection sampling with the base model, creating a self-improvement loop. On hard reasoning problems, PrefixRL reaches the same training reward 2x faster than the strongest baseline (SFT on off-policy data then RL), even after accounting for the compute spent on the initial rejection sampling, and increases the final reward by 3x. The gains transfer to held-out benchmarks, and PrefixRL is still effective when off-policy traces are derived from a different model family, validating its flexibility in practical settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>复用计算量：通过条件化极离策略前缀扩展强化学习在难题上的应用</div>
<div class="mono" style="margin-top:8px">传统大语言模型推理的强化学习方法在难题上浪费算力，因为正确的同策略轨迹稀少、策略梯度消失且学习停滞。为引导更高效的强化学习，我们考虑以离策略轨迹形式复用旧采样计算量（来自先前的推理或强化学习训练）。标准离策略方法通过离策略数据进行监督，导致强化学习优化过程不稳定。我们提出PrefixRL方法：以成功离策略轨迹的前缀为条件，运行同策略强化学习来完成轨迹，从而规避离策略不稳定性。该方法通过调节离策略前缀长度来控制问题难度，从而增强难题上的学习信号。我们证明PrefixRL目标不仅与标准强化学习目标一致，且具有更高样本效率。实证中发现反向泛化现象：仅在前缀问题上训练可泛化至分布外无前缀任务，且学习策略常与前缀策略不同。实验中通过基础模型的拒绝采样获取离策略轨迹，形成自我改进循环。在复杂推理问题上，即使计入初始拒绝采样的计算成本，PrefixRL达到相同训练奖励的速度仍比最强基线（离策略数据监督微调后强化学习）快2倍，最终奖励提升3倍。该优势可迁移至保留基准测试，且当离策略轨迹源自不同模型家族时仍保持有效，验证了其在实际场景中的灵活性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the inefficiency of standard reinforcement learning (RL) for large language models on hard reasoning problems, where on-policy learning stalls due to rare correct traces and vanishing gradients. To overcome this, the authors propose PrefixRL, a method that reuses off-policy traces from prior inference by conditioning on their prefixes and performing on-policy RL to complete them, thereby avoiding instabilities from direct off-policy supervision while modulating problem difficulty. Experimental results show that PrefixRL achieves training rewards 2x faster than strong baselines and improves final rewards by 3x on hard problems, with gains transferring to held-out benchmarks and demonstrating flexibility across model families.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在困难推理问题上标准强化学习效率低下的问题，提出了一种名为PrefixRL的方法。该方法通过重用先前推理生成的离策略轨迹，以其前缀为条件进行在策略强化学习来完成后续部分，从而避免直接离策略监督的不稳定性并调节问题难度。实验结果表明，PrefixRL在困难问题上比最强基线快2倍达到相同训练奖励，并将最终奖励提高3倍，其增益可迁移到保留基准测试中，且在不同模型家族生成的离策略轨迹上仍有效，展现了实际应用的灵活性。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Objective Reinforcement Learning for Efficient Tactical Decision Making for Trucks in Highway Traffic</div>
<div class="meta-line">Authors: Deepthi Pathare, Leo Laine, Morteza Haghir Chehreghani</div>
<div class="meta-line">First: 2026-01-26T18:50:21+00:00 · Latest: 2026-01-26T18:50:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18783v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18783v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Balancing safety, efficiency, and operational costs in highway driving poses a challenging decision-making problem for heavy-duty vehicles. A central difficulty is that conventional scalar reward formulations, obtained by aggregating these competing objectives, often obscure the structure of their trade-offs. We present a Proximal Policy Optimization based multi-objective reinforcement learning framework that learns a continuous set of policies explicitly representing these trade-offs and evaluates it on a scalable simulation platform for tactical decision making in trucks. The proposed approach learns a continuous set of Pareto-optimal policies that capture the trade-offs among three conflicting objectives: safety, quantified in terms of collisions and successful completion; energy efficiency and time efficiency, quantified using energy cost and driver cost, respectively. The resulting Pareto frontier is smooth and interpretable, enabling flexibility in choosing driving behavior along different conflicting objectives. This framework allows seamless transitions between different driving policies without retraining, yielding a robust and adaptive decision-making strategy for autonomous trucking applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向高速公路卡车战术决策的多目标强化学习效率优化研究</div>
<div class="mono" style="margin-top:8px">在高速公路驾驶中平衡安全性、效率与运营成本对重型车辆构成复杂的决策难题。传统标量奖励函数通过聚合竞争性目标往往模糊其权衡结构。本研究提出基于近端策略优化的多目标强化学习框架，通过可扩展仿真平台学习能显式表征目标权衡的连续策略集。该方法生成的三目标帕累托最优策略集涵盖：以碰撞率与任务完成度衡量的安全性、能耗成本量化的能源效率、驾驶员成本量化的时间效率。所得平滑可解释的帕累托前沿支持沿不同竞争目标灵活选择驾驶行为，无需重新训练即可实现策略间无缝切换，为自动驾驶卡车提供鲁棒自适应决策方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of balancing safety, efficiency, and cost in highway driving for heavy-duty trucks, where traditional scalar reward functions obscure critical trade-offs. The authors propose a multi-objective reinforcement learning framework based on Proximal Policy Optimization to learn a continuous set of Pareto-optimal policies that explicitly represent trade-offs among safety, energy efficiency, and time efficiency. Experimental evaluation on a scalable simulation platform demonstrates that the method produces a smooth and interpretable Pareto frontier, allowing flexible selection of driving behaviors without retraining, thereby enabling robust and adaptive tactical decision-making for autonomous trucking.</div>
<div class="mono" style="margin-top:8px">本文针对重型卡车在高速公路驾驶中平衡安全性、效率和成本的挑战，传统标量奖励函数常掩盖关键权衡。作者提出了一种基于近端策略优化的多目标强化学习框架，以学习一组连续的帕累托最优策略，明确表征安全性、能源效率和时间效率之间的权衡。在可扩展仿真平台上的实验评估表明，该方法能生成平滑且可解释的帕累托前沿，允许无需重新训练即可灵活选择驾驶行为，从而为自动驾驶卡车应用提供稳健且自适应的战术决策策略。</div>
</details>
</div>
<div class="card">
<div class="title">POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration</div>
<div class="meta-line">Authors: Yuxiao Qu, Amrith Setlur, Virginia Smith, Ruslan Salakhutdinov, Aviral Kumar</div>
<div class="meta-line">First: 2026-01-26T18:47:21+00:00 · Latest: 2026-01-26T18:47:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18779v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18779v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without improving solvability. A natural alternative is to leverage transfer from easier problems. However, we show that mixing easy and hard problems during RL training is counterproductive due to ray interference, where optimization focuses on already-solvable problems in a way that actively inhibits progress on harder ones. To address this challenge, we introduce Privileged On-Policy Exploration (POPE), an approach that leverages human- or other oracle solutions as privileged information to guide exploration on hard problems, unlike methods that use oracle solutions as training targets (e.g., off-policy RL methods or warmstarting from SFT). POPE augments hard problems with prefixes of oracle solutions, enabling RL to obtain non-zero rewards during guided rollouts. Crucially, the resulting behaviors transfer back to the original, unguided problems through a synergy between instruction-following and reasoning. Empirically, POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>POPE：通过特权在线策略探索学习解决难题的推理能力</div>
<div class="mono" style="margin-top:8px">强化学习（RL）提升了大型语言模型（LLM）的推理能力，但现有先进方法仍难以从许多训练问题中学习。在难题上，在线策略RL很少能探索出正确的轨迹，导致零奖励且缺乏驱动改进的学习信号。我们发现，经典RL中解决探索问题的自然方法（如熵奖励、放宽重要性比率裁剪或直接优化pass@k目标）均无法解决此问题，且常破坏优化稳定性而不提升可解性。一种自然替代方案是利用简单问题的迁移学习，但我们证明在RL训练中混合简单与难题会因优化射线干扰而适得其反——优化会聚焦于已可解问题，反而阻碍对难题的进展。为此，我们提出特权在线策略探索（POPE），该方法利用人类或其他先知解决方案作为特权信息引导难题探索，而非将其作为训练目标（如离线策略RL或从监督微调预热）。POPE通过为难题添加先知解的前缀，使RL在引导轨迹中获得非零奖励。关键在于，通过指令遵循与推理的协同作用，习得的行为能迁移回原始无引导问题。实验表明，POPE显著扩展了可解问题集，并在挑战性推理基准上大幅提升性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the failure of standard reinforcement learning (RL) methods to learn on hard reasoning problems, where on-policy exploration rarely yields any correct rollouts, resulting in no learning signal. The proposed method, Privileged On-Policy Exploration (POPE), leverages oracle solutions as privileged information to guide exploration by augmenting hard problems with prefixes of these solutions, enabling RL to obtain non-zero rewards during guided rollouts and facilitating transfer back to the original problems. Experimental results show that POPE substantially improves performance on challenging reasoning benchmarks and expands the set of solvable problems compared to prior approaches.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决标准强化学习方法在困难推理问题上无法学习的问题，因为在线探索很少产生正确轨迹，导致缺乏学习信号。所提出的方法——特权在线探索（POPE）——利用人类或其他预言机解决方案作为特权信息来引导探索，通过为困难问题添加预言机解决方案的前缀，使强化学习在引导轨迹中获得非零奖励，并促进知识迁移回原始问题。实验结果表明，与现有方法相比，POPE显著提升了在挑战性推理基准上的性能，并扩大了可解决问题的范围。</div>
</details>
</div>
<div class="card">
<div class="title">Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability</div>
<div class="meta-line">Authors: Shobhita Sundaram, John Quan, Ariel Kwiatkowski, Kartik Ahuja, Yann Ollivier, Julia Kempe</div>
<div class="meta-line">First: 2026-01-26T18:46:56+00:00 · Latest: 2026-01-26T18:46:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18778v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18778v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL. A teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. Our study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>让模型学会自我教学：可学习性边缘的推理研究</div>
<div class="mono" style="margin-top:8px">模型能否突破自身的学习瓶颈？针对大型推理模型的强化学习微调方法在初始成功率较低的数据集上容易停滞，导致训练信号匮乏。本研究探讨一个核心问题：预训练大语言模型能否利用潜在知识为无法解决的问题生成自动化课程？为此我们设计了SOAR框架：一种通过元强化学习挖掘教学信号的自改进框架。模型的教师副本为学生副本生成合成问题，并根据其在少量难题子集上的进步获得奖励。关键在于，SOAR将课程设计锚定于可测量的学生进展而非内在代理奖励。在数学基准最难题集（0/128成功率）上的实验揭示三项核心发现：首先，通过激发预训练模型生成有效阶梯问题的潜在能力，可实现双层元强化学习，在稀疏二元奖励下解锁学习能力；其次，基于实际进展的奖励机制优于先前大语言模型自对弈中的内在奖励方案，能稳定避免其常见的失稳与多样性崩溃问题；第三，对生成问题的分析表明，结构质量与问题明确性比答案正确性对学习进展更为关键。这些发现表明，生成有效阶梯问题的能力并不以预先解决难题为前提，为无需额外标注数据即可突破推理瓶颈提供了理论路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of reinforcement learning methods stalling when finetuning large reasoning models on datasets with very low initial success rates, which provide little training signal. To overcome this, the authors propose SOAR, a self-improvement framework using meta-reinforcement learning where a teacher model generates synthetic problems for a student model, with rewards based on the student&#x27;s measured progress on a small set of hard problems rather than intrinsic proxy rewards. Experimental results on the hardest subsets of mathematical benchmarks, starting from zero success, demonstrate that SOAR enables effective bi-level meta-RL under sparse rewards, outperforms prior intrinsic reward schemes in stability and diversity, and reveals that the structural quality of generated questions is more critical for learning progress than their solution correctness, suggesting a path to escape reasoning plateaus without additional curated data.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习方法在微调大型推理模型时，因数据集初始成功率极低、训练信号不足而陷入停滞的问题展开研究。为此，作者提出了SOAR框架，采用元强化学习方法，让一个教师模型为学生模型生成合成问题，并以学生在少量难题上的实测进步作为奖励，而非依赖内在代理奖励。在数学基准测试最难子集上的实验结果表明，SOAR能够在稀疏奖励下实现有效的双层元强化学习，其基于实测进步的奖励机制在稳定性和多样性上优于先前基于内在奖励的方法，且分析显示生成问题的结构质量比其解答正确性对学习进展更为关键，这为无需额外标注数据即可突破推理瓶颈提供了可行路径。</div>
</details>
</div>
<div class="card">
<div class="title">Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory</div>
<div class="meta-line">Authors: Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu, Yuefeng Huang, Xinyi Wang, Jiannan Cao, Jianwei Yin, Xuhong Zhang</div>
<div class="meta-line">First: 2026-01-26T18:42:33+00:00 · Latest: 2026-01-26T18:42:33+00:00</div>
<div class="meta-line">Comments: Dep-Search 1st version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18771v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18771v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs&#x27; ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Dep-Search：基于持久化记忆的依赖感知推理轨迹学习框架</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）在复杂推理任务中展现出卓越能力，尤其是在结合搜索机制系统探索外部知识库时。该领域已从传统检索增强生成（RAG）框架演进至更复杂的搜索框架，通过显式搜索策略协调多步推理。然而，现有搜索框架仍严重依赖隐式自然语言推理来确定搜索策略及跨推理步骤的信息利用方式。这种隐式推理依赖在管理子问题间依赖关系、高效复用已检索知识、通过强化学习优化搜索策略等方面存在根本性挑战。为突破这些局限，我们提出Dep-Search——一种依赖感知搜索框架，通过GRPO整合结构化推理、检索与持久化记忆，实现了对现有搜索框架的超越。Dep-Search引入显式控制机制，使模型能够：分解具有依赖关系的问题、按需检索信息、从记忆访问历史知识、将长推理上下文总结为可复用的记忆单元。在七个多样化问答数据集上的实验表明，Dep-Search显著提升LLMs处理复杂多跳推理任务的能力，在不同规模模型上均取得对基准模型的实质性改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for Dep-Search arises from the limitations of existing search-augmented LLMs, which rely on implicit natural language reasoning for search strategies, leading to challenges in managing dependencies between sub-questions and reusing retrieved knowledge efficiently. The method introduces a dependency-aware search framework that integrates structured reasoning, retrieval, and persistent memory via GRPO, enabling explicit control for question decomposition with dependencies, on-demand retrieval, access to stored knowledge, and summarization of reasoning contexts into reusable memory entries. Main experimental results on seven diverse QA datasets show that Dep-Search significantly enhances LLMs&#x27; performance in complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across various model scales.</div>
<div class="mono" style="margin-top:8px">Dep-Search的提出动机源于现有搜索增强大语言模型依赖隐式自然语言推理进行搜索策略的局限性，这导致在管理子问题依赖关系和高效重用检索知识方面存在挑战。该方法引入了一个依赖感知的搜索框架，通过GRPO整合结构化推理、检索和持久记忆，实现了对具有依赖关系的问题分解、按需检索、访问存储知识以及将长推理上下文总结为可重用记忆条目的显式控制。在七个多样化问答数据集上的主要实验结果表明，Dep-Search显著提升了大语言模型处理复杂多跳推理任务的能力，在不同模型规模上均实现了对强基线的实质性改进。</div>
</details>
</div>
<div class="card">
<div class="title">Trust, Don&#x27;t Trust, or Flip: Robust Preference-Based Reinforcement Learning with Multi-Expert Feedback</div>
<div class="meta-line">Authors: Seyed Amir Hosseini, Maryam Abdolali, Amirhosein Tavakkoli, Fardin Ayar, Ehsan Javanmardi, Manabu Tsukada, Mahdi Javanmardi</div>
<div class="meta-line">First: 2026-01-26T18:21:48+00:00 · Latest: 2026-01-26T18:21:48+00:00</div>
<div class="meta-line">Comments: Equal contribution: Seyed Amir Hosseini and Maryam Abdolali. Corresponding author: Maryam Abdolali (maryam.abdolali@kntu.ac.ir)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18751v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18751v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Preference-based reinforcement learning (PBRL) offers a promising alternative to explicit reward engineering by learning from pairwise trajectory comparisons. However, real-world preference data often comes from heterogeneous annotators with varying reliability; some accurate, some noisy, and some systematically adversarial. Existing PBRL methods either treat all feedback equally or attempt to filter out unreliable sources, but both approaches fail when faced with adversarial annotators who systematically provide incorrect preferences. We introduce TriTrust-PBRL (TTP), a unified framework that jointly learns a shared reward model and expert-specific trust parameters from multi-expert preference feedback. The key insight is that trust parameters naturally evolve during gradient-based optimization to be positive (trust), near zero (ignore), or negative (flip), enabling the model to automatically invert adversarial preferences and recover useful signal rather than merely discarding corrupted feedback. We provide theoretical analysis establishing identifiability guarantees and detailed gradient analysis that explains how expert separation emerges naturally during training without explicit supervision. Empirically, we evaluate TTP on four diverse domains spanning manipulation tasks (MetaWorld) and locomotion (DM Control) under various corruption scenarios. TTP achieves state-of-the-art robustness, maintaining near-oracle performance under adversarial corruption while standard PBRL methods fail catastrophically. Notably, TTP outperforms existing baselines by successfully learning from mixed expert pools containing both reliable and adversarial annotators, all while requiring no expert features beyond identification indices and integrating seamlessly with existing PBRL pipelines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>信任、不信任或翻转：基于多专家反馈的鲁棒偏好强化学习</div>
<div class="mono" style="margin-top:8px">基于偏好的强化学习（PBRL）通过从成对轨迹比较中学习，为显式奖励工程提供了有前景的替代方案。然而，现实世界的偏好数据常来自可靠性各异的异构标注者：部分准确、部分含噪声、部分系统性地具有对抗性。现有PBRL方法要么平等对待所有反馈，要么尝试过滤不可靠来源，但面对系统性提供错误偏好的对抗性标注者时，两种方法均会失效。我们提出TriTrust-PBRL（TTP）框架，该统一框架能够从多专家偏好反馈中联合学习共享奖励模型和专家特定信任参数。核心洞见在于：信任参数在基于梯度的优化过程中会自然演化为正值（信任）、接近零值（忽略）或负值（翻转），使模型能自动反转对抗性偏好并恢复有效信号，而非简单丢弃被污染的反馈。我们通过理论分析建立了可识别性保证，并通过梯度分析阐明专家分离如何在无显式监督的训练过程中自然涌现。在涵盖操作任务（MetaWorld）与运动控制（DM Control）的四个多样化领域及多种污染场景下的实证评估表明，TTP实现了最先进的鲁棒性——在对抗性污染下保持接近理论最优的性能，而标准PBRL方法则完全失效。值得注意的是，TTP仅需专家标识索引且无需额外专家特征，即可成功从包含可靠与对抗性标注者的混合专家池中学习，并完全兼容现有PBRL流程，其性能显著超越现有基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of learning robust reward models from heterogeneous and potentially adversarial human preferences in preference-based reinforcement learning (PBRL). The authors propose TriTrust-PBRL (TTP), a framework that jointly learns a shared reward model and expert-specific trust parameters, which adaptively assign positive (trust), near-zero (ignore), or negative (flip) weights to each annotator&#x27;s feedback, thereby enabling the inversion of systematically adversarial preferences. Theoretically, the method provides identifiability guarantees and shows that expert separation emerges naturally during gradient-based optimization. Experimentally, TTP is evaluated on manipulation and locomotion tasks under various corruption scenarios, achieving state-of-the-art robustness by maintaining near-oracle performance even with adversarial annotators, outperforming existing baselines that fail catastrophically in such settings.</div>
<div class="mono" style="margin-top:8px">本文针对基于偏好的强化学习（PBRL）中，从异构且可能具有对抗性的人类偏好数据中学习鲁棒奖励模型的挑战，提出了TriTrust-PBRL（TTP）框架。该方法联合学习共享的奖励模型和专家特定的信任参数，这些参数能自适应地为每个标注者的反馈分配正值（信任）、接近零值（忽略）或负值（翻转），从而能够反转系统性的对抗性偏好。理论分析提供了可识别性保证，并表明专家分离在基于梯度的优化过程中自然出现。在实验部分，TTP在多种操纵和运动任务的不同污染场景下进行评估，结果显示其实现了最先进的鲁棒性，即使在存在对抗性标注者的情况下也能保持接近最优的性能，显著优于在此类设置中完全失效的现有基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models</div>
<div class="meta-line">Authors: Siyan Zhao, Zhihui Xie, Mengchen Liu, Jing Huang, Guan Pang, Feiyu Chen, Aditya Grover</div>
<div class="meta-line">First: 2026-01-26T17:56:50+00:00 · Latest: 2026-01-26T17:56:50+00:00</div>
<div class="meta-line">Comments: 13 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18734v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18734v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Knowledge distillation improves large language model (LLM) reasoning by compressing the knowledge of a teacher LLM to train smaller LLMs. On-policy distillation advances this approach by having the student sample its own trajectories while a teacher LLM provides dense token-level supervision, addressing the distribution mismatch between training and inference in off-policy distillation methods. However, on-policy distillation typically requires a separate, often larger, teacher LLM and does not explicitly leverage ground-truth solutions available in reasoning datasets. Inspired by the intuition that a sufficiently capable LLM can rationalize external privileged reasoning traces and teach its weaker self (i.e., the version without access to privileged information), we introduce On-Policy Self-Distillation (OPSD), a framework where a single model acts as both teacher and student by conditioning on different contexts. The teacher policy conditions on privileged information (e.g., verified reasoning traces) while the student policy sees only the question; training minimizes the per-token divergence between these distributions over the student&#x27;s own rollouts. We demonstrate the efficacy of our method on multiple mathematical reasoning benchmarks, achieving 4-8x token efficiency compared to reinforcement learning methods such as GRPO and superior performance over off-policy distillation methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自蒸馏推理器：大语言模型的在线自蒸馏方法</div>
<div class="mono" style="margin-top:8px">知识蒸馏通过压缩教师大语言模型（LLM）的知识来训练较小LLM，从而提升推理能力。在线蒸馏通过让学生模型采样自身轨迹，同时由教师LLM提供密集的令牌级监督，解决了离线蒸馏方法中训练与推理的分布不匹配问题。然而，在线蒸馏通常需要独立且规模更大的教师LLM，且未显式利用推理数据集中可用的真实解。受“能力足够的LLM可合理化外部特权推理轨迹并教导其较弱自身（即无法访问特权信息的版本）”这一直觉启发，我们提出在线自蒸馏（OPSD）框架：单一模型通过不同上下文条件同时扮演教师和学生角色。教师策略以特权信息（如已验证的推理轨迹）为条件，学生策略仅接收问题；训练过程通过最小化学生自身推演中两种策略分布的每令牌差异来实现。我们在多个数学推理基准测试中验证了该方法的效果，相比GRPO等强化学习方法实现了4-8倍的令牌效率，且性能优于离线蒸馏方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces On-Policy Self-Distillation (OPSD), motivated by the limitations of existing knowledge distillation methods for large language models (LLMs) in reasoning tasks, which often rely on separate teacher models and fail to fully utilize ground-truth solutions. The method enables a single LLM to act as both teacher and student: the teacher conditions on privileged information like verified reasoning traces, while the student sees only the question, and training minimizes token-level divergence over the student&#x27;s own sampled trajectories. Experimental results on multiple mathematical reasoning benchmarks show that OPSD achieves 4-8x token efficiency compared to reinforcement learning approaches like GRPO and outperforms off-policy distillation methods.</div>
<div class="mono" style="margin-top:8px">本文提出了基于策略的自蒸馏方法，其动机在于现有大型语言模型知识蒸馏方法在推理任务中的局限性，这些方法通常依赖独立的教师模型且未能充分利用真实解。该方法让单一模型同时充当教师和学生：教师基于特权信息（如已验证的推理轨迹）进行条件生成，而学生仅看到问题，训练通过最小化学生自身采样轨迹上的词元级差异来实现。在多个数学推理基准测试中的实验结果表明，该方法相比GRPO等强化学习方法实现了4-8倍的词元效率提升，并优于离策略蒸馏方法。</div>
</details>
</div>
<div class="card">
<div class="title">Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale</div>
<div class="meta-line">Authors: Henry Bell, Caroline Zhang, Mohammed Mobasserul Haque, Dhaval Potdar, Samia Zaman, Brandon Fain</div>
<div class="meta-line">First: 2026-01-26T17:54:54+00:00 · Latest: 2026-01-26T17:54:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18730v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18730v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The constitutional framework of alignment aims to align large language models (LLMs) with value-laden principles written in natural language (such as to avoid using biased language). Prior work has focused on parameter fine-tuning techniques, such as reinforcement learning from human feedback (RLHF), to instill these principles. However, these approaches are computationally demanding, require careful engineering and tuning, and often require difficult-to-obtain human annotation data. We propose \textsc{reflect}, an inference-time framework for constitutional alignment that does not require any training or data, providing a plug-and-play approach for aligning an instruction-tuned model to a set of principles. \textsc{reflect} operates entirely in-context, combining a (i) constitution-conditioned base response with post-generation (ii) self-evaluation, (iii)(a) self-critique, and (iii)(b) final revision. \textsc{reflect}&#x27;s technique of explicit in-context reasoning over principles during post-generation outperforms standard few-shot prompting and provides transparent reasoning traces. Our results demonstrate that \textsc{reflect} significantly improves LLM conformance to diverse and complex principles, including principles quite distinct from those emphasized in the model&#x27;s original parameter fine-tuning, without sacrificing factual reasoning. \textsc{reflect} is particularly effective at reducing the rate of rare but significant violations of principles, thereby improving safety and robustness in the tail end of the distribution of generations. Finally, we show that \textsc{reflect} naturally generates useful training data for traditional parameter fine-tuning techniques, allowing for efficient scaling and the reduction of inference-time computational overhead in long-term deployment scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Reflect：基于透明原则指导的大规模宪法对齐推理框架</div>
<div class="mono" style="margin-top:8px">对齐的宪法框架旨在使大语言模型（LLM）与用自然语言编写的价值负载原则（例如避免使用偏见性语言）保持一致。先前的研究主要侧重于参数微调技术，如基于人类反馈的强化学习（RLHF），以灌输这些原则。然而，这些方法计算成本高，需要精心设计和调优，且通常依赖难以获取的人工标注数据。我们提出\textsc{reflect}，一种用于宪法对齐的推理时框架，无需任何训练或数据，为指令调优模型提供即插即用的原则对齐方法。\textsc{reflect}完全在上下文中运行，结合了（i）基于宪法条件的初始响应与生成后的（ii）自我评估、（iii）（a）自我批判及（iii）（b）最终修订。\textsc{reflect}在生成后对原则进行显式上下文推理的技术优于标准少样本提示，并提供透明的推理轨迹。实验结果表明，\textsc{reflect}显著提升了LLM对多样复杂原则的遵循度，包括与模型原始参数微调重点迥异的原则，且不牺牲事实推理能力。\textsc{reflect}在降低原则罕见但严重违反的发生率方面尤为有效，从而提升了生成分布尾端的安全性与鲁棒性。最后，我们证明\textsc{reflect}能自然生成适用于传统参数微调技术的训练数据，为长期部署场景实现高效扩展并降低推理时计算开销。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Reflect, an inference-time framework designed to align large language models with constitutional principles without requiring additional training or human-annotated data, addressing the computational and engineering burdens of prior fine-tuning methods like RLHF. The method operates in-context by generating a base response conditioned on principles, followed by self-evaluation, self-critique, and revision, which provides transparent reasoning traces. Experimental results show that Reflect significantly enhances model adherence to diverse and complex principles, including those not emphasized during original fine-tuning, reduces rare but severe principle violations to improve safety and robustness, and generates useful data for efficient fine-tuning to lower long-term inference costs.</div>
<div class="mono" style="margin-top:8px">本文提出了Reflect框架，旨在无需额外训练或人工标注数据的情况下，通过推理时对齐使大语言模型遵循宪法原则，以解决传统微调方法（如RLHF）的计算和工程负担。该方法在上下文中运行，首先生成基于原则的初始响应，然后进行自我评估、自我批评和修订，从而提供透明的推理轨迹。实验结果表明，Reflect显著提升了模型对多样复杂原则的遵从性，包括那些在原始微调中未强调的原则，减少了罕见但严重的违规行为以增强安全性和鲁棒性，并能生成有效数据以支持高效微调，降低长期部署的推理开销。</div>
</details>
</div>
<div class="card">
<div class="title">Health-SCORE: Towards Scalable Rubrics for Improving Health-LLMs</div>
<div class="meta-line">Authors: Zhichao Yang, Sepehr Janghorbani, Dongxu Zhang, Jun Han, Qian Qian, Andrew Ressler, Gregory D. Lyng, Sanjit Singh Batra, Robert E. Tillman</div>
<div class="meta-line">First: 2026-01-26T17:34:10+00:00 · Latest: 2026-01-26T17:34:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18706v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18706v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Rubrics are essential for evaluating open-ended LLM responses, especially in safety-critical domains such as healthcare. However, creating high-quality and domain-specific rubrics typically requires significant human expertise time and development cost, making rubric-based evaluation and training difficult to scale. In this work, we introduce Health-SCORE, a generalizable and scalable rubric-based training and evaluation framework that substantially reduces rubric development costs without sacrificing performance. We show that Health-SCORE provides two practical benefits beyond standalone evaluation: it can be used as a structured reward signal to guide reinforcement learning with safety-aware supervision, and it can be incorporated directly into prompts to improve response quality through in-context learning. Across open-ended healthcare tasks, Health-SCORE achieves evaluation quality comparable to human-created rubrics while significantly lowering development effort, making rubric-based evaluation and training more scalable.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Health-SCORE：面向可扩展的评估准则以改进健康领域大语言模型</div>
<div class="mono" style="margin-top:8px">评估准则对于评估开放式大语言模型响应至关重要，尤其在医疗等安全关键领域。然而，创建高质量且领域特定的评估准则通常需要大量专家人力与开发成本，使得基于准则的评估与训练难以规模化。本研究提出Health-SCORE——一个可泛化、可扩展的基于准则的训练与评估框架，能在保持性能的同时大幅降低准则开发成本。我们证明Health-SCORE除独立评估外还具有两项实际优势：可作为结构化奖励信号引导具备安全监督的强化学习，并能直接嵌入提示词通过上下文学习提升响应质量。在开放式医疗任务中，Health-SCORE实现了与人工创建准则相当的评估质量，同时显著降低开发成本，使基于准则的评估与训练更具可扩展性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that creating high-quality, domain-specific rubrics for evaluating open-ended LLM responses in healthcare is costly and time-consuming, hindering scalable evaluation and training. The method introduces Health-SCORE, a scalable framework that reduces rubric development costs while maintaining performance, and demonstrates its utility not only for evaluation but also as a structured reward signal for reinforcement learning and for improving responses via in-context learning. The main experimental results show that across open-ended healthcare tasks, Health-SCORE achieves evaluation quality comparable to human-created rubrics with significantly lower development effort, thereby enhancing the scalability of rubric-based approaches.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，为医疗保健等安全关键领域评估开放式大语言模型回答时，创建高质量、领域特定的评分标准通常需要大量专业人力与开发成本，这使得基于评分标准的评估和训练难以规模化。方法上，研究提出了Health-SCORE这一可泛化、可扩展的评分标准框架，它在不牺牲性能的前提下大幅降低了开发成本，并证明该框架不仅能用于评估，还可作为结构化奖励信号指导具有安全监督的强化学习，以及通过上下文学习直接提升回答质量。主要实验结果表明，在多项开放式医疗任务中，Health-SCORE达到了与人工创建评分标准相当的评估质量，同时显著减少了开发工作量，从而提高了评分标准评估与训练的可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning</div>
<div class="meta-line">Authors: Peng Xia, Jinglu Wang, Yibo Peng, Kaide Zeng, Zihan Dong, Xian Wu, Xiangru Tang, Hongtu Zhu, Yun Li, Linjun Zhang, Shujie Liu, Yan Lu, Huaxiu Yao</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-05-31T13:22:55+00:00 · Latest: 2026-01-26T17:15:26+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.00555v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.00555v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential in multimodal diagnostic tasks. However, existing single-agent models struggle to generalize across diverse medical specialties, limiting their performance. Recent efforts introduce multi-agent collaboration frameworks inspired by clinical workflows, where general practitioners (GPs) and specialists interact in a fixed sequence. Despite improvements, these static pipelines lack flexibility and adaptability in reasoning. To address this, we propose MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that enables dynamic, optimized collaboration among medical agents. Specifically, we train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to assign patients to appropriate specialties, while the attending physician integrates the judgments from multi-specialists and its own knowledge to make final decisions. To address the inconsistency in specialist outputs, we introduce a curriculum learning (CL)-guided RL strategy with dynamic entropy regulation, progressively teaching the attending physician to balance between imitating specialists and correcting their mistakes. Experiments on five medical VQA benchmarks demonstrate that MMedAgent-RL outperforms both open-source and proprietary Med-LVLMs. Notably, it achieves an average performance gain of 23.6% over strong baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MMedAgent-RL：优化多智能体协作以实现多模态医学推理</div>
<div class="mono" style="margin-top:8px">医学大型视觉语言模型（Med-LVLMs）在多模态诊断任务中展现出强大潜力。然而，现有单智能体模型难以泛化至不同医学专科，限制了其性能。近期研究借鉴临床工作流程，引入了全科医生与专科医生按固定顺序交互的多智能体协作框架。尽管有所改进，这些静态流程在推理中缺乏灵活性与适应性。为此，我们提出MMedAgent-RL——一种基于强化学习（RL）的多智能体框架，可实现医学智能体间的动态优化协作。具体而言，我们通过RL训练两个基于Qwen2.5-VL的全科医生智能体：分诊医生学习将患者分配至合适专科，而主治医生则整合多专科判断与自身知识做出最终决策。为解决专科输出不一致问题，我们引入课程学习（CL）引导的RL策略，通过动态熵调节逐步指导主治医生在模仿专科医生与纠正其错误之间取得平衡。在五个医学VQA基准上的实验表明，MMedAgent-RL性能优于开源及专有Med-LVLMs，较基线模型平均提升23.6%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limited generalization of single-agent medical vision-language models across diverse specialties, this paper introduces MMedAgent-RL, a reinforcement learning-based multi-agent framework designed to optimize dynamic collaboration among medical agents. The method employs two GP agents trained via RL: a triage doctor that assigns patients to specialists and an attending physician that integrates multi-specialist judgments, enhanced by a curriculum learning strategy with dynamic entropy regulation to balance imitation and correction of specialist outputs. Experimental results on five medical VQA benchmarks show that MMedAgent-RL outperforms existing open-source and proprietary models, achieving an average performance gain of 23.6% over strong baselines.</div>
<div class="mono" style="margin-top:8px">针对单智能体医疗视觉语言模型在不同医学专科间泛化能力有限的问题，本文提出了MMedAgent-RL，一种基于强化学习的多智能体框架，旨在优化医疗智能体间的动态协作。该方法通过强化学习训练两个全科医生智能体：分诊医生负责将患者分配给专科医生，主治医生则整合多专科判断并利用自身知识做出最终决策，同时引入课程学习引导的动态熵调节策略，以平衡对专科输出的模仿与纠错。在五个医疗视觉问答基准上的实验结果表明，MMedAgent-RL超越了现有开源和专有模型，平均性能较基线提升23.6%。</div>
</details>
</div>
<div class="card">
<div class="title">Exploration vs Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward</div>
<div class="meta-line">Authors: Peter Chen, Xiaopeng Li, Ziniu Li, Wotao Yin, Xi Chen, Tianyi Lin</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-12-18T18:59:27+00:00 · Latest: 2026-01-26T17:06:02+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16912v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.16912v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探索与利用：通过剪裁、熵与伪奖励重思RLVR</div>
<div class="mono" style="margin-top:8px">本文研究了可验证奖励强化学习（RLVR）中的探索-利用权衡，该框架旨在提升大语言模型（LLM）的推理能力。近期研究表明，RLVR可通过两种看似矛盾的机制激发LLM的数学推理能力：伪奖励（通过奖励与真实答案无关的结果来抑制利用）和熵最小化（通过推动模型产生更自信、确定性的输出来抑制探索）。这揭示了一个令人困惑的动态：既抑制利用又抑制探索反而提升了推理性能，但其背后的协调机制尚不明确。我们聚焦两个核心问题：（i）策略熵如何影响性能；（ii）伪奖励是否通过剪裁偏差与模型污染的相互作用产生增益。实验表明，伪奖励下的剪裁偏差会降低策略熵，从而产生更自信、确定性的输出，而仅靠熵最小化不足以提升性能。我们进一步提出奖励错配模型，解释为何伪奖励能在污染环境之外提升性能。这些发现阐明了伪奖励获益的机制，并为更有效的RLVR训练提供了原则。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR) for improving large language model reasoning, motivated by the paradoxical observation that both spurious rewards and entropy minimization enhance performance despite seemingly opposing mechanisms. The method analyzes how clipping bias under spurious rewards reduces policy entropy to produce more confident outputs, while also proposing a reward-misalignment model to explain benefits beyond contaminated settings. Experimental results show that clipping bias-driven entropy reduction is key to performance gains, whereas entropy minimization alone is insufficient, clarifying how spurious rewards improve reasoning in RLVR training.</div>
<div class="mono" style="margin-top:8px">本文研究用于提升大语言模型推理能力的可验证奖励强化学习中的探索与利用权衡，其动机源于一个矛盾现象：虚假奖励和熵最小化虽机制看似对立，却都能提升性能。方法上，分析了虚假奖励下的裁剪偏差如何降低策略熵以产生更自信的输出，并提出了奖励失配模型来解释超出污染设置的收益。实验结果表明，裁剪偏差驱动的熵减少是性能提升的关键，而单独的熵最小化则不足，这阐明了虚假奖励在RLVR训练中改善推理的机制。</div>
</details>
</div>
<div class="card">
<div class="title">ART for Diffusion Sampling: A Reinforcement Learning Approach to Timestep Schedule</div>
<div class="meta-line">Authors: Yilie Huang, Wenpin Tang, Xunyu Zhou</div>
<div class="meta-line">First: 2026-01-26T16:56:40+00:00 · Latest: 2026-01-26T16:56:40+00:00</div>
<div class="meta-line">Comments: 17 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18681v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18681v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider time discretization for score-based diffusion models to generate samples from a learned reverse-time dynamic on a finite grid. Uniform and hand-crafted grids can be suboptimal given a budget on the number of time steps. We introduce Adaptive Reparameterized Time (ART) that controls the clock speed of a reparameterized time variable, leading to a time change and uneven timesteps along the sampling trajectory while preserving the terminal time. The objective is to minimize the aggregate error arising from the discretized Euler scheme. We derive a randomized control companion, ART-RL, and formulate time change as a continuous-time reinforcement learning (RL) problem with Gaussian policies. We then prove that solving ART-RL recovers the optimal ART schedule, which in turn enables practical actor--critic updates to learn the latter in a data-driven way. Empirically, based on the official EDM pipeline, ART-RL improves Fréchet Inception Distance on CIFAR-10 over a wide range of budgets and transfers to AFHQv2, FFHQ, and ImageNet without the need of retraining.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ART扩散采样：基于强化学习的时间步调度方法</div>
<div class="mono" style="margin-top:8px">本文研究基于分数的扩散模型在有限时间网格上生成样本时的时间离散化问题。在给定时间步数预算下，均匀或人工设计的网格可能并非最优。我们提出自适应重参数化时间（ART）方法，通过控制重参数化时间变量的时钟速率，在保持终止时间不变的前提下实现采样轨迹上的时间变换与非均匀时间步划分。目标是最小化离散化欧拉格式产生的累积误差。我们推导出随机控制对应框架ART-RL，将时间变换建模为具有高斯策略的连续时间强化学习问题，并证明求解ART-RL可获得最优ART调度方案，进而通过数据驱动的行动者-评论家更新实现实际学习。基于官方EDM框架的实验表明，ART-RL在CIFAR-10数据集上能显著提升Fréchet起始距离指标，且无需重新训练即可迁移至AFHQv2、FFHQ和ImageNet数据集。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the suboptimality of uniform or hand-crafted timestep schedules in diffusion model sampling, where a fixed budget on the number of steps can limit sample quality. The authors propose Adaptive Reparameterized Time (ART), a method that adaptively controls the clock speed of a reparameterized time variable to create an uneven, optimized timestep schedule that minimizes discretization error from the Euler scheme. They derive a reinforcement learning variant, ART-RL, framing time change as a continuous-time RL problem with Gaussian policies, and prove it recovers the optimal ART schedule. Experimental results using the EDM pipeline show that ART-RL improves Fréchet Inception Distance on CIFAR-10 across various step budgets and transfers effectively to other datasets like AFHQv2, FFHQ, and ImageNet without retraining.</div>
<div class="mono" style="margin-top:8px">本文针对扩散模型采样中均匀或手工设计的步长时间表可能次优的问题，在固定步数预算下，采样质量可能受限。作者提出了自适应重参数化时间（ART）方法，通过自适应控制重参数化时间变量的时钟速度，创建非均匀的优化时间步长调度，以最小化欧拉格式的离散化误差。他们推导了强化学习变体ART-RL，将时间变化构建为连续时间强化学习问题，采用高斯策略，并证明其能恢复最优ART调度。基于EDM管线的实验结果表明，ART-RL在CIFAR-10上显著提升了Fréchet Inception Distance，适用于多种步数预算，并能无需重新训练就迁移到AFHQv2、FFHQ和ImageNet等数据集。</div>
</details>
</div>
<div class="card">
<div class="title">MiST: Understanding the Role of Mid-Stage Scientific Training in Developing Chemical Reasoning Models</div>
<div class="meta-line">Authors: Andres M Bran, Tong Xie, Shai Pranesh, Jeffrey Meng, Xuan Vu Nguyen, Jeremy Goumaz, David Ming Segura, Ruizhi Xu, Dongzhan Zhou, Wenjie Zhang, Bram Hoex, Philippe Schwaller</div>
<div class="meta-line">First: 2025-12-24T15:15:18+00:00 · Latest: 2026-01-26T16:31:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21231v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.21231v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models can develop reasoning capabilities through online fine-tuning with rule-based rewards. However, recent studies reveal a critical constraint: reinforcement learning succeeds only when the base model already assigns non-negligible probability to correct answers -- a property we term &#x27;latent solvability&#x27;. This work investigates the emergence of chemical reasoning capabilities and what these prerequisites mean for chemistry. We identify two necessary conditions for RL-based chemical reasoning: 1) Symbolic competence, and 2) Latent chemical knowledge. We propose mid-stage scientific training (MiST): a set of mid-stage training techniques to satisfy these, including data-mixing with SMILES/CIF-aware pre-processing, continued pre-training on 2.9B tokens, and supervised fine-tuning on 1B tokens. These steps raise the latent-solvability score on 3B and 7B models by up to 1.8x, and enable RL to lift top-1 accuracy from 10.9 to 63.9% on organic reaction naming, and from 40.6 to 67.4% on inorganic material generation. Similar results are observed for other challenging chemical tasks, while producing interpretable reasoning traces. Our results define clear prerequisites for chemical reasoning training and highlight the broader role of mid-stage training in unlocking reasoning capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MiST：理解中期科学训练在化学推理模型发展中的作用</div>
<div class="mono" style="margin-top:8px">大型语言模型可通过基于规则的奖励进行在线微调来发展推理能力。然而，近期研究揭示了一个关键限制：强化学习仅在基础模型已对正确答案赋予不可忽略概率时才能成功——这一特性我们称为‘潜在可解性’。本研究探讨了化学推理能力的涌现及其先决条件对化学领域的意义。我们确定了基于强化学习的化学推理的两个必要条件：1）符号能力，2）潜在化学知识。我们提出了中期科学训练（MiST）：一套满足这些条件的中期训练技术，包括采用SMILES/CIF感知预处理的数据混合、基于29亿词元的持续预训练，以及基于10亿词元的监督微调。这些步骤将30亿和70亿参数模型的潜在可解性分数提升至多1.8倍，并使强化学习在有机反应命名任务上将Top-1准确率从10.9%提升至63.9%，在无机材料生成任务上从40.6%提升至67.4%。其他具有挑战性的化学任务也观察到类似效果，同时生成可解释的推理轨迹。我们的研究明确了化学推理训练的必备前提，并凸显了中期训练在解锁推理能力方面的广泛作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work is motivated by the observation that reinforcement learning (RL) for chemical reasoning in large language models requires the base model to already assign some probability to correct answers, a property termed &#x27;latent solvability&#x27;. To establish the necessary preconditions of symbolic competence and latent chemical knowledge, the authors propose MiST, a set of mid-stage training techniques involving data-mixing with chemical format pre-processing, continued pre-training on 2.9B tokens, and supervised fine-tuning on 1B tokens. The main experimental results show that MiST increases the latent-solvability score by up to 1.8x for 3B and 7B models, enabling RL to significantly improve top-1 accuracy from 10.9% to 63.9% on organic reaction naming and from 40.6% to 67.4% on inorganic material generation, with similar gains on other challenging chemical tasks while producing interpretable reasoning traces.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于发现，基于强化学习训练大语言模型进行化学推理时，要求基础模型已对正确答案赋予一定的概率，这一特性被称为“潜在可解性”。为满足符号能力和潜在化学知识这两个必要前提，作者提出了MiST，即一套中期科学训练技术，包括对SMILES/CIF格式数据进行混合预处理、在29亿词元上继续预训练，以及在10亿词元上进行监督微调。主要实验结果表明，MiST将3B和7B模型的潜在可解性分数提升了最高1.8倍，从而使强化学习能够将有机反应命名的Top-1准确率从10.9%大幅提升至63.9%，将无机材料生成的准确率从40.6%提升至67.4%，在其他具有挑战性的化学任务上也观察到类似提升，同时模型能生成可解释的推理轨迹。</div>
</details>
</div>
<div class="card">
<div class="title">AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning</div>
<div class="meta-line">Authors: Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu, Ranjay Krishna, Yu Cheng</div>
<div class="meta-line">First: 2026-01-26T16:04:43+00:00 · Latest: 2026-01-26T16:04:43+00:00</div>
<div class="meta-line">Comments: 28 pages, 10 figures and 13 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18631v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18631v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AdaReasoner：面向迭代式视觉推理的动态工具编排框架</div>
<div class="mono" style="margin-top:8px">当人类面临超出即时能力的问题时，会借助工具解决，这为提升多模态大语言模型（MLLMs）的视觉推理能力提供了可行范式。有效的推理关键在于：即使面对新工具或新任务，也能准确判断使用何种工具、何时调用工具以及如何在多步骤中组合工具。本文提出 \textbf{AdaReasoner}——一类将工具使用作为通用推理技能（而非针对特定工具或显式监督行为）进行学习的多模态模型系列。AdaReasoner 通过以下技术实现：（i）可扩展的数据构建流程，使模型接触长跨度、多步骤的工具交互；（ii）Tool-GRPO 强化学习算法，基于最终任务成功率优化工具选择与序列编排；（iii）自适应学习机制，动态调节工具使用策略。这些组件共同使模型能够从任务上下文与中间结果推断工具效用，实现多工具协同及对未见工具的泛化。实验表明，AdaReasoner 展现出强大的工具自适应与泛化能力：尽管从未接受相关显式训练，它能自主采用有益工具、抑制无关工具，并根据任务需求动态调整工具使用频率。这些能力使其在多项挑战性基准测试中达到最先进水平：7B 基础模型平均性能提升 24.9%，在 VSP 与 Jigsaw 等多项任务上超越 GPT-5 等强效专有系统。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces AdaReasoner, a family of multimodal models designed to enhance visual reasoning by enabling dynamic tool orchestration as a general skill, motivated by the need for models to autonomously select, sequence, and adapt tools across iterative reasoning steps without explicit supervision. The method combines a scalable data curation pipeline for multi-step tool interactions, a reinforcement learning algorithm (Tool-GRPO) that optimizes tool use based on end-task success, and an adaptive learning mechanism to regulate tool usage dynamically. Experimental results show that AdaReasoner achieves state-of-the-art performance on challenging benchmarks, improving a 7B base model by an average of 24.9% and surpassing proprietary systems like GPT-5 on tasks such as VSP and Jigsaw, while demonstrating strong generalization to unseen tools and adaptive tool usage behaviors.</div>
<div class="mono" style="margin-top:8px">本文提出了AdaReasoner，这是一个多模态模型系列，旨在通过将动态工具编排作为通用技能来增强视觉推理，其动机是模型需要在无需显式监督的情况下自主选择、排序和适应工具以进行迭代推理。方法结合了用于多步工具交互的可扩展数据整理流程、基于最终任务成功优化工具使用的强化学习算法（Tool-GRPO）以及动态调节工具使用的自适应学习机制。实验结果表明，AdaReasoner在多个挑战性基准测试中取得了最先进的性能，将7B基础模型平均提升了24.9%，并在VSP和Jigsaw等任务上超越了GPT-5等专有系统，同时展现出对未见工具的强泛化能力和自适应工具使用行为。</div>
</details>
</div>
<div class="card">
<div class="title">Rank-1 Approximation of Inverse Fisher for Natural Policy Gradients in Deep Reinforcement Learning</div>
<div class="meta-line">Authors: Yingxiao Huo, Satya Prakash Dash, Radu Stoican, Samuel Kaski, Mingfei Sun</div>
<div class="meta-line">First: 2026-01-26T16:02:18+00:00 · Latest: 2026-01-26T16:02:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18626v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18626v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Natural gradients have long been studied in deep reinforcement learning due to their fast convergence properties and covariant weight updates. However, computing natural gradients requires inversion of the Fisher Information Matrix (FIM) at each iteration, which is computationally prohibitive in nature. In this paper, we present an efficient and scalable natural policy optimization technique that leverages a rank-1 approximation to full inverse-FIM. We theoretically show that under certain conditions, a rank-1 approximation to inverse-FIM converges faster than policy gradients and, under some conditions, enjoys the same sample complexity as stochastic policy gradient methods. We benchmark our method on a diverse set of environments and show that it achieves superior performance to standard actor-critic and trust-region baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度强化学习中自然策略梯度的逆费舍尔矩阵秩-1近似方法</div>
<div class="mono" style="margin-top:8px">自然梯度因其快速收敛特性和协变权重更新特性，在深度强化学习中长期受到研究。然而，计算自然梯度需要在每次迭代中求取费舍尔信息矩阵的逆，这在计算上具有天然的高昂代价。本文提出一种高效可扩展的自然策略优化技术，利用秩-1近似替代完整的逆费舍尔矩阵。我们从理论上证明，在特定条件下，逆费舍尔矩阵的秩-1近似比策略梯度收敛更快，并且在某些条件下具有与随机策略梯度方法相同的样本复杂度。我们在多种环境中对本方法进行基准测试，结果表明其性能优于标准的演员-评论家方法和信赖域基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the computational challenge of inverting the Fisher Information Matrix (FIM) for natural policy gradients in deep reinforcement learning, which is essential for fast convergence but prohibitively expensive. The proposed method introduces an efficient natural policy optimization technique that uses a rank-1 approximation to the full inverse-FIM, making it scalable. Experimental results across diverse environments demonstrate that this approach achieves superior performance compared to standard actor-critic and trust-region baselines, while theoretical analysis shows it can converge faster than policy gradients and match their sample complexity under certain conditions.</div>
<div class="mono" style="margin-top:8px">本文的动机在于解决深度强化学习中自然策略梯度所需计算费舍尔信息矩阵逆矩阵的挑战，该计算对快速收敛至关重要但成本高昂。所提出的方法采用一种高效的策略优化技术，利用对完整逆费舍尔信息矩阵的秩-1近似，从而实现了可扩展性。在多种环境中的实验结果表明，该方法相比标准的演员-评论家和信赖域基线取得了更优的性能，同时理论分析表明，在特定条件下，它能比策略梯度收敛更快，并具有相同的样本复杂度。</div>
</details>
</div>
<div class="card">
<div class="title">Learning long term climate-resilient transport adaptation pathways under direct and indirect flood impacts using reinforcement learning</div>
<div class="meta-line">Authors: Miguel Costa, Arthur Vandervoort, Carolin Schmidt, Morten W. Petersen, Martin Drews, Karyn Morrissey, Francisco C. Pereira</div>
<div class="meta-line">First: 2026-01-26T15:32:40+00:00 · Latest: 2026-01-26T15:32:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18586v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18586v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Climate change is expected to intensify rainfall and other hazards, increasing disruptions in urban transportation systems. Designing effective adaptation strategies is challenging due to the long-term, sequential nature of infrastructure investments, deep uncertainty, and complex cross-sector interactions. We propose a generic decision-support framework that couples an integrated assessment model (IAM) with reinforcement learning (RL) to learn adaptive, multi-decade investment pathways under uncertainty. The framework combines long-term climate projections (e.g., IPCC scenario pathways) with models that map projected extreme-weather drivers (e.g. rain) into hazard likelihoods (e.g. flooding), propagate hazards into urban infrastructure impacts (e.g. transport disruption), and value direct and indirect consequences for service performance and societal costs. Embedded in a reinforcement-learning loop, it learns adaptive climate adaptation policies that trade off investment and maintenance expenditures against avoided impacts. In collaboration with Copenhagen Municipality, we demonstrate the approach on pluvial flooding in the inner city for the horizon of 2024 to 2100. The learned strategies yield coordinated spatial-temporal pathways and improved robustness relative to conventional optimization baselines, namely inaction and random action, illustrating the framework&#x27;s transferability to other hazards and cities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习探索直接与间接洪水影响下的长期气候韧性交通适应路径</div>
<div class="mono" style="margin-top:8px">气候变化预计将加剧降雨等灾害，导致城市交通系统中断频发。由于基础设施投资具有长期性、时序性、深度不确定性及复杂的跨部门交互特征，设计有效的适应策略面临挑战。本研究提出一个通用决策支持框架，将综合评估模型与强化学习相结合，以在不确定性环境下探索适应性的数十年投资路径。该框架整合长期气候预测（如IPCC情景路径）与多级模型：将极端天气驱动因子（如降雨）映射为灾害概率（如洪水），推演灾害对城市基础设施的影响（如交通中断），并量化服务性能与社会成本的直接与间接后果。通过强化学习闭环，该框架能学习权衡投资维护成本与灾害规避效益的气候适应策略。在与哥本哈根市政府合作中，我们以2024-2100年为研究期，针对内城雨洪问题进行了实证验证。相较于传统优化基线（无作为与随机行动），学习得到的策略产生了协调的时空路径并提升了系统鲁棒性，证明了该框架可推广至其他灾害类型与城市区域。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to design long-term, resilient urban transport adaptation strategies under climate-induced flood uncertainties, this paper proposes a decision-support framework that couples an integrated assessment model with reinforcement learning. The method integrates long-term climate projections, hazard mapping, and impact valuation to learn multi-decade investment pathways that balance costs against avoided direct and indirect flood impacts. Experimental application to pluvial flooding in Copenhagen from 2024 to 2100 demonstrates that the learned adaptive policies yield coordinated spatio-temporal pathways and improved robustness compared to conventional baselines like inaction, highlighting the framework&#x27;s transferability to other urban hazards.</div>
<div class="mono" style="margin-top:8px">针对气候变化引发的洪水不确定性下设计长期、有韧性的城市交通适应策略的需求，本文提出了一个将综合评估模型与强化学习相结合的支持决策框架。该方法整合了长期气候预测、灾害映射和影响评估，以学习权衡成本与避免直接和间接洪水影响的数十年投资路径。在哥本哈根2024年至2100年内涝问题的实验应用中表明，学习到的自适应策略产生了协调的时空路径，相较于不作为等传统基线方法具有更强的鲁棒性，凸显了该框架对其他城市灾害的可迁移性。</div>
</details>
</div>
<div class="card">
<div class="title">K-Myriad: Jump-starting reinforcement learning with unsupervised parallel agents</div>
<div class="meta-line">Authors: Vincenzo De Paola, Mirco Mutti, Riccardo Zamboni, Marcello Restelli</div>
<div class="meta-line">First: 2026-01-26T15:26:40+00:00 · Latest: 2026-01-26T15:26:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18580v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18580v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Parallelization in Reinforcement Learning is typically employed to speed up the training of a single policy, where multiple workers collect experience from an identical sampling distribution. This common design limits the potential of parallelization by neglecting the advantages of diverse exploration strategies. We propose K-Myriad, a scalable and unsupervised method that maximizes the collective state entropy induced by a population of parallel policies. By cultivating a portfolio of specialized exploration strategies, K-Myriad provides a robust initialization for Reinforcement Learning, leading to both higher training efficiency and the discovery of heterogeneous solutions. Experiments on high-dimensional continuous control tasks, with large-scale parallelization, demonstrate that K-Myriad can learn a broad set of distinct policies, highlighting its effectiveness for collective exploration and paving the way towards novel parallelization strategies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>K-Myriad：基于无监督并行智能体的强化学习快速启动方法</div>
<div class="mono" style="margin-top:8px">强化学习中的并行化通常用于加速单一策略的训练，即多个工作器从相同的采样分布中收集经验。这种常见设计因忽视多样化探索策略的优势而限制了并行化的潜力。我们提出K-Myriad——一种可扩展的无监督方法，通过并行策略群体最大化集体状态熵。通过培育专业化探索策略组合，K-Myriad为强化学习提供稳健的初始化方案，既能提升训练效率，又能发现异构解决方案。在高维连续控制任务中进行大规模并行实验表明，K-Myriad能够学习到大量差异化策略，凸显了其在集体探索方面的有效性，并为新型并行化策略开辟了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the observation that typical parallel reinforcement learning methods, which use identical sampling distributions across workers, fail to leverage diverse exploration strategies, thus limiting training efficiency and solution diversity. The proposed method, K-Myriad, is an unsupervised approach that maximizes the collective state entropy across a population of parallel policies to cultivate a portfolio of specialized exploration strategies, thereby providing a robust initialization for reinforcement learning. Experimental results on high-dimensional continuous control tasks with large-scale parallelization show that K-Myriad learns a broad set of distinct policies, enhancing training efficiency and enabling the discovery of heterogeneous solutions, which validates its effectiveness for collective exploration.</div>
<div class="mono" style="margin-top:8px">本文的动机是观察到典型的并行强化学习方法在各工作节点上使用相同的采样分布，未能利用多样化的探索策略，从而限制了训练效率和解决方案的多样性。所提出的方法K-Myriad是一种无监督方法，通过最大化并行策略群体诱导的集体状态熵，培养一系列专业化的探索策略，从而为强化学习提供稳健的初始化。在高维连续控制任务上进行的大规模并行实验结果表明，K-Myriad能够学习到一组广泛且不同的策略，提高了训练效率并促进了异构解决方案的发现，这验证了其在集体探索方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Just-In-Time Reinforcement Learning: Continual Learning in LLM Agents Without Gradient Updates</div>
<div class="meta-line">Authors: Yibo Li, Zijie Lin, Ailin Deng, Xuan Zhang, Yufei He, Shuo Ji, Tri Cao, Bryan Hooi</div>
<div class="meta-line">First: 2026-01-26T14:16:51+00:00 · Latest: 2026-01-26T14:16:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18510v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18510v1">PDF</a> · <a href="https://github.com/liushiliushi/JitRL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Language Model (LLM) agents excel at general tasks, they inherently struggle with continual adaptation due to the frozen weights after deployment. Conventional reinforcement learning (RL) offers a solution but incurs prohibitive computational costs and the risk of catastrophic forgetting. We introduce Just-In-Time Reinforcement Learning (JitRL), a training-free framework that enables test-time policy optimization without any gradient updates. JitRL maintains a dynamic, non-parametric memory of experiences and retrieves relevant trajectories to estimate action advantages on-the-fly. These estimates are then used to directly modulate the LLM&#x27;s output logits. We theoretically prove that this additive update rule is the exact closed-form solution to the KL-constrained policy optimization objective. Extensive experiments on WebArena and Jericho demonstrate that JitRL establishes a new state-of-the-art among training-free methods. Crucially, JitRL outperforms the performance of computationally expensive fine-tuning methods (e.g., WebRL) while reducing monetary costs by over 30 times, offering a scalable path for continual learning agents. The code is available at https://github.com/liushiliushi/JitRL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>即时强化学习：无需梯度更新的大语言模型智能体持续学习</div>
<div class="mono" style="margin-top:8px">尽管大语言模型（LLM）智能体在通用任务上表现出色，但由于部署后权重固定，其持续适应能力存在固有局限。传统强化学习（RL）虽能提供解决方案，但计算成本高昂且存在灾难性遗忘风险。本文提出即时强化学习（JitRL），一种无需训练即可在测试时进行策略优化的免梯度更新框架。JitRL通过动态非参数化记忆库存储经验，实时检索相关轨迹以估算动作优势值，并直接用于调节LLM的输出逻辑值。理论证明该加法更新规则是KL约束策略优化目标的精确闭式解。在WebArena和Jericho上的大量实验表明，JitRL在免训练方法中达到最优性能，且显著超越计算密集型微调方法（如WebRL），同时降低超过30倍成本，为持续学习智能体提供了可扩展路径。代码已开源：https://github.com/liushiliushi/JitRL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of enabling continual adaptation in deployed Large Language Model (LLM) agents, which have frozen weights and thus cannot learn from new experiences via gradient updates. To overcome the computational cost and catastrophic forgetting issues of conventional reinforcement learning, the authors propose Just-In-Time Reinforcement Learning (JitRL), a training-free framework that performs test-time policy optimization without gradient updates. JitRL uses a dynamic, non-parametric memory to store experiences and retrieves relevant trajectories to estimate action advantages on-the-fly, which are then used to directly modulate the LLM&#x27;s output logits. Theoretically, this additive update is proven to be the exact closed-form solution to a KL-constrained policy optimization objective. Experiments on WebArena and Jericho benchmarks show that JitRL sets a new state-of-the-art among training-free methods, even outperforming computationally expensive fine-tuning approaches like WebRL while reducing monetary costs by over 30 times, offering a scalable path for continual learning in agents.</div>
<div class="mono" style="margin-top:8px">本文旨在解决大型语言模型（LLM）智能体在部署后因权重冻结而难以持续适应新任务的问题。针对传统强化学习方法计算成本高且易导致灾难性遗忘的缺陷，作者提出了即时强化学习（JitRL），这是一种无需训练、在测试时进行策略优化且不依赖梯度更新的框架。JitRL通过动态非参数化记忆存储经验，并检索相关轨迹以实时估计动作优势值，进而直接调整LLM的输出对数概率。理论上，这种加法更新被证明是KL约束策略优化目标的精确闭式解。在WebArena和Jericho基准测试上的实验表明，JitRL在无需训练的方法中达到了新的最优性能，甚至超越了计算昂贵的微调方法（如WebRL），同时将成本降低了30倍以上，为智能体的持续学习提供了一条可扩展的路径。</div>
</details>
</div>
<div class="card">
<div class="title">Pretrain Value, Not Reward: Decoupled Value Policy Optimization</div>
<div class="meta-line">Authors: Chenghua Huang, Lu Wang, Fangkai Yang, Pu Zhao, Zhixu Li, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang</div>
<div class="meta-line">First: 2025-02-24T08:11:33+00:00 · Latest: 2026-01-26T14:09:10+00:00</div>
<div class="meta-line">Comments: 16 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.16944v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.16944v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we explore how directly pretraining a value model simplifies and stabilizes reinforcement learning from human feedback (RLHF). In reinforcement learning, value estimation is the key to policy optimization, distinct from reward supervision. The value function predicts the \emph{return-to-go} of a partial answer, that is, how promising the partial answer is if it were continued to completion. In RLHF, however, the standard pipeline first pretrains a reward model and then learns a value function online, even though no new reward signals are available once preference data is collected. This makes critic learning redundant, as the process of training a reward model and then deriving a value model is informationally equivalent to directly pretraining a value model. Importantly, this requires no additional supervision, and our value model is trained on exactly the same data used for reward modeling. Building on this insight, we introduce \emph{Decoupled Value Policy Optimization} (DVPO), a framework that pretrains a \emph{Global Value Model} (GVM) offline and freezes it as a universal critic for policy learning. The GVM provides stable, fine-grained credit assignment without critic drift or trajectory sampling. Experiments across MT-Bench, Alpaca-Eval, and Arena-Hard demonstrate that DVPO matches or surpasses state-of-the-art RLHF methods. These results highlight RLHF can be reframed as policy-only optimization guided by a single pretrained value model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>预训练价值而非奖励：解耦价值策略优化</div>
<div class="mono" style="margin-top:8px">本文探讨了直接预训练价值模型如何简化和稳定基于人类反馈的强化学习（RLHF）。在强化学习中，价值估计是策略优化的关键，不同于奖励监督。价值函数预测部分答案的“剩余回报”，即该部分答案若延续至完成时的潜在价值。然而在RLHF标准流程中，通常先预训练奖励模型，再在线学习价值函数，尽管收集偏好数据后并无新的奖励信号。这使得评论家学习变得冗余，因为训练奖励模型再推导价值模型的过程，在信息层面等同于直接预训练价值模型。关键的是，这无需额外监督，我们的价值模型训练数据与奖励建模完全相同。基于此，我们提出《解耦价值策略优化》（DVPO）框架，该框架离线预训练《全局价值模型》（GVM）并将其冻结为策略学习的通用评论家。GVM提供稳定、细粒度的信用分配，避免评论家漂移或轨迹采样。在MT-Bench、Alpaca-Eval和Arena-Hard上的实验表明，DVPO达到或超越了当前最先进的RLHF方法。这些结果证明，RLHF可重构为仅需单个预训练价值模型指导的纯策略优化过程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the observation that the standard RLHF pipeline&#x27;s separate reward model pretraining and online value function learning is redundant, this paper proposes Decoupled Value Policy Optimization (DVPO), a method that directly pretrains a Global Value Model (GVM) offline on preference data to predict the return-to-go of partial answers, thereby serving as a frozen, universal critic. The method simplifies and stabilizes RLHF by providing fine-grained credit assignment without critic drift, eliminating the need for online critic learning or trajectory sampling. Experimental results on benchmarks including MT-Bench, Alpaca-Eval, and Arena-Hard show that DVPO matches or surpasses state-of-the-art RLHF methods, demonstrating that RLHF can be effectively reframed as policy-only optimization guided by a single pretrained value model.</div>
<div class="mono" style="margin-top:8px">本文的动机是观察到标准RLHF流程中分别预训练奖励模型和在线学习价值函数是冗余的，因此提出了解耦价值策略优化（DVPO）方法，该方法直接在偏好数据上离线预训练一个全局价值模型（GVM）来预测部分答案的预期回报，从而作为一个冻结的通用评判者。该方法通过提供细粒度的信用分配且无需在线学习评判者或轨迹采样，简化并稳定了RLHF。在MT-Bench、Alpaca-Eval和Arena-Hard等基准测试上的实验结果表明，DVPO达到或超越了最先进的RLHF方法，证明RLHF可以有效地重构为由单一预训练价值模型指导的纯策略优化。</div>
</details>
</div>
<div class="card">
<div class="title">STaR: Towards Effective and Stable Table Reasoning via Slow-Thinking Large Language Models</div>
<div class="meta-line">Authors: Huajian Zhang, Mingyue Cheng, Yucong Luo, Xiaoyu Tao</div>
<div class="meta-line">First: 2025-11-14T12:34:17+00:00 · Latest: 2026-01-26T13:35:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.11233v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.11233v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Table reasoning with large language models (LLMs) plays a critical role in building intelligent systems capable of understanding and analyzing tabular data. Despite recent progress, existing methods still face key limitations: their reasoning processes lacks depth and explicit multi-step reasoning, often relying solely on implicit language model understanding. In addition, their reasoning processes suffer from instability, primarily caused by model uncertainty. In this work, we propose STaR, a novel slow-thinking model that can achieve effective and stable table reasoning. To enable effective multi-step reasoning, we design a two-stage training framework consisting of supervised fine-tuning (SFT) warm-up followed by reinforced fine-tuning (RFT). Specifically, in the SFT stage, we construct a high-quality dataset through automatic self-verification. In the RFT stage, we introduce a difficulty-aware reinforcement learning mechanism to further enhance reasoning capabilities. Furthermore, to improve reasoning stability, we introduce trajectory-level uncertainty quantification, which fuses token-level confidence with answer-level consistency, enabling the selection of better reasoning trajectories. Extensive experiments demonstrate that STaR-8B achieves state-of-the-art performance on in-domain benchmarks and exhibits strong generalization to out-of-domain datasets, highlighting its potential for enhancing both effectiveness and stability in table reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>STaR：基于慢思考大语言模型实现高效稳定的表格推理</div>
<div class="mono" style="margin-top:8px">基于大语言模型的表格推理在构建能够理解和分析表格数据的智能系统中至关重要。尽管已有进展，现有方法仍面临关键局限：其推理过程缺乏深度和显式的多步推理，往往仅依赖语言模型的隐式理解；同时，推理过程存在不稳定性，主要由模型不确定性导致。本研究提出STaR——一种新型慢思考模型，能够实现高效且稳定的表格推理。为实现有效的多步推理，我们设计了包含监督微调预热和强化微调的两阶段训练框架：在监督微调阶段，通过自动自验证构建高质量数据集；在强化微调阶段，引入难度感知强化学习机制以增强推理能力。此外，为提升推理稳定性，我们提出轨迹级不确定性量化方法，融合词元级置信度与答案级一致性，从而筛选更优推理轨迹。大量实验表明，STaR-8B在领域内基准测试中达到最先进性能，并在领域外数据集上展现出强大泛化能力，凸显了其在提升表格推理效能与稳定性方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses limitations in table reasoning with large language models, where existing methods lack explicit multi-step reasoning and suffer from instability due to model uncertainty. To overcome this, the authors propose STaR, a slow-thinking model that employs a two-stage training framework: supervised fine-tuning with a self-verified dataset for warm-up, followed by reinforced fine-tuning with a difficulty-aware mechanism to enhance reasoning. Additionally, they introduce trajectory-level uncertainty quantification, combining token confidence and answer consistency to improve stability. Experimental results show that STaR-8B achieves state-of-the-art performance on in-domain benchmarks and generalizes well to out-of-domain datasets, demonstrating improved effectiveness and stability in table reasoning.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型在表格推理中存在的局限性，即现有方法缺乏显式的多步推理且因模型不确定性导致不稳定，提出了STaR这一慢思考模型。该方法采用两阶段训练框架：首先通过自动自验证构建高质量数据集进行监督微调预热，然后引入难度感知的强化微调机制以增强推理能力。此外，通过轨迹级不确定性量化，融合词元置信度和答案一致性，以提升推理稳定性。实验结果表明，STaR-8B在领域内基准测试中达到最先进性能，并在领域外数据集上表现出强泛化能力，凸显了其在表格推理中提升有效性和稳定性的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Control Policy Smoothness by Aligning Actions with Predictions from Preceding States</div>
<div class="meta-line">Authors: Kyoleen Kwak, Hyoseok Hwang</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2026-01-26T13:34:34+00:00 · Latest: 2026-01-26T13:34:34+00:00</div>
<div class="meta-line">Comments: Accepted at AAAI-26. 7 pages (excluding references), 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18479v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18479v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep reinforcement learning has proven to be a powerful approach to solving control tasks, but its characteristic high-frequency oscillations make it difficult to apply in real-world environments. While prior methods have addressed action oscillations via architectural or loss-based methods, the latter typically depend on heuristic or synthetic definitions of state similarity to promote action consistency, which often fail to accurately reflect the underlying system dynamics. In this paper, we propose a novel loss-based method by introducing a transition-induced similar state. The transition-induced similar state is defined as the distribution of next states transitioned from the previous state. Since it utilizes only environmental feedback and actually collected data, it better captures system dynamics. Building upon this foundation, we introduce Action Smoothing by Aligning Actions with Predictions from Preceding States (ASAP), an action smoothing method that effectively mitigates action oscillations. ASAP enforces action smoothness by aligning the actions with those taken in transition-induced similar states and by penalizing second-order differences to suppress high-frequency oscillations. Experiments in Gymnasium and Isaac-Lab environments demonstrate that ASAP yields smoother control and improved policy performance over existing methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过对齐动作与前序状态预测增强控制策略平滑性</div>
<div class="mono" style="margin-top:8px">深度强化学习已被证明是解决控制任务的有效方法，但其特有的高频振荡特性使其难以应用于现实环境。现有方法虽通过架构或损失函数设计缓解动作振荡，但后者通常依赖启发式或人工定义的状态相似性来促进动作一致性，往往难以准确反映系统动态。本文提出一种基于损失函数的新方法，引入转移诱导相似状态——定义为从前一状态转移得到的下一状态分布。该方法仅利用环境反馈和实际采集数据，能更准确地捕捉系统动态。在此基础上，我们提出通过对齐动作与前序状态预测实现动作平滑的方法，该方法通过将当前动作与转移诱导相似状态中的动作对齐，并惩罚二阶差分以抑制高频振荡，有效缓解动作振荡。在Gymnasium和Isaac-Lab环境中的实验表明，该方法相比现有方法能产生更平滑的控制效果并提升策略性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of high-frequency action oscillations in deep reinforcement learning policies, which hinder real-world deployment, by proposing a novel loss-based method called ASAP. The motivation stems from the limitations of prior heuristic approaches to defining state similarity for promoting action consistency. The method introduces a transition-induced similar state, derived from the distribution of next states based on actual environmental transitions, to better capture system dynamics and align actions from consecutive states while penalizing second-order differences to suppress oscillations. Experimental results in Gymnasium and Isaac-Lab environments show that ASAP produces smoother control policies and improves overall policy performance compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本文针对深度强化学习中策略存在高频动作振荡、阻碍实际应用的问题，提出了一种名为ASAP的新型基于损失函数的方法。其动机源于现有方法在定义状态相似性以促进动作一致性时依赖启发式或合成方式，难以准确反映系统动力学。该方法引入了转移诱导相似状态，该状态基于从先前状态转移得到的下一状态分布，仅利用环境反馈和实际采集数据，从而更好地捕捉系统动态，并通过对齐连续状态的动作并惩罚二阶差分来抑制高频振荡。在Gymnasium和Isaac-Lab环境中的实验表明，ASAP相比现有方法能产生更平滑的控制策略并提升策略性能。</div>
</details>
</div>
<div class="card">
<div class="title">Tandem Training for Language Models</div>
<div class="meta-line">Authors: Robert West, Ashton Anderson, Ece Kamar, Eric Horvitz</div>
<div class="meta-line">Venue: Proceedings of the 2026 Conference of the European Chapter of the Association for Computational Linguistics (EACL)</div>
<div class="meta-line">First: 2025-10-15T13:48:16+00:00 · Latest: 2026-01-26T13:19:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.13551v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.13551v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As language models continue to rapidly improve, we can expect their actions and reasoning to become difficult or impossible for weaker agents and humans to follow, undermining interpretability and oversight. With an eye on long-term futures, we pursue methods that encourage models to produce solutions that remain intelligible to weaker collaborators. We formalize intelligibility as handoff robustness: a strong model&#x27;s solution is intelligible to a weaker model if randomly handing off control to the weaker model along the solution path does not cause failure. Building on this criterion, we introduce tandem training for language models, a reinforcement learning (RL) paradigm in which rollout tokens are intermittently and randomly sampled from a frozen weak model rather than the strong model being trained. Because rollouts succeed only when the strong model&#x27;s actions and reasoning process can be continued by the weak model -- when the two can co-construct a successful solution -- optimizing standard RL objectives with tandem training implicitly incentivizes both correctness and intelligibility. In the GSM8K math reasoning task, tandem training reliably teaches models to abandon jargon and adapt their language to weaker partners while keeping task accuracy high. Our results demonstrate a promising route to building AI systems that remain auditable by weaker agents, with implications for human--AI collaboration and multi-agent communication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言模型的串联训练</div>
<div class="mono" style="margin-top:8px">随着语言模型持续快速进步，其行为和推理可能变得难以被较弱智能体或人类理解，从而削弱可解释性与监督。着眼于长远发展，我们探索促使模型生成对较弱合作者保持可理解解决方案的方法。我们将可理解性形式化为交接鲁棒性：若在解决方案路径中随机将控制权移交给较弱模型不会导致失败，则强模型的解决方案对较弱模型是可理解的。基于此标准，我们提出语言模型的串联训练——一种强化学习范式，其中推演过程的令牌间歇且随机地来自一个冻结的较弱模型，而非正在训练的强模型。由于推演仅在强模型的行为和推理过程能被较弱模型延续时（即两者能协同构建成功解决方案时）才能成功，通过串联训练优化标准强化学习目标，可同时隐式激励正确性与可理解性。在GSM8K数学推理任务中，串联训练能可靠地引导模型放弃专业术语、使语言适应较弱合作者，同时保持高任务准确率。我们的研究结果为构建能被较弱智能体审计的人工智能系统提供了可行路径，对人类-人工智能协作及多智能体通信具有启示意义。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the concern that increasingly advanced language models may produce solutions too complex for weaker agents or humans to understand, undermining oversight, this paper formalizes intelligibility as handoff robustness, where a strong model&#x27;s solution is considered intelligible if a weaker model can successfully take over control during its execution. The method introduces tandem training, a reinforcement learning paradigm where rollout tokens are intermittently sampled from a frozen weak model instead of the strong model being trained, incentivizing the strong model to produce solutions that the weak model can continue. Experimental results on the GSM8K math reasoning task show that tandem training effectively teaches models to adapt their language to be more accessible to weaker partners while maintaining high task accuracy, demonstrating a promising approach for building auditable AI systems.</div>
<div class="mono" style="margin-top:8px">本文的动机是担忧日益先进的语言模型可能产生过于复杂的解决方案，导致较弱智能体或人类难以理解，从而削弱监督能力；为此，研究将可理解性形式化为“交接鲁棒性”，即强模型的解决方案若能在执行过程中被弱模型成功接管，则被视为可理解。方法上提出了串联训练，这是一种强化学习范式，在训练强模型时，间歇性地从冻结的弱模型中采样生成令牌，而非仅依赖强模型自身，以此激励强模型生成弱模型可延续的解决方案。在GSM8K数学推理任务上的实验结果表明，串联训练能有效教导模型调整语言以适应较弱合作伙伴，同时保持高任务准确率，为构建可审计的AI系统提供了一条有前景的路径。</div>
</details>
</div>
<div class="card">
<div class="title">OffSeeker: Online Reinforcement Learning Is Not All You Need for Deep Research Agents</div>
<div class="meta-line">Authors: Yuhang Zhou, Kai Zheng, Qiguang Chen, Mengkang Hu, Qingfeng Sun, Can Xu, Jingjing Chen</div>
<div class="meta-line">First: 2026-01-26T13:13:59+00:00 · Latest: 2026-01-26T13:13:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18467v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18467v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep research agents have shown remarkable potential in handling long-horizon tasks. However, state-of-the-art performance typically relies on online reinforcement learning (RL), which is financially expensive due to extensive API calls. While offline training offers a more efficient alternative, its progress is hindered by the scarcity of high-quality research trajectories. In this paper, we demonstrate that expensive online reinforcement learning is not all you need to build powerful research agents. To bridge this gap, we introduce a fully open-source suite designed for effective offline training. Our core contributions include DeepForge, a ready-to-use task synthesis framework that generates large-scale research queries without heavy preprocessing; and a curated collection of 66k QA pairs, 33k SFT trajectories, and 21k DPO pairs. Leveraging these resources, we train OffSeeker (8B), a model developed entirely offline. Extensive evaluations across six benchmarks show that OffSeeker not only leads among similar-sized agents but also remains competitive with 30B-parameter systems trained via heavy online RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OffSeeker：深度研究智能体并非仅需在线强化学习</div>
<div class="mono" style="margin-top:8px">深度研究智能体在处理长周期任务方面展现出显著潜力，但当前最优性能通常依赖在线强化学习（RL），其频繁的API调用导致高昂成本。离线训练虽为高效替代方案，却受限于高质量研究轨迹的稀缺性。本文论证了构建强大研究智能体无需完全依赖昂贵的在线强化学习。为此，我们推出了一套专为高效离线训练设计的全开源工具集。核心贡献包括：DeepForge——无需繁重预处理即可生成大规模研究查询的即用型任务合成框架；以及精心整理的6.6万组问答对、3.3万条监督微调轨迹和2.1万组直接偏好优化数据。基于这些资源，我们训练出完全离线开发的OffSeeker（80亿参数）模型。在六项基准测试中的广泛评估表明，OffSeeker不仅领先于同规模智能体，更能与通过密集在线强化学习训练的300亿参数系统保持竞争力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the high financial cost of using online reinforcement learning (RL) to train deep research agents, which requires extensive API calls, and the scarcity of high-quality trajectories for offline training. To address this, the authors introduce a fully open-source suite for offline training, featuring DeepForge, a task synthesis framework that generates large-scale research queries without heavy preprocessing, and a curated dataset of QA pairs, SFT trajectories, and DPO pairs. Experimental results across six benchmarks demonstrate that their offline-trained model, OffSeeker (8B), outperforms similar-sized agents and remains competitive with 30B-parameter systems trained via heavy online RL.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，训练深度研究智能体时，依赖在线强化学习会因大量API调用而产生高昂财务成本，而离线训练又受限于高质量研究轨迹的稀缺。为解决这一问题，作者提出了一个完全开源的离线训练套件，其核心包括DeepForge（一个无需繁重预处理即可生成大规模研究查询的任务合成框架）以及一个包含问答对、监督微调轨迹和直接偏好优化对的精选数据集。在六个基准测试上的实验结果表明，完全离线训练的模型OffSeeker（8B）在同等规模智能体中领先，并能与通过大量在线强化学习训练的30B参数系统保持竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Behaviors of Evolutionary Algorithms on GPUs: When Does Parallelism Pay Off?</div>
<div class="meta-line">Authors: Xinmeng Yu, Tao Jiang, Ran Cheng, Yaochu Jin, Kay Chen Tan</div>
<div class="meta-line">First: 2026-01-26T12:55:21+00:00 · Latest: 2026-01-26T12:55:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18446v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18446v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evolutionary algorithms (EAs) are increasingly implemented on graphics processing units (GPUs) to leverage parallel processing capabilities for enhanced efficiency. However, existing studies largely emphasize the raw speedup obtained by porting individual algorithms from CPUs to GPUs. Consequently, these studies offer limited insight into when and why GPU parallelism fundamentally benefits EAs. To address this gap, we investigate how GPU parallelism alters the behavior of EAs beyond simple acceleration metrics. We conduct a systematic empirical study of 16 representative EAs on 30 benchmark problems. Specifically, we compare CPU and GPU executions across a wide range of problem dimensionalities and population sizes. Our results reveal that the impact of GPU acceleration is highly heterogeneous and depends strongly on algorithmic structure. We further demonstrate that conventional fixed-budget evaluation based on the number of function evaluations (FEs) is inadequate for GPU execution. In contrast, fixed-time evaluation uncovers performance characteristics that are unobservable under small or practically constrained FE budgets, particularly for adaptive and exploration-oriented algorithms. Moreover, we identify distinct scaling regimes in which GPU parallelism is beneficial, saturates, or degrades as problem dimensionality and population size increase. Crucially, we show that large populations enabled by GPUs not only improve hardware utilization but also reveal algorithm-specific convergence and diversity dynamics that are difficult to observe under CPU-constrained settings. Consequently, our findings indicate that GPU parallelism is not strictly an implementation detail, but a pivotal factor that influences how EAs should be evaluated, compared, and designed for modern computing platforms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>进化算法在GPU上的扩展行为：并行化何时产生效益？</div>
<div class="mono" style="margin-top:8px">进化算法（EAs）越来越多地在图形处理器（GPU）上实现，以利用并行处理能力提升效率。然而，现有研究主要关注将单个算法从CPU移植到GPU所获得的原始加速比，因此对GPU并行化何时及为何从根本上使EAs受益的见解有限。为填补这一空白，我们研究了GPU并行化如何改变EAs的行为，超越简单的加速指标。我们对30个基准问题上的16种代表性EAs进行了系统性实证研究，特别比较了CPU和GPU在不同问题维度和种群规模下的执行情况。结果表明，GPU加速的影响高度异质，且强烈依赖于算法结构。我们进一步证明，基于函数评估次数（FEs）的传统固定预算评估不适用于GPU执行；相比之下，固定时间评估揭示了在小规模或实际受限的FE预算下无法观察到的性能特征，尤其对于自适应和探索导向的算法。此外，我们识别了不同的扩展机制：随着问题维度和种群规模增加，GPU并行化可能带来效益、达到饱和或导致性能下降。关键的是，我们发现GPU支持的大规模种群不仅提高了硬件利用率，还揭示了在CPU受限设置下难以观察到的算法特定收敛与多样性动态。因此，我们的研究结果表明，GPU并行化不仅是实现细节，更是影响EAs在现代计算平台上如何评估、比较和设计的关键因素。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the fundamental impact of GPU parallelism on evolutionary algorithms (EAs), moving beyond simple speedup metrics to understand when and why GPUs benefit EA performance. The authors systematically compare 16 EAs on 30 benchmark problems across CPUs and GPUs, varying problem dimensionality and population size. Key results show that GPU acceleration&#x27;s effect is highly heterogeneous and algorithm-dependent, that fixed-time evaluation reveals performance traits missed by conventional fixed-budget (function evaluation) analysis, and that GPUs enable distinct scaling regimes and large-population dynamics which can alter algorithm convergence and diversity, influencing how EAs should be evaluated and designed.</div>
<div class="mono" style="margin-top:8px">本研究探讨了GPU并行性对进化算法的根本性影响，超越了简单的加速指标，旨在理解GPU何时以及为何能提升算法性能。作者在30个基准问题上系统比较了16种进化算法在CPU和GPU上的表现，并改变了问题维度和种群规模。主要实验结果表明：GPU加速的效果高度异质且依赖于算法结构；基于固定时间的评估能揭示传统固定评估次数（函数评估）分析所忽略的性能特征；GPU支持的不同规模扩展机制以及大种群运行动态可以改变算法的收敛性和多样性，从而影响进化算法在现代计算平台上的评估与设计方式。</div>
</details>
</div>
<div class="card">
<div class="title">Emergent Cooperation in Quantum Multi-Agent Reinforcement Learning Using Communication</div>
<div class="meta-line">Authors: Michael Kölle, Christian Reff, Leo Sünkel, Julian Hager, Gerhard Stenzel, Claudia Linnhoff-Popien</div>
<div class="meta-line">First: 2026-01-26T12:21:05+00:00 · Latest: 2026-01-26T12:21:05+00:00</div>
<div class="meta-line">Comments: Accepted at IEEE ICC 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18419v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18419v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Emergent cooperation in classical Multi-Agent Reinforcement Learning has gained significant attention, particularly in the context of Sequential Social Dilemmas (SSDs). While classical reinforcement learning approaches have demonstrated capability for emergent cooperation, research on extending these methods to Quantum Multi-Agent Reinforcement Learning remains limited, particularly through communication. In this paper, we apply communication approaches to quantum Q-Learning agents: the Mutual Acknowledgment Token Exchange (MATE) protocol, its extension Mutually Endorsed Distributed Incentive Acknowledgment Token Exchange (MEDIATE), the peer rewarding mechanism Gifting, and Reinforced Inter-Agent Learning (RIAL). We evaluate these approaches in three SSDs: the Iterated Prisoner&#x27;s Dilemma, Iterated Stag Hunt, and Iterated Game of Chicken. Our experimental results show that approaches using MATE with temporal-difference measure (MATE\textsubscript{TD}), AutoMATE, MEDIATE-I, and MEDIATE-S achieved high cooperation levels across all dilemmas, demonstrating that communication is a viable mechanism for fostering emergent cooperation in Quantum Multi-Agent Reinforcement Learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于通信的量子多智能体强化学习中的涌现合作</div>
<div class="mono" style="margin-top:8px">经典多智能体强化学习中的涌现合作已受到广泛关注，尤其在序列社会困境（SSDs）背景下。尽管经典强化学习方法已展现出实现涌现合作的能力，但将这些方法扩展到量子多智能体强化学习的研究仍显不足，特别是通过通信机制的研究。本文在量子Q学习智能体中应用了多种通信方法：互认令牌交换（MATE）协议、其扩展版本互认分布式激励令牌交换（MEDIATE）、同伴奖励机制Gifting以及强化智能体间学习（RIAL）。我们在三种SSD环境中评估这些方法：迭代囚徒困境、迭代猎鹿博弈和迭代斗鸡博弈。实验结果表明，采用时差度量的MATE（MATE\textsubscript{TD}）、AutoMATE、MEDIATE-I和MEDIATE-S方法在所有困境中均实现了高水平合作，证明通信是促进量子多智能体强化学习中涌现合作的有效机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the success of emergent cooperation in classical multi-agent reinforcement learning for sequential social dilemmas, this study explores whether similar cooperative behaviors can be fostered in a quantum multi-agent reinforcement learning setting through communication. The method applies several communication protocols—including MATE, MEDIATE, Gifting, and RIAL—to quantum Q-learning agents and evaluates them in three classic dilemma games: the Iterated Prisoner&#x27;s Dilemma, Iterated Stag Hunt, and Iterated Game of Chicken. The main experimental results demonstrate that specific protocols, namely MATE with temporal-difference measure, AutoMATE, MEDIATE-I, and MEDIATE-S, achieved high levels of cooperation across all tested dilemmas, confirming communication as an effective mechanism for promoting cooperation in quantum multi-agent systems.</div>
<div class="mono" style="margin-top:8px">本研究受经典多智能体强化学习中涌现合作行为的启发，旨在探索在量子多智能体强化学习框架下，能否通过通信机制在序列社会困境中促成类似的合作。方法上，将多种通信协议（包括MATE、MEDIATE、Gifting和RIAL）应用于量子Q学习智能体，并在迭代囚徒困境、迭代猎鹿博弈和迭代斗鸡博弈这三种经典困境游戏中进行了评估。主要实验结果表明，特定的协议，如基于时序差分度量的MATE、AutoMATE、MEDIATE-I和MEDIATE-S，在所有测试困境中都实现了高水平的合作，从而证实了通信是促进量子多智能体系统合作的有效机制。</div>
</details>
</div>
<div class="card">
<div class="title">daVinci-Dev: Agent-native Mid-training for Software Engineering</div>
<div class="meta-line">Authors: Ji Zeng, Dayuan Fu, Tiantian Mi, Yumin Zhuang, Yaxing Huang, Xuefeng Li, Lyumanshan Ye, Muhang Xie, Qishuo Hua, Zhen Huang, Mohan Jiang, Hanning Wang, Jifan Lin, Yang Xiao, Jie Sun, Yunze Wu, Pengfei Liu</div>
<div class="meta-line">First: 2026-01-26T12:20:18+00:00 · Latest: 2026-01-26T12:20:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18418v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18418v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model&#x27;s agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>daVinci-Dev：面向软件工程的智能体原生中期训练</div>
<div class="mono" style="margin-top:8px">近期，大语言模型（LLM）能力前沿已从单轮代码生成转向智能体化软件工程——即模型能自主导航、编辑和测试复杂代码库的范式。尽管后训练方法已成为代码智能体的主流方案，但**智能体中期训练**——在模拟真实智能体工作流程的大规模数据上进行中期训练（MT）——虽比单纯依赖昂贵的强化学习更能规模化地培养基础智能体行为，却因资源需求巨大而尚未得到充分探索。实现有效智能体中期训练的核心挑战在于静态训练数据与真实开发中动态、反馈丰富的环境之间的分布不匹配。为此，我们系统研究了智能体中期训练，建立了大规模智能体开发的数据合成原则与训练方法。我们方法的核心是**智能体原生数据**——包含两种互补轨迹的监督数据：**上下文原生轨迹**完整保留智能体经历的信息流，提供广泛覆盖与多样性；**环境原生轨迹**从可执行代码库收集，其观测源自实际工具调用与测试执行，提供深度与交互真实性。我们在`SWE-Bench Verified`上验证了模型的智能体能力。实验表明，在采用对齐基座模型与智能体框架的两种后训练设置下，我们的方法以不足半数中期训练词元（731亿）超越了先前开源软件工程中期训练方案`Kimi-Dev`。除相对优势外，我们表现最佳的320亿与720亿参数模型分别达到**56.1%**与**58.5%**的问题解决率……</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the shift from single-turn code generation to agentic software engineering, where LLMs autonomously handle complex repositories, and identifies a gap in scalable training methods; it proposes agentic mid-training using agent-native data, which combines contextually-native trajectories for broad coverage and environmentally-native trajectories from executable repositories for authentic interaction. The method involves synthesizing this data to mirror real agentic workflows and training models at scale, with experimental results on SWE-Bench Verified showing that their 32B and 72B models achieve resolution rates of 56.1% and 58.5%, respectively, outperforming the prior Kimi-Dev approach while using less than half the mid-training tokens.</div>
<div class="mono" style="margin-top:8px">本文的动机在于大型语言模型能力前沿已从单轮代码生成转向代理式软件工程，即模型自主处理复杂代码库，但现有训练方法在可扩展性上存在不足；为此提出代理式中期训练，采用代理原生数据，包括保留完整信息流的上下文原生轨迹和来自可执行仓库的环境原生轨迹，以模拟真实工作流程。该方法通过合成数据并进行大规模训练，在SWE-Bench Verified上的实验结果表明，其32B和72B模型分别达到56.1%和58.5%的解决率，优于之前的Kimi-Dev方法，且中期训练令牌使用量减少一半以上。</div>
</details>
</div>
<div class="card">
<div class="title">Noise-based reward-modulated learning</div>
<div class="meta-line">Authors: Jesús García Fernández, Nasir Ahmad, Marcel van Gerven</div>
<div class="meta-line">First: 2025-03-31T11:35:23+00:00 · Latest: 2026-01-26T11:58:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.23972v4">Abs</a> · <a href="https://arxiv.org/pdf/2503.23972v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The pursuit of energy-efficient and adaptive artificial intelligence (AI) has positioned neuromorphic computing as a promising alternative to conventional computing. However, achieving learning on these platforms requires techniques that prioritize local information while enabling effective credit assignment. Here, we propose noise-based reward-modulated learning (NRL), a novel synaptic plasticity rule that mathematically unifies reinforcement learning and gradient-based optimization with biologically-inspired local updates. NRL addresses the computational bottleneck of exact gradients by approximating them through stochastic neural activity, transforming the inherent noise of biological and neuromorphic substrates into a functional resource. Drawing inspiration from biological learning, our method uses reward prediction errors as its optimization target to generate increasingly advantageous behavior, and eligibility traces to facilitate retrospective credit assignment. Experimental validation on reinforcement tasks, featuring immediate and delayed rewards, shows that NRL achieves performance comparable to baselines optimized using backpropagation, although with slower convergence, while showing significantly superior performance and scalability in multi-layer networks compared to reward-modulated Hebbian learning (RMHL), the most prominent similar approach. While tested on simple architectures, the results highlight the potential of noise-driven, brain-inspired learning for low-power adaptive systems, particularly in computing substrates with locality constraints. NRL offers a theoretically grounded paradigm well-suited for the event-driven characteristics of next-generation neuromorphic AI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于噪声的奖励调制学习</div>
<div class="mono" style="margin-top:8px">追求节能和自适应人工智能（AI）使神经形态计算成为传统计算的有前景替代方案。然而，在这些平台上实现学习需要优先利用局部信息并实现有效信用分配的技术。本文提出基于噪声的奖励调制学习（NRL），这是一种新颖的突触可塑性规则，通过受生物启发的局部更新，在数学上统一了强化学习和基于梯度的优化。NRL通过随机神经活动近似精确梯度，将生物和神经形态基底固有的噪声转化为功能资源，从而解决精确梯度的计算瓶颈。受生物学习启发，该方法以奖励预测误差为优化目标生成渐进优势行为，并利用资格迹促进回溯信用分配。在包含即时和延迟奖励的强化任务实验中，NRL取得了与反向传播优化基线相当的性能（尽管收敛较慢），同时在多层网络中相比最突出的同类方法——奖励调制赫布学习（RMHL）展现出显著优越的性能和可扩展性。虽然在简单架构中测试，结果凸显了噪声驱动、类脑学习在低功耗自适应系统中的潜力，尤其适用于具有局部性约束的计算基底。NRL为契合下一代神经形态AI事件驱动特性的系统提供了理论完备的范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for energy-efficient and adaptive artificial intelligence on neuromorphic platforms, this paper introduces noise-based reward-modulated learning (NRL), a synaptic plasticity rule that unifies reinforcement learning and gradient-based optimization using local, biologically-inspired updates. The method leverages stochastic neural activity to approximate gradients, turning inherent noise into a functional resource, and employs reward prediction errors with eligibility traces for credit assignment. Experimental results on reinforcement tasks with immediate and delayed rewards show that NRL achieves performance comparable to backpropagation-optimized baselines, albeit with slower convergence, and outperforms reward-modulated Hebbian learning in multi-layer networks, demonstrating superior scalability and potential for low-power adaptive systems.</div>
<div class="mono" style="margin-top:8px">本文旨在为神经形态计算平台开发节能且自适应的人工智能，提出了噪声驱动的奖励调制学习（NRL），这是一种通过局部生物启发更新统一强化学习和梯度优化的突触可塑性规则。该方法利用随机神经活动近似梯度，将固有噪声转化为功能资源，并采用奖励预测误差和资格迹进行信用分配。在包含即时和延迟奖励的强化学习任务上的实验结果表明，NRL的性能与基于反向传播的基线方法相当，尽管收敛较慢，但在多层网络中显著优于奖励调制赫布学习，展现出更好的可扩展性，为低功耗自适应系统提供了潜力。</div>
</details>
</div>
<div class="card">
<div class="title">AI Agent for Reverse-Engineering Legacy Finite-Difference Code and Translating to Devito</div>
<div class="meta-line">Authors: Yinghan Hou, Zongyou Yang</div>
<div class="meta-line">First: 2026-01-26T11:31:00+00:00 · Latest: 2026-01-26T11:31:00+00:00</div>
<div class="meta-line">Comments: 14 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18381v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18381v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To facilitate the transformation of legacy finite difference implementations into the Devito environment, this study develops an integrated AI agent framework. Retrieval-Augmented Generation (RAG) and open-source Large Language Models are combined through multi-stage iterative workflows in the system&#x27;s hybrid LangGraph architecture. The agent constructs an extensive Devito knowledge graph through document parsing, structure-aware segmentation, extraction of entity relationships, and Leiden-based community detection. GraphRAG optimisation enhances query performance across semantic communities that include seismic wave simulation, computational fluid dynamics, and performance tuning libraries. A reverse engineering component derives three-level query strategies for RAG retrieval through static analysis of Fortran source code. To deliver precise contextual information for language model guidance, the multi-stage retrieval pipeline performs parallel searching, concept expansion, community-scale retrieval, and semantic similarity analysis. Code synthesis is governed by Pydantic-based constraints to guarantee structured outputs and reliability. A comprehensive validation framework integrates conventional static analysis with the G-Eval approach, covering execution correctness, structural soundness, mathematical consistency, and API compliance. The overall agent workflow is implemented on the LangGraph framework and adopts concurrent processing to support quality-based iterative refinement and state-aware dynamic routing. The principal contribution lies in the incorporation of feedback mechanisms motivated by reinforcement learning, enabling a transition from static code translation toward dynamic and adaptive analytical behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向逆向工程遗留有限差分代码并转换为Devito的AI智能体</div>
<div class="mono" style="margin-top:8px">为促进将遗留有限差分实现迁移至Devito环境，本研究开发了集成化AI智能体框架。系统通过混合LangGraph架构中的多阶段迭代工作流，结合检索增强生成技术与开源大语言模型。该智能体通过文档解析、结构感知分割、实体关系提取及基于Leiden的社区检测，构建了完整的Devito知识图谱。GraphRAG优化提升了在包含地震波模拟、计算流体力学和性能调优库等语义社区中的查询性能。逆向工程组件通过对Fortran源代码的静态分析，为RAG检索推导出三级查询策略。多阶段检索管道通过并行搜索、概念扩展、社区级检索和语义相似度分析，为语言模型提供精准的上下文引导信息。代码合成受基于Pydantic的约束机制管控，确保结构化输出与可靠性。验证框架整合传统静态分析与G-Eval方法，涵盖执行正确性、结构完整性、数学一致性和API合规性。整体工作流基于LangGraph框架实现，采用并发处理以支持基于质量的迭代优化和状态感知动态路由。核心贡献在于引入受强化学习启发的反馈机制，实现了从静态代码翻译到动态自适应分析行为的转变。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study is motivated by the need to modernize legacy finite-difference code by translating it into the Devito framework, a task that is complex and error-prone if done manually. The method introduces an integrated AI agent that combines Retrieval-Augmented Generation (RAG) with open-source Large Language Models within a multi-stage workflow on a hybrid LangGraph architecture; it constructs a Devito knowledge graph for enhanced retrieval and employs a reverse-engineering component to analyze Fortran source code, with code synthesis constrained by Pydantic for reliability. The main experimental results, validated through a framework combining static analysis and G-Eval, demonstrate that the agent successfully handles code translation across domains like seismic wave simulation and computational fluid dynamics, achieving correctness, structural soundness, and API compliance through iterative refinement and dynamic routing enabled by reinforcement learning-inspired feedback mechanisms.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决将遗留有限差分代码转换为Devito框架的现代化需求，这一过程若手动完成则复杂且易错。方法上，该研究开发了一个集成AI智能体，在混合LangGraph架构中通过多阶段工作流结合检索增强生成（RAG）与开源大语言模型；它构建了Devito知识图谱以提升检索性能，并利用逆向工程组件分析Fortran源代码，同时通过Pydantic约束确保代码合成的可靠性。主要实验结果通过结合静态分析和G-Eval的验证框架表明，该智能体成功处理了地震波模拟和计算流体动力学等领域的代码翻译，在基于强化学习启发的反馈机制驱动下，通过迭代优化和动态路由实现了执行正确性、结构健全性和API合规性。</div>
</details>
</div>
<div class="card">
<div class="title">Shared Spatial Memory Through Predictive Coding</div>
<div class="meta-line">Authors: Zhengru Fang, Yu Guo, Jingjing Wang, Yuang Zhang, Haonan An, Yinhai Wang, Wenbo Ding, Yuguang Fang</div>
<div class="meta-line">First: 2025-11-06T10:12:46+00:00 · Latest: 2026-01-26T11:24:30+00:00</div>
<div class="meta-line">Comments: We have prepared the open-source code and video demonstration pages: 1. Code: github.com/fangzr/SSM-PC 2. Demo: fangzr.github.io/SSM-PC/index.html</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.04235v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.04235v3">PDF</a> · <a href="http://github.com/fangzr/SSM-PC">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Constructing a consistent shared spatial memory is a critical challenge in multi-agent systems, where partial observability and limited bandwidth often lead to catastrophic failures in coordination. We introduce a multi-agent predictive coding framework that formulates coordination as the minimization of mutual uncertainty among agents. Through an information bottleneck objective, this framework prompts agents to learn not only who and what to communicate but also when. At the foundation of this framework lies a grid-cell-like metric as internal spatial coding for self-localization, emerging spontaneously from self-supervised motion prediction. Building upon this internal spatial code, agents gradually develop a bandwidth-efficient communication mechanism and specialized neural populations that encode partners&#x27; locations-an artificial analogue of hippocampal social place cells (SPCs). These social representations are further utilized by a hierarchical reinforcement learning policy that actively explores to reduce joint uncertainty. On the Memory-Maze benchmark, our approach shows exceptional resilience to bandwidth constraints: success degrades gracefully from 73.5% to 64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast baseline collapses from 67.6% to 28.6%. Our findings establish a theoretically principled and biologically plausible basis for how complex social representations emerge from a unified predictive drive, leading to collective intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于预测编码的共享空间记忆</div>
<div class="mono" style="margin-top:8px">在多智能体系统中，构建一致的共享空间记忆是关键挑战，局部可观测性和有限带宽常导致协调灾难性失败。我们提出一种多智能体预测编码框架，将协调问题转化为智能体间互不确定性的最小化。通过信息瓶颈目标，该框架促使智能体不仅学习与谁沟通、沟通什么，还学习何时沟通。框架基础采用类网格细胞度量作为自定位的内部空间编码，通过自监督运动预测自发涌现。基于此内部空间编码，智能体逐步发展出带宽高效的通信机制和编码同伴位置的特化神经集群——海马社会位置细胞的人工模拟。这些社会表征被分层强化学习策略进一步利用，通过主动探索降低联合不确定性。在Memory-Maze基准测试中，本方法展现出对带宽限制的卓越鲁棒性：当带宽从128比特/步缩减至4比特/步时，成功率从73.5%平缓降至64.4%，而全广播基线则从67.6%崩溃至28.6%。本研究为复杂社会表征如何从统一预测驱动中涌现并形成集体智能，建立了理论原则明确且生物学合理的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of building a consistent shared spatial memory in multi-agent systems under partial observability and limited bandwidth, this paper introduces a multi-agent predictive coding framework that formulates coordination as minimizing mutual uncertainty via an information bottleneck. The method involves agents learning an internal grid-cell-like spatial code from self-supervised motion prediction, which then supports the emergence of bandwidth-efficient communication and specialized neural populations akin to social place cells to encode partners&#x27; locations; these representations are used by a hierarchical reinforcement learning policy for active exploration. Key experimental results on the Memory-Maze benchmark demonstrate exceptional resilience to bandwidth constraints: success rates degrade gracefully from 73.5% to 64.4% as bandwidth shrinks from 128 to 4 bits/step, outperforming a baseline that collapses from 67.6% to 28.6%, thereby establishing a principled basis for emergent social representations and collective intelligence.</div>
<div class="mono" style="margin-top:8px">本文针对多智能体系统中部分可观测性和有限带宽导致协调失败的问题，旨在构建一致的共享空间记忆，提出了一种多智能体预测编码框架，将协调形式化为通过信息瓶颈最小化相互不确定性。该方法使智能体通过自监督运动预测学习出类似网格细胞的内部空间编码，进而催生带宽高效的通信机制和类似社交位置细胞的神经群体以编码同伴位置，并利用分层强化学习策略进行主动探索。在Memory-Maze基准测试中的主要实验结果表明，该方法对带宽限制具有卓越的韧性：当带宽从128比特/步缩减至4比特/步时，成功率从73.5%平缓下降至64.4%，而全广播基线则从67.6%骤降至28.6%，这为社交表征的涌现和集体智能提供了理论原则与生物学上合理的依据。</div>
</details>
</div>
<div class="card">
<div class="title">Temp-R1: A Unified Autonomous Agent for Complex Temporal KGQA via Reverse Curriculum Reinforcement Learning</div>
<div class="meta-line">Authors: Zhaoyan Gong, Zhiqiang Liu, Songze Li, Xiaoke Guo, Yuanxiang Liu, Xinle Deng, Zhizhen Liu, Lei Liang, Huajun Chen, Wen Zhang</div>
<div class="meta-line">First: 2026-01-26T09:23:53+00:00 · Latest: 2026-01-26T09:23:53+00:00</div>
<div class="meta-line">Comments: Work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18296v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18296v1">PDF</a> · <a href="https://github.com/zjukg/Temp-R1">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Temporal Knowledge Graph Question Answering (TKGQA) is inherently challenging, as it requires sophisticated reasoning over dynamic facts with multi-hop dependencies and complex temporal constraints. Existing methods rely on fixed workflows and expensive closed-source APIs, limiting flexibility and scalability. We propose Temp-R1, the first autonomous end-to-end agent for TKGQA trained through reinforcement learning. To address cognitive overload in single-action reasoning, we expand the action space with specialized internal actions alongside external action. To prevent shortcut learning on simple questions, we introduce reverse curriculum learning that trains on difficult questions first, forcing the development of sophisticated reasoning before transferring to easier cases. Our 8B-parameter Temp-R1 achieves state-of-the-art performance on MultiTQ and TimelineKGQA, improving 19.8% over strong baselines on complex questions. Our work establishes a new paradigm for autonomous temporal reasoning agents. Our code will be publicly available soon at https://github.com/zjukg/Temp-R1.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Temp-R1：基于逆向课程强化学习的复杂时序知识图谱问答统一自主智能体</div>
<div class="mono" style="margin-top:8px">时序知识图谱问答（TKGQA）因需对具有多跳依赖和复杂时序约束的动态事实进行精细推理而具有本质挑战性。现有方法依赖固定流程和昂贵的闭源API，限制了灵活性与可扩展性。我们提出首个通过强化学习训练的端到端自主TKGQA智能体Temp-R1。为解决单步推理中的认知过载问题，我们在外部动作基础上扩展了包含专用内部动作的动作空间。为避免简单问题上的捷径学习，我们引入逆向课程学习机制，优先训练困难问题，迫使智能体在迁移至简单案例前先发展复杂推理能力。参数量达80亿的Temp-R1在MultiTQ和TimelineKGQA数据集上取得最先进性能，在复杂问题上较基线模型提升19.8%。本研究为自主时序推理智能体确立了新范式，代码即将发布于https://github.com/zjukg/Temp-R1。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to overcome the limitations of existing Temporal Knowledge Graph Question Answering (TKGQA) methods, which rely on rigid workflows and costly APIs, by creating a more flexible and scalable autonomous agent. The method introduces Temp-R1, an end-to-end agent trained with reinforcement learning, which expands the action space with specialized internal actions and employs a reverse curriculum learning strategy that starts training on difficult questions to foster sophisticated reasoning before tackling easier ones. The main experimental results show that the 8B-parameter Temp-R1 achieves state-of-the-art performance on the MultiTQ and TimelineKGQA benchmarks, with a 19.8% improvement over strong baselines on complex questions, establishing a new paradigm for autonomous temporal reasoning agents.</div>
<div class="mono" style="margin-top:8px">本研究的动机是克服现有时序知识图谱问答方法依赖固定流程和昂贵API的限制，旨在开发更灵活、可扩展的自主智能体。方法上提出了Temp-R1，这是一种通过强化学习训练的端到端智能体，它通过引入专门的内部动作扩展了动作空间，并采用逆向课程学习策略，即先训练困难问题以培养复杂推理能力，再迁移至简单问题。主要实验结果表明，参数量为80亿的Temp-R1在MultiTQ和TimelineKGQA基准测试中取得了最先进的性能，在复杂问题上比强基线提升了19.8%，为自主时序推理智能体确立了新范式。</div>
</details>
</div>
<div class="card">
<div class="title">TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment</div>
<div class="meta-line">Authors: Zhewen Tan, Wenhan Yu, Jianfeng Si, Tongxin Liu, Kaiqi Guan, Huiyan Jin, Jiawen Tao, Xiaokun Yuan, Duohe Ma, Xiangzheng Zhang, Tong Yang, Lin Sun</div>
<div class="meta-line">First: 2026-01-26T09:21:43+00:00 · Latest: 2026-01-26T09:21:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18292v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18292v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent years, safety risks associated with large language models have become increasingly prominent, highlighting the urgent need to mitigate the generation of toxic and harmful content. The mainstream paradigm for LLM safety alignment typically adopts a collaborative framework involving three roles: an attacker for adversarial prompt generation, a defender for safety defense, and an evaluator for response assessment. In this paper, we propose a closed-loop reinforcement learning framework called TriPlay-RL that enables iterative and co-improving collaboration among three roles with near-zero manual annotation. Experimental results show that the attacker preserves high output diversity while achieving a 20%-50% improvement in adversarial effectiveness; the defender attains 10%-30% gains in safety performance without degrading general reasoning capability; and the evaluator continuously refines its fine-grained judgment ability through iterations, accurately distinguishing unsafe responses, simple refusals, and useful guidance. Overall, our framework establishes an efficient and scalable paradigm for LLM safety alignment, enabling continuous co-evolution within a unified learning loop.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TriPlay-RL：面向大语言模型安全对齐的三角色自博弈强化学习</div>
<div class="mono" style="margin-top:8px">近年来，大语言模型的安全风险日益凸显，亟需抑制其生成有害内容。主流的安全对齐范式通常采用包含攻击者（生成对抗性提示）、防御者（实施安全防护）和评估者（评估响应）的三角色协作框架。本文提出一种名为TriPlay-RL的闭环强化学习框架，可在近乎无需人工标注的情况下实现三角色的迭代式协同优化。实验表明：攻击者在保持高输出多样性的同时，对抗效果提升20%-50%；防御者在安全性能上获得10%-30%的增益，且不影响通用推理能力；评估者通过迭代持续优化细粒度判别能力，能准确区分不安全响应、简单拒绝和有效指导。该框架为大语言模型安全对齐构建了一个高效可扩展的范式，实现了统一学习循环内的持续协同进化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the growing safety risks of large language models (LLMs) and the need to reduce toxic content generation, this paper introduces TriPlay-RL, a closed-loop reinforcement learning framework that facilitates iterative collaboration among three roles—an attacker generating adversarial prompts, a defender implementing safety measures, and an evaluator assessing responses—with minimal manual annotation. The method enables these roles to co-evolve within a unified learning loop, where experimental results demonstrate that the attacker improves adversarial effectiveness by 20%-50% while maintaining high output diversity, the defender achieves 10%-30% gains in safety performance without compromising general reasoning, and the evaluator refines its judgment to accurately differentiate unsafe responses, simple refusals, and useful guidance. Overall, the framework establishes an efficient and scalable paradigm for continuous safety alignment in LLMs.</div>
<div class="mono" style="margin-top:8px">针对大语言模型日益突出的安全风险及减少有害内容生成的迫切需求，本文提出了TriPlay-RL，一种闭环强化学习框架，通过攻击者生成对抗性提示、防御者实施安全防护、评估者进行响应评估的三角色协作，在近乎零人工标注下实现迭代协同改进。该方法使三角色在统一学习循环中共同进化，实验结果表明，攻击者在保持高输出多样性的同时将对抗有效性提升20%-50%，防御者在安全性能上获得10%-30%的增益且不损害通用推理能力，评估者通过迭代持续优化细粒度判断能力，能准确区分不安全响应、简单拒绝和有用指导。该框架为大语言模型的安全对齐建立了一个高效可扩展的持续协同进化范式。</div>
</details>
</div>
<div class="card">
<div class="title">The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models</div>
<div class="meta-line">Authors: Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao, Yeguo Hua, Tianyi Chen, Jun Song, Cheng Yu, Bo Zheng, Gao Huang</div>
<div class="meta-line">First: 2026-01-21T16:41:58+00:00 · Latest: 2026-01-26T08:29:32+00:00</div>
<div class="meta-line">Comments: Code and pre-trained models: https://github.com/LeapLabTHU/JustGRPO</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15165v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.15165v2">PDF</a> · <a href="https://github.com/LeapLabTHU/JustGRPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://nzl-thu.github.io/the-flexibility-trap">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation motivates a rethink of RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning can be better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>灵活性陷阱：为何任意顺序限制扩散语言模型的推理潜力</div>
<div class="mono" style="margin-top:8px">扩散大语言模型（dLLMs）打破了传统LLM严格的从左到右约束，实现了按任意顺序生成词元。直观上，这种灵活性意味着其解空间严格包含了固定自回归轨迹，理论上为数学和编程等通用任务解锁了更优的推理潜力。因此，许多研究利用强化学习（RL）来激发dLLMs的推理能力。本文揭示了一个反直觉的现实：当前形式的任意顺序生成非但没有扩展dLLMs的推理边界，反而使其收窄。我们发现dLLMs倾向于利用这种顺序灵活性来规避对探索至关重要的高不确定性词元，导致解空间过早坍缩。这一观察促使我们重新思考针对dLLMs的RL方法——现有方法往往投入大量复杂度（如处理组合轨迹和难解似然）以维持这种灵活性。我们证明，通过主动放弃任意顺序并改用标准组相对策略优化（GRPO），反而能更好地激发有效推理。我们的方法JustGRPO极简却出奇有效（例如在GSM8K上达到89.1%准确率），同时完全保留了dLLMs的并行解码能力。项目页面：https://nzl-thu.github.io/the-flexibility-trap</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper challenges the assumption that arbitrary token generation order in diffusion language models inherently enhances reasoning, showing instead that it can lead models to avoid uncertain but critical tokens, thereby prematurely collapsing the solution space. The authors propose a minimalist method, JustGRPO, which intentionally forgoes arbitrary order generation and applies standard Group Relative Policy Optimization, effectively eliciting reasoning while retaining parallel decoding. Experimental results demonstrate its effectiveness, achieving 89.1% accuracy on the GSM8K benchmark.</div>
<div class="mono" style="margin-top:8px">本文挑战了扩散语言模型中任意顺序生成令牌能增强推理能力的假设，揭示其反而可能导致模型规避关键但不确定的令牌，从而过早压缩解空间。作者提出了一种极简方法JustGRPO，有意放弃任意顺序生成，采用标准组相对策略优化，在保持并行解码的同时有效激发推理能力。实验结果显示该方法效果显著，在GSM8K基准上达到了89.1%的准确率。</div>
</details>
</div>
<div class="card">
<div class="title">GeoVLMath: Enhancing Geometry Reasoning in Vision-Language Models via Cross-Modal Reward for Auxiliary Line Creation</div>
<div class="meta-line">Authors: Shasha Guo, Liang Pang, Xi Wang, Yanling Wang, Huawei Shen, Jing Zhang</div>
<div class="meta-line">First: 2025-10-13T05:33:51+00:00 · Latest: 2026-01-26T07:35:43+00:00</div>
<div class="meta-line">Comments: 19 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.11020v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.11020v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Auxiliary lines are essential for solving complex geometric problems but remain challenging for large vision-language models (LVLMs). Recent attempts construct auxiliary lines via code-driven rendering, a strategy that relies on accurate and executable code generation to produce visual renderings of the auxiliary lines for subsequent reasoning. However, in complex solid geometry settings, such a strong dependence on precise specifications substantially restricts the robustness of this strategy. Alternatively, we turn to a simpler and more stable solution, representing auxiliary-line constructions as structured textual descriptions. To bridge the gap between textual descriptions and spatial structure, we propose a reinforcement learning framework that enhances diagram-text alignment. The core is a cross-modal reward model that evaluates how well the generated auxiliary-line description matches the ground-truth auxiliary-line diagram. The reward signal drives a GRPO-based RL stage to yield informative auxiliary-line descriptions for the reasoning. To support the training and evaluation, we develop a scalable data pipeline and construct AuxSolidMath, a dataset of 3,018 real-exam geometry problems with paired diagrams and aligned textual fields. Based on this framework, we derive GeoVLMath, an LVLM for solving complex solid geometry.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GeoVLMath：通过跨模态奖励增强视觉语言模型的几何推理能力——基于辅助线构建</div>
<div class="mono" style="margin-top:8px">辅助线对解决复杂几何问题至关重要，但对大型视觉语言模型（LVLM）仍具挑战。现有方法通过代码驱动渲染构建辅助线，该策略依赖精确可执行的代码生成来呈现辅助线视觉化以支持后续推理。然而在复杂立体几何场景中，这种对精确规范的高度依赖严重限制了策略的鲁棒性。为此，我们转向更简洁稳定的解决方案：将辅助线构建表示为结构化文本描述。为弥合文本描述与空间结构的鸿沟，我们提出强化学习框架以增强图文对齐，其核心是评估生成辅助线描述与真实辅助线图示匹配度的跨模态奖励模型。奖励信号驱动基于GRPO的强化学习阶段，生成具有信息量的辅助线描述以支持推理。为支撑训练与评估，我们开发了可扩展数据流水线，构建了包含3,018道真实考题、配备图示与对齐文本字段的立体几何数据集AuxSolidMath。基于此框架，我们最终得到用于解决复杂立体几何问题的LVLM模型GeoVLMath。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of enabling large vision-language models (LVLMs) to create auxiliary lines for solving complex geometry problems, where existing code-driven rendering methods are fragile due to their reliance on precise code generation. The authors propose a reinforcement learning framework that represents auxiliary lines as structured textual descriptions and uses a cross-modal reward model to align these descriptions with ground-truth diagrams, training the model via GRPO-based reinforcement learning to produce informative descriptions for reasoning. Experimental results are supported by a new dataset, AuxSolidMath, containing 3,018 real-exam geometry problems, and the derived model, GeoVLMath, demonstrates enhanced performance in solving complex solid geometry tasks.</div>
<div class="mono" style="margin-top:8px">本文针对大型视觉语言模型在解决复杂几何问题时创建辅助线所面临的挑战，现有基于代码驱动渲染的方法因依赖精确代码生成而显得脆弱。作者提出一种强化学习框架，将辅助线表示为结构化文本描述，并利用跨模态奖励模型来对齐这些描述与真实几何图形，通过基于GRPO的强化学习训练模型以生成用于推理的信息性描述。实验结果基于新构建的AuxSolidMath数据集，该数据集包含3,018个真实考试几何问题，所推导的GeoVLMath模型在复杂立体几何任务中表现出增强的推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Yunjue Agent Tech Report: A Fully Reproducible, Zero-Start In-Situ Self-Evolving Agent System for Open-Ended Tasks</div>
<div class="meta-line">Authors: Haotian Li, Shijun Yang, Weizhen Qi, Silei Zhao, Rui Hua, Mingzhu Song, Xiaojian Yang, Chao Peng</div>
<div class="meta-line">First: 2026-01-26T07:27:47+00:00 · Latest: 2026-01-26T07:27:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18226v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18226v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Conventional agent systems often struggle in open-ended environments where task distributions continuously drift and external supervision is scarce. Their reliance on static toolsets or offline training lags behind these dynamics, leaving the system&#x27;s capability boundaries rigid and unknown. To address this, we propose the In-Situ Self-Evolving paradigm. This approach treats sequential task interactions as a continuous stream of experience, enabling the system to distill short-term execution feedback into long-term, reusable capabilities without access to ground-truth labels. Within this framework, we identify tool evolution as the critical pathway for capability expansion, which provides verifiable, binary feedback signals. Within this framework, we develop Yunjue Agent, a system that iteratively synthesizes, optimizes, and reuses tools to navigate emerging challenges. To optimize evolutionary efficiency, we further introduce a Parallel Batch Evolution strategy. Empirical evaluations across five diverse benchmarks under a zero-start setting demonstrate significant performance gains over proprietary baselines. Additionally, complementary warm-start evaluations confirm that the accumulated general knowledge can be seamlessly transferred to novel domains. Finally, we propose a novel metric to monitor evolution convergence, serving as a function analogous to training loss in conventional optimization. We open-source our codebase, system traces, and evolved tools to facilitate future research in resilient, self-evolving intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>云觉智能体技术报告：面向开放任务的完全可复现、零启动原位自进化智能体系统</div>
<div class="mono" style="margin-top:8px">传统智能体系统在任务分布持续漂移且外部监督稀缺的开放环境中常面临挑战。其依赖静态工具集或离线训练的模式难以适应动态变化，导致系统能力边界僵化且未知。为此，我们提出原位自进化范式，将序列化任务交互视为连续经验流，使系统能在无真实标签的情况下，将短期执行反馈提炼为长期可复用的能力。在此框架中，我们将工具进化定位为能力扩展的关键路径，其可提供可验证的二元反馈信号。基于此，我们开发了云觉智能体系统，通过迭代合成、优化和复用工具以应对新兴挑战。为提升进化效率，我们进一步提出并行批量进化策略。在零启动设置下对五个多样化基准的实证评估显示，该系统较专有基线模型取得显著性能提升。补充性热启动实验也证实，系统积累的通用知识可无缝迁移至新领域。最后，我们提出一种监测进化收敛的新指标，其功能类似于传统优化中的训练损失。我们开源了代码库、系统轨迹及进化工具，以促进韧性自进化智能研究的未来发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of conventional agent systems in open-ended environments with drifting task distributions and scarce supervision, this paper proposes the In-Situ Self-Evolving paradigm, which treats sequential task interactions as a continuous experience stream to distill short-term feedback into long-term, reusable capabilities without ground-truth labels. The method centers on tool evolution as a pathway for capability expansion and introduces the Yunjue Agent system, which iteratively synthesizes, optimizes, and reuses tools, enhanced by a Parallel Batch Evolution strategy for efficiency. Experimental results across five benchmarks in a zero-start setting show significant performance gains over proprietary baselines, with warm-start evaluations confirming transferable general knowledge, and a novel metric is proposed to monitor evolution convergence, with the codebase and tools made open-source.</div>
<div class="mono" style="margin-top:8px">针对传统智能体系统在任务分布持续变化且外部监督稀缺的开放环境中能力受限的问题，本文提出了原位自进化范式，将序列化任务交互视为连续经验流，使系统能将短期执行反馈提炼为无需真实标签的长期可复用能力。该方法以工具进化为核心扩展途径，开发了云雀智能体系统，通过迭代合成、优化和重用工具来应对新挑战，并采用并行批量进化策略提升效率。在零启动设置下的五个多样化基准测试中，实验结果表明其性能显著优于专有基线，补充的热启动评估验证了所积累的通用知识可无缝迁移至新领域，同时提出了一种监测进化收敛的新指标，相关代码库与进化工具均已开源。</div>
</details>
</div>
<div class="card">
<div class="title">ShopSimulator: Evaluating and Exploring RL-Driven LLM Agent for Shopping Assistants</div>
<div class="meta-line">Authors: Pei Wang, Yanan Wu, Xiaoshuai Song, Weixun Wang, Gengru Chen, Zhongwen Li, Kezhong Yan, Ken Deng, Qi Liu, Shuaibing Zhao, Shaopan Xiong, Xuepeng Liu, Xuefeng Chen, Wanxi Deng, Wenbo Su, Bo Zheng</div>
<div class="meta-line">First: 2026-01-26T07:24:28+00:00 · Latest: 2026-01-26T07:24:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18225v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18225v1">PDF</a> · <a href="https://github.com/ShopAgent-Team/ShopSimulator">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language model (LLM)-based agents are increasingly deployed in e-commerce shopping. To perform thorough, user-tailored product searches, agents should interpret personal preferences, engage in multi-turn dialogues, and ultimately retrieve and discriminate among highly similar products. However, existing research has yet to provide a unified simulation environment that consistently captures all of these aspects, and always focuses solely on evaluation benchmarks without training support. In this paper, we introduce ShopSimulator, a large-scale and challenging Chinese shopping environment. Leveraging ShopSimulator, we evaluate LLMs across diverse scenarios, finding that even the best-performing models achieve less than 40% full-success rate. Error analysis reveals that agents struggle with deep search and product selection in long trajectories, fail to balance the use of personalization cues, and to effectively engage with users. Further training exploration provides practical guidance for overcoming these weaknesses, with the combination of supervised fine-tuning (SFT) and reinforcement learning (RL) yielding significant performance improvements. Code and data will be released at https://github.com/ShopAgent-Team/ShopSimulator.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ShopSimulator：面向购物助手的RL驱动LLM智能体评估与探索</div>
<div class="mono" style="margin-top:8px">基于大语言模型（LLM）的智能体在电子商务购物场景中日益普及。为执行全面且个性化的产品搜索，智能体需解析用户偏好、进行多轮对话，并最终在高度相似的产品中进行检索与甄别。然而，现有研究尚未提供能系统涵盖上述所有维度的统一仿真环境，且往往仅聚焦评估基准而缺乏训练支持。本文提出ShopSimulator——一个大规模、高挑战性的中文购物仿真环境。基于该环境，我们对多种场景下的LLM进行评估，发现即使最优模型的完全成功率仍低于40%。错误分析表明，智能体在长轨迹任务中难以实现深度搜索与产品选择，无法平衡个性化线索的运用，且与用户的有效互动能力不足。进一步的训练探索为克服这些缺陷提供了实践指导：监督微调（SFT）与强化学习（RL）相结合可显著提升模型性能。代码与数据将在https://github.com/ShopAgent-Team/ShopSimulator发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the lack of a unified simulation environment for evaluating and training LLM-based shopping assistants that need to interpret preferences, conduct multi-turn dialogues, and discriminate among similar products, this paper introduces ShopSimulator, a large-scale Chinese shopping environment. The method involves using this simulator to evaluate various LLMs and then exploring training strategies like supervised fine-tuning (SFT) and reinforcement learning (RL) to address identified weaknesses. Main experimental results show that even top models achieve under 40% full-success rate, struggling with deep search, product selection in long trajectories, balancing personalization cues, and user engagement; however, combining SFT and RL yields significant performance improvements.</div>
<div class="mono" style="margin-top:8px">本文的动机是现有研究缺乏一个统一的模拟环境来评估和训练基于大语言模型的购物助手，这些助手需要理解用户偏好、进行多轮对话并区分相似商品。为此，本文提出了ShopSimulator，一个大规模的中文购物模拟环境。方法上，利用该模拟器评估了多种大语言模型，并探索了监督微调（SFT）和强化学习（RL）等训练策略以解决发现的缺陷。主要实验结果表明，即使表现最佳的模型完全成功率也不足40%，在深度搜索、长轨迹中的商品选择、平衡个性化线索以及用户互动方面存在困难；但结合SFT和RL的训练能带来显著的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents</div>
<div class="meta-line">Authors: Zhihan Liu, Lin Guan, Yixin Nie, Kai Zhang, Zhuoqun Hao, Lin Chen, Asli Celikyilmaz, Zhaoran Wang, Na Zhang</div>
<div class="meta-line">First: 2026-01-26T07:07:03+00:00 · Latest: 2026-01-26T07:07:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18217v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18217v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>降低泛化税：面向大语言模型智能体的强化学习训练跨领域泛化研究</div>
<div class="mono" style="margin-top:8px">通用型大语言模型智能体通常在有限环境集上进行后训练，却需部署至更广泛、未知的领域。本研究针对测试领域未知时智能体后训练的挑战展开探究，重点分析强化学习环境特性与建模选择对领域外性能的关键影响。首先，我们识别出与跨领域泛化强相关的两个环境维度：（i）状态信息丰富度，即智能体从状态中需处理的信息量；（ii）规划复杂度，通过基础策略下的目标可达性与轨迹长度估算。值得注意的是，领域真实性与文本相似度并非主要因素；例如，简单的网格世界推箱子游戏在SciWorld中产生的泛化效果甚至优于更贴近现实的ALFWorld。基于此发现，我们进一步证明仅提升状态信息丰富度即可有效增强跨领域鲁棒性，并提出一种低开销、普适的随机化技术：在状态中添加少量与目标无关的干扰特征以丰富状态信息，同时保持任务不变。除环境特性外，我们还检验了多项建模选择：（a）监督微调预热或中期训练虽能防止强化学习中的灾难性遗忘，但会削弱对未包含在中期训练数据混合领域中的泛化能力；（b）在强化学习中启用逐步推理虽不总能提升领域内性能，但对保持泛化能力具有关键作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of training generalist LLM agents for deployment in unknown domains by investigating how reinforcement learning (RL) environment properties and modeling choices affect cross-domain generalization. The authors identify two key environment axes—state information richness and planning complexity—as strong correlates of generalization, finding that domain realism and text similarity are less critical; for example, training on the simple Sokoban grid-world generalizes better to SciWorld than on the more realistic ALFWorld. Motivated by this, they propose a low-overhead randomization technique that enriches states with distractive, goal-irrelevant features to boost robustness. Additionally, experimental results show that SFT warmup or mid-training helps prevent catastrophic forgetting but can harm generalization to excluded domains, while enabling step-by-step thinking during RL is crucial for preserving generalization even when in-domain performance does not improve.</div>
<div class="mono" style="margin-top:8px">本研究针对通用大语言模型智能体在未知领域部署的挑战，探究了强化学习环境属性与建模选择如何影响跨领域泛化能力。作者发现两个关键环境轴——状态信息丰富度和规划复杂度——与泛化能力强相关，而领域真实性和文本相似性则影响较小；例如，在简单的Sokoban网格世界训练比在更真实的ALFWorld训练能更好地泛化到SciWorld。基于此，他们提出一种低开销的随机化技术，通过向状态添加分散注意力的、与目标无关的特征来增强鲁棒性。实验结果表明，监督微调预热或中期训练有助于防止灾难性遗忘，但会损害对未包含领域的泛化；而在强化学习中启用逐步思考对于保持泛化能力至关重要，即使域内性能未必提升。</div>
</details>
</div>
<div class="card">
<div class="title">PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR</div>
<div class="meta-line">Authors: James Burgess, Jan N. Hansen, Duo Peng, Yuhui Zhang, Alejandro Lozano, Min Woo Sun, Emma Lundberg, Serena Yeung-Levy</div>
<div class="meta-line">First: 2026-01-26T06:46:16+00:00 · Latest: 2026-01-26T06:46:16+00:00</div>
<div class="meta-line">Comments: EACL 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18207v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18207v1">PDF</a> · <a href="https://huggingface.co/collections/jmhb/papersearchqa">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on https://huggingface.co/collections/jmhb/papersearchqa. Finally, our data creation methods are scalable and easily extendable to other scientific domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PaperSearchQA：基于RLVR的科研文献检索与推理学习框架</div>
<div class="mono" style="margin-top:8px">检索智能体是通过语言模型对知识库（或网络）进行推理检索以回答问题的系统；当前主流方法仅采用可验证奖励的强化学习监督最终答案准确性。现有RLVR检索智能体多面向通用领域问答，限制了其在科学、工程、医学等专业AI系统中的适用性。本研究提出训练面向科研文献检索与推理的智能体——这既能检验技术性问答能力，又直接服务于科研工作者，其能力对未来AI科学家系统至关重要。具体而言，我们发布了包含1600万篇生物医学论文摘要的检索语料库，并构建了含6万样本、可从该语料库获取答案的挑战性事实问答数据集PaperSearchQA及基准测试。在此环境中训练的检索智能体性能超越非强化学习检索基线；定量分析进一步揭示了智能体在规划、推理、自我验证等方面的有趣行为。本研究的语料库、数据集及基准测试可通过RLVR训练常用代码库Search-R1使用，并发布于https://huggingface.co/collections/jmhb/papersearchqa。最后，我们的数据构建方法具备可扩展性，能便捷迁移至其他科学领域。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation is to develop search agents capable of answering technical questions in scientific domains, moving beyond general-domain QA to support real scientists and future AI systems. The method involves creating PaperSearchQA, a dataset of 60k factoid questions answerable from a corpus of 16 million biomedical paper abstracts, and training language model agents using reinforcement learning with verifiable rewards (RLVR) in this environment. The main experimental results show that these RLVR-trained agents outperform non-RL retrieval baselines, demonstrating behaviors such as planning, reasoning, and self-verification, with the resources made publicly available for further research.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发能够回答科学领域技术问题的搜索智能体，超越通用领域问答，以支持实际科学家和未来人工智能系统。方法包括构建PaperSearchQA数据集，其中包含6万个可从1600万篇生物医学论文摘要语料库中回答的事实型问题，并在此环境中使用可验证奖励的强化学习训练语言模型智能体。主要实验结果表明，这些经过RLVR训练的智能体优于非强化学习的检索基线，展现出规划、推理和自我验证等行为，相关资源已公开供进一步研究使用。</div>
</details>
</div>
<div class="card">
<div class="title">When Sharpening Becomes Collapse: Sampling Bias and Semantic Coupling in RL with Verifiable Rewards</div>
<div class="meta-line">Authors: Mingyuan Fan, Weiguang Han, Daixin Wang, Cen Chen, Zhiqiang Zhang, Jun Zhou</div>
<div class="meta-line">First: 2026-01-22T03:15:57+00:00 · Latest: 2026-01-26T06:32:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15609v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.15609v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) is a central paradigm for turning large language models (LLMs) into reliable problem solvers, especially in logic-heavy domains. Despite its empirical success, it remains unclear whether RLVR elicits novel capabilities or merely sharpens the distribution over existing knowledge. We study this by formalizing over-sharpening, a phenomenon where the policy collapses onto limited modes, suppressing valid alternatives. At a high level, we discover finite-batch updates intrinsically bias learning toward sampled modes, triggering a collapse that propagates globally via semantic coupling. To mitigate this, we propose inverse-success advantage calibration to prioritize difficult queries and distribution-level calibration to diversify sampling via a memory network. Empirical evaluations validate that our strategies can effectively improve generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当锐化演变为坍缩：可验证奖励强化学习中的采样偏差与语义耦合</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）是将大语言模型转化为可靠问题求解器的核心范式，尤其在逻辑密集型领域表现突出。尽管实证研究已证实其有效性，但RLVR究竟是激发了新能力，还是仅强化了现有知识的分布，仍不明确。本研究通过形式化“过度锐化”现象对此展开探讨——即策略坍缩至有限模式、压制有效替代方案的过程。研究发现，有限批次更新本质上会使学习偏向已采样模式，进而通过语义耦合引发全局性坍缩。为缓解此问题，我们提出逆成功优势校准（优先处理困难查询）与分布级校准（通过记忆网络实现采样多样化）两种方法。实证评估表明，这些策略能有效提升模型的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the phenomenon of over-sharpening in Reinforcement Learning with Verifiable Rewards (RLVR), a method used to enhance large language models for logic-heavy tasks, motivated by the need to determine whether RLVR develops new capabilities or merely narrows existing knowledge. The authors formalize over-sharpening as a policy collapse onto limited modes, attributing it to sampling bias in finite-batch updates and its propagation through semantic coupling. To address this, they propose two calibration strategies: inverse-success advantage calibration to prioritize difficult queries and distribution-level calibration using a memory network to diversify sampling. Experimental results demonstrate that these methods effectively mitigate collapse and improve the model&#x27;s generalization performance.</div>
<div class="mono" style="margin-top:8px">本文研究了可验证奖励强化学习中的过度锐化现象，该方法用于增强大语言模型在逻辑密集型任务中的表现，其动机是需厘清RLVR是激发了新能力还是仅压缩了现有知识。作者将过度锐化形式化为策略坍缩到有限模式，并将其归因于有限批次更新中的采样偏差以及通过语义耦合的全局传播。为缓解此问题，他们提出了两种校准策略：逆成功优势校准以优先处理困难查询，以及通过记忆网络进行分布级校准以多样化采样。实验结果表明，这些方法能有效减轻坍缩并提升模型的泛化性能。</div>
</details>
</div>
<div class="card">
<div class="title">A Regularized Actor-Critic Algorithm for Bi-Level Reinforcement Learning</div>
<div class="meta-line">Authors: Sihan Zeng, Sujay Bhatt, Sumitra Ganesh, Alec Koppel</div>
<div class="meta-line">First: 2026-01-23T02:12:24+00:00 · Latest: 2026-01-26T05:27:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16399v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.16399v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study a structured bi-level optimization problem where the upper-level objective is a smooth function and the lower-level problem is policy optimization in a Markov decision process (MDP). The upper-level decision variable parameterizes the reward of the lower-level MDP, and the upper-level objective depends on the optimal induced policy. Existing methods for bi-level optimization and RL often require second-order information, impose strong regularization at the lower level, or inefficiently use samples through nested-loop procedures. In this work, we propose a single-loop, first-order actor-critic algorithm that optimizes the bi-level objective via a penalty-based reformulation. We introduce into the lower-level RL objective an attenuating entropy regularization, which enables asymptotically unbiased upper-level hyper-gradient estimation without solving the unregularized RL problem exactly. We establish the finite-time and finite-sample convergence of the proposed algorithm to a stationary point of the original, unregularized bi-level optimization problem through a novel lower-level residual analysis under a special type of Polyak-Lojasiewicz condition. We validate the performance of our method through experiments on a GridWorld goal position problem and on happy tweet generation through reinforcement learning from human feedback (RLHF).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种用于双层强化学习的正则化行动者-评论家算法</div>
<div class="mono" style="margin-top:8px">本文研究一个结构化的双层优化问题，其中上层目标函数为光滑函数，下层问题是在马尔可夫决策过程中的策略优化。上层决策变量参数化下层MDP的奖励函数，上层目标依赖于下层诱导的最优策略。现有双层优化与强化学习方法通常需要二阶信息、对下层施加强正则化约束，或通过嵌套循环流程低效使用样本。本研究提出一种单循环一阶行动者-评论家算法，通过基于惩罚的重构形式优化双层目标。我们在下层RL目标中引入衰减熵正则化，使得无需精确求解无正则化RL问题即可实现渐近无偏的上层超梯度估计。通过特殊类型Polyak-Lojasiewicz条件下的新型下层残差分析，我们证明了所提算法在有限时间和有限样本条件下收敛至原始无正则化双层优化问题的稳定点。通过在GridWorld目标定位问题及基于人类反馈的强化学习生成积极推文任务上的实验，验证了方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the computational challenges in bi-level reinforcement learning, where an upper-level objective depends on the optimal policy of a lower-level Markov decision process whose reward is parameterized by the upper variable. The authors propose a single-loop, first-order actor-critic algorithm that reformulates the problem using a penalty approach and incorporates an attenuating entropy regularization in the lower-level RL objective, enabling asymptotically unbiased hyper-gradient estimation without exact solution of the unregularized RL problem. Experimental validation on a GridWorld task and a reinforcement learning from human feedback (RLHF) application for happy tweet generation demonstrates the method&#x27;s effectiveness, with theoretical convergence guarantees to a stationary point under a Polyak-Lojasiewicz condition.</div>
<div class="mono" style="margin-top:8px">本文针对双层强化学习中的计算难题展开研究，其中上层目标函数依赖于下层马尔可夫决策过程的最优策略，而下层奖励由上层的参数化变量决定。作者提出了一种单循环、一阶的演员-评论家算法，通过惩罚重构将问题转化，并在下层RL目标中引入衰减的熵正则化，从而无需精确求解未正则化的RL问题即可实现渐近无偏的超梯度估计。在GridWorld目标定位任务和基于人类反馈的强化学习（RLHF）生成快乐推文的实验中验证了该方法的性能，并在Polyak-Lojasiewicz条件下提供了收敛到原问题稳定点的理论保证。</div>
</details>
</div>
<div class="card">
<div class="title">FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning</div>
<div class="meta-line">Authors: Zhaopeng Qiu, Shuang Yu, Jingqi Zhang, Shuai Zhang, Xue Huang, Jingyi Yang, Junjie Lai</div>
<div class="meta-line">First: 2026-01-26T05:12:05+00:00 · Latest: 2026-01-26T05:12:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18150v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18150v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) for large language models (LLMs) is increasingly bottlenecked by rollout (generation), where long output sequence lengths make attention and KV-cache memory dominate end-to-end step time. FP8 offers an attractive lever for accelerating RL by reducing compute cost and memory traffic during rollout, but applying FP8 in RL introduces unique engineering and algorithmic challenges: policy weights change every step (requiring repeated quantization and weight synchronization into the inference engine) and low-precision rollouts can deviate from the higher-precision policy assumed by the trainer, causing train-inference mismatch and potential instability. This report presents a practical FP8 rollout stack for LLM RL, implemented in the veRL ecosystem with support for common training backends (e.g., FSDP/Megatron-LM) and inference engines (e.g., vLLM/SGLang). We (i) enable FP8 W8A8 linear-layer rollout using blockwise FP8 quantization, (ii) extend FP8 to KV-cache to remove long-context memory bottlenecks via per-step QKV scale recalibration, and (iii) mitigate mismatch using importance-sampling-based rollout correction (token-level TIS/MIS variants). Across dense and MoE models, these techniques deliver up to 44% rollout throughput gains while preserving learning behavior comparable to BF16 baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FP8-RL：面向大语言模型强化学习的实用稳定低精度技术栈</div>
<div class="mono" style="margin-top:8px">大语言模型（LLM）的强化学习（RL）日益受限于生成阶段：长输出序列导致注意力机制与KV缓存内存成为端到端步骤的主要耗时瓶颈。FP8通过降低生成阶段的计算成本与内存流量，为加速RL提供了有效途径，但在RL中应用FP8面临独特的工程与算法挑战：策略权重每步更新（需重复量化并同步至推理引擎），且低精度生成可能偏离训练器预设的高精度策略，引发训练-推理失配及潜在不稳定问题。本报告提出面向LLM RL的实用FP8生成技术栈，在veRL生态中实现并兼容主流训练后端（如FSDP/Megatron-LM）与推理引擎（如vLLM/SGLang）。我们（i）采用分块FP8量化实现W8A8线性层生成，（ii）通过逐步QKV缩放重校准将FP8扩展至KV缓存以消除长上下文内存瓶颈，（iii）利用基于重要性采样的生成校正（词元级TIS/MIS变体）缓解失配效应。在稠密与混合专家模型上的实验表明，该技术栈在保持与BF16基线相当学习性能的同时，可实现最高44%的生成吞吐量提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the computational and memory bottlenecks in reinforcement learning (RL) for large language models (LLMs), particularly during the rollout phase where long sequences increase attention and KV-cache overhead. The method introduces a practical FP8 rollout stack that employs blockwise FP8 quantization for linear layers, extends FP8 to the KV-cache with per-step scale recalibration, and mitigates train-inference mismatch using importance-sampling-based correction techniques. Experimental results show that across dense and mixture-of-experts models, this approach achieves up to 44% rollout throughput improvements while maintaining learning performance comparable to BF16 baselines.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大型语言模型强化学习中计算和内存瓶颈问题，特别是在生成阶段，长序列导致注意力机制和KV缓存开销显著增加。方法上，提出了一个实用的FP8生成堆栈，采用分块FP8量化处理线性层，通过每步重新校准尺度将FP8扩展至KV缓存以消除长上下文内存瓶颈，并利用基于重要性采样的校正技术来缓解训练与推理间的失配。实验结果表明，在密集和混合专家模型中，该技术实现了高达44%的生成吞吐量提升，同时保持了与BF16基线相当的学习性能。</div>
</details>
</div>
<div class="card">
<div class="title">Exact Minimum-Volume Confidence Set Intersection for Multinomial Outcomes</div>
<div class="meta-line">Authors: Heguang Lin, Binhao Chen, Mengze Li, Daniel Pimentel-Alarcón, Matthew L. Malloy</div>
<div class="meta-line">First: 2026-01-26T05:07:47+00:00 · Latest: 2026-01-26T05:07:47+00:00</div>
<div class="meta-line">Comments: 15 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18145v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18145v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Computation of confidence sets is central to data science and machine learning, serving as the workhorse of A/B testing and underpinning the operation and analysis of reinforcement learning algorithms. Among all valid confidence sets for the multinomial parameter, minimum-volume confidence sets (MVCs) are optimal in that they minimize average volume, but they are defined as level sets of an exact p-value that is discontinuous and difficult to compute. Rather than attempting to characterize the geometry of MVCs directly, this paper studies a practically motivated decision problem: given two observed multinomial outcomes, can one certify whether their MVCs intersect? We present a certified, tolerance-aware algorithm for this intersection problem. The method exploits the fact that likelihood ordering induces halfspace constraints in log-odds coordinates, enabling adaptive geometric partitioning of parameter space and computable lower and upper bounds on p-values over each cell. For three categories, this yields an efficient and provably sound algorithm that either certifies intersection, certifies disjointness, or returns an indeterminate result when the decision lies within a prescribed margin. We further show how the approach extends to higher dimensions. The results demonstrate that, despite their irregular geometry, MVCs admit reliable certified decision procedures for core tasks in A/B testing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多项分布结果的最小体积置信集精确交集</div>
<div class="mono" style="margin-top:8px">置信集的计算是数据科学与机器学习的核心，作为A/B测试的主要工具，并支撑强化学习算法的运行与分析。在多项分布参数的所有有效置信集中，最小体积置信集（MVCs）通过最小化平均体积达到最优，但其定义为精确p值的水平集，该p值不连续且难以计算。本文不直接刻画MVCs的几何特性，而是研究一个实际驱动的决策问题：给定两个观测到的多项分布结果，能否判定其MVCs是否相交？我们提出一种可验证、容错感知的算法来解决此交集问题。该方法利用似然排序在对数几率坐标中诱导半空间约束，实现对参数空间的自适应几何划分，并计算每个单元上p值的可计算上下界。对于三类情况，该算法高效且可证明可靠，能够验证交集、验证不相交，或在决策处于预设容差范围内时返回不确定结果。我们进一步展示了该方法如何扩展到更高维度。结果表明，尽管MVCs几何结构不规则，但其在A/B测试核心任务中允许可靠的验证决策过程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for reliable confidence sets in A/B testing and reinforcement learning, this paper addresses the challenge of determining whether minimum-volume confidence sets (MVCs) for multinomial parameters intersect, given their discontinuous p-values and complex geometry. The method introduces a certified, tolerance-aware algorithm that leverages likelihood ordering to impose halfspace constraints in log-odds coordinates, enabling adaptive geometric partitioning of parameter space and computable bounds on p-values per cell. Experimental results show that for three categories, the algorithm efficiently certifies intersection, disjointness, or an indeterminate outcome within a margin, with extensions to higher dimensions, demonstrating that MVCs support practical certified decision procedures for A/B testing tasks.</div>
<div class="mono" style="margin-top:8px">本文受A/B测试和强化学习中可靠置信集需求的驱动，针对多项式参数的最小体积置信集（MVC）因其不连续p值和复杂几何形状而难以直接处理的问题，研究了如何判定两个观测结果对应的MVC是否相交。方法提出了一种经过认证的、容错感知的算法，利用似然排序在log-odds坐标中引入半空间约束，实现对参数空间的自适应几何划分，并计算每个单元上p值的可计算上下界。实验结果表明，对于三类情况，该算法能高效地认证相交、不相交或在预设容差内返回不确定结果，并可扩展至高维，证明了MVC尽管几何形状不规则，仍能支持A/B测试核心任务的可靠认证决策过程。</div>
</details>
</div>
<div class="card">
<div class="title">Enhance the Safety in Reinforcement Learning by ADRC Lagrangian Methods</div>
<div class="meta-line">Authors: Mingxu Zhang, Huicheng Zhang, Jiaming Ji, Yaodong Yang, Ying Sun</div>
<div class="meta-line">First: 2026-01-26T04:54:57+00:00 · Latest: 2026-01-26T04:54:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18142v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18142v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safe reinforcement learning (Safe RL) seeks to maximize rewards while satisfying safety constraints, typically addressed through Lagrangian-based methods. However, existing approaches, including PID and classical Lagrangian methods, suffer from oscillations and frequent safety violations due to parameter sensitivity and inherent phase lag. To address these limitations, we propose ADRC-Lagrangian methods that leverage Active Disturbance Rejection Control (ADRC) for enhanced robustness and reduced oscillations. Our unified framework encompasses classical and PID Lagrangian methods as special cases while significantly improving safety performance. Extensive experiments demonstrate that our approach reduces safety violations by up to 74%, constraint violation magnitudes by 89%, and average costs by 67\%, establishing superior effectiveness for Safe RL in complex environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于ADRC拉格朗日方法增强强化学习的安全性</div>
<div class="mono" style="margin-top:8px">安全强化学习（Safe RL）旨在最大化奖励的同时满足安全约束，通常通过基于拉格朗日的方法处理。然而，现有方法（包括PID和经典拉格朗日方法）因参数敏感性和固有相位滞后，存在振荡和频繁安全违规问题。为解决这些局限，我们提出ADRC-拉格朗日方法，利用自抗扰控制（ADRC）增强鲁棒性并减少振荡。我们的统一框架将经典及PID拉格朗日方法作为特例涵盖，同时显著提升安全性能。大量实验表明，该方法将安全违规减少高达74%，约束违规幅度降低89%，平均成本削减67%，在复杂环境中为安全强化学习确立了卓越的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing Lagrangian-based safe reinforcement learning methods, which suffer from oscillations and frequent safety violations due to parameter sensitivity and phase lag, this paper introduces ADRC-Lagrangian methods that integrate Active Disturbance Rejection Control (ADRC) to enhance robustness and stability. The proposed unified framework generalizes classical and PID Lagrangian approaches, actively compensating for disturbances to reduce oscillations. Experimental results demonstrate that the method significantly improves safety performance, reducing safety violations by up to 74%, constraint violation magnitudes by 89%, and average costs by 67% in complex environments.</div>
<div class="mono" style="margin-top:8px">针对现有基于拉格朗日的安全强化学习方法因参数敏感性和相位滞后导致振荡和频繁安全违规的问题，本文提出了ADRC-拉格朗日方法，通过引入自抗扰控制（ADRC）来增强鲁棒性和稳定性。该方法构建了一个统一框架，将经典和PID拉格朗日方法作为特例，并主动补偿干扰以减少振荡。实验结果表明，该方法在复杂环境中显著提升了安全性能，将安全违规减少高达74%，约束违规幅度降低89%，平均成本下降67%。</div>
</details>
</div>
<div class="card">
<div class="title">Tail Distribution of Regret in Optimistic Reinforcement Learning</div>
<div class="meta-line">Authors: Sajad Khodadadian, Mehrdad Moharrami</div>
<div class="meta-line">First: 2025-11-23T02:23:09+00:00 · Latest: 2026-01-26T04:16:22+00:00</div>
<div class="meta-line">Comments: 17 pages, 0 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.18247v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.18247v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We derive instance-dependent tail bounds for the regret of optimism-based reinforcement learning in finite-horizon tabular Markov decision processes with unknown transition dynamics. Focusing on a UCBVI-type algorithm, we characterize the tail distribution of the cumulative regret $R_K$ over $K$ episodes, rather than only its expectation or a single high-probability quantile. We analyze two natural exploration-bonus schedules: (i) a $K$-dependent scheme that explicitly incorporates the total number of episodes $K$, and (ii) a $K$-independent scheme that depends only on the current episode index. For both settings, we obtain an upper bound on $\Pr(R_K \ge x)$ that exhibits a distinctive two-regime structure: a sub-Gaussian tail starting from an instance-dependent scale $m_K$ up to a transition threshold, followed by a sub-Weibull tail beyond that point. We further derive corresponding instance-dependent bounds on the expected regret $\mathbb{E}[R_K]$. The proposed algorithm depends on a tuning parameter $α$, which balances the expected regret and the range over which the regret exhibits a sub-Gaussian tail. To the best of our knowledge, our results provide one of the first comprehensive tail-regret guarantees for a standard optimistic algorithm in episodic reinforcement learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>乐观强化学习中遗憾的尾部分布</div>
<div class="mono" style="margin-top:8px">我们在未知转移动态的有限时域表格化马尔可夫决策过程中，推导了基于乐观策略的强化学习算法遗憾的实例依赖性尾部边界。聚焦于UCBVI类算法，我们刻画了K幕中累积遗憾$R_K$的尾部分布，而非仅关注其期望或单一高概率分位数。我们分析了两种自然的探索奖励调度方案：(i)显式包含总幕数$K$的$K$依赖方案；(ii)仅依赖当前幕索引的$K$独立方案。针对两种设置，我们获得了$\Pr(R_K \ge x)$的上界，该上界呈现出独特的双区域结构：从实例依赖尺度$m_K$开始至转移阈值呈现亚高斯尾部，超过该点后呈现亚威布尔尾部。我们进一步推导了期望遗憾$\mathbb{E}[R_K]$的相应实例依赖性边界。所提算法依赖调节参数$α$，该参数平衡了期望遗憾与遗憾呈现亚高斯尾部的范围。据我们所知，我们的结果为幕式强化学习中标准乐观算法提供了首批全面的尾部遗憾保证之一。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to understand the full tail distribution of cumulative regret in optimistic RL, beyond just its expectation or a single high-probability bound. The method analyzes a UCBVI-type algorithm in finite-horizon tabular MDPs with unknown transitions, examining two exploration-bonus schedules: one dependent on the total number of episodes K and another independent of K. The main experimental results show that the tail probability Pr(R_K ≥ x) exhibits a two-regime structure: a sub-Gaussian tail up to an instance-dependent transition threshold, followed by a heavier sub-Weibull tail beyond it, with the algorithm&#x27;s tuning parameter α allowing a trade-off between expected regret and the range of the sub-Gaussian behavior.</div>
<div class="mono" style="margin-top:8px">本文的动机是需要理解乐观强化学习中累积遗憾的完整尾部分布，而不仅仅是其期望或单一的高概率界。方法分析了在具有未知转移的有限视野表格MDP中的一种UCBVI型算法，研究了两种探索奖励方案：一种依赖于总回合数K，另一种独立于K。主要实验结果表明，尾部概率Pr(R_K ≥ x)呈现两阶段结构：在实例相关的转换阈值之前是亚高斯尾部，之后是更重的亚韦布尔尾部，并且算法的调节参数α允许在期望遗憾和亚高斯行为的范围之间进行权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Static Datasets: Robust Offline Policy Optimization via Vetted Synthetic Transitions</div>
<div class="meta-line">Authors: Pedram Agand, Mo Chen</div>
<div class="meta-line">First: 2026-01-26T03:38:27+00:00 · Latest: 2026-01-26T03:38:27+00:00</div>
<div class="meta-line">Comments: 11 pages, 2 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18107v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18107v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline Reinforcement Learning (ORL) holds immense promise for safety-critical domains like industrial robotics, where real-time environmental interaction is often prohibitive. A primary obstacle in ORL remains the distributional shift between the static dataset and the learned policy, which typically mandates high degrees of conservatism that can restrain potential policy improvements. We present MoReBRAC, a model-based framework that addresses this limitation through Uncertainty-Aware latent synthesis. Instead of relying solely on the fixed data, MoReBRAC utilizes a dual-recurrent world model to synthesize high-fidelity transitions that augment the training manifold. To ensure the reliability of this synthetic data, we implement a hierarchical uncertainty pipeline integrating Variational Autoencoder (VAE) manifold detection, model sensitivity analysis, and Monte Carlo (MC) dropout. This multi-layered filtering process guarantees that only transitions residing within high-confidence regions of the learned dynamics are utilized. Our results on D4RL Gym-MuJoCo benchmarks reveal significant performance gains, particularly in ``random&#x27;&#x27; and ``suboptimal&#x27;&#x27; data regimes. We further provide insights into the role of the VAE as a geometric anchor and discuss the distributional trade-offs encountered when learning from near-optimal datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越静态数据集：基于验证合成转移的稳健离线策略优化</div>
<div class="mono" style="margin-top:8px">离线强化学习（ORL）在工业机器人等安全关键领域具有巨大潜力，这些领域通常难以进行实时环境交互。ORL的主要障碍在于静态数据集与学习策略之间的分布偏移，这通常要求高度保守性，从而限制了策略的潜在改进。我们提出了MoReBRAC，一种基于模型的框架，通过不确定性感知的潜在合成来解决这一限制。MoReBRAC不依赖固定数据，而是利用双循环世界模型合成高保真转移，以扩展训练流形。为确保合成数据的可靠性，我们实施了分层不确定性流程，整合了变分自编码器（VAE）流形检测、模型敏感性分析和蒙特卡洛（MC）丢弃法。这种多层过滤过程确保仅使用位于学习动态高置信区域的转移。我们在D4RL Gym-MuJoCo基准测试中的结果显示显著性能提升，尤其在“随机”和“次优”数据机制中。我们进一步探讨了VAE作为几何锚点的作用，并讨论了从近最优数据集学习时遇到的分布权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the distributional shift problem in offline reinforcement learning, which limits policy improvement due to reliance on static datasets, particularly in safety-critical domains like robotics. The method, named MoReBRAC, introduces a model-based framework that uses a dual-recurrent world model to generate synthetic transitions for data augmentation, coupled with a hierarchical uncertainty pipeline involving VAE manifold detection, model sensitivity analysis, and MC dropout to vet these synthetic samples for reliability. Experimental results on D4RL Gym-MuJoCo benchmarks demonstrate significant performance gains, especially in random and suboptimal data regimes, while also providing insights into the VAE&#x27;s role as a geometric anchor and trade-offs in near-optimal datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决离线强化学习中的分布偏移问题，该问题因依赖静态数据集而限制了策略改进，在机器人等安全关键领域尤为突出。方法上，提出了名为MoReBRAC的基于模型的框架，利用双循环世界模型生成合成转移以进行数据增强，并通过集成VAE流形检测、模型敏感性分析和MC丢弃的分层不确定性管道来验证这些合成样本的可靠性。在D4RL Gym-MuJoCo基准测试上的实验结果显示，特别是在随机和次优数据机制中性能显著提升，同时深入分析了VAE作为几何锚点的作用及在近最优数据集中的分布权衡。</div>
</details>
</div>
<div class="card">
<div class="title">RRPO: Robust Reward Policy Optimization for LLM-based Emotional TTS</div>
<div class="meta-line">Authors: Cong Wang, Changfeng Gao, Yang Xiang, Zhihao Du, Keyu An, Han Zhao, Qian Chen, Xiangang Li, Yingming Gao, Ya Li</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2025-12-04T08:12:49+00:00 · Latest: 2026-01-26T03:09:00+00:00</div>
<div class="meta-line">Comments: Accepted by ICASSP 2026. Copyright 2026 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.04552v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.04552v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://lrwinr.github.io/RRPO-CosyVoice">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Differentiable reinforcement learning (RL) frameworks like DiffRO offer a powerful approach for controllable text-to-speech (TTS), but are vulnerable to reward hacking, particularly for nuanced tasks like emotion control. The policy model can exploit a vanilla Reward Model (RM) by generating acoustic artifacts to achieve spurious rewards, but at the cost of degrading perceptual quality. To address this, we propose Robust Reward Policy Optimization (RRPO), a novel framework that employs a hybrid regularization scheme. This scheme develops a robust RM whose reward signal is more reliably aligned with human perception, compelling the policy to abandon detrimental shortcuts and instead learn the complex features of genuine emotions. Our ablation study confirms the enhanced robustness of our RM, as evidenced by its strong cross-lingual generalization. The subjective evaluation demonstrates that this robust RM effectively mitigates reward hacking, leading to significant improvements in both emotional expressiveness and naturalness over all baselines. Demo page: https://lrwinr.github.io/RRPO-CosyVoice.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RRPO：基于大语言模型的鲁棒奖励策略优化情感语音合成</div>
<div class="mono" style="margin-top:8px">DiffRO等可微分强化学习框架为可控文本转语音提供了强大方法，但在情感控制等精细任务中易受奖励攻击影响。策略模型可能通过生成声学伪影来利用普通奖励模型获取虚假奖励，却会损害感知质量。为此，我们提出鲁棒奖励策略优化框架，采用混合正则化方案构建鲁棒奖励模型，使其奖励信号更可靠地符合人类感知，促使策略放弃有害捷径并学习真实情感的复杂特征。消融实验证实了奖励模型的增强鲁棒性，其跨语言泛化能力即为明证。主观评估表明，该鲁棒奖励模型有效缓解了奖励攻击，在情感表现力和自然度上均显著超越所有基线。演示页面：https://lrwinr.github.io/RRPO-CosyVoice。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the vulnerability of differentiable reinforcement learning frameworks like DiffRO to reward hacking in emotion-controlled text-to-speech (TTS), where policies exploit acoustic artifacts to achieve spurious rewards at the cost of perceptual quality, this paper proposes Robust Reward Policy Optimization (RRPO). The method employs a hybrid regularization scheme to develop a robust reward model (RM) whose signals are more reliably aligned with human perception, compelling the policy to learn genuine emotional features instead of detrimental shortcuts. Experimental results from an ablation study confirm the enhanced robustness and strong cross-lingual generalization of the RM, while subjective evaluations demonstrate that RRPO effectively mitigates reward hacking, leading to significant improvements in emotional expressiveness and naturalness over all baselines.</div>
<div class="mono" style="margin-top:8px">针对可微分强化学习框架在情感控制文本到语音任务中易受奖励攻击的问题，即策略模型可能通过生成声学伪影获取虚假奖励而损害感知质量，本文提出了鲁棒奖励策略优化方法。该方法采用混合正则化方案，构建一个奖励信号与人类感知更可靠对齐的鲁棒奖励模型，迫使策略学习真实情感特征而非有害捷径。消融实验证实了该奖励模型增强的鲁棒性和强大的跨语言泛化能力，主观评估表明该方法有效缓解了奖励攻击，在情感表现力和自然度上均显著优于所有基线模型。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion Model-based Reinforcement Learning for Version Age of Information Scheduling: Average and Tail-Risk-Sensitive Control</div>
<div class="meta-line">Authors: Haoyuan Pan, Sizhao Chen, Zhaorui Wang, Tse-Tin Chan</div>
<div class="meta-line">First: 2026-01-26T01:59:46+00:00 · Latest: 2026-01-26T01:59:46+00:00</div>
<div class="meta-line">Comments: 16 pages, 11 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18069v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18069v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ensuring timely and semantically accurate information delivery is critical in real-time wireless systems. While Age of Information (AoI) quantifies temporal freshness, Version Age of Information (VAoI) captures semantic staleness by accounting for version evolution between transmitters and receivers. Existing VAoI scheduling approaches primarily focus on minimizing average VAoI, overlooking rare but severe staleness events that can compromise reliability under stochastic packet arrivals and unreliable channels. This paper investigates both average-oriented and tail-risk-sensitive VAoI scheduling in a multi-user status update system with long-term transmission cost constraints. We first formulate the average VAoI minimization problem as a constrained Markov decision process and introduce a deep diffusion-based Soft Actor-Critic (D2SAC) algorithm. By generating actions through a diffusion-based denoising process, D2SAC enhances policy expressiveness and establishes a strong baseline for mean performance. Building on this foundation, we put forth RS-D3SAC, a risk-sensitive deep distributional diffusion-based Soft Actor-Critic algorithm. RS-D3SAC integrates a diffusion-based actor with a quantile-based distributional critic, explicitly modeling the full VAoI return distribution. This enables principled tail-risk optimization via Conditional Value-at-Risk (CVaR) while satisfying long-term transmission cost constraints. Extensive simulations show that, while D2SAC reduces average VAoI, RS-D3SAC consistently achieves substantial reductions in CVaR without sacrificing mean performance. The dominant gain in tail-risk reduction stems from the distributional critic, with the diffusion-based actor providing complementary refinement to stabilize and enrich policy decisions, highlighting their effectiveness for robust and risk-aware VAoI scheduling in multi-user wireless systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于扩散模型的强化学习在信息版本年龄调度中的应用：均值与尾部风险敏感控制</div>
<div class="mono" style="margin-top:8px">在实时无线系统中，确保信息传递的及时性与语义准确性至关重要。信息年龄（AoI）量化了时间新鲜度，而信息版本年龄（VAoI）通过考虑收发端之间的版本演化，捕捉了语义陈旧性。现有VAoI调度方法主要聚焦于最小化平均VAoI，忽略了在随机数据包到达和不可靠信道下可能损害系统可靠性的罕见但严重的陈旧事件。本文研究了具有长期传输成本约束的多用户状态更新系统中，面向均值与尾部风险敏感的VAoI调度问题。我们首先将平均VAoI最小化问题建模为约束马尔可夫决策过程，并提出一种基于深度扩散的柔性演员-评论家（D2SAC）算法。通过基于扩散的去噪过程生成动作，D2SAC增强了策略表达能力，并为均值性能建立了强基准。在此基础上，我们提出RS-D3SAC算法——一种风险敏感的深度分布式扩散柔性演员-评论家算法。RS-D3SAC将基于扩散的演员网络与基于分位数的分布式评论家网络相结合，显式建模完整的VAoI回报分布。这使得通过条件风险价值（CVaR）进行原则性尾部风险优化成为可能，同时满足长期传输成本约束。大量仿真表明，D2SAC能降低平均VAoI，而RS-D3SAC在不牺牲均值性能的前提下持续实现CVaR的显著降低。尾部风险降低的主要增益源于分布式评论家网络，基于扩散的演员网络则通过互补性优化来稳定和丰富策略决策，凸显了二者在多用户无线系统中实现鲁棒且风险感知的VAoI调度的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for robust scheduling in multi-user wireless systems by focusing on both average and tail-risk-sensitive Version Age of Information (VAoI), which measures semantic staleness beyond just temporal freshness. The method introduces two algorithms: first, a deep diffusion-based Soft Actor-Critic (D2SAC) for minimizing average VAoI under transmission cost constraints, leveraging a diffusion process for enhanced policy expressiveness; second, a risk-sensitive variant (RS-D3SAC) that combines a diffusion-based actor with a quantile-based distributional critic to explicitly optimize tail risks via Conditional Value-at-Risk. Experimental results demonstrate that D2SAC effectively reduces average VAoI, while RS-D3SAC achieves significant reductions in tail risk without compromising average performance, highlighting the synergy between distributional critics and diffusion actors for reliable scheduling.</div>
<div class="mono" style="margin-top:8px">本文针对多用户无线系统中需兼顾平均与尾部风险敏感性的版本信息年龄调度问题展开研究，VAoI用于衡量信息语义过时性而不仅是时间新鲜度。方法上提出了两种算法：一是基于深度扩散的Soft Actor-Critic（D2SAC），通过扩散去噪过程生成动作以最小化平均VAoI并满足传输成本约束；二是风险敏感算法RS-D3SAC，结合扩散执行器与分位数分布评论器，显式建模VAoI回报分布以通过条件风险价值优化尾部风险。实验结果表明，D2SAC能有效降低平均VAoI，而RS-D3SAC在保持平均性能的同时大幅减少尾部风险，证明了分布评论器与扩散执行器协同提升调度鲁棒性的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Visual Attention Reasoning via Hierarchical Search and Self-Verification</div>
<div class="meta-line">Authors: Wei Cai, Jian Zhao, Yuchen Yuan, Tianle Zhang, Ming Zhu, Haichuan Tang, Xuelong Li</div>
<div class="meta-line">First: 2025-10-21T13:18:44+00:00 · Latest: 2026-01-26T01:56:26+00:00</div>
<div class="meta-line">Comments: The paper is withdrawn by the authors after discovering a flaw in the theoretical derivation presented in the Method section. This incorrect step leads to conclusions that are not supported by the corrected derivation. The authors plan to reconstruct the argument and will release an updated version once the issue is fully resolved</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.18619v4">Abs</a> · <a href="https://arxiv.org/pdf/2510.18619v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) frequently hallucinate due to their reliance on fragile, linear reasoning and weak visual grounding. We propose Visual Attention Reasoning (VAR), a reinforcement learning framework that reformulates reasoning as a hierarchical search with self-verification. VAR enforces traceable evidence grounding by generating explicit bounding boxes, guided by a novel reward function combining geometric precision and semantic sufficiency. Furthermore, it replaces linear Chain-of-Thought with a tree-search policy capable of backtracking to correct logical errors. Theoretical analysis validates the framework&#x27;s reliability, and extensive experiments demonstrate that VAR significantly outperforms state-of-the-art methods on complex hallucination and safety benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于分层搜索与自验证的视觉注意力推理</div>
<div class="mono" style="margin-top:8px">多模态大语言模型（MLLMs）因其依赖脆弱、线性的推理机制和薄弱的视觉基础而频繁产生幻觉。我们提出视觉注意力推理（VAR），一种强化学习框架，将推理重构为具有自验证功能的分层搜索过程。VAR通过生成显式边界框来强制实现可追溯的证据基础，并受一种结合几何精度与语义充分性的新型奖励函数引导。此外，它用能够回溯以修正逻辑错误的树搜索策略取代了线性的思维链。理论分析验证了该框架的可靠性，大量实验表明，VAR在复杂幻觉和安全基准测试中显著优于现有最优方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Visual Attention Reasoning (VAR), a reinforcement learning framework designed to address the hallucination issues in Multimodal Large Language Models (MLLMs) caused by linear reasoning and weak visual grounding. The method reformulates reasoning as a hierarchical search with self-verification, generating explicit bounding boxes for traceable evidence and employing a tree-search policy that allows backtracking to correct errors, guided by a reward function based on geometric precision and semantic sufficiency. Experimental results show that VAR significantly outperforms state-of-the-art methods on complex hallucination and safety benchmarks, though the paper was later withdrawn due to a flaw in the theoretical derivation, with plans for an updated version.</div>
<div class="mono" style="margin-top:8px">该论文提出了视觉注意力推理（VAR）框架，旨在解决多模态大语言模型因线性推理和视觉基础薄弱而产生的幻觉问题。该方法通过强化学习将推理重构为具有自验证的分层搜索，生成显式边界框以实现可追溯的证据基础，并采用允许回溯纠正逻辑错误的树搜索策略，其奖励函数结合了几何精度和语义充分性。实验结果表明，VAR在复杂幻觉和安全基准测试中显著优于现有先进方法，但论文因理论推导存在缺陷后被作者撤回，计划发布修正后的版本。</div>
</details>
</div>
<div class="card">
<div class="title">EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization</div>
<div class="meta-line">Authors: Wei-Po Hsin, Ren-Hao Deng, Yao-Ting Hsieh, En-Ming Huang, Shih-Hao Hung</div>
<div class="meta-line">First: 2026-01-26T01:53:54+00:00 · Latest: 2026-01-26T01:53:54+00:00</div>
<div class="meta-line">Comments: 17 pages, 6 figures, 8 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18067v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18067v1">PDF</a> · <a href="https://github.com/weiber2002/ICRTL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Verilog&#x27;s design cycle is inherently labor-intensive and necessitates extensive domain expertise. Although Large Language Models (LLMs) offer a promising pathway toward automation, their limited training data and intrinsic sequential reasoning fail to capture the strict formal logic and concurrency inherent in hardware systems. To overcome these barriers, we present EvolVE, the first framework to analyze multiple evolution strategies on chip design tasks, revealing that Monte Carlo Tree Search (MCTS) excels at maximizing functional correctness, while Idea-Guided Refinement (IGR) proves superior for optimization. We further leverage Structured Testbench Generation (STG) to accelerate the evolutionary process. To address the lack of complex optimization benchmarks, we introduce IC-RTL, targeting industry-scale problems derived from the National Integrated Circuit Contest. Evaluations establish EvolVE as the new state-of-the-art, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2. Furthermore, on the industry-scale IC-RTL suite, our framework surpasses reference implementations authored by contest participants, reducing the Power, Performance, Area (PPA) product by up to 66% in Huffman Coding and 17% in the geometric mean across all problems. The source code of the IC-RTL benchmark is available at https://github.com/weiber2002/ICRTL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EvolVE：基于大语言模型的Verilog生成与优化进化搜索框架</div>
<div class="mono" style="margin-top:8px">Verilog设计流程本质上是劳动密集型的，且需要深厚的领域专业知识。尽管大语言模型为实现自动化提供了可行路径，但其有限的训练数据和固有的顺序推理能力难以捕捉硬件系统严格的逻辑形式与并发特性。为突破这些限制，我们提出了EvolVE——首个在芯片设计任务中系统分析多种进化策略的框架，研究发现蒙特卡洛树搜索在最大化功能正确性方面表现卓越，而思路引导优化在性能提升方面更具优势。我们进一步采用结构化测试平台生成技术加速进化过程。针对复杂优化基准缺失的问题，我们推出了源自全国集成电路设计竞赛的工业级问题集IC-RTL。评估结果表明，EvolVE在VerilogEval v2和RTLLM v2上分别达到98.1%和92%的准确率，确立了新的技术标杆。在工业级IC-RTL测试集中，本框架超越竞赛参与者编写的参考方案，在哈夫曼编码任务中将功耗-性能-面积综合指标降低达66%，所有问题的几何平均优化幅度达17%。IC-RTL基准测试源代码已发布于https://github.com/weiber2002/ICRTL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the labor-intensive and expertise-dependent nature of Verilog hardware design by proposing EvolVE, a framework that uses evolutionary search to enhance Large Language Model (LLM) capabilities for Verilog generation and optimization. The method analyzes multiple evolution strategies, finding that Monte Carlo Tree Search (MCTS) maximizes functional correctness while Idea-Guided Refinement (IGR) excels at optimization, and incorporates Structured Testbench Generation (STG) to speed up evolution. Experimental results show state-of-the-art performance, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2, and on the new industry-scale IC-RTL benchmark, it reduces the Power, Performance, Area product by up to 66% in Huffman Coding and 17% on geometric mean compared to contest implementations.</div>
<div class="mono" style="margin-top:8px">本文针对Verilog硬件设计过程劳动密集且依赖专业知识的挑战，提出了EvolVE框架，该框架利用进化搜索增强大语言模型在Verilog生成与优化方面的能力。方法分析了多种进化策略，发现蒙特卡洛树搜索在最大化功能正确性上表现优异，而思想引导精炼在优化方面更优，并采用结构化测试平台生成来加速进化过程。实验结果表明其达到了最先进的性能，在VerilogEval v2和RTLLM v2上分别取得98.1%和92%的准确率，并在新的工业级基准IC-RTL上，相比竞赛实现，将哈夫曼编码的功耗、性能、面积乘积降低了高达66%，所有问题的几何平均降低了17%。</div>
</details>
</div>
<div class="card">
<div class="title">Joint Resource Optimization, Computation Offloading and Resource Slicing for Multi-Edge Traffic-Cognitive Networks</div>
<div class="meta-line">Authors: Ting Xiaoyang, Minfeng Zhang, Shu gonglee, Saimin Chen Zhang</div>
<div class="meta-line">First: 2024-11-26T11:51:10+00:00 · Latest: 2026-01-26T00:16:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.17782v2">Abs</a> · <a href="https://arxiv.org/pdf/2411.17782v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The evolving landscape of edge computing envisions platforms operating as dynamic intermediaries between application providers and edge servers (ESs), where task offloading is coupled with payments for computational services. Ensuring efficient resource utilization and meeting stringent Quality of Service (QoS) requirements necessitates incentivizing ESs while optimizing the platforms operational objectives. This paper investigates a multi-agent system where both the platform and ESs are self-interested entities, addressing the joint optimization of revenue maximization, resource allocation, and task offloading. We propose a novel Stackelberg game-based framework to model interactions between stakeholders and solve the optimization problem using a Bayesian Optimization-based centralized algorithm. Recognizing practical challenges in information collection due to privacy concerns, we further design a decentralized solution leveraging neural network optimization and a privacy-preserving information exchange protocol. Extensive numerical evaluations demonstrate the effectiveness of the proposed mechanisms in achieving superior performance compared to existing baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多边缘流量感知网络的联合资源优化、计算卸载与资源切片</div>
<div class="mono" style="margin-top:8px">边缘计算的发展趋势是构建作为应用提供商与边缘服务器（ES）间动态中介的平台，其中任务卸载需支付计算服务费用。为确保资源高效利用并满足严格的服务质量（QoS）要求，需在优化平台运营目标的同时激励ES参与。本文研究一个平台与ES均为自利实体的多智能体系统，协同优化收益最大化、资源分配和任务卸载。我们提出一种基于斯塔克尔伯格博弈的新框架建模利益相关者间的交互，并采用基于贝叶斯优化的集中式算法求解优化问题。针对隐私顾虑导致信息收集的实际挑战，进一步设计了结合神经网络优化与隐私保护信息交换协议的分布式解决方案。大量数值评估表明，所提机制相比现有基线方法能实现更优性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient resource utilization and QoS assurance in multi-agent edge computing systems where platforms and edge servers act as self-interested entities, this paper proposes a joint optimization framework for revenue, resource allocation, and task offloading. The method employs a Stackelberg game to model stakeholder interactions, solved via a Bayesian Optimization-based centralized algorithm, and further introduces a decentralized solution using neural network optimization with a privacy-preserving protocol to address practical information collection challenges. Experimental results from numerical evaluations show that the proposed mechanisms outperform existing baselines in achieving superior performance.</div>
<div class="mono" style="margin-top:8px">本文针对多智能体边缘计算系统中平台和边缘服务器作为自利实体时，需确保资源高效利用和服务质量保证的动机，提出了一个联合优化收入、资源分配和任务卸载的框架。方法采用斯塔克尔伯格博弈建模利益相关者交互，通过基于贝叶斯优化的集中式算法求解，并进一步设计了利用神经网络优化和隐私保护信息交换协议的分散式解决方案，以应对实际信息收集的挑战。大量数值评估的实验结果表明，所提机制在实现优越性能方面优于现有基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">LLMs as Layout Designers: Enhanced Spatial Reasoning for Content-Aware Layout Generation</div>
<div class="meta-line">Authors: Sha Li, Stefano Petrangeli, Yu Shen, Xiang Chen, Naren Ramakrishnan</div>
<div class="meta-line">First: 2025-09-21T03:02:59+00:00 · Latest: 2026-01-25T23:11:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.16891v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.16891v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Language Models (LLMs) have demonstrated impressive reasoning and planning abilities in textual domains and can effectively follow instructions for complex tasks, their ability to understand and manipulate spatial relationships remains limited. Such capabilities are crucial for content-aware graphic layout design, where the goal is to arrange heterogeneous elements onto a canvas so that final design remains visually balanced and structurally feasible. This problem requires precise coordination of placement, alignment, and structural organization of multiple elements within a constrained visual space. To address this limitation, we introduce LaySPA, a reinforcement learning-based framework that augments LLM-based agents with explicit spatial reasoning capabilities for layout design. LaySPA employs hybrid reward signals that jointly capture geometric constraints, structural fidelity, and visual quality, enabling agents to navigate the canvas, model inter-element relationships, and optimize spatial arrangements. Through group-relative policy optimization, the agent generates content-aware layouts that reflect salient regions, respect spatial constraints, and produces an interpretable reasoning trace explaining placement decisions and a structured layout specification. Experimental results show that LaySPA substantially improves the generation of structurally valid and visually appealing layouts, outperforming larger general-purpose LLMs and achieving performance comparable to state-of-the-art specialized layout models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型作为布局设计师：增强空间推理能力以实现内容感知的布局生成</div>
<div class="mono" style="margin-top:8px">尽管大语言模型在文本领域展现出卓越的推理与规划能力，并能有效执行复杂任务指令，但其理解和处理空间关系的能力仍显不足。这种能力对于内容感知的图形布局设计至关重要，其目标是在画布上合理排布异构元素，使最终设计既保持视觉平衡又具备结构可行性。该问题需要在受限视觉空间内精确协调多个元素的位置、对齐与结构组织。为突破此局限，我们提出LaySPA——一种基于强化学习的框架，通过显式空间推理能力增强基于大语言模型的布局设计智能体。LaySPA采用混合奖励信号，协同捕捉几何约束、结构保真度与视觉质量，使智能体能够导航画布、建模元素间关系并优化空间排布。通过组相对策略优化，智能体生成的内容感知布局能反映显著区域、遵循空间约束，同时生成可解释的推理轨迹以说明排布决策，并提供结构化布局规范。实验结果表明，LaySPA显著提升了生成结构有效且视觉美观布局的能力，其性能超越规模更大的通用大语言模型，并与专业布局模型的最新成果相媲美。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limited spatial reasoning capabilities of Large Language Models (LLMs) in content-aware graphic layout design, where arranging heterogeneous elements on a canvas requires balancing visual appeal and structural feasibility. To overcome this, the authors propose LaySPA, a reinforcement learning framework that enhances LLM-based agents with explicit spatial reasoning through hybrid reward signals capturing geometric constraints, structural fidelity, and visual quality, enabling optimized layout generation via group-relative policy optimization. Experimental results demonstrate that LaySPA significantly improves the production of structurally valid and visually appealing layouts, outperforming larger general-purpose LLMs and matching the performance of specialized state-of-the-art layout models.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在内容感知图形布局设计中的空间推理能力不足问题展开研究，该任务需在画布上排列异构元素以实现视觉平衡与结构可行性。为此，作者提出了LaySPA框架，通过强化学习增强基于LLM的智能体，利用混合奖励信号捕捉几何约束、结构保真度和视觉质量，结合组相对策略优化来生成内容感知布局。实验结果表明，LaySPA显著提升了结构有效性和视觉吸引力的布局生成效果，优于更大的通用LLM，并与专业的最先进布局模型性能相当。</div>
</details>
</div>
<div class="card">
<div class="title">Quantum Architecture Search with Unsupervised Representation Learning</div>
<div class="meta-line">Authors: Yize Sun, Zixin Wu, Volker Tresp, Yunpu Ma</div>
<div class="meta-line">First: 2024-01-21T19:53:17+00:00 · Latest: 2026-01-25T21:59:01+00:00</div>
<div class="meta-line">Comments: 12 Pages, quantum architecture search, unsupervised representation learning</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2401.11576v6">Abs</a> · <a href="https://arxiv.org/pdf/2401.11576v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised representation learning presents new opportunities for advancing Quantum Architecture Search (QAS) on Noisy Intermediate-Scale Quantum (NISQ) devices. QAS is designed to optimize quantum circuits for Variational Quantum Algorithms (VQAs). Most QAS algorithms tightly couple the search space and search algorithm, typically requiring the evaluation of numerous quantum circuits, resulting in high computational costs and limiting scalability to larger quantum circuits. Predictor-based QAS algorithms mitigate this issue by estimating circuit performance based on structure or embedding. However, these methods often demand time-intensive labeling to optimize gate parameters across many circuits, which is crucial for training accurate predictors. Inspired by the classical neural architecture search algorithm Arch2vec, we investigate the potential of unsupervised representation learning for QAS without relying on predictors. Our framework decouples unsupervised architecture representation learning from the search process, enabling the learned representations to be applied across various downstream tasks. Additionally, it integrates an improved quantum circuit graph encoding scheme, addressing the limitations of existing representations and enhancing search efficiency. This predictor-free approach removes the need for large labeled datasets. During the search, we employ REINFORCE and Bayesian Optimization to explore the latent representation space and compare their performance against baseline methods. We further validate our approach by executing the best-discovered MaxCut circuits on IBM&#x27;s ibm_sherbrooke quantum processor, confirming that the architectures retain optimal performance even under real hardware noise. Our results demonstrate that the framework efficiently identifies high-performing quantum circuits with fewer search iterations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于无监督表征学习的量子架构搜索</div>
<div class="mono" style="margin-top:8px">无监督表征学习为在含噪声中等规模量子（NISQ）设备上推进量子架构搜索（QAS）提供了新机遇。QAS旨在为变分量子算法（VQAs）优化量子电路。多数QAS算法将搜索空间与搜索算法紧密耦合，通常需要评估大量量子电路，导致计算成本高昂并限制了对更大规模量子电路的扩展能力。基于预测器的QAS算法通过根据结构或嵌入估计电路性能来缓解此问题，但这类方法常需耗时标注以优化众多电路的栅极参数，这对训练准确预测器至关重要。受经典神经架构搜索算法Arch2vec启发，我们探索了不依赖预测器的无监督表征学习在QAS中的应用潜力。本框架将无监督架构表征学习与搜索过程解耦，使学习到的表征可应用于多种下游任务，并整合了改进的量子电路图编码方案，以克服现有表征的局限并提升搜索效率。这种无需预测器的方法消除了对大规模标注数据集的需求。在搜索过程中，我们采用REINFORCE和贝叶斯优化探索潜在表征空间，并与基线方法进行性能比较。通过在IBM ibm_sherbrooke量子处理器上执行最优发现的MaxCut电路，我们进一步验证了该方法，证实了所获架构在真实硬件噪声下仍保持最优性能。实验结果表明，该框架能以更少搜索迭代高效识别高性能量子电路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the high computational cost and scalability limitations of existing Quantum Architecture Search (QAS) methods, which often require extensive labeled data for predictor training, this paper proposes a predictor-free framework leveraging unsupervised representation learning. The method decouples architecture representation learning from the search process, employing an improved quantum circuit graph encoding to generate reusable representations, and utilizes REINFORCE and Bayesian Optimization for efficient exploration in the latent space. Experimental results show that this approach identifies high-performing quantum circuits with fewer iterations, and validation on IBM&#x27;s ibm_sherbrooke processor confirms that the discovered MaxCut circuits maintain optimal performance under real hardware noise.</div>
<div class="mono" style="margin-top:8px">针对现有量子架构搜索方法计算成本高、可扩展性有限且通常需要大量标注数据训练预测器的问题，本文提出了一种基于无监督表示学习的无预测器框架。该方法将架构表示学习与搜索过程解耦，采用改进的量子电路图编码生成可重用的表示，并利用REINFORCE和贝叶斯优化在潜在空间中进行高效探索。实验结果表明，该方法能以更少的搜索迭代找到高性能量子电路，并在IBM的ibm_sherbrooke处理器上验证了所发现的MaxCut电路在真实硬件噪声下仍保持最优性能。</div>
</details>
</div>
<div class="card">
<div class="title">SD-E$^2$: Semantic Exploration for Reasoning Under Token Budgets</div>
<div class="meta-line">Authors: Kshitij Mishra, Nils Lukas, Salem Lahlou</div>
<div class="meta-line">First: 2026-01-25T20:21:52+00:00 · Latest: 2026-01-25T20:21:52+00:00</div>
<div class="meta-line">Comments: Accepted at EACL 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17982v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17982v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Small language models (SLMs) struggle with complex reasoning because exploration is expensive under tight compute budgets. We introduce Semantic Diversity-Exploration-Exploitation (SD-E$^2$), a reinforcement learning framework that makes exploration explicit by optimizing semantic diversity in generated reasoning trajectories. Using a frozen sentence-embedding model, SD-E$^2$ assigns a diversity reward that captures (i) the coverage of semantically distinct solution strategies and (ii) their average pairwise dissimilarity in embedding space, rather than surface-form novelty. This diversity reward is combined with outcome correctness and solution efficiency in a z-score-normalized multi-objective objective that stabilizes training. On GSM8K, SD-E$^2$ surpasses the base Qwen2.5-3B-Instruct and strong GRPO baselines (GRPO-CFL and GRPO-CFEE) by +27.4, +5.2, and +1.5 percentage points, respectively, while discovering on average 9.8 semantically distinct strategies per question. We further improve MedMCQA to 49.64% versus 38.37% for the base model and show gains on the harder AIME benchmark (1983-2025), reaching 13.28% versus 6.74% for the base. These results indicate that rewarding semantic novelty yields a more compute-efficient exploration-exploitation signal for training reasoning-capable SLMs. By introducing cognitive adaptation-adjusting the reasoning process structure rather than per-token computation-SD-E$^2$ offers a complementary path to efficiency gains in resource-constrained models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SD-E$^2$：面向令牌预算约束下推理的语义探索方法</div>
<div class="mono" style="margin-top:8px">小型语言模型（SLMs）在严格的计算预算下难以进行复杂推理，因为探索过程代价高昂。我们提出了语义多样性探索-利用（SD-E$^2$）强化学习框架，通过优化生成推理轨迹的语义多样性使探索过程显式化。SD-E$^2$利用冻结的句子嵌入模型分配多样性奖励，该奖励捕获：（i）语义不同解题策略的覆盖度，以及（ii）嵌入空间中策略间的平均成对相异性，而非表面形式的新颖性。该多样性奖励与结果正确性、求解效率结合，通过z分数归一化形成多目标优化函数以稳定训练。在GSM8K数据集上，SD-E$^2$相比基础模型Qwen2.5-3B-Instruct及强基线GRPO（GRPO-CFL与GRPO-CFEE）分别提升27.4、5.2和1.5个百分点，平均每个问题发现9.8种语义不同的策略。在MedMCQA上，模型性能从基础模型的38.37%提升至49.64%；在更难的AIME基准（1983-2025）上，从基础模型的6.74%提升至13.28%。结果表明，奖励语义新颖性能为训练具备推理能力的SLMs提供更高效的计算探索-利用信号。通过引入认知适应——调整推理过程结构而非逐令牌计算——SD-E$^2$为资源受限模型的效率提升提供了互补路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of enabling small language models (SLMs) to perform complex reasoning efficiently under tight computational budgets, where exploration of different reasoning paths is costly. It introduces SD-E^2, a reinforcement learning framework that explicitly optimizes for semantic diversity in reasoning trajectories by using a frozen sentence-embedding model to reward coverage and pairwise dissimilarity of distinct solution strategies, combined with correctness and efficiency in a normalized multi-objective objective. Experimental results show that SD-E^2 significantly improves performance over base models and strong baselines, achieving gains of up to +27.4 percentage points on GSM8K, discovering an average of 9.8 distinct strategies per question, and also boosting results on MedMCQA and the AIME benchmark, demonstrating that semantic diversity rewards lead to more compute-efficient exploration for reasoning in SLMs.</div>
<div class="mono" style="margin-top:8px">本文针对小型语言模型在严格计算预算下进行复杂推理时探索成本高昂的问题，提出了一种强化学习框架SD-E^2，通过优化推理轨迹的语义多样性来显式促进探索。该方法利用冻结的句子嵌入模型，奖励不同解决方案策略的覆盖度和嵌入空间中的平均成对差异度，而非表面形式的新颖性，并将此多样性奖励与结果正确性和求解效率结合在标准化多目标函数中以稳定训练。实验结果表明，SD-E^2在GSM8K上显著超越基础模型和强基线，提升最高达27.4个百分点，平均每个问题发现9.8种语义不同的策略，并在MedMCQA和更难的AIME基准上取得进步，验证了语义多样性奖励能为资源受限模型提供更高效的计算探索信号。</div>
</details>
</div>
<div class="card">
<div class="title">Decentralized Learning Strategies for Estimation Error Minimization with Graph Neural Networks</div>
<div class="meta-line">Authors: Xingran Chen, Navid NaderiAlizadeh, Alejandro Ribeiro, Shirin Saeedi Bidokhti</div>
<div class="meta-line">First: 2026-01-19T02:18:45+00:00 · Latest: 2026-01-25T19:29:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12662v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.12662v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We address real-time sampling and estimation of autoregressive Markovian sources in dynamic yet structurally similar multi-hop wireless networks. Each node caches samples from others and communicates over wireless collision channels, aiming to minimize time-average estimation error via decentralized policies. Due to the high dimensionality of action spaces and complexity of network topologies, deriving optimal policies analytically is intractable. To address this, we propose a graphical multi-agent reinforcement learning framework for policy optimization. Theoretically, we demonstrate that our proposed policies are transferable, allowing a policy trained on one graph to be effectively applied to structurally similar graphs. Numerical experiments demonstrate that (i) our proposed policy outperforms state-of-the-art baselines; (ii) the trained policies are transferable to larger networks, with performance gains increasing with the number of agents; (iii) the graphical training procedure withstands non-stationarity, even when using independent learning techniques; and (iv) recurrence is pivotal in both independent learning and centralized training and decentralized execution, and improves the resilience to non-stationarity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于图神经网络的估计误差最小化去中心化学习策略</div>
<div class="mono" style="margin-top:8px">本研究针对动态但结构相似的多跳无线网络中自回归马尔可夫源的实时采样与估计问题。各节点缓存来自其他节点的样本，并通过无线碰撞信道进行通信，旨在通过去中心化策略最小化时间平均估计误差。由于动作空间的高维性与网络拓扑的复杂性，解析推导最优策略具有挑战性。为此，我们提出一种基于图结构的多智能体强化学习框架进行策略优化。理论分析表明，所提策略具有可迁移性，在单一图结构上训练的策略可有效应用于结构相似的图。数值实验验证：(i) 所提策略性能优于现有基线方法；(ii) 训练策略可迁移至更大规模网络，且性能增益随智能体数量增加而提升；(iii) 图训练过程能适应非平稳环境，即使采用独立学习技术；(iv) 循环机制在独立学习与集中训练分散执行模式中均发挥关键作用，并增强了对非平稳环境的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the challenge of minimizing time-average estimation error in decentralized multi-hop wireless networks, where nodes sample and estimate autoregressive Markovian sources in real-time, but analytical derivation of optimal policies is intractable due to high-dimensional action spaces and complex topologies. The method introduces a graphical multi-agent reinforcement learning framework to optimize decentralized sampling and communication policies. Experimental results show that the proposed policy outperforms state-of-the-art baselines, is transferable to larger structurally similar networks with increasing performance gains, withstands non-stationarity, and benefits from recurrence to enhance resilience.</div>
<div class="mono" style="margin-top:8px">本文旨在解决去中心化多跳无线网络中最小化时间平均估计误差的挑战，其中节点实时采样和估计自回归马尔可夫源，但由于高维动作空间和复杂拓扑，解析推导最优策略不可行。方法上，提出了一个图多智能体强化学习框架来优化去中心化的采样和通信策略。实验结果表明，所提策略优于现有基线，可迁移到结构相似的更大网络且性能增益随智能体数量增加，能够抵御非平稳性，并通过循环机制提升了抗非平稳性的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Agentic AI for Self-Driving Laboratories in Soft Matter: Taxonomy, Benchmarks,and Open Challenges</div>
<div class="meta-line">Authors: Xuanzhou Chen, Audrey Wang, Stanley Yin, Hanyang Jiang, Dong Zhang</div>
<div class="meta-line">First: 2026-01-25T17:44:19+00:00 · Latest: 2026-01-25T17:44:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17920v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17920v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Self-driving laboratories (SDLs) close the loop between experiment design, automated execution, and data-driven decision making, and they provide a demanding testbed for agentic AI under expensive actions, noisy and delayed feedback, strict feasibility and safety constraints, and non-stationarity. This survey uses soft matter as a representative setting but focuses on the AI questions that arise in real laboratories. We frame SDL autonomy as an agent environment interaction problem with explicit observations, actions, costs, and constraints, and we use this formulation to connect common SDL pipelines to established AI principles. We review the main method families that enable closed loop experimentation, including Bayesian optimization and active learning for sample efficient experiment selection, planning and reinforcement learning for long horizon protocol optimization, and tool using agents that orchestrate heterogeneous instruments and software. We emphasize verifiable and provenance aware policies that support debugging, reproducibility, and safe operation. We then propose a capability driven taxonomy that organizes systems by decision horizon, uncertainty modeling, action parameterization, constraint handling, failure recovery, and human involvement. To enable meaningful comparison, we synthesize benchmark task templates and evaluation metrics that prioritize cost aware performance, robustness to drift, constraint violation behavior, and reproducibility. Finally, we distill lessons from deployed SDLs and outline open challenges in multi-modal representation, calibrated uncertainty, safe exploration, and shared benchmark infrastructure.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>软物质自驱实验室中的智能体人工智能：分类体系、基准测试与开放挑战</div>
<div class="mono" style="margin-top:8px">自驱实验室通过实验设计、自动化执行与数据驱动决策的闭环，为智能体人工智能在昂贵操作、噪声延迟反馈、严格可行性与安全约束及非平稳性等复杂条件下提供了高要求测试平台。本综述以软物质为典型场景，聚焦真实实验室中涌现的人工智能问题：将SDL自主性构建为包含显式观测、行动、成本与约束的智能体-环境交互问题，并借此框架将常见SDL流程与既有AI原则相衔接。系统回顾了实现闭环实验的核心方法体系，包括面向样本高效实验选择的贝叶斯优化与主动学习、用于长周期协议优化的规划与强化学习，以及协调异构仪器与软件的智能体工具调用。重点关注支持调试、可复现性与安全运行的可验证及溯源感知策略。进而提出以能力为导向的分类体系，通过决策视野、不确定性建模、行动参数化、约束处理、故障恢复及人机协同等维度对系统进行归类。为建立有效比较基准，综合构建了优先考虑成本感知性能、漂移鲁棒性、约束违反行为与可复现性的基准任务模板与评估指标。最后，总结已部署SDL的经验教训，并展望多模态表征、校准不确定性、安全探索与共享基准设施等开放挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This survey paper is motivated by the need to advance agentic AI for self-driving laboratories (SDLs), which automate experimental workflows under real-world constraints like cost, noise, and safety, using soft matter as a representative domain. The method involves framing SDL autonomy as an agent-environment interaction, reviewing AI techniques such as Bayesian optimization and reinforcement learning for experiment selection and protocol optimization, and emphasizing verifiable, provenance-aware policies. The main experimental results are synthesized through a proposed capability taxonomy, benchmark task templates, and evaluation metrics that highlight performance under cost, robustness to drift, and reproducibility, while identifying open challenges in areas like safe exploration and benchmark infrastructure.</div>
<div class="mono" style="margin-top:8px">本综述论文旨在推动用于自驱动实验室的代理人工智能发展，这些实验室在成本、噪声和安全等现实约束下自动化实验流程，并以软物质为代表领域。方法包括将自驱动实验室自主性构建为代理-环境交互问题，综述了贝叶斯优化和强化学习等人工智能技术用于实验选择和协议优化，并强调可验证、可溯源的策略。主要实验成果通过提出的能力分类法、基准任务模板和评估指标综合体现，重点关注成本感知性能、对漂移的鲁棒性以及可重复性，同时指出了在多模态表示、校准不确定性、安全探索和共享基准设施等方面的开放挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Yukthi Opus: A Multi-Chain Hybrid Metaheuristic for Large-Scale NP-Hard Optimization</div>
<div class="meta-line">Authors: SB Danush Vikraman, Hannah Abigail, Prasanna Kesavraj, Gajanan V Honnavar</div>
<div class="meta-line">First: 2026-01-05T06:51:08+00:00 · Latest: 2026-01-25T17:00:22+00:00</div>
<div class="meta-line">Comments: 22 pages, 9 figures, includes extensive ablation studies and benchmark comparisons</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01832v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.01832v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Yukthi Opus (YO), a multi-chain hybrid metaheuristic designed for NP-hard optimization under explicit evaluation budget constraints. YO integrates three complementary mechanisms in a structured two-phase architecture: Markov Chain Monte Carlo (MCMC) for global exploration, greedy local search for exploitation, and simulated annealing with adaptive reheating to enable controlled escape from local minima. A dedicated burn-in phase allocates evaluations to probabilistic exploration, after which a hybrid optimization loop refines promising candidates. YO further incorporates a spatial blacklist mechanism to avoid repeated evaluation of poor regions and a multi-chain execution strategy to improve robustness and reduce sensitivity to initialization.
  We evaluate YO on three benchmarks: the Rastrigin function (5D) with ablation studies, the Traveling Salesman Problem with 50 to 200 cities, and the Rosenbrock function (5D) with comparisons against established optimizers including CMA-ES, Bayesian optimization, and accelerated particle swarm optimization. Results show that MCMC exploration and greedy refinement are critical for solution quality, while simulated annealing and multi-chain execution primarily improve stability and variance reduction. Overall, YO achieves competitive performance on large and multimodal problems while maintaining predictable evaluation budgets, making it suitable for expensive black-box optimization settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Yukthi Opus：一种面向大规模NP难优化问题的多链混合元启发式算法</div>
<div class="mono" style="margin-top:8px">本文提出Yukthi Opus（YO），一种在显式评估预算约束下针对NP难优化问题设计的多链混合元启发式算法。YO采用结构化的两阶段架构集成三种互补机制：马尔可夫链蒙特卡洛（MCMC）实现全局探索，贪婪局部搜索进行开发，以及带自适应重加热的模拟退火实现可控的局部极小值逃逸。专用预热阶段将评估资源分配给概率性探索，随后混合优化循环对潜力解进行精细化处理。YO进一步引入空间黑名单机制以避免重复评估劣质区域，并采用多链执行策略提升鲁棒性、降低对初始化的敏感性。
我们在三个基准测试中评估YO：含消融实验的5维Rastrigin函数、50至200城市的旅行商问题，以及5维Rosenbrock函数（与CMA-ES、贝叶斯优化、加速粒子群优化等成熟优化器对比）。结果表明MCMC探索与贪婪精细化对解质量至关重要，而模拟退火与多链执行主要提升稳定性并降低方差。总体而言，YO在保持可预测评估预算的同时，于大规模多模态问题上取得具有竞争力的性能，适用于高成本黑箱优化场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Yukthi Opus (YO), a multi-chain hybrid metaheuristic motivated by the need to solve large-scale NP-hard optimization problems under strict evaluation budget constraints, such as in expensive black-box settings. The method employs a structured two-phase architecture integrating Markov Chain Monte Carlo for global exploration, greedy local search for exploitation, and simulated annealing with adaptive reheating to escape local minima, alongside a spatial blacklist and multi-chain execution for robustness. Experimental results on benchmarks including the Rastrigin and Rosenbrock functions and the Traveling Salesman Problem demonstrate that YO achieves competitive performance, with MCMC and greedy refinement being key to solution quality, while simulated annealing and multi-chain strategies enhance stability and reduce variance compared to optimizers like CMA-ES and Bayesian optimization.</div>
<div class="mono" style="margin-top:8px">本文提出了Yukthi Opus（YO），一种多链混合元启发式方法，其动机是在严格的评估预算约束下解决大规模NP难优化问题，适用于昂贵的黑盒场景。该方法采用结构化的两阶段架构，整合了马尔可夫链蒙特卡洛进行全局探索、贪婪局部搜索进行利用，以及带自适应重热的模拟退火来逃离局部极小值，同时通过空间黑名单和多链执行策略提高鲁棒性。在Rastrigin函数、Rosenbrock函数和旅行商问题等基准测试上的实验结果表明，YO取得了有竞争力的性能，其中MCMC和贪婪细化对解的质量至关重要，而模拟退火和多链策略则增强了稳定性并降低了方差，优于CMA-ES和贝叶斯优化等现有优化器。</div>
</details>
</div>
<div class="card">
<div class="title">Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities</div>
<div class="meta-line">Authors: Abd Ullah Khan, Adnan Shahid, Haejoon Jung, Hyundong Shin</div>
<div class="meta-line">First: 2025-12-25T15:40:52+00:00 · Latest: 2026-01-25T16:37:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21717v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.21717v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Space-air-ground-integrated network (SAGIN)-enabled multiconnectivity (MC) is emerging as a key enabler for next-generation networks, enabling users to simultaneously utilize multiple links across multi-layer non-terrestrial networks (NTN) and multi-radio access technology (multi-RAT) terrestrial networks (TN). However, the heterogeneity of TN and NTN introduces complex architectural challenges that complicate MC implementation. Specifically, the diversity of link types, spanning air-to-air, air-to-space, space-to-space, space-to-ground, and ground-to-ground communications, renders optimal resource allocation highly complex. Recent advancements in reinforcement learning (RL) and agentic artificial intelligence (AI) have shown remarkable effectiveness in optimal decision-making in complex and dynamic environments. In this paper, we review the current developments in SAGIN-enabled MC and outline the key challenges associated with its implementation. We further highlight the transformative potential of AI-driven approaches for resource optimization in a heterogeneous SAGIN environment. To this end, we present a case study on resource allocation optimization enabled by agentic RL for SAGIN-enabled MC involving diverse radio access technologies (RATs). Results show that learning-based methods can effectively handle complex scenarios and substantially enhance network performance in terms of latency and capacity while incurring a moderate increase in power consumption as an acceptable tradeoff. Finally, open research problems and future directions are presented to realize efficient SAGIN-enabled MC.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>空天地一体化网络多连接技术：现状、挑战、AI驱动方案与机遇</div>
<div class="mono" style="margin-top:8px">基于空天地一体化网络（SAGIN）的多连接（MC）技术正成为下一代网络的关键赋能手段，使用户能够同时利用多层非地面网络（NTN）与多无线接入技术（多-RAT）地面网络（TN）的多条链路。然而，TN与NTN的异构性带来了复杂的架构挑战，增加了MC实施的难度。具体而言，链路类型的多样性（涵盖空对空、空对天、天对天、天对地及地对地通信）使得最优资源分配高度复杂。强化学习（RL）与智能体人工智能（AI）的最新进展在复杂动态环境的最优决策中展现出显著成效。本文综述了SAGIN赋能的MC技术当前发展，阐述了其实施过程中的关键挑战，并重点探讨了AI驱动方法在异构SAGIN环境中资源优化的变革潜力。为此，我们通过一个案例研究，展示了基于智能体RL的资源分配优化在涉及多无线接入技术（RAT）的SAGIN赋能MC中的应用。结果表明，基于学习的方法能有效处理复杂场景，在延迟与容量方面显著提升网络性能，同时以适度的功耗增加作为可接受的权衡。最后，本文提出了实现高效SAGIN赋能MC的开放研究问题与未来方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to address the complex architectural challenges of implementing multiconnectivity in heterogeneous space-air-ground-integrated networks (SAGIN), where diverse link types complicate optimal resource allocation. The method involves a comprehensive review of current SAGIN developments and the proposal of AI-driven solutions, specifically highlighting a case study using agentic reinforcement learning for resource optimization across multiple radio access technologies. The main experimental results from the case study demonstrate that such learning-based methods can effectively manage complex scenarios, significantly improving network latency and capacity, albeit with a moderate and acceptable increase in power consumption.</div>
<div class="mono" style="margin-top:8px">本文的动机在于解决在异构的空天地一体化网络（SAGIN）中实现多连接所面临的复杂架构挑战，其中多样化的链路类型使得最优资源分配高度复杂。方法包括对当前SAGIN发展的全面综述，并提出了人工智能驱动的解决方案，特别通过一个案例研究，展示了利用智能体强化学习在多种无线接入技术中进行资源优化。主要实验结果表明，此类基于学习的方法能有效处理复杂场景，显著提升网络时延和容量性能，尽管会带来适度的、可接受的功耗增加作为权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Evolving Interdependent Operators with Large Language Models for Multi-Objective Combinatorial Optimization</div>
<div class="meta-line">Authors: Junhao Qiu, Xin Chen, Liang Ge, Liyong Lin, Zhichao Lu, Qingfu Zhang</div>
<div class="meta-line">First: 2026-01-25T16:31:07+00:00 · Latest: 2026-01-25T16:31:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17899v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17899v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neighborhood search operators are critical to the performance of Multi-Objective Evolutionary Algorithms (MOEAs) and rely heavily on expert design. Although recent LLM-based Automated Heuristic Design (AHD) methods have made notable progress, they primarily optimize individual heuristics or components independently, lacking explicit exploration and exploitation of dynamic coupling relationships between multiple operators. In this paper, multi-operator optimization in MOEAs is formulated as a Markov decision process, enabling the improvement of interdependent operators through sequential decision-making. To address this, we propose the Evolution of Operator Combination (E2OC) framework for MOEAs, which achieves the co-evolution of design strategies and executable codes. E2OC employs Monte Carlo Tree Search to progressively search combinations of operator design strategies and adopts an operator rotation mechanism to identify effective operator configurations while supporting the integration of mainstream AHD methods as the underlying designer. Experimental results across AHD tasks with varying objectives and problem scales show that E2OC consistently outperforms state-of-the-art AHD and other multi-heuristic co-design frameworks, demonstrating strong generalization and sustained optimization capability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型的多目标组合优化中演化相互依赖算子</div>
<div class="mono" style="margin-top:8px">邻域搜索算子的设计对多目标进化算法（MOEA）的性能至关重要，且高度依赖专家经验。尽管近期基于大语言模型的自动启发式设计方法取得了显著进展，但这些方法主要独立优化单一启发式或组件，未能显式探索和利用多算子间的动态耦合关系。本文将MOEA中的多算子优化建模为马尔可夫决策过程，通过序列化决策实现相互依赖算子的协同改进。为此，我们提出了面向MOEA的算子组合演化框架E2OC，实现了设计策略与可执行代码的协同演化。E2OC采用蒙特卡洛树搜索渐进式探索算子设计策略组合，并引入算子轮换机制以识别高效算子配置，同时支持集成主流自动启发式设计方法作为底层设计器。在不同目标与问题规模的自动启发式设计任务实验中，E2OC始终优于当前最先进的自动启发式设计方法及其他多启发式协同设计框架，展现出强大的泛化能力与持续优化性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of existing LLM-based Automated Heuristic Design (AHD) methods, which optimize individual operators independently without exploiting their dynamic interdependencies in Multi-Objective Evolutionary Algorithms (MOEAs). The authors formulate multi-operator optimization as a Markov decision process and propose the Evolution of Operator Combination (E2OC) framework, which uses Monte Carlo Tree Search to co-evolve design strategies and executable codes through an operator rotation mechanism. Experiments across various AHD tasks demonstrate that E2OC consistently outperforms state-of-the-art AHD and multi-heuristic co-design frameworks, showing strong generalization and sustained optimization capabilities.</div>
<div class="mono" style="margin-top:8px">本文针对现有基于大语言模型的自动启发式设计方法在多目标进化算法中独立优化单个算子、未能利用算子间动态耦合关系的局限性展开研究。作者将多算子优化建模为马尔可夫决策过程，提出了算子组合进化框架，该框架采用蒙特卡洛树搜索通过算子轮换机制共同演化设计策略与可执行代码。在不同目标和问题规模的自动启发式设计任务上的实验结果表明，该框架持续优于最先进的自动启发式设计及其他多启发式协同设计框架，展现出强大的泛化能力和持续优化性能。</div>
</details>
</div>
<div class="card">
<div class="title">Aligning Medical Conversational AI through Online Reinforcement Learning with Information-Theoretic Rewards</div>
<div class="meta-line">Authors: Tanvi Verma, Yang Zhou, Rick Siow Mong Goh, Yong Liu</div>
<div class="meta-line">First: 2026-01-25T13:19:16+00:00 · Latest: 2026-01-25T13:19:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17828v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17828v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Information Gain Fine-Tuning (IGFT), a novel approach for training medical conversational AI to conduct effective patient interviews and generate comprehensive History of Present Illness (HPI) without requiring pre-collected human conversations. IGFT combines online Group Relative Policy Optimization (GRPO) with information-theoretic rewards, enabling models to learn from self-generated conversations with simulated patients. Unlike existing approaches that rely on expensive expert-annotated conversations or static datasets, our online RL framework allows models to discover effective questioning strategies through exploration. Our key innovation is an information gain reward function that tracks which clinical entities such as symptoms, temporal patterns, and medical history, are revealed during conversation. Each question&#x27;s reward is computed based on its expected information gain combined with GPT-4o-mini quality assessments across dimensions including clinical relevance, patient engagement, and specificity. This hybrid approach ensures models learn to ask targeted, clinically appropriate questions that efficiently gather diagnostic information. We fine-tune two models using LoRA: Llama-3.1-8B-Instruct and DeepSeek-R1-Distill-Qwen-7B (a reasoning-optimized model). Training exclusively on Avey data containing concise HPIs, we evaluate generalization to MIMIC data with longer, more elaborate HPIs. DeepSeek-R1-Distill-Qwen-7B (IGFT) achieves F1 scores of 0.408 on Avey (10.9% improvement over base) and 0.289 on MIMIC (12.9% improvement), while Llama-3.1-8B-Instruct (IGFT) reaches 0.384 and 0.336 respectively. Both models outperform OpenAI&#x27;s model on MIMIC and surpass medical domain-specific baselines like HuatuoGPT and UltraMedical, which were optimized for single-turn medical QA rather than multi-turn conversations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于信息论奖励的在线强化学习对齐医疗对话AI</div>
<div class="mono" style="margin-top:8px">我们提出信息增益微调（IGFT），一种无需预收集人类对话即可训练医疗对话AI进行有效患者访谈并生成全面现病史（HPI）的新方法。IGFT将在线组相对策略优化（GRPO）与信息论奖励相结合，使模型能够通过与模拟患者的自生成对话进行学习。与依赖昂贵专家标注对话或静态数据集的现有方法不同，我们的在线强化学习框架允许模型通过探索发现有效提问策略。核心创新是信息增益奖励函数，该函数追踪对话中揭示的临床实体（如症状、时间模式、病史）。每个问题的奖励基于其预期信息增益，并结合GPT-4o-mini在临床相关性、患者参与度和特异性等维度的质量评估计算。这种混合方法确保模型学会提出有针对性的、临床适宜的问题，以高效收集诊断信息。我们使用LoRA微调两个模型：Llama-3.1-8B-Instruct和DeepSeek-R1-Distill-Qwen-7B（推理优化模型）。在仅包含简洁HPI的Avey数据上训练后，评估模型对更长更详细HPI的MIMIC数据的泛化能力。DeepSeek-R1-Distill-Qwen-7B（IGFT）在Avey上获得0.408的F1分数（较基线提升10.9%），在MIMIC上获得0.289（提升12.9%）；Llama-3.1-8B-Instruct（IGFT）则分别达到0.384和0.336。两个模型在MIMIC上均优于OpenAI模型，并超越了如华佗GPT和UltraMedical等专为单轮医疗问答优化的领域基线模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces Information Gain Fine-Tuning (IGFT), a method to train medical conversational AI for patient interviews without relying on pre-collected human dialogues, motivated by the high cost of expert annotations. The approach combines online reinforcement learning (Group Relative Policy Optimization) with a novel information-theoretic reward function that quantifies the clinical information, such as symptoms and medical history, revealed during conversations, supplemented by GPT-4o-mini assessments for quality. Experimental results show that fine-tuning models like Llama-3.1-8B-Instruct and DeepSeek-R1-Distill-Qwen-7B with IGFT leads to significant improvements in F1 scores on both Avey and MIMIC datasets, outperforming domain-specific baselines and demonstrating effective generalization to more complex medical histories.</div>
<div class="mono" style="margin-top:8px">本文提出了信息增益微调（IGFT）方法，旨在无需依赖预先收集的人类对话数据来训练医疗对话AI进行患者问诊，其动机是解决专家标注成本高昂的问题。该方法结合了在线强化学习（组相对策略优化）与一种新颖的信息论奖励函数，该函数量化对话中揭示的临床信息（如症状和病史），并辅以GPT-4o-mini的质量评估。实验结果表明，使用IGFT对Llama-3.1-8B-Instruct和DeepSeek-R1-Distill-Qwen-7B等模型进行微调后，在Avey和MIMIC数据集上的F1分数均有显著提升，优于特定医疗领域的基线模型，并展现出对更复杂病史的有效泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Empowering LLMs for Structure-Based Drug Design via Exploration-Augmented Latent Inference</div>
<div class="meta-line">Authors: Xuanning Hu, Anchen Li, Qianli Xing, Jinglong Ji, Hao Tuo, Bo Yang</div>
<div class="meta-line">First: 2026-01-20T08:10:48+00:00 · Latest: 2026-01-25T12:08:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15333v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.15333v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) possess strong representation and reasoning capabilities, but their application to structure-based drug design (SBDD) is limited by insufficient understanding of protein structures and unpredictable molecular generation. To address these challenges, we propose Exploration-Augmented Latent Inference for LLMs (ELILLM), a framework that reinterprets the LLM generation process as an encoding, latent space exploration, and decoding workflow. ELILLM explicitly explores portions of the design problem beyond the model&#x27;s current knowledge while using a decoding module to handle familiar regions, generating chemically valid and synthetically reasonable molecules. In our implementation, Bayesian optimization guides the systematic exploration of latent embeddings, and a position-aware surrogate model efficiently predicts binding affinity distributions to inform the search. Knowledge-guided decoding further reduces randomness and effectively imposes chemical validity constraints. We demonstrate ELILLM on the CrossDocked2020 benchmark, showing strong controlled exploration and high binding affinity scores compared with seven baseline methods. These results demonstrate that ELILLM can effectively enhance LLMs capabilities for SBDD.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过探索增强的潜在推理赋能大语言模型用于基于结构的药物设计</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）具备强大的表征与推理能力，但其在基于结构的药物设计（SBDD）中的应用受限于对蛋白质结构的理解不足和分子生成的不确定性。为应对这些挑战，我们提出面向LLMs的探索增强潜在推理框架（ELILLM），该框架将LLM生成过程重新诠释为编码、潜在空间探索和解码的工作流程。ELILLM在模型现有知识范围之外显式探索设计问题的部分空间，同时利用解码模块处理熟悉区域，从而生成化学有效且合成合理的分子。在我们的实现中，贝叶斯优化指导潜在嵌入的系统性探索，位置感知代理模型高效预测结合亲和力分布以指导搜索过程。知识引导的解码进一步减少随机性，并有效施加化学有效性约束。我们在CrossDocked2020基准测试中验证ELILLM，相较于七种基线方法，展现出强大的可控探索能力和高结合亲和力评分。这些结果表明ELILLM能有效增强LLMs在SBDD任务中的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to overcome the limitations of Large Language Models (LLMs) in structure-based drug design (SBDD), specifically their insufficient understanding of protein structures and unpredictable molecular generation. The proposed method, Exploration-Augmented Latent Inference for LLMs (ELILLM), reinterprets LLM generation as an encoding, latent space exploration, and decoding workflow, using Bayesian optimization for systematic exploration and a position-aware surrogate model to predict binding affinity. The main experimental results on the CrossDocked2020 benchmark show that ELILLM achieves strong controlled exploration and higher binding affinity scores compared to seven baseline methods, effectively enhancing LLM capabilities for SBDD.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大语言模型在基于结构的药物设计中存在的不足，特别是其对蛋白质结构理解不足和分子生成不可预测的问题。所提出的方法ELILLM将大语言模型的生成过程重新解释为编码、潜在空间探索和解码的工作流程，利用贝叶斯优化进行系统探索，并采用位置感知的代理模型预测结合亲和力以指导搜索。在CrossDocked2020基准测试上的主要实验结果表明，与七种基线方法相比，ELILLM实现了强大的可控探索和更高的结合亲和力得分，有效提升了大语言模型在基于结构的药物设计中的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Vision-Proprioception Fusion with Mamba2 in End-to-End Reinforcement Learning for Motion Control</div>
<div class="meta-line">Authors: Xiaowen Tao, Yinuo Wang, Jinzhao Zhou</div>
<div class="meta-line">First: 2025-09-09T11:05:44+00:00 · Latest: 2026-01-25T07:52:38+00:00</div>
<div class="meta-line">Comments: 6 figures and 8 tables. This paper has been accepted by Advanced Engineering Informatics</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.07593v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.07593v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">End-to-end reinforcement learning (RL) for motion control trains policies directly from sensor inputs to motor commands, enabling unified controllers for different robots and tasks. However, most existing methods are either blind (proprioception-only) or rely on fusion backbones with unfavorable compute-memory trade-offs. Recurrent controllers struggle with long-horizon credit assignment, and Transformer-based fusion incurs quadratic cost in token length, limiting temporal and spatial context. We present a vision-driven cross-modal RL framework built on SSD-Mamba2, a selective state-space backbone that applies state-space duality (SSD) to enable both recurrent and convolutional scanning with hardware-aware streaming and near-linear scaling. Proprioceptive states and exteroceptive observations (e.g., depth tokens) are encoded into compact tokens and fused by stacked SSD-Mamba2 layers. The selective state-space updates retain long-range dependencies with markedly lower latency and memory use than quadratic self-attention, enabling longer look-ahead, higher token resolution, and stable training under limited compute. Policies are trained end-to-end under curricula that randomize terrain and appearance and progressively increase scene complexity. A compact, state-centric reward balances task progress, energy efficiency, and safety. Across diverse motion-control scenarios, our approach consistently surpasses strong state-of-the-art baselines in return, safety (collisions and falls), and sample efficiency, while converging faster at the same compute budget. These results suggest that SSD-Mamba2 provides a practical fusion backbone for resource-constrained robotic and autonomous systems in engineering informatics applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于Mamba2的视觉-本体感知融合在运动控制端到端强化学习中的应用</div>
<div class="mono" style="margin-top:8px">运动控制的端到端强化学习（RL）直接从传感器输入训练策略生成运动指令，可为不同机器人和任务提供统一控制器。然而，现有方法多为盲控（仅本体感知）或依赖计算-内存权衡不佳的融合骨干网络。循环控制器难以处理长时程信用分配，而基于Transformer的融合因令牌长度产生二次计算成本，限制了时空上下文利用。本文提出基于SSD-Mamba2的视觉驱动跨模态RL框架，该选择性状态空间骨干网络应用状态空间对偶（SSD）实现循环与卷积扫描，具备硬件感知流式处理和近线性扩展能力。本体感知状态与外感知观测（如深度令牌）被编码为紧凑令牌，通过堆叠的SSD-Mamba2层进行融合。选择性状态空间更新在显著低于二次自注意力机制的延迟和内存消耗下保持长程依赖，支持更长前瞻窗口、更高令牌分辨率及有限算力下的稳定训练。策略在随机化地形外观并渐进增加场景复杂度的课程设置中进行端到端训练。紧凑的以状态为中心的奖励函数平衡任务进度、能效与安全性。在多样化运动控制场景中，本方法在回报率、安全性（碰撞与跌落）和样本效率上持续超越强基线模型，并在相同计算预算下收敛更快。结果表明SSD-Mamba2为工程信息学应用中资源受限的机器人及自主系统提供了实用的融合骨干网络。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of existing end-to-end reinforcement learning methods for motion control, which often rely on either proprioception-only inputs or computationally expensive fusion backbones like Transformers, leading to poor compute-memory trade-offs and difficulty with long-horizon tasks. The authors propose a vision-driven cross-modal RL framework built on SSD-Mamba2, a selective state-space backbone that employs state-space duality to enable efficient recurrent and convolutional scanning with near-linear scaling, fusing proprioceptive and exteroceptive observations into compact tokens. Experimental results across diverse motion-control scenarios demonstrate that this approach consistently outperforms state-of-the-art baselines in terms of return, safety (reducing collisions and falls), and sample efficiency, while converging faster under the same compute budget, indicating its practicality for resource-constrained robotic systems.</div>
<div class="mono" style="margin-top:8px">本文针对现有运动控制的端到端强化学习方法存在的局限性展开研究，这些方法通常仅依赖本体感知输入或依赖计算成本高昂的融合骨干网络（如Transformer），导致计算-内存权衡不佳且难以处理长时程任务。作者提出了一种基于SSD-Mamba2的视觉驱动跨模态强化学习框架，该选择性状态空间骨干网络利用状态空间对偶性，实现了高效的循环和卷积扫描，具有近线性缩放特性，能够将本体感知和外体感知观测融合为紧凑的令牌。在多种运动控制场景下的实验结果表明，该方法在回报、安全性（减少碰撞和跌倒）和样本效率方面 consistently 优于现有先进基线，且在相同计算预算下收敛更快，证明了其在资源受限机器人系统中的实用性。</div>
</details>
</div>
<div class="card">
<div class="title">SeRL: Self-Play Reinforcement Learning for Large Language Models with Limited Data</div>
<div class="meta-line">Authors: Wenkai Fang, Shunyu Liu, Yang Zhou, Kongcheng Zhang, Tongya Zheng, Kaixuan Chen, Mingli Song, Dacheng Tao</div>
<div class="meta-line">First: 2025-05-25T13:28:04+00:00 · Latest: 2026-01-25T06:57:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.20347v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.20347v2">PDF</a> · <a href="https://github.com/wantbook-book/SeRL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances have demonstrated the effectiveness of Reinforcement Learning (RL) in improving the reasoning capabilities of Large Language Models (LLMs). However, existing works inevitably rely on high-quality instructions and verifiable rewards for effective training, both of which are often difficult to obtain in specialized domains. In this paper, we propose Self-play Reinforcement Learning (SeRL) to bootstrap LLM training with limited initial data. Specifically, SeRL comprises two complementary modules: self-instruction and self-rewarding. The former module generates additional instructions based on the available data at each training step, employing robust online filtering strategies to ensure instruction quality, diversity, and difficulty. The latter module introduces a simple yet effective majority-voting mechanism to estimate response rewards for additional instructions, eliminating the need for external annotations. Finally, SeRL performs conventional RL based on the generated data, facilitating iterative self-play learning. Extensive experiments on various reasoning benchmarks and across different LLM backbones demonstrate that the proposed SeRL yields results superior to its counterparts and achieves performance on par with those obtained by high-quality data with verifiable rewards. Our code is available at https://github.com/wantbook-book/SeRL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SeRL：面向有限数据大语言模型的自博弈强化学习</div>
<div class="mono" style="margin-top:8px">近期研究表明，强化学习（RL）能有效提升大语言模型（LLM）的推理能力。然而，现有方法均依赖于高质量指令与可验证奖励进行有效训练，这两者在专业领域中往往难以获取。本文提出自博弈强化学习（SeRL），旨在利用有限初始数据引导LLM训练。SeRL包含两个互补模块：自指令生成与自奖励机制。前者基于各训练步骤的可用数据生成额外指令，并采用鲁棒的在线过滤策略确保指令质量、多样性与难度；后者通过简单有效的多数投票机制估算对额外指令的响应奖励，无需外部标注。最终，SeRL基于生成数据执行常规强化学习，实现迭代式自博弈训练。在多种推理基准测试及不同LLM骨干模型上的实验表明，SeRL优于现有方法，其性能可与使用高质量可验证奖励数据所获结果相媲美。代码已开源：https://github.com/wantbook-book/SeRL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of applying reinforcement learning to large language models in specialized domains where high-quality instructions and verifiable rewards are scarce. It introduces SeRL, a self-play reinforcement learning method that bootstraps training with limited initial data through two modules: self-instruction, which generates and filters additional instructions for quality and diversity, and self-rewarding, which uses majority voting to estimate rewards without external annotations. Experimental results across various reasoning benchmarks and LLM backbones show that SeRL outperforms existing methods and matches the performance of models trained with high-quality, reward-verified data.</div>
<div class="mono" style="margin-top:8px">本文针对在专业领域中应用强化学习训练大语言模型时高质量指令和可验证奖励稀缺的挑战，提出了SeRL方法，这是一种自对弈强化学习框架，利用有限初始数据自举训练。该方法包含两个互补模块：自指令模块通过在线过滤策略生成并确保指令的质量、多样性和难度；自奖励模块采用简单有效的多数投票机制来估计响应奖励，无需外部标注。在多个推理基准测试和不同大语言模型骨干上的实验表明，SeRL优于现有方法，其性能与使用高质量可验证奖励数据训练的结果相当。</div>
</details>
</div>
<div class="card">
<div class="title">SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL</div>
<div class="meta-line">Authors: Harper Hua, Zhen Han, Zhengyuan Shen, Jeremy Lee, Patrick Guan, Qi Zhu, Sullam Jeoung, Yueyan Chen, Yunfei Bai, Shuai Wang, Vassilis Ioannidis, Huzefa Rangwala</div>
<div class="meta-line">First: 2026-01-25T05:16:52+00:00 · Latest: 2026-01-25T05:16:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17699v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17699v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While large language models (LLMs) have substantially improved Text-to-SQL generation, a pronounced gap remains between AI systems and human experts on challenging benchmarks such as BIRD-SQL. We argue this gap stems largely from the prevailing single-pass paradigm, which lacks the iterative reasoning, schema exploration, and error-correction behaviors that humans naturally employ. To address this limitation, we introduce SQL-Trail, a multi-turn reinforcement learning (RL) agentic framework for Text-to-SQL. Rather than producing a query in one shot, SQL-Trail interacts with the database environment and uses execution feedback to iteratively refine its predictions. Our approach centers on two key ideas: (i) an adaptive turn-budget allocation mechanism that scales the agent&#x27;s interaction depth to match question difficulty, and (ii) a composite reward panel that jointly incentivizes SQL correctness and efficient exploration. Across benchmarks, SQL-Trail sets a new state of the art and delivers strong data efficiency--up to 18x higher than prior single-pass RL state-of-the-art methods. Notably, our 7B and 14B models outperform substantially larger proprietary systems by 5% on average, underscoring the effectiveness of interactive, agentic workflows for robust Text-to-SQL generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SQL-Trail：基于交错反馈的多轮强化学习文本到SQL生成</div>
<div class="mono" style="margin-top:8px">尽管大语言模型显著提升了文本到SQL的生成能力，但在BIRD-SQL等复杂基准测试中，AI系统与人类专家之间仍存在明显差距。我们认为这一差距主要源于当前主流的单次生成范式缺乏人类自然运用的迭代推理、模式探索和错误修正机制。为此，我们提出SQL-Trail——一个面向文本到SQL的多轮强化学习智能体框架。该方法摒弃单次查询生成模式，通过与数据库环境交互并利用执行反馈迭代优化预测结果。其核心创新包括：（1）自适应轮次预算分配机制，可根据问题难度动态调整智能体交互深度；（2）复合奖励机制，协同激励SQL正确性与探索效率。实验表明，SQL-Trail在多项基准测试中刷新最优性能，并实现高达先前单轮强化学习方法18倍的数据效率。值得注意的是，我们的70亿和140亿参数模型平均超越规模更大的商用系统5%，这印证了交互式智能体工作流对构建鲁棒文本到SQL生成系统的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the performance gap between large language models and human experts on complex Text-to-SQL benchmarks like BIRD-SQL, which stems from the lack of iterative reasoning and error correction in single-pass generation, this paper introduces SQL-Trail, a multi-turn reinforcement learning framework. The method employs an agent that interacts with a database environment, using execution feedback for iterative refinement, and features an adaptive turn-budget allocation to match question difficulty and a composite reward to incentivize both SQL correctness and exploration efficiency. Experimental results show that SQL-Trail achieves state-of-the-art performance across benchmarks, with up to 18x higher data efficiency than prior single-pass RL methods, and notably, its 7B and 14B models outperform substantially larger proprietary systems by an average of 5%.</div>
<div class="mono" style="margin-top:8px">本文的动机是大型语言模型在复杂文本到SQL基准（如BIRD-SQL）上与人类专家存在性能差距，这源于单次生成缺乏迭代推理和错误纠正。为此，论文提出了SQL-Trail，一个多轮强化学习框架，其方法基于智能体与数据库环境交互，利用执行反馈进行迭代优化，并采用自适应轮次预算分配以匹配问题难度，以及复合奖励机制以同时激励SQL正确性和探索效率。主要实验结果表明，SQL-Trail在多个基准测试中达到了最先进的性能，数据效率比之前的单轮强化学习方法高出最多18倍，且其7B和14B模型平均以5%的优势显著超越了规模大得多的专有系统。</div>
</details>
</div>
<div class="card">
<div class="title">Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis</div>
<div class="meta-line">Authors: Hao Li, He Cao, Shenyao Peng, Zijing Liu, Bin Feng, Yu Wang, Zhiyuan Yan, Yonghong Tian, Yu Li, Li Yuan</div>
<div class="meta-line">First: 2026-01-25T04:23:34+00:00 · Latest: 2026-01-25T04:23:34+00:00</div>
<div class="meta-line">Comments: Working in Progress, 13 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17687v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17687v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language models are revolutionizing the biochemistry domain, assisting scientists in drug design and chemical synthesis with high efficiency. Yet current approaches struggle between small language models prone to hallucination and limited knowledge retention, and large cloud-based language models plagued by privacy risks and high inference costs. To bridge this gap, we introduce ChemCRAFT, a novel framework leveraging agentic reinforcement learning to decouple chemical reasoning from knowledge storage. Instead of forcing the model to memorize vast chemical data, our approach empowers the language model to interact with a sandbox for precise information retrieval. This externalization of knowledge allows a locally deployable small model to achieve superior performance with minimal inference costs. To enable small language models for agent-calling ability, we build an agentic trajectory construction pipeline and a comprehensive chemical-agent sandbox. Based on sandbox interactions, we constructed ChemToolDataset, the first large-scale chemical tool trajectory dataset. Simultaneously, we propose SMILES-GRPO to build a dense chemical reward function, promoting the model&#x27;s ability to call chemical agents. Evaluations across diverse aspects of drug design show that ChemCRAFT outperforms current cloud-based LLMs in molecular structure analysis, molecular optimization, and synthesis pathway prediction, demonstrating that scientific reasoning is not solely an emergent ability of model scale, but a learnable policy of tool orchestration. This work establishes a cost-effective and privacy-preserving paradigm for AI-aided chemistry, opening new avenues for accelerating molecular discovery with locally deployable agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>智能体强化学习赋能新一代化学语言模型实现分子设计与合成</div>
<div class="mono" style="margin-top:8px">语言模型正高效革新生物化学领域，助力科学家进行药物设计与化学合成。然而现有方法面临两难：小型语言模型易产生幻觉且知识储备有限，而大型云端语言模型则受隐私风险和高推理成本困扰。为弥合这一鸿沟，我们提出ChemCRAFT框架，通过智能体强化学习实现化学推理与知识存储的解耦。该方法不强制模型记忆海量化学数据，而是赋予语言模型与沙盒交互的能力以进行精准信息检索。这种知识外部化机制使可本地部署的小型模型能以极低推理成本实现卓越性能。为赋予小型语言模型调用智能体的能力，我们构建了智能体轨迹构建流程和综合性化学智能体沙盒。基于沙盒交互，我们创建了首个大规模化学工具轨迹数据集ChemToolDataset。同时，我们提出SMILES-GRPO算法构建密集化学奖励函数，提升模型调用化学智能体的能力。在药物设计多维度评估中，ChemCRAFT在分子结构分析、分子优化及合成路径预测方面均超越现有云端大语言模型，证明科学推理不仅是模型规模涌现的能力，更是可学习的工具编排策略。本研究为AI辅助化学建立了高性价比且保护隐私的新范式，为通过本地可部署智能体加速分子发现开辟了新途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the trade-offs between hallucination-prone small language models and privacy-risky, costly large cloud models in chemistry, this paper introduces ChemCRAFT, a framework that uses agentic reinforcement learning to decouple chemical reasoning from knowledge storage. The method externalizes knowledge by enabling a small local model to interact with a chemical-agent sandbox for precise retrieval, supported by a novel trajectory construction pipeline and the SMILES-GRPO reward function to enhance agent-calling ability. Experimental results demonstrate that ChemCRAFT outperforms current cloud-based LLMs in molecular structure analysis, optimization, and synthesis pathway prediction, showing that scientific reasoning can be achieved cost-effectively with local deployment.</div>
<div class="mono" style="margin-top:8px">针对化学领域中小型语言模型易产生幻觉而大型云端模型存在隐私风险和高成本的问题，本文提出了ChemCRAFT框架，利用智能体强化学习将化学推理与知识存储解耦。该方法通过让小型本地模型与化学智能体沙盒交互以精确检索信息，并构建了轨迹流水线和SMILES-GRPO奖励函数来提升智能体调用能力。实验结果表明，ChemCRAFT在分子结构分析、分子优化和合成路径预测方面优于当前云端大模型，证明了科学推理可通过本地部署以低成本实现。</div>
</details>
</div>
<div class="card">
<div class="title">DIML: Differentiable Inverse Mechanism Learning from Behaviors of Multi-Agent Learning Trajectories</div>
<div class="meta-line">Authors: Zhiyu An, Wan Du</div>
<div class="meta-line">First: 2026-01-25T03:49:25+00:00 · Latest: 2026-01-25T03:49:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17678v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17678v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study inverse mechanism learning: recovering an unknown incentive-generating mechanism from observed strategic interaction traces of self-interested learning agents. Unlike inverse game theory and multi-agent inverse reinforcement learning, which typically infer utility/reward parameters inside a structured mechanism, our target includes unstructured mechanism -- a (possibly neural) mapping from joint actions to per-agent payoffs. Unlike differentiable mechanism design, which optimizes mechanisms forward, we infer mechanisms from behavior in an observational setting. We propose DIML, a likelihood-based framework that differentiates through a model of multi-agent learning dynamics and uses the candidate mechanism to generate counterfactual payoffs needed to predict observed actions. We establish identifiability of payoff differences under a conditional logit response model and prove statistical consistency of maximum likelihood estimation under standard regularity conditions. We evaluate DIML with simulated interactions of learning agents across unstructured neural mechanisms, congestion tolling, public goods subsidies, and large-scale anonymous games. DIML reliably recovers identifiable incentive differences and supports counterfactual prediction, where its performance rivals tabular enumeration oracle in small environments and its convergence scales to large, hundred-participant environments. Code to reproduce our experiments is open-sourced.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DIML：基于多智能体学习轨迹行为的可微分逆向机制学习</div>
<div class="mono" style="margin-top:8px">本研究提出逆向机制学习：从自利学习智能体观察到的策略交互轨迹中恢复未知的激励生成机制。与通常从结构化机制内部推断效用/奖励参数的逆向博弈论和多智能体逆向强化学习不同，我们的目标包含非结构化机制——从联合动作到个体收益的（可能基于神经网络的）映射。与通过前向优化机制的可微分机制设计不同，我们在观测环境中从行为反推机制。我们提出DIML这一基于似然估计的框架，通过对多智能体学习动力学模型进行微分，并利用候选机制生成预测观测动作所需的反事实收益。我们在条件逻辑响应模型下建立了收益差异的可识别性，并在标准正则条件下证明了最大似然估计的统计一致性。通过模拟学习智能体在非结构化神经机制、拥堵收费、公共物品补贴及大规模匿名博弈中的交互，验证了DIML能可靠恢复可识别的激励差异并支持反事实预测——其性能在小型环境中媲美表格枚举基准，在百参与者规模的大型环境中仍保持收敛性。实验代码已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of inverse mechanism learning, aiming to deduce an unknown incentive-generating mechanism from observed strategic interactions among self-interested learning agents, moving beyond traditional methods like inverse game theory that focus on structured mechanisms. The proposed DIML framework employs a likelihood-based approach that differentiates through multi-agent learning dynamics, using candidate mechanisms to generate counterfactual payoffs for predicting actions, with identifiability established under a conditional logit response model and statistical consistency proven under standard conditions. Experimental results across unstructured neural mechanisms, congestion tolling, public goods subsidies, and large-scale anonymous games show that DIML reliably recovers incentive differences, supports counterfactual prediction, matches oracle performance in small settings, and scales effectively to environments with hundreds of participants.</div>
<div class="mono" style="margin-top:8px">本文研究逆向机制学习问题，旨在从自利学习智能体的观察到的战略交互轨迹中推断未知的激励生成机制，超越了传统方法如逆向博弈论对结构化机制的关注。提出的DIML框架采用基于似然的方法，通过对多智能体学习动态进行微分，利用候选机制生成反事实收益以预测行为，并在条件逻辑响应模型下建立了可识别性，在标准正则条件下证明了最大似然估计的统计一致性。实验在非结构化神经机制、拥堵收费、公共物品补贴和大规模匿名博弈等场景中进行，结果表明DIML能可靠地恢复可识别的激励差异，支持反事实预测，在小型环境中性能媲美枚举基准，并能扩展到数百参与者的复杂环境。</div>
</details>
</div>
<div class="card">
<div class="title">Not All Steps are Informative: On the Linearity of LLMs&#x27; RLVR Training</div>
<div class="meta-line">Authors: Tianle Wang, Zhongyuan Wu, Shenghao Jin, Hao Xu, Wei Chen, Ning Miao</div>
<div class="meta-line">First: 2026-01-08T03:06:18+00:00 · Latest: 2026-01-25T03:44:00+00:00</div>
<div class="meta-line">Comments: pre-print</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04537v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.04537v2">PDF</a> · <a href="https://github.com/Miaow-Lab/RLVR-Linearity">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) has become a central component of large language model (LLM) post-training. Unlike supervised fine-tuning (SFT), RLVR lets an LLM generate multiple candidate solutions and reinforces those that lead to a verifiably correct final answer. However, in practice, RLVR often requires thousands of training steps to reach strong performance, incurring substantial computation largely attributed to prolonged exploration. In this work, we make a surprising observation: during RLVR, LLMs evolve in a strongly linear manner. Specifically, both model weights and model output log-probabilities exhibit strong linear correlations with RL training steps. This suggests that RLVR predominantly amplifies trends that emerge early in training, rather than continuously discovering new behaviors throughout the entire optimization trajectory. Motivated by this linearity, we investigate whether future model states can be predicted from intermediate checkpoints via extrapolation, avoiding continued expensive training. We show that Weight Extrapolation produces models with performance comparable to standard RL training while requiring significantly less computation. Moreover, Logits Extrapolation consistently outperforms continued RL training on mathematics and code benchmarks by extrapolating beyond the step range where RL training remains stable. Our code is available at https://github.com/Miaow-Lab/RLVR-Linearity</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>并非所有训练步骤都具备信息量：论大语言模型RLVR训练的线性特性</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）已成为大语言模型（LLM）后训练的核心组成部分。与监督微调（SFT）不同，RLVR允许LLM生成多个候选解决方案，并对那些能产生可验证正确答案的路径进行强化。然而在实践中，RLVR通常需要数千个训练步骤才能达到强劲性能，其巨大的计算成本主要源于漫长的探索过程。本研究发现了一个令人惊讶的现象：在RLVR训练过程中，LLM呈现出强烈的线性演化特征。具体表现为模型权重和输出对数概率均与RL训练步数呈现强线性相关。这表明RLVR主要放大训练早期出现的趋势，而非在整个优化轨迹中持续发现新行为。基于这种线性特性，我们探究了能否通过中间检查点外推预测未来模型状态，从而避免持续的高成本训练。实验表明：权重外推法产生的模型性能与标准RL训练相当，同时显著降低计算需求；而对数概率外推法在数学和代码基准测试中持续优于传统RL训练，其外推范围可超越RL训练保持稳定的步数区间。代码已开源：https://github.com/Miaow-Lab/RLVR-Linearity</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper investigates the linear evolution of large language models (LLMs) during reinforcement learning with verifiable rewards (RLVR), a costly post-training method that often requires extensive exploration. Motivated by the observation that model weights and output log-probabilities change linearly with training steps, indicating RLVR mainly amplifies early trends rather than discovering new behaviors, the authors propose extrapolation techniques to predict future model states from intermediate checkpoints. Experimental results show that Weight Extrapolation achieves performance comparable to standard RL training with less computation, while Logits Extrapolation outperforms continued training on mathematics and code benchmarks by extrapolating beyond stable training ranges.</div>
<div class="mono" style="margin-top:8px">本文研究了大型语言模型（LLM）在可验证奖励强化学习（RLVR）中的线性演化现象，这是一种计算成本高昂的后训练方法，通常需要大量探索。受模型权重和输出对数概率随训练步骤线性变化的观察启发，表明RLVR主要放大早期趋势而非持续发现新行为，作者提出从中间检查点通过外推预测未来模型状态的方法。实验结果表明，权重外推在减少计算量的同时达到了与标准RL训练相当的性能，而对数概率外推通过超越稳定训练范围的外推，在数学和代码基准上优于持续训练。</div>
</details>
</div>
<div class="card">
<div class="title">Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning</div>
<div class="meta-line">Authors: Sanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, JeongYeon Nam, Donghyun Kwak</div>
<div class="meta-line">First: 2025-04-04T11:52:05+00:00 · Latest: 2026-01-25T03:41:27+00:00</div>
<div class="meta-line">Comments: To appear in EACL 2026 (main conference)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.03380v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.03380v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in reinforcement learning with verifiable rewards (RLVR) show that large language models enhance their reasoning abilities when trained with verifiable signals. However, due to reward sparsity, effectiveness depends heavily on selecting samples of appropriate difficulty. In this work, we present a formal analysis of online difficulty-aware filtering and establish its theoretical foundations. We show that expected policy improvement is lower-bounded by the variance of task-level success probabilities, implying that selecting tasks of intermediate difficulty maximizes learning efficiency. Building on this, we demonstrate that balanced filtering maximizes this lower bound, leading to superior performance and sample efficiency. Evaluations across multiple math reasoning benchmarks validate that balanced filtering consistently enhances convergence speed and final performance, achieving up to +12% gains in less than half the training steps of standard GRPO. By extending our analysis to various reward distributions, we provide a principled foundation for future RLVR curriculum strategies, confirmed through both theoretical analysis and extensive empirical results.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向推理强化学习的在线难度过滤方法</div>
<div class="mono" style="margin-top:8px">近期基于可验证奖励的强化学习研究表明，大语言模型在可验证信号训练下能显著提升推理能力。但由于奖励稀疏性，其效果高度依赖于选择适当难度的样本。本文对在线难度感知过滤进行了形式化分析并建立理论基础，证明策略改进期望值受任务级成功概率方差的下界约束，这意味着选择中等难度任务能最大化学习效率。基于此，我们论证了平衡过滤可使该下界最大化，从而获得更优的性能与样本效率。在多个数学推理基准上的评估表明，平衡过滤能持续提升收敛速度与最终性能，在不足标准GRPO一半训练步数的情况下实现最高+12%的性能增益。通过将分析扩展至多种奖励分布，本研究为未来RLVR课程策略提供了理论依据，并通过理论分析与大量实证结果得到验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of reward sparsity in reinforcement learning with verifiable rewards (RLVR), which makes sample difficulty selection critical for enhancing reasoning in large language models, this work formally analyzes online difficulty-aware filtering. The method establishes a theoretical foundation showing that expected policy improvement is lower-bounded by the variance of task success probabilities, leading to a balanced filtering strategy that selects tasks of intermediate difficulty to maximize learning efficiency. Experimental results across multiple math reasoning benchmarks demonstrate that this approach consistently improves convergence speed and final performance, achieving gains of up to +12% in less than half the training steps compared to standard methods like GRPO, with theoretical and empirical validation extending to various reward distributions.</div>
<div class="mono" style="margin-top:8px">本研究针对可验证奖励强化学习（RLVR）中奖励稀疏性导致样本难度选择对提升大语言模型推理能力至关重要的挑战，对在线难度感知过滤进行了形式化分析。该方法建立了理论基础，表明策略改进的期望下界取决于任务成功概率的方差，从而提出一种平衡过滤策略，选择中等难度任务以最大化学习效率。在多个数学推理基准上的实验结果表明，该方法能持续提升收敛速度和最终性能，相比标准GRPO等方法，在不到一半的训练步数内实现了高达+12%的性能增益，并通过理论和实证分析验证了其对不同奖励分布的适用性。</div>
</details>
</div>
<div class="card">
<div class="title">The Landscape of Agentic Reinforcement Learning for LLMs: A Survey</div>
<div class="meta-line">Authors: Guibin Zhang, Hejia Geng, Xiaohang Yu, Zhenfei Yin, Zaibin Zhang, Zelin Tan, Heng Zhou, Zhongzhi Li, Xiangyuan Xue, Yijiang Li, Yifan Zhou, Yang Chen, Chen Zhang, Yutao Fan, Zihu Wang, Songtao Huang, Francisco Piedrahita-Velez, Yue Liao, Hongru Wang, Mengyue Yang, Heng Ji, Jun Wang, Shuicheng Yan, Philip Torr, Lei Bai</div>
<div class="meta-line">First: 2025-09-02T17:46:26+00:00 · Latest: 2026-01-24T22:41:54+00:00</div>
<div class="meta-line">Comments: Published on Transactions on Machine Learning Research: https://openreview.net/forum?id=RY19y2RI1O</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.02547v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.02547v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied to large language models (LLM RL), reframing LLMs from passive sequence generators into autonomous, decision-making agents embedded in complex, dynamic worlds. This survey formalizes this conceptual shift by contrasting the degenerate single-step Markov Decision Processes (MDPs) of LLM-RL with the temporally extended, partially observable Markov decision processes (POMDPs) that define Agentic RL. Building on this foundation, we propose a comprehensive twofold taxonomy: one organized around core agentic capabilities, including planning, tool use, memory, reasoning, self-improvement, and perception, and the other around their applications across diverse task domains. Central to our thesis is that reinforcement learning serves as the critical mechanism for transforming these capabilities from static, heuristic modules into adaptive, robust agentic behavior. To support and accelerate future research, we consolidate the landscape of open-source environments, benchmarks, and frameworks into a practical compendium. By synthesizing over five hundred recent works, this survey charts the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose AI agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向大语言模型的智能体强化学习研究综述</div>
<div class="mono" style="margin-top:8px">智能体强化学习的出现标志着从传统应用于大语言模型的强化学习向新范式的转变，将大语言模型从被动的序列生成器重塑为嵌入复杂动态世界的自主决策智能体。本综述通过对比大语言模型强化学习中简化的单步马尔可夫决策过程与定义智能体强化学习的时序扩展部分可观测马尔可夫决策过程，正式确立了这一概念转变。在此基础上，我们提出一个全面的双重分类体系：一个围绕核心智能体能力（包括规划、工具使用、记忆、推理、自我改进和感知）组织，另一个围绕其在多样化任务领域的应用展开。本文的核心论点是：强化学习是将这些能力从静态启发式模块转化为自适应、鲁棒智能体行为的关键机制。为支持和加速未来研究，我们将开源环境、基准测试和框架整合为实用手册。通过综合五百余篇近期研究成果，本综述勾勒了这一快速发展领域的轮廓，并指出了将影响可扩展通用人工智能智能体发展的机遇与挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This survey paper addresses the paradigm shift from conventional reinforcement learning for large language models (LLM RL) to agentic reinforcement learning (Agentic RL), which reframes LLMs as autonomous decision-making agents operating in complex, dynamic environments rather than passive sequence generators. The method involves formalizing this shift by contrasting single-step Markov Decision Processes with temporally extended, partially observable Markov decision processes, and proposing a twofold taxonomy organized around core agentic capabilities and their applications across diverse domains. The main experimental results are synthesized from over five hundred recent works, consolidating the landscape of open-source environments, benchmarks, and frameworks to highlight the opportunities and challenges in developing scalable, general-purpose AI agents.</div>
<div class="mono" style="margin-top:8px">本综述论文针对从传统大语言模型强化学习向智能体强化学习的范式转变，将大语言模型重新定义为在复杂动态环境中自主决策的智能体，而非被动序列生成器。研究方法通过对比单步马尔可夫决策过程与时间扩展的部分可观测马尔可夫决策过程来形式化这一转变，并提出围绕核心智能体能力及其跨领域应用的双重分类体系。主要实验成果综合了五百余篇近期研究，整合了开源环境、基准测试和框架的现状，以揭示开发可扩展通用人工智能智能体所面临的机遇与挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Athena: Synergizing Data Prefetching and Off-Chip Prediction via Online Reinforcement Learning</div>
<div class="meta-line">Authors: Rahul Bera, Zhenrong Lang, Caroline Hengartner, Konstantinos Kanellopoulos, Rakesh Kumar, Mohammad Sadrosadati, Onur Mutlu</div>
<div class="meta-line">Venue: 32nd IEEE International Symposium on High-Performance Computer Architecture (HPCA), 2026</div>
<div class="meta-line">First: 2026-01-24T22:39:07+00:00 · Latest: 2026-01-24T22:39:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17615v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17615v1">PDF</a> · <a href="https://github.com/CMU-SAFARI/Athena">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Prefetching and off-chip prediction are two techniques proposed to hide long memory access latencies in high-performance processors. In this work, we demonstrate that: (1) prefetching and off-chip prediction often provide complementary performance benefits, yet (2) naively combining them often fails to realize their full performance potential, and (3) existing prefetcher control policies leave significant room for performance improvement behind.
  Our goal is to design a holistic framework that can autonomously learn to coordinate an off-chip predictor with multiple prefetchers employed at various cache levels. To this end, we propose a new technique called Athena, which models the coordination between prefetchers and off-chip predictor (OCP) as a reinforcement learning (RL) problem. Athena acts as the RL agent that observes multiple system-level features (e.g., prefetcher/OCP accuracy, bandwidth usage) over an epoch of program execution, and uses them as state information to select a coordination action (i.e., enabling the prefetcher and/or OCP, and adjusting prefetcher aggressiveness). At the end of every epoch, Athena receives a numerical reward that measures the change in multiple system-level metrics (e.g., number of cycles taken to execute an epoch). Athena uses this reward to autonomously and continuously learn a policy to coordinate prefetchers with OCP.
  Our extensive evaluation using a diverse set of memory-intensive workloads shows that Athena consistently outperforms prior state-of-the-art coordination policies across a wide range of system configurations with various combinations of underlying prefetchers, OCPs, and main memory bandwidths, while incurring only modest storage overhead. Athena is freely available at https://github.com/CMU-SAFARI/Athena.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>雅典娜：通过在线强化学习协同数据预取与片外预测</div>
<div class="mono" style="margin-top:8px">预取与片外预测是两种用于隐藏高性能处理器中长内存访问延迟的技术。本研究证明：(1) 预取与片外预测常能提供互补的性能收益，但(2) 简单组合二者往往无法充分发挥其性能潜力，且(3) 现有预取器控制策略仍存在显著的性能提升空间。
我们旨在设计一个整体框架，能自主学习协调片外预测器与多级缓存中多个预取器的工作。为此，我们提出名为雅典娜的新技术，将预取器与片外预测器（OCP）的协同建模为强化学习（RL）问题。雅典娜作为RL智能体，在程序执行的每个周期内观测多维度系统特征（如预取器/OCP准确率、带宽使用率），将其作为状态信息来选择协同动作（即启用预取器和/或OCP，并调整预取器激进程度）。每个周期结束时，雅典娜接收量化奖励以评估多项系统指标的变化（如周期执行耗时），并据此持续自主学习预取器与OCP的协同策略。
通过对多样化内存密集型工作负载的广泛评估表明：在涵盖不同底层预取器、OCP及主存带宽组合的系统配置中，雅典娜始终优于现有先进协同策略，且仅产生适度存储开销。雅典娜已在https://github.com/CMU-SAFARI/Athena开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the complementary yet often underutilized performance benefits of data prefetching and off-chip prediction in mitigating memory latency, noting that naive combinations and existing control policies leave significant performance gains unrealized. The method, named Athena, employs online reinforcement learning to autonomously coordinate multiple prefetchers and an off-chip predictor by observing system-level features over execution epochs and selecting coordination actions, such as enabling components and adjusting prefetcher aggressiveness, based on a reward signal derived from metrics like cycle counts. Experimental results across diverse memory-intensive workloads demonstrate that Athena consistently outperforms prior state-of-the-art coordination policies under various system configurations, including different prefetchers, off-chip predictors, and memory bandwidths, while maintaining modest storage overhead.</div>
<div class="mono" style="margin-top:8px">本文的动机在于数据预取和片外预测在隐藏内存访问延迟方面具有互补但常未充分利用的性能优势，指出简单组合及现有控制策略未能实现其全部潜力。该方法名为Athena，采用在线强化学习，通过观察程序执行周期中的系统级特征并选择协调动作（如启用组件和调整预取器激进程度），基于周期数等指标得出的奖励信号，自主协调多个预取器与片外预测器。实验结果表明，在多种内存密集型工作负载下，Athena在不同预取器、片外预测器和内存带宽的系统配置中，始终优于先前的先进协调策略，同时仅产生适中的存储开销。</div>
</details>
</div>
<div class="card">
<div class="title">Deep Intrinsic Surprise-Regularized Control (DISRC): A Biologically Inspired Mechanism for Efficient Deep Q-Learning in Sparse Environments</div>
<div class="meta-line">Authors: Yash Kini, Shiv Davay, Shreya Polavarapu</div>
<div class="meta-line">First: 2026-01-24T21:25:08+00:00 · Latest: 2026-01-24T21:25:08+00:00</div>
<div class="meta-line">Comments: 5 pages, 3 figures, 1 table. Preprint version of work submitted, accepted, and presented at IEEE URTC. Accepted and pending publication in IEEE Xplore</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17598v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17598v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep reinforcement learning (DRL) has driven major advances in autonomous control. Still, standard Deep Q-Network (DQN) agents tend to rely on fixed learning rates and uniform update scaling, even as updates are modulated by temporal-difference (TD) error. This rigidity destabilizes convergence, especially in sparse-reward settings where feedback is infrequent. We introduce Deep Intrinsic Surprise-Regularized Control (DISRC), a biologically inspired augmentation to DQN that dynamically scales Q-updates based on latent-space surprise. DISRC encodes states via a LayerNorm-based encoder and computes a deviation-based surprise score relative to a moving latent setpoint. Each update is then scaled in proportion to both TD error and surprise intensity, promoting plasticity during early exploration and stability as familiarity increases. We evaluate DISRC on two sparse-reward MiniGrid environments, which included MiniGrid-DoorKey-8x8 and MiniGrid-LavaCrossingS9N1, under identical settings as a vanilla DQN baseline. In DoorKey, DISRC reached the first successful episode (reward &gt; 0.8) 33% faster than the vanilla DQN baseline (79 vs. 118 episodes), with lower reward standard deviation (0.25 vs. 0.34) and higher reward area under the curve (AUC: 596.42 vs. 534.90). These metrics reflect faster, more consistent learning - critical for sparse, delayed reward settings. In LavaCrossing, DISRC achieved a higher final reward (0.95 vs. 0.93) and the highest AUC of all agents (957.04), though it converged more gradually. These preliminary results establish DISRC as a novel mechanism for regulating learning intensity in off-policy agents, improving both efficiency and stability in sparse-reward domains. By treating surprise as an intrinsic learning signal, DISRC enables agents to modulate updates based on expectation violations, enhancing decision quality when conventional value-based methods fall short.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度内在惊奇正则化控制（DISRC）：一种稀疏环境下高效深度Q学习的生物启发机制</div>
<div class="mono" style="margin-top:8px">深度强化学习（DRL）推动了自主控制领域的重大进展。然而，标准深度Q网络（DQN）智能体往往依赖固定的学习率和均匀的更新缩放，即使更新受时序差分（TD）误差调节。这种刚性会破坏收敛稳定性，尤其在反馈稀疏的稀疏奖励环境中。我们提出深度内在惊奇正则化控制（DISRC），这是一种对DQN的生物启发式增强方法，基于潜在空间惊奇动态缩放Q更新。DISRC通过基于层归一化的编码器编码状态，并计算相对于移动潜在设定点的偏差型惊奇分数。每个更新随后按TD误差和惊奇强度比例缩放，从而在早期探索阶段促进可塑性，在熟悉度增加时提升稳定性。我们在两个稀疏奖励MiniGrid环境（包括MiniGrid-DoorKey-8x8和MiniGrid-LavaCrossingS9N1）中，以与原始DQN基线相同的设置评估DISRC。在DoorKey环境中，DISRC比原始DQN基线（118轮）提前33%达到首次成功回合（奖励&gt;0.8）（79轮），且奖励标准差更低（0.25对比0.34），曲线下面积（AUC）更高（596.42对比534.90）。这些指标反映了更快速、更稳定的学习过程——对稀疏延迟奖励环境至关重要。在LavaCrossing环境中，DISRC取得了更高的最终奖励（0.95对比0.93）和所有智能体中最高的AUC（957.04），尽管收敛速度较平缓。这些初步结果确立了DISRC作为调节离线策略智能体学习强度的新机制，在稀疏奖励领域同时提升了效率与稳定性。通过将惊奇视为内在学习信号，DISRC使智能体能基于预期违背调节更新，从而在传统基于价值的方法不足时提升决策质量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the instability of standard Deep Q-Networks (DQN) in sparse-reward environments due to rigid update scaling, this paper introduces Deep Intrinsic Surprise-Regularized Control (DISRC), a biologically inspired method that dynamically scales Q-updates based on a latent-space surprise score computed from state encodings relative to a moving setpoint. Experimental results on sparse-reward MiniGrid tasks show that DISRC accelerates learning, achieving the first successful episode 33% faster in DoorKey with higher consistency and area under the curve, and attains higher final rewards in LavaCrossing, demonstrating improved efficiency and stability over vanilla DQN.</div>
<div class="mono" style="margin-top:8px">针对标准深度Q网络（DQN）在稀疏奖励环境中因更新缩放僵化而导致收敛不稳定的问题，本文提出了深度内在惊喜正则化控制（DISRC），这是一种受生物学启发的增强方法，它基于潜在空间惊喜分数动态调整Q更新，该分数通过状态编码相对于移动设定点的偏差计算。在稀疏奖励MiniGrid环境上的实验结果表明，DISRC在DoorKey任务中首次成功回合的学习速度比基准DQN快33%，且具有更高的一致性和曲线下面积，在LavaCrossing任务中获得了更高的最终奖励，证明了其相对于原始DQN在效率和稳定性上的提升。</div>
</details>
</div>
<div class="card">
<div class="title">Stabilizing Policy Gradient Methods via Reward Profiling</div>
<div class="meta-line">Authors: Shihab Ahmed, El Houcine Bergou, Aritra Dutta, Yue Wang</div>
<div class="meta-line">First: 2025-11-20T18:35:51+00:00 · Latest: 2026-01-24T21:24:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16629v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.16629v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Policy gradient methods, which have been extensively studied in the last decade, offer an effective and efficient framework for reinforcement learning problems. However, their performances can often be unsatisfactory, suffering from unreliable reward improvements and slow convergence, due to high variance in gradient estimations. In this paper, we propose a universal reward profiling framework that can be seamlessly integrated with any policy gradient algorithm, where we selectively update the policy based on high-confidence performance estimations. We theoretically justify that our technique will not slow down the convergence of the baseline policy gradient methods, but with high probability, will result in stable and monotonic improvements of their performance. Empirically, on eight continuous-control benchmarks (Box2D and MuJoCo/PyBullet), our profiling yields up to 1.5x faster convergence to near-optimal returns, up to 1.75x reduction in return variance on some setups. Our profiling approach offers a general, theoretically grounded path to more reliable and efficient policy learning in complex environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过奖励剖析稳定策略梯度方法</div>
<div class="mono" style="margin-top:8px">策略梯度方法在过去十年中得到广泛研究，为强化学习问题提供了高效框架。然而，由于梯度估计的高方差，其性能常因奖励改进不稳定和收敛缓慢而不尽如人意。本文提出一种通用的奖励剖析框架，可无缝集成至任意策略梯度算法中，通过基于高置信度性能估计的选择性策略更新，理论上确保不会减缓基线方法的收敛速度，且能以高概率实现稳定、单调的性能提升。在八个连续控制基准测试（Box2D和MuJoCo/PyBullet）中，该框架使收敛至近最优回报的速度提升最高达1.5倍，部分场景下回报方差降低最高达1.75倍，为复杂环境中实现更可靠高效的策略学习提供了理论完备的通用路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the high variance and unstable convergence of policy gradient methods in reinforcement learning, this paper proposes a universal reward profiling framework that selectively updates policies based on high-confidence performance estimations. The method integrates seamlessly with existing policy gradient algorithms and is theoretically shown to maintain convergence speed while ensuring stable, monotonic performance improvements. Experimental results on eight continuous-control benchmarks demonstrate up to 1.5x faster convergence to near-optimal returns and up to 1.75x reduction in return variance, offering a general approach for more reliable policy learning.</div>
<div class="mono" style="margin-top:8px">针对强化学习中策略梯度方法因梯度估计方差高而导致收敛不稳定和性能不佳的问题，本文提出了一种通用的奖励分析框架，通过基于高置信度性能估计选择性更新策略来提升稳定性。该方法可无缝集成到现有策略梯度算法中，理论分析表明它不会降低基线方法的收敛速度，且能以高概率实现稳定、单调的性能改进。在八个连续控制基准测试中的实验结果显示，该方法使收敛速度提升至1.5倍，回报方差降低达1.75倍，为复杂环境中的策略学习提供了一条通用且理论可靠的路径。</div>
</details>
</div>
<div class="card">
<div class="title">Quantum-Inspired Episode Selection for Monte Carlo Reinforcement Learning via QUBO Optimization</div>
<div class="meta-line">Authors: Hadi Salloum, Ali Jnadi, Yaroslav Kholodov, Alexander Gasnikov</div>
<div class="meta-line">First: 2026-01-24T19:47:48+00:00 · Latest: 2026-01-24T19:47:48+00:00</div>
<div class="meta-line">Comments: Proceedings of Machine Learning Research tbd: 1_13, 2025 International Conference on Computational Optimization</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17570v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17570v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Monte Carlo (MC) reinforcement learning suffers from high sample complexity, especially in environments with sparse rewards, large state spaces, and correlated trajectories. We address these limitations by reformulating episode selection as a Quadratic Unconstrained Binary Optimization (QUBO) problem and solving it with quantum-inspired samplers. Our method, MC+QUBO, integrates a combinatorial filtering step into standard MC policy evaluation: from each batch of trajectories, we select a subset that maximizes cumulative reward while promoting state-space coverage. This selection is encoded as a QUBO, where linear terms favor high-reward episodes and quadratic terms penalize redundancy. We explore both Simulated Quantum Annealing (SQA) and Simulated Bifurcation (SB) as black-box solvers within this framework. Experiments in a finite-horizon GridWorld demonstrate that MC+QUBO outperforms vanilla MC in convergence speed and final policy quality, highlighting the potential of quantum-inspired optimization as a decision-making subroutine in reinforcement learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于QUBO优化的量子启发式蒙特卡洛强化学习情节选择方法</div>
<div class="mono" style="margin-top:8px">蒙特卡洛强化学习存在样本复杂度高的问题，在奖励稀疏、状态空间庞大且轨迹相关的环境中尤为突出。本研究通过将情节选择重构为二次无约束二进制优化问题，并采用量子启发式采样器求解，提出MC+QUBO方法。该方法在标准MC策略评估中引入组合筛选步骤：从每批轨迹中选择能最大化累积奖励并提升状态空间覆盖度的子集。该选择过程被编码为QUBO问题，其中线性项偏好高奖励情节，二次项惩罚冗余轨迹。我们探索了模拟量子退火与模拟分岔两种黑箱求解器在此框架中的应用。有限时域网格世界的实验表明，MC+QUBO在收敛速度与最终策略质量上均优于传统MC方法，凸显了量子启发式优化作为强化学习决策子程序的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the high sample complexity of Monte Carlo reinforcement learning in sparse-reward, large-state-space environments by reformulating episode selection as a Quadratic Unconstrained Binary Optimization problem. The proposed MC+QUBO method selects a subset of trajectories from each batch to maximize cumulative reward while promoting state-space coverage, using quantum-inspired samplers like Simulated Quantum Annealing and Simulated Bifurcation as solvers. Experimental results in a GridWorld environment show that MC+QUBO achieves faster convergence and higher final policy quality compared to standard Monte Carlo methods, demonstrating the utility of quantum-inspired optimization for reinforcement learning decision-making.</div>
<div class="mono" style="margin-top:8px">本文针对蒙特卡洛强化学习在稀疏奖励、大状态空间环境中样本复杂度高的问题，将情节选择重新表述为二次无约束二进制优化问题。提出的MC+QUBO方法从每批轨迹中选择一个子集，以最大化累积奖励并促进状态空间覆盖，使用模拟量子退火和模拟分岔等量子启发采样器作为求解器。在GridWorld环境中的实验结果表明，与标准蒙特卡洛方法相比，MC+QUBO实现了更快的收敛速度和更高的最终策略质量，证明了量子启发优化在强化学习决策中的潜力。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260127_0320.html">20260127_0320</a>
<a href="archive/20260126_0313.html">20260126_0313</a>
<a href="archive/20260125_0321.html">20260125_0321</a>
<a href="archive/20260124_0344.html">20260124_0344</a>
<a href="archive/20260123_0345.html">20260123_0345</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0408.html">20260121_0408</a>
<a href="archive/20260120_0316.html">20260120_0316</a>
<a href="archive/20260119_0312.html">20260119_0312</a>
<a href="archive/20260118_0323.html">20260118_0323</a>
<a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
