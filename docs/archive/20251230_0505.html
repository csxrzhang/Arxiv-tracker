<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-12-30 05:05</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251230_0505</div>
    <div class="row"><div class="card">
<div class="title">Cost-aware Stopping for Bayesian Optimization</div>
<div class="meta-line">Authors: Qian Xie, Linda Cai, Alexander Terenin, Peter I. Frazier, Ziv Scully</div>
<div class="meta-line">First: 2025-07-16T17:54:14+00:00 · Latest: 2025-12-26T18:48:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.12453v4">Abs</a> · <a href="https://arxiv.org/pdf/2507.12453v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In automated machine learning, scientific discovery, and other applications of Bayesian optimization, deciding when to stop evaluating expensive black-box functions in a cost-aware manner is an important but underexplored practical consideration. A natural performance metric for this purpose is the cost-adjusted simple regret, which captures the trade-off between solution quality and cumulative evaluation cost. While several heuristic or adaptive stopping rules have been proposed, they lack guarantees ensuring stopping before incurring excessive function evaluation costs. We propose a principled cost-aware stopping rule for Bayesian optimization that adapts to varying evaluation costs without heuristic tuning. Our rule is grounded in a theoretical connection to state-of-the-art cost-aware acquisition functions, namely the Pandora&#x27;s Box Gittins Index (PBGI) and log expected improvement per cost (LogEIPC). We prove a theoretical guarantee bounding the expected cost-adjusted simple regret incurred by our stopping rule when paired with either acquisition function. Across synthetic and empirical tasks, including hyperparameter optimization and neural architecture size search, pairing our stopping rule with PBGI or LogEIPC usually matches or outperforms other acquisition-function--stopping-rule pairs in terms of cost-adjusted simple regret.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>贝叶斯优化的成本感知停止策略</div>
<div class="mono" style="margin-top:8px">在自动化机器学习、科学发现及其他贝叶斯优化应用中，如何以成本感知的方式决定何时停止评估昂贵的黑盒函数，是一个重要但尚未充分探索的实际问题。为此，成本调整简单遗憾是一种自然的性能度量指标，它捕捉了解决方案质量与累积评估成本之间的权衡。尽管已有多种启发式或自适应停止规则被提出，但它们缺乏确保在产生过高函数评估成本前停止的理论保证。本文提出一种基于原则的成本感知停止规则，适用于贝叶斯优化，能够自适应变化的评估成本而无需启发式调参。该规则的理论基础源于与前沿成本感知采集函数（即潘多拉盒吉廷斯指数和成本对数期望改进）的理论关联。我们证明了当该停止规则与任一采集函数结合时，其预期成本调整简单遗憾的理论上界。在合成与实证任务（包括超参数优化和神经网络架构规模搜索）中，将本停止规则与潘多拉盒吉廷斯指数或成本对数期望改进结合使用，通常在成本调整简单遗憾方面达到或优于其他采集函数-停止规则组合。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the practical need for cost-aware stopping in Bayesian optimization, where evaluating expensive black-box functions requires balancing solution quality with cumulative costs, as measured by cost-adjusted simple regret. The authors propose a principled stopping rule that adapts to varying evaluation costs without heuristic tuning, grounded in theoretical connections to state-of-the-art cost-aware acquisition functions like Pandora&#x27;s Box Gittins Index and log expected improvement per cost. They provide a theoretical guarantee bounding the expected cost-adjusted simple regret and demonstrate through synthetic and empirical tasks, including hyperparameter optimization and neural architecture search, that their rule paired with these acquisition functions typically matches or outperforms other combinations in terms of cost-adjusted simple regret.</div>
<div class="mono" style="margin-top:8px">本文针对贝叶斯优化中成本感知停止的实际需求，其中评估昂贵的黑盒函数需要在解决方案质量与累积成本之间取得平衡，以成本调整简单遗憾为衡量指标。作者提出了一种无需启发式调优、能自适应不同评估成本的原则性停止规则，其理论基础与最先进的成本感知采集函数（如潘多拉盒吉廷斯指数和对数期望改进每成本）相关联。他们提供了理论保证，界定了该停止规则在预期成本调整简单遗憾上的表现，并通过合成和实证任务（包括超参数优化和神经架构搜索）证明，该规则与这些采集函数结合通常能匹配或优于其他组合在成本调整简单遗憾方面的性能。</div>
</details>
</div>
<div class="card">
<div class="title">A study of EHVI vs fixed scalarization for molecule design</div>
<div class="meta-line">Authors: Anabel Yong, Austin Tripp, Layla Hosseini-Gerami, Brooks Paige</div>
<div class="meta-line">Venue: NeurIPS</div>
<div class="meta-line">First: 2025-07-18T07:12:19+00:00 · Latest: 2025-12-24T14:56:07+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS AI4Science Workshop 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.13704v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.13704v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-objective Bayesian optimization (MOBO) provides a principled framework for navigating trade-offs in molecular design. However, its empirical advantages over scalarized alternatives remain underexplored. We benchmark a simple Pareto-based MOBO strategy - Expected Hypervolume Improvement (EHVI) - against a simple fixed-weight scalarized baseline using Expected Improvement (EI), under a tightly controlled setup with identical Gaussian Process surrogates and molecular representations. Across three molecular optimization tasks, EHVI consistently outperforms scalarized EI in terms of Pareto front coverage, convergence speed, and chemical diversity. While scalarization encompasses flexible variants - including random or adaptive schemes - our results show that even strong deterministic instantiations can underperform in low-data regimes. These findings offer concrete evidence for the practical advantages of Pareto-aware acquisition in de novo molecular optimization, especially when evaluation budgets are limited and trade-offs are nontrivial.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分子设计中EHVI与固定标量化方法的对比研究</div>
<div class="mono" style="margin-top:8px">多目标贝叶斯优化（MOBO）为分子设计中的权衡探索提供了理论框架，但其相较于标量化方法的实证优势尚未充分研究。我们在严格控制的实验设置下（使用相同的高斯过程代理模型与分子表示），将基于帕累托的简单MOBO策略——期望超体积改进（EHVI）与采用期望改进（EI）的固定权重标量化基线进行对比。在三个分子优化任务中，EHVI在帕累托前沿覆盖度、收敛速度和化学多样性方面均持续优于标量化EI。尽管标量化方法包含随机或自适应等灵活变体，但实验表明即使在强确定性设定下，其在低数据量场景中仍可能表现不佳。这些发现为帕累托感知采集策略在从头分子优化中的实际优势提供了具体证据，尤其在评估预算有限且权衡关系复杂时更为显著。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the empirical advantages of multi-objective Bayesian optimization (MOBO) over scalarization methods in molecular design, motivated by the need to better understand their performance in navigating trade-offs. The method compares a Pareto-based MOBO strategy using Expected Hypervolume Improvement (EHVI) against a fixed-weight scalarized baseline with Expected Improvement (EI), employing identical Gaussian Process surrogates and molecular representations in a controlled setup. Experimental results across three molecular optimization tasks show that EHVI consistently outperforms scalarized EI in Pareto front coverage, convergence speed, and chemical diversity, highlighting the practical benefits of Pareto-aware acquisition, particularly in low-data regimes with limited evaluation budgets.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究多目标贝叶斯优化（MOBO）在分子设计中相较于标量化方法的实证优势，动机在于更好地理解其在权衡取舍中的性能。方法上，在严格控制设置下，使用相同的高斯过程代理和分子表示，比较了基于帕累托的MOBO策略（采用预期超体积改进EHVI）与固定权重标量化基线（采用预期改进EI）。在三个分子优化任务上的实验结果表明，EHVI在帕累托前沿覆盖、收敛速度和化学多样性方面均一致优于标量化EI，这凸显了帕累托感知获取策略的实用优势，特别是在评估预算有限且权衡关系复杂的低数据场景中。</div>
</details>
</div>
<div class="card">
<div class="title">Neural Probe-Based Hallucination Detection for Large Language Models</div>
<div class="meta-line">Authors: Shize Liang, Hongzhi Wang</div>
<div class="meta-line">First: 2025-12-24T05:10:19+00:00 · Latest: 2025-12-24T05:10:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20949v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20949v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models(LLMs) excel at text generation and knowledge question-answering tasks, but they are prone to generating hallucinated content, severely limiting their application in high-risk domains. Current hallucination detection methods based on uncertainty estimation and external knowledge retrieval suffer from the limitation that they still produce erroneous content at high confidence levels and rely heavily on retrieval efficiency and knowledge coverage. In contrast, probe methods that leverage the model&#x27;s hidden-layer states offer real-time and lightweight advantages. However, traditional linear probes struggle to capture nonlinear structures in deep semantic spaces.To overcome these limitations, we propose a neural network-based framework for token-level hallucination detection. By freezing language model parameters, we employ lightweight MLP probes to perform nonlinear modeling of high-level hidden states. A multi-objective joint loss function is designed to enhance detection stability and semantic disambiguity. Additionally, we establish a layer position-probe performance response model, using Bayesian optimization to automatically search for optimal probe insertion layers and achieve superior training results.Experimental results on LongFact, HealthBench, and TriviaQA demonstrate that MLP probes significantly outperform state-of-the-art methods in accuracy, recall, and detection capability under low false-positive conditions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于神经探针的大语言模型幻觉检测</div>
<div class="mono" style="margin-top:8px">大语言模型在文本生成和知识问答任务中表现出色，但容易产生幻觉内容，严重限制了其在高风险领域的应用。当前基于不确定性估计和外部知识检索的幻觉检测方法存在局限：它们仍会在高置信度下生成错误内容，且高度依赖检索效率和知识覆盖范围。相比之下，利用模型隐藏层状态的探针方法具有实时性和轻量化优势。然而，传统线性探针难以捕捉深层语义空间中的非线性结构。为克服这些局限，我们提出一种基于神经网络的词元级幻觉检测框架：通过冻结语言模型参数，采用轻量级MLP探针对高层隐藏状态进行非线性建模；设计多目标联合损失函数以提升检测稳定性和语义消歧能力；同时建立层位置-探针性能响应模型，利用贝叶斯优化自动搜索最优探针插入层，实现更优训练效果。在LongFact、HealthBench和TriviaQA数据集上的实验表明，MLP探针在低误报条件下，其准确率、召回率和检测能力均显著优于现有先进方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the critical issue of hallucination in large language models (LLMs), which limits their safe deployment in high-risk domains, by moving beyond existing detection methods that rely on uncertainty estimation or external knowledge retrieval and are often inefficient or unreliable. The method introduces a neural network-based framework for token-level hallucination detection, which freezes the LLM parameters and employs lightweight MLP probes to nonlinearly model high-level hidden states, enhanced by a multi-objective joint loss function and a Bayesian optimization strategy to automatically search for optimal probe insertion layers. Experimental results on datasets including LongFact, HealthBench, and TriviaQA show that the proposed MLP probes significantly outperform state-of-the-art methods in accuracy, recall, and detection capability under low false-positive conditions.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大语言模型生成幻觉内容这一关键问题，该问题限制了其在高风险领域的应用，现有基于不确定性估计或外部知识检索的检测方法存在效率低和可靠性不足的局限。方法上提出了一种基于神经网络的词元级幻觉检测框架，通过冻结语言模型参数，使用轻量级MLP探针对高层隐藏状态进行非线性建模，并设计了多目标联合损失函数以增强检测稳定性，同时利用贝叶斯优化自动搜索最优探针插入层。在LongFact、HealthBench和TriviaQA数据集上的实验结果表明，MLP探针在准确率、召回率以及低误报条件下的检测能力上显著优于现有先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">GLUE: Generative Latent Unification of Expertise-Informed Engineering Models</div>
<div class="meta-line">Authors: Tim Aebersold, Soheyl Massoudi, Mark D. Fuge</div>
<div class="meta-line">First: 2025-12-22T15:23:19+00:00 · Latest: 2025-12-22T15:23:19+00:00</div>
<div class="meta-line">Comments: 11 pages, 10 figures. Preprint. Submitted to Computer-Aided Engineering (Elsevier)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19469v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19469v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Engineering complex systems (aircraft, buildings, vehicles) requires accounting for geometric and performance couplings across subsystems. As generative models proliferate for specialized domains (wings, structures, engines), a key research gap is how to coordinate frozen, pre-trained submodels to generate full-system designs that are feasible, diverse, and high-performing. We introduce Generative Latent Unification of Expertise-Informed Engineering Models (GLUE), which orchestrates pre-trained, frozen subsystem generators while enforcing system-level feasibility, optimality, and diversity. We propose and benchmark (i) data-driven GLUE models trained on pre-generated system-level designs and (ii) a data-free GLUE model trained online on a differentiable geometry layer. On a UAV design problem with five coupling constraints, we find that data-driven approaches yield diverse, high-performing designs but require large datasets to satisfy constraints reliably. The data-free approach is competitive with Bayesian optimization and gradient-based optimization in performance and feasibility while training a full generative model in only 10 min on a RTX 4090 GPU, requiring more than two orders of magnitude fewer geometry evaluations and FLOPs than the data-driven method. Ablations focused on data-free training show that subsystem output continuity affects coordination, and equality constraints can trigger mode collapse unless mitigated. By integrating unmodified, domain-informed submodels into a modular generative workflow, this work provides a viable path for scaling generative design to complex, real-world engineering systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GLUE：基于专家知识的工程模型生成式潜在统一框架</div>
<div class="mono" style="margin-top:8px">复杂工程系统（飞机、建筑、车辆）的设计需考虑跨子系统的几何与性能耦合。随着专业领域（机翼、结构、发动机）生成模型的激增，如何协调冻结的预训练子模型以生成可行、多样且高性能的全系统设计成为关键研究缺口。本文提出基于专家知识的工程模型生成式潜在统一框架（GLUE），通过协调预训练冻结的子系统生成器，同时保障系统级可行性、最优性与多样性。我们提出并评估了两种方法：（1）基于预生成系统级设计数据训练的GLUE模型；（2）在可微分几何层上在线训练的无数据GLUE模型。在包含五个耦合约束的无人机设计问题中，数据驱动方法能生成多样且高性能的设计，但需要大量数据才能可靠满足约束；无数据方法在性能与可行性方面与贝叶斯优化和梯度优化相当，且在RTX 4090 GPU上仅需10分钟即可完成完整生成模型训练，其几何评估次数和浮点运算量比数据驱动方法低两个数量级以上。针对无数据训练的消融实验表明：子系统输出连续性影响协调效果，等式约束可能引发模式坍塌需加以缓解。通过将未经修改的领域专家子模型整合至模块化生成工作流，本研究为生成式设计扩展至复杂现实工程系统提供了可行路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the challenge of coordinating multiple pre-trained, specialized generative models for engineering subsystems into a unified framework that can produce feasible, diverse, and high-performing full-system designs. The method introduces Generative Latent Unification of Expertise-Informed Engineering Models (GLUE), which orchestrates frozen subsystem generators through two approaches: a data-driven model trained on pre-generated system designs and a data-free model trained online using a differentiable geometry layer. Experimental results on a UAV design problem with five coupling constraints show that the data-driven approach yields diverse, high-performing designs but requires large datasets to reliably satisfy constraints, while the data-free method is competitive with Bayesian and gradient-based optimization in performance and feasibility, training a full generative model in only 10 minutes on an RTX 4090 GPU with significantly fewer geometry evaluations and computational operations.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决如何协调多个预训练的工程子系统专业生成模型，以形成一个统一框架，从而生成可行、多样且高性能的全系统设计。方法上提出了生成式潜在统一专家知识工程模型（GLUE），通过两种途径协调冻结的子系统生成器：一种是基于预生成系统设计训练的数据驱动模型，另一种是利用可微分几何层在线训练的无数据模型。在具有五个耦合约束的无人机设计问题上的实验结果表明，数据驱动方法能产生多样且高性能的设计，但需要大量数据才能可靠满足约束；而无数据方法在性能和可行性上与贝叶斯优化及基于梯度的优化相当，仅需在RTX 4090 GPU上训练10分钟即可完成完整生成模型，且几何评估和计算操作量显著减少。</div>
</details>
</div>
<div class="card">
<div class="title">ESSA: Evolutionary Strategies for Scalable Alignment</div>
<div class="meta-line">Authors: Daria Korotyshova, Boris Shaposhnikov, Alexey Malakhov, Alexey Khokhulin, Nikita Surnachev, Kirill Ovcharenko, George Bredis, Alexey Gorbatovski, Viacheslav Sinii, Daniil Gavrilov</div>
<div class="meta-line">First: 2025-07-06T16:23:07+00:00 · Latest: 2025-12-22T14:35:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.04453v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.04453v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Alignment of Large Language Models (LLMs) typically relies on Reinforcement Learning from Human Feedback (RLHF) with gradient-based optimizers such as Proximal Policy Optimization (PPO) or Group Relative Policy Optimization (GRPO). While effective, these methods require complex distributed training, large memory budgets, and careful hyperparameter tuning, all of which become increasingly difficult at billion-parameter scale. We present ESSA, Evolutionary Strategies for Scalable Alignment, a gradient-free framework that aligns LLMs using only forward inference and black-box optimization. ESSA focuses optimization on Low-Rank Adapters (LoRA) and further compresses their parameter space by optimizing only the singular values from an singular value decomposition (SVD) of each adapter matrix. This dimensionality reduction makes evolutionary search practical even for very large models and allows efficient operation in quantized INT4 and INT8 inference mode. Across these benchmarks ESSA improves the test accuracy of Qwen2.5-Math-7B by 12.6% on GSM8K and 14.8% on PRM800K, and raises the accuracy of LLaMA3.1-8B on IFEval by 22.5%, all compared with GRPO. In large-scale settings ESSA shows stronger scaling than gradient-based methods: on Qwen2.5-32B for PRM800K it reaches near-optimal accuracy twice as fast on 16 GPUs and six times as fast on 128 GPUs compared with GRPO. These results position evolutionary strategies as a compelling, hardware-friendly alternative to gradient-based LLM alignment, combining competitive quality with substantially reduced wall-clock time and engineering overhead.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ESSA：面向可扩展对齐的进化策略</div>
<div class="mono" style="margin-top:8px">大语言模型（LLM）的对齐通常依赖于基于人类反馈的强化学习（RLHF），并采用如近端策略优化（PPO）或组相对策略优化（GRPO）等基于梯度的优化器。这些方法虽有效，但需要复杂的分布式训练、大量内存预算以及精细的超参数调优，在数十亿参数规模下实施难度显著增加。本文提出ESSA（面向可扩展对齐的进化策略），这是一种无需梯度的框架，仅通过前向推理和黑盒优化实现LLM对齐。ESSA将优化集中于低秩适配器（LoRA），并通过仅优化每个适配器矩阵奇异值分解（SVD）后的奇异值，进一步压缩其参数空间。这种降维处理使得进化搜索即使对于超大规模模型也具备可行性，并能在量化INT4和INT8推理模式下高效运行。在多项基准测试中，ESSA将Qwen2.5-Math-7B在GSM8K上的测试准确率提升12.6%，在PRM800K上提升14.8%；将LLaMA3.1-8B在IFEval上的准确率提高22.5%（均与GRPO对比）。在大规模场景中，ESSA展现出比梯度方法更强的扩展性：在PRM800K任务上，Qwen2.5-32B使用16张GPU时达到接近最优准确率的速度是GRPO的2倍，使用128张GPU时达到6倍。这些结果表明，进化策略作为基于梯度的LLM对齐方法的硬件友好型替代方案，兼具竞争力质量与显著缩短的实耗时间及工程开销。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces ESSA, a gradient-free evolutionary strategy framework designed to address the scalability and engineering challenges of gradient-based alignment methods like RLHF, which require complex distributed training and extensive hyperparameter tuning for large language models. ESSA optimizes only the singular values from an SVD decomposition of Low-Rank Adapters (LoRA), drastically reducing the parameter space and enabling efficient operation in quantized INT4/INT8 inference modes. Experimental results show that ESSA significantly improves test accuracy on benchmarks such as GSM8K and PRM800K for models like Qwen2.5-Math-7B and LLaMA3.1-8B, outperforming GRPO, and demonstrates superior scaling efficiency, achieving near-optimal accuracy up to six times faster on 128 GPUs for the Qwen2.5-32B model.</div>
<div class="mono" style="margin-top:8px">本文提出了ESSA，一种无需梯度的进化策略框架，旨在解决基于梯度的对齐方法（如RLHF）在大型语言模型上面临的可扩展性和工程挑战，这些方法需要复杂的分布式训练和大量超参数调优。ESSA仅优化低秩适配器（LoRA）矩阵经奇异值分解后的奇异值，大幅减少了参数空间，并支持在INT4/INT8量化推理模式下高效运行。实验结果表明，ESSA在GSM8K和PRM800K等基准测试上显著提升了Qwen2.5-Math-7B和LLaMA3.1-8B等模型的测试准确率，优于GRPO方法，并展现出更强的扩展效率，在128个GPU上为Qwen2.5-32B模型实现近最优准确率的速度比GRPO快六倍。</div>
</details>
</div>
<div class="card">
<div class="title">From Black-Box Tuning to Guided Optimization via Hyperparameters Interaction Analysis</div>
<div class="meta-line">Authors: Moncef Garouani, Ayah Barhrhouj</div>
<div class="meta-line">Venue: 2025 IEEE 37th International Conference on Tools with Artificial Intelligence (ICTAI)</div>
<div class="meta-line">First: 2025-12-22T10:28:22+00:00 · Latest: 2025-12-22T10:28:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19246v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19246v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hyperparameters tuning is a fundamental, yet computationally expensive, step in optimizing machine learning models. Beyond optimization, understanding the relative importance and interaction of hyperparameters is critical to efficient model development. In this paper, we introduce MetaSHAP, a scalable semi-automated eXplainable AI (XAI) method, that uses meta-learning and Shapley values analysis to provide actionable and dataset-aware tuning insights. MetaSHAP operates over a vast benchmark of over 09 millions evaluated machine learning pipelines, allowing it to produce interpretable importance scores and actionable tuning insights that reveal how much each hyperparameter matters, how it interacts with others and in which value ranges its influence is concentrated. For a given algorithm and dataset, MetaSHAP learns a surrogate performance model from historical configurations, computes hyperparameters interactions using SHAP-based analysis, and derives interpretable tuning ranges from the most influential hyperparameters. This allows practitioners not only to prioritize which hyperparameters to tune, but also to understand their directionality and interactions. We empirically validate MetaSHAP on a diverse benchmark of 164 classification datasets and 14 classifiers, demonstrating that it produces reliable importance rankings and competitive performance when used to guide Bayesian optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从黑盒调优到基于超参数交互分析的引导式优化</div>
<div class="mono" style="margin-top:8px">超参数调优是优化机器学习模型的基础步骤，但计算成本高昂。除优化外，理解超参数的相对重要性和交互作用对高效模型开发至关重要。本文提出MetaSHAP——一种可扩展的半自动化可解释人工智能方法，通过元学习和沙普利值分析提供可操作且感知数据集的调优洞见。MetaSHAP基于超过900万个已评估机器学习流程的基准库运行，能生成可解释的重要性评分和可操作的调优建议，揭示各超参数的影响程度、交互关系及关键影响区间。针对特定算法和数据集，MetaSHAP从历史配置中学习代理性能模型，采用基于SHAP的分析计算超参数交互作用，并从最具影响力的超参数中推导可解释的调优范围。这不仅帮助实践者确定调优优先级，还能理解参数方向性与交互关系。我们在包含164个分类数据集和14种分类器的多样化基准上实证验证MetaSHAP，证明其能生成可靠的重要性排序，并在指导贝叶斯优化时实现具有竞争力的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to move beyond computationally expensive black-box hyperparameter tuning and to understand hyperparameter importance and interactions for efficient model development, this paper introduces MetaSHAP, a scalable semi-automated XAI method. The method leverages meta-learning and Shapley values analysis on a vast benchmark of over 9 million evaluated machine learning pipelines to learn surrogate performance models, compute hyperparameter interactions via SHAP, and derive interpretable tuning ranges. Experimental validation on 164 classification datasets and 14 classifiers demonstrates that MetaSHAP provides reliable hyperparameter importance rankings and achieves competitive performance when guiding Bayesian optimization.</div>
<div class="mono" style="margin-top:8px">本文的动机是超越计算成本高昂的黑盒超参数调优，并理解超参数的重要性与交互作用以提升模型开发效率。为此，论文提出了MetaSHAP，一种可扩展的半自动化可解释人工智能方法。该方法基于包含超过900万个已评估机器学习管道的大规模基准，利用元学习和Shapley值分析来学习代理性能模型，通过SHAP计算超参数交互，并推导出可解释的调优范围。在164个分类数据集和14个分类器上的实验验证表明，MetaSHAP能提供可靠的超参数重要性排序，并在指导贝叶斯优化时实现有竞争力的性能。</div>
</details>
</div>
<div class="card">
<div class="title">OPBO: Order-Preserving Bayesian Optimization</div>
<div class="meta-line">Authors: Wei Peng, Jianchen Hu, Kang Liu, Qiaozhu Zhai</div>
<div class="meta-line">First: 2025-12-22T02:45:41+00:00 · Latest: 2025-12-22T02:45:41+00:00</div>
<div class="meta-line">Comments: 13 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.18980v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.18980v1">PDF</a> · <a href="https://github.com/pengwei222/OPBO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bayesian optimization is an effective method for solving expensive black-box optimization problems. Most existing methods use Gaussian processes (GP) as the surrogate model for approximating the black-box objective function, it is well-known that it can fail in high-dimensional space (e.g., dimension over 500). We argue that the reliance of GP on precise numerical fitting is fundamentally ill-suited in high-dimensional space, where it leads to prohibitive computational complexity. In order to address this, we propose a simple order-preserving Bayesian optimization (OPBO) method, where the surrogate model preserves the order, instead of the value, of the black-box objective function. Then we can use a simple but effective OP neural network (NN) to replace GP as the surrogate model. Moreover, instead of searching for the best solution from the acquisition model, we select good-enough solutions in the ordinal set to reduce computational cost. The experimental results show that for high-dimensional (over 500) black-box optimization problems, the proposed OPBO significantly outperforms traditional BO methods based on regression NN and GP. The source code is available at https://github.com/pengwei222/OPBO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OPBO：保序贝叶斯优化</div>
<div class="mono" style="margin-top:8px">贝叶斯优化是解决昂贵黑盒优化问题的有效方法。现有方法大多采用高斯过程作为代理模型来近似黑盒目标函数，但众所周知，在高维空间（如维度超过500）中该方法可能失效。我们认为，高斯过程对精确数值拟合的依赖本质上不适用于高维空间，会导致计算复杂度急剧上升。为此，我们提出了一种简单的保序贝叶斯优化方法，其代理模型保留黑盒目标函数的序关系而非具体数值。随后，我们可以用简单高效的保序神经网络替代高斯过程作为代理模型。此外，我们通过从序集中选择足够优的解来替代从采集模型中搜索最优解，以降低计算成本。实验结果表明，对于高维（超过500维）黑盒优化问题，所提出的OPBO方法显著优于基于回归神经网络和高斯过程的传统贝叶斯优化方法。源代码发布于https://github.com/pengwei222/OPBO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the computational inefficiency of Gaussian process (GP) surrogate models in high-dimensional Bayesian optimization (BO), where precise numerical fitting becomes prohibitive. To address this, the authors propose Order-Preserving Bayesian Optimization (OPBO), which uses a neural network surrogate model that preserves only the order of the objective function values rather than their exact magnitudes, thereby simplifying the modeling task. Experimental results demonstrate that OPBO significantly outperforms traditional BO methods based on GP and regression neural networks on high-dimensional black-box optimization problems with dimensions over 500.</div>
<div class="mono" style="margin-top:8px">本文的动机在于高维贝叶斯优化中高斯过程代理模型的计算效率低下，精确数值拟合变得难以承受。为解决此问题，作者提出了保序贝叶斯优化方法，该方法使用神经网络作为代理模型，仅保留目标函数值的顺序而非精确数值，从而简化了建模任务。实验结果表明，在维度超过500的高维黑盒优化问题上，所提出的方法显著优于基于高斯过程和回归神经网络的传统贝叶斯优化方法。</div>
</details>
</div>
<div class="card">
<div class="title">Hardware-Aware DNN Compression for Homogeneous Edge Devices</div>
<div class="meta-line">Authors: Kunlong Zhang, Guiying Li, Ning Lu, Peng Yang, Ke Tang</div>
<div class="meta-line">Venue: Proc. International Conference on Data-driven Optimization of Complex Systems (DOCS), 2025</div>
<div class="meta-line">First: 2025-01-25T15:14:18+00:00 · Latest: 2025-12-21T06:16:00+00:00</div>
<div class="meta-line">Comments: Published at the International Conference on Data-driven Optimization of Complex Systems (DOCS 2025). The final published version is available via DOI: 10.1109/DOCS67533.2025.11200827</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.15240v2">Abs</a> · <a href="https://arxiv.org/pdf/2501.15240v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying deep neural networks (DNNs) across homogeneous edge devices (the devices with the same SKU labeled by the manufacturer) often assumes identical performance among them. However, once a device model is widely deployed, the performance of each device becomes different after a period of running. This is caused by the differences in user configurations, environmental conditions, manufacturing variances, battery degradation, etc. Existing DNN compression methods have not taken this scenario into consideration and can not guarantee good compression results in all homogeneous edge devices. To address this, we propose Homogeneous-Device Aware Pruning (HDAP), a hardware-aware DNN compression framework explicitly designed for homogeneous edge devices, aiming to achieve optimal average performance of the compressed model across all devices. To deal with the difficulty of time-consuming hardware-aware evaluations for thousands or millions of homogeneous edge devices, HDAP partitions all the devices into several device clusters, which can dramatically reduce the number of devices to evaluate and use the surrogate-based evaluation instead of hardware evaluation in real-time. Experiments on ResNet50 and MobileNetV1 with the ImageNet dataset show that HDAP consistently achieves lower average inference latency compared with state-of-the-art methods, with substantial speedup gains (e.g., 2.86 $\times$ speedup at 1.0G FLOPs for ResNet50) on the homogeneous device clusters. HDAP offers an effective solution for scalable, high-performance DNN deployment methods for homogeneous edge devices.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向同构边缘设备的硬件感知深度神经网络压缩</div>
<div class="mono" style="margin-top:8px">在同构边缘设备（制造商标注相同SKU的设备）上部署深度神经网络时，通常假设设备间性能一致。然而，设备型号大规模部署后，因用户配置、环境条件、制造差异、电池老化等因素，各设备实际性能会逐渐分化。现有DNN压缩方法未考虑此场景，难以保证在所有同构设备上均获得良好压缩效果。为此，我们提出同构设备感知剪枝框架，该硬件感知的DNN压缩框架专为同构边缘设备设计，旨在使压缩模型在所有设备上获得最优平均性能。针对海量同构设备硬件评估耗时难题，HDAP将设备划分为若干集群，大幅减少待评估设备数量，并采用代理评估替代实时硬件评估。在ImageNet数据集上对ResNet50和MobileNetV1的实验表明，相较于前沿方法，HDAP在同构设备集群上持续实现更低的平均推理延迟，并获得显著加速增益（如ResNet50在1.0G FLOPs时实现2.86倍加速）。HDAP为同构边缘设备提供了可扩展的高性能DNN部署方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of deploying deep neural networks (DNNs) on homogeneous edge devices, which, despite sharing the same manufacturer model, exhibit performance variations over time due to factors like user configurations and hardware degradation, a scenario overlooked by existing compression methods. To ensure optimal average performance across all such devices, the authors propose Homogeneous-Device Aware Pruning (HDAP), a hardware-aware compression framework that clusters devices to reduce evaluation overhead and employs surrogate-based assessments instead of real-time hardware testing. Experimental results on ResNet50 and MobileNetV1 with ImageNet demonstrate that HDAP achieves lower average inference latency than state-of-the-art methods, delivering significant speedups, such as a 2.86× acceleration for ResNet50 at 1.0G FLOPs, thus providing a scalable solution for high-performance DNN deployment on homogeneous edge devices.</div>
<div class="mono" style="margin-top:8px">本文针对同构边缘设备部署深度神经网络（DNN）的挑战展开研究，这些设备尽管制造商型号相同，但由于用户配置、硬件退化等因素，长期运行后性能会出现差异，而现有压缩方法未考虑此场景。为确保所有设备上的平均性能最优，作者提出了同构设备感知剪枝（HDAP），这是一种硬件感知的压缩框架，通过将设备聚类以减少评估开销，并采用基于代理的评估替代实时硬件测试。在ImageNet数据集上对ResNet50和MobileNetV1的实验结果表明，HDAP相比先进方法实现了更低的平均推理延迟，获得了显著的加速效果，例如在1.0G FLOPs下ResNet50加速2.86倍，从而为同构边缘设备的高性能DNN部署提供了可扩展的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">A Neural Surrogate-Enhanced Multi-Method Framework for Robust Wing Design Optimization</div>
<div class="meta-line">Authors: Arash Fath Lipaei, Melika Sabzikari</div>
<div class="meta-line">First: 2025-09-29T17:26:37+00:00 · Latest: 2025-12-19T18:34:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.08582v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.08582v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces a modular and scalable design optimization framework for the wing design process that enables faster early-phase design while ensuring aerodynamic stability. The pipeline starts with the generation of initial wing geometries and then proceeds to optimize the wing using several algorithms. Aerodynamic performance is assessed using a Vortex Lattice Method (VLM) applied to a carefully selected dataset of wing configurations. These results are employed to develop surrogate neural network models, which can predict lift and drag rapidly and accurately. The stability evaluation is implemented by setting the control surfaces and components to fixed positions in order to have realistic flight dynamics. The approach unifies and compares several optimization techniques, including Particle Swarm Optimization (PSO), Genetic Algorithms (GA), gradient-based MultiStart methods, Bayesian optimization, and Lipschitz optimization. Each method ensures constraint management via adaptive strategies and penalty functions, where the targets for lift and design feasibility are enforced. The progression of aerodynamic characteristics and geometries over the optimization iterations will be investigated in order to clarify each algorithm&#x27;s convergence characteristics and performance efficiency. Our results show improvement in aerodynamic qualities and robust stability properties, offering a mechanism for wing design at speed and precision. In the interest of reproducibility and community development, the complete implementation is publicly available at Github.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种基于神经代理增强的多方法稳健机翼设计优化框架</div>
<div class="mono" style="margin-top:8px">本文提出了一种模块化、可扩展的机翼设计优化框架，旨在加速早期设计阶段并确保气动稳定性。该流程首先生成初始机翼几何构型，随后采用多种算法进行优化。气动性能通过涡格法（VLM）对精选的机翼构型数据集进行评估，并利用其结果构建代理神经网络模型，以实现快速精准的升阻力预测。稳定性评估通过将控制面与组件固定于特定位置来模拟真实飞行动力学。该框架整合并比较了多种优化技术，包括粒子群优化（PSO）、遗传算法（GA）、基于梯度的多起点方法、贝叶斯优化及利普希茨优化。各方法均通过自适应策略与罚函数实现约束管理，确保升力目标与设计可行性。研究通过分析优化迭代中气动特性与几何形态的演变，阐明各算法的收敛特性与性能效率。实验结果表明，该方法能有效提升气动品质与稳健稳定性，为机翼设计提供了高效精准的解决方案。为促进可重复性与社区发展，完整代码已公开于Github。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for faster and more robust early-phase wing design, this paper introduces a modular optimization framework that integrates multiple algorithms with neural surrogate models. The method begins by generating initial wing geometries and uses a Vortex Lattice Method to compute aerodynamic data, which then trains surrogate neural networks for rapid lift and drag prediction; stability is assessed via fixed control surfaces. It unifies and compares Particle Swarm Optimization, Genetic Algorithms, gradient-based MultiStart, Bayesian optimization, and Lipschitz optimization, employing adaptive constraint management. Experimental results demonstrate improvements in aerodynamic performance and stability, with the framework enabling efficient and precise wing design optimization.</div>
<div class="mono" style="margin-top:8px">本文旨在实现更快、更稳健的早期机翼设计，提出了一种模块化优化框架，将多种算法与神经代理模型相结合。方法首先生成初始机翼几何形状，利用涡格法计算气动数据以训练代理神经网络，从而快速预测升力和阻力；通过固定控制面评估稳定性。该框架统一并比较了粒子群优化、遗传算法、基于梯度的多起点方法、贝叶斯优化和利普希茨优化，采用自适应约束管理策略。实验结果表明，该方法提升了气动性能和稳定性，为机翼设计提供了高效且精确的优化机制。</div>
</details>
</div>
<div class="card">
<div class="title">Generative Multi-Objective Bayesian Optimization with Scalable Batch Evaluations for Sample-Efficient De Novo Molecular Design</div>
<div class="meta-line">Authors: Madhav R. Muthyala, Farshud Sorourifar, Tianhong Tan, You Peng, Joel A. Paulson</div>
<div class="meta-line">First: 2025-12-19T14:59:27+00:00 · Latest: 2025-12-19T14:59:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17659v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17659v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Designing molecules that must satisfy multiple, often conflicting objectives is a central challenge in molecular discovery. The enormous size of chemical space and the cost of high-fidelity simulations have driven the development of machine learning-guided strategies for accelerating design with limited data. Among these, Bayesian optimization (BO) offers a principled framework for sample-efficient search, while generative models provide a mechanism to propose novel, diverse candidates beyond fixed libraries. However, existing methods that couple the two often rely on continuous latent spaces, which introduces both architectural entanglement and scalability challenges. This work introduces an alternative, modular &quot;generate-then-optimize&quot; framework for de novo multi-objective molecular design/discovery. At each iteration, a generative model is used to construct a large, diverse pool of candidate molecules, after which a novel acquisition function, qPMHI (multi-point Probability of Maximum Hypervolume Improvement), is used to optimally select a batch of candidates most likely to induce the largest Pareto front expansion. The key insight is that qPMHI decomposes additively, enabling exact, scalable batch selection via only simple ranking of probabilities that can be easily estimated with Monte Carlo sampling. We benchmark the framework against state-of-the-art latent-space and discrete molecular optimization methods, demonstrating significant improvements across synthetic benchmarks and application-driven tasks. Specifically, in a case study related to sustainable energy storage, we show that our approach quickly uncovers novel, diverse, and high-performing organic (quinone-based) cathode materials for aqueous redox flow battery applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成式多目标贝叶斯优化与可扩展批量评估用于高效样本的从头分子设计</div>
<div class="mono" style="margin-top:8px">设计需满足多个常相互冲突目标的分子是分子发现中的核心挑战。化学空间的巨大规模与高保真模拟的高成本推动了机器学习引导策略的发展，以在有限数据下加速设计。其中，贝叶斯优化（BO）为样本高效搜索提供了原则性框架，而生成模型则提供了超越固定库、提出新颖多样候选分子的机制。然而，现有结合两者的方法常依赖连续潜在空间，这既引入了架构纠缠，也带来了可扩展性挑战。本研究提出了一种模块化的“先生成后优化”替代框架，用于从头多目标分子设计/发现。在每次迭代中，生成模型用于构建一个庞大且多样化的候选分子池，随后采用一种新颖的采集函数——qPMHI（多点最大超体积改进概率），以最优方式选择最可能引发帕累托前沿最大扩展的一批候选分子。关键洞见在于qPMHI具有可加性分解特性，仅需通过蒙特卡洛采样轻松估计的概率进行简单排序，即可实现精确且可扩展的批量选择。我们将该框架与最先进的潜在空间和离散分子优化方法进行基准测试，在合成基准和应用驱动任务中均展现出显著改进。具体而言，在一项与可持续能源存储相关的案例研究中，我们展示了该方法能快速发现用于水系氧化还原液流电池应用的新型、多样且高性能的有机（醌基）正极材料。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of designing molecules that must satisfy multiple conflicting objectives, a task complicated by the vast chemical space and costly simulations. The authors propose a modular &quot;generate-then-optimize&quot; framework that first uses a generative model to create a diverse pool of candidate molecules, then employs a novel acquisition function called qPMHI to efficiently select batches of candidates that maximize Pareto front expansion. Experimental results demonstrate that this approach outperforms existing latent-space and discrete optimization methods on synthetic benchmarks and, in a case study on sustainable energy storage, successfully discovers novel, high-performing organic cathode materials for redox flow batteries.</div>
<div class="mono" style="margin-top:8px">本文针对分子设计中需满足多个冲突目标的挑战，该任务因化学空间巨大和模拟成本高昂而复杂化。作者提出了一种模块化的“先生成后优化”框架，首先使用生成模型创建多样化的候选分子池，然后采用一种名为qPMHI的新型采集函数来高效选择能最大化帕累托前沿扩展的候选批次。实验结果表明，该方法在合成基准测试中优于现有的潜空间和离散优化方法，并在可持续能源存储的案例研究中，成功发现了用于液流电池的新型高性能有机阴极材料。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
