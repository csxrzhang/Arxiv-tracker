<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-26 04:05</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260226_0405</div>
    <div class="row"><div class="card">
<div class="title">Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models</div>
<div class="meta-line">Authors: Siddarth Venkatraman, Vineet Jain, Sarthak Mittal, Vedant Shah, Johan Obando-Ceron, Yoshua Bengio, Brian R. Bartoldson, Bhavya Kailkhura, Guillaume Lajoie, Glen Berseth, Nikolay Malkin, Moksh Jain</div>
<div class="meta-line">First: 2025-09-30T17:58:03+00:00 · Latest: 2026-02-24T18:58:30+00:00</div>
<div class="meta-line">Comments: 23 pages, 10 figures. Project page: https://rsa-llm.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.26626v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.26626v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://rsa-llm.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Test-time scaling methods improve the capabilities of large language models (LLMs) by increasing the amount of compute used during inference to make a prediction. Inference-time compute can be scaled in parallel by choosing among multiple independent solutions or sequentially through self-refinement. We propose Recursive Self-Aggregation (RSA), a test-time scaling method inspired by evolutionary methods that combines the benefits of both parallel and sequential scaling. Each step of RSA refines a population of candidate reasoning chains through aggregation of subsets to yield a population of improved solutions, which are then used as the candidate pool for the next iteration. Empirically, RSA delivers substantial performance gains with increasing compute budgets across diverse tasks, model families and sizes. Notably, RSA with Gemini 3 Flash attains performance near the top of the ARC-AGI-2 public leaderboard. RSA also enables Qwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning models, including DeepSeek-R1 and o3-mini (high), outperforming purely parallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning Gym, LiveCodeBench-v6, and SuperGPQA. We further propose a novel aggregation-aware reinforcement learning approach that yields significant performance gains by training the model to combine solutions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>递归自聚合解锁大型语言模型的深度思考能力</div>
<div class="mono" style="margin-top:8px">测试时扩展方法通过增加推理阶段的计算量来提升大型语言模型（LLMs）的能力。推理计算可通过并行选择多个独立解或通过自精炼进行序列化扩展。我们提出递归自聚合（RSA），这是一种受进化方法启发的测试时扩展方法，兼具并行与序列扩展的优势。RSA的每一步通过聚合子集来精炼候选推理链群体，产生改进解群体，并作为下一轮迭代的候选池。实证表明，RSA在不同任务、模型系列和规模上均能随计算预算增加带来显著性能提升。值得注意的是，采用Gemini 3 Flash的RSA在ARC-AGI-2公开排行榜上达到接近顶端的性能。RSA还使Qwen3-4B-Instruct-2507在AIME-25、HMMT-25、Reasoning Gym、LiveCodeBench-v6和SuperGPQA等任务中，与包括DeepSeek-R1和o3-mini（高配版）在内的大型推理模型取得竞争性表现，且优于纯并行与序列扩展策略。我们进一步提出一种新颖的聚合感知强化学习方法，通过训练模型融合解决方案实现显著性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to enhance large language models&#x27; (LLMs) reasoning capabilities through increased inference-time compute, this paper introduces Recursive Self-Aggregation (RSA), a test-time scaling method that merges parallel and sequential scaling by iteratively refining a population of candidate reasoning chains through subset aggregation. The method demonstrates substantial performance improvements across diverse tasks, model families, and sizes with greater compute budgets, notably enabling models like Gemini 3 Flash to achieve near-top performance on the ARC-AGI-2 leaderboard and allowing smaller models such as Qwen3-4B-Instruct-2507 to compete with larger reasoning models on benchmarks including AIME-25 and SuperGPQA, while also proposing an aggregation-aware reinforcement learning approach for further gains.</div>
<div class="mono" style="margin-top:8px">本文旨在通过增加推理时计算来提升大语言模型的深度推理能力，提出了递归自聚合方法，该方法结合了并行与顺序扩展的优势，通过迭代地对候选推理链子集进行聚合来优化解决方案。实验结果表明，随着计算预算增加，该方法在多样化任务、模型系列和规模上均带来显著性能提升，特别是使Gemini 3 Flash在ARC-AGI-2公开排行榜上接近顶尖水平，并让Qwen3-4B-Instruct-2507等较小模型在AIME-25、SuperGPQA等基准测试中与更大推理模型竞争；此外，还提出了一种聚合感知的强化学习方法以进一步优化性能。</div>
</details>
</div>
<div class="card">
<div class="title">Squint: Fast Visual Reinforcement Learning for Sim-to-Real Robotics</div>
<div class="meta-line">Authors: Abdulaziz Almuzairee, Henrik I. Christensen</div>
<div class="meta-line">First: 2026-02-24T18:58:11+00:00 · Latest: 2026-02-24T18:58:11+00:00</div>
<div class="meta-line">Comments: For website and code, see https://aalmuzairee.github.io/squint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21203v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21203v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://aalmuzairee.github.io/squint">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual reinforcement learning is appealing for robotics but expensive -- off-policy methods are sample-efficient yet slow; on-policy methods parallelize well but waste samples. Recent work has shown that off-policy methods can train faster than on-policy methods in wall-clock time for state-based control. Extending this to vision remains challenging, where high-dimensional input images complicate training dynamics and introduce substantial storage and encoding overhead. To address these challenges, we introduce Squint, a visual Soft Actor Critic method that achieves faster wall-clock training than prior visual off-policy and on-policy methods. Squint achieves this via parallel simulation, a distributional critic, resolution squinting, layer normalization, a tuned update-to-data ratio, and an optimized implementation. We evaluate on the SO-101 Task Set, a new suite of eight manipulation tasks in ManiSkill3 with heavy domain randomization, and demonstrate sim-to-real transfer to a real SO-101 robot. We train policies for 15 minutes on a single RTX 3090 GPU, with most tasks converging in under 6 minutes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Squint：面向仿真到真实机器人的快速视觉强化学习</div>
<div class="mono" style="margin-top:8px">视觉强化学习在机器人领域前景广阔但成本高昂——离策略方法样本效率高但速度慢；同策略方法并行性好但样本利用率低。近期研究表明，在基于状态的控制任务中，离策略方法在挂钟时间上可训练得更快。但将其扩展至视觉领域仍具挑战性：高维输入图像使训练动态复杂化，并带来显著的存储与编码开销。为应对这些挑战，我们提出Squint——一种视觉软演员-评论家方法，其挂钟训练速度超越现有视觉离策略与同策略方法。Squint通过并行仿真、分布式评论家、分辨率压缩、层归一化、优化的更新-数据比率及高效实现达成这一目标。我们在SO-101任务集（ManiSkill3中八个采用重度领域随机化的操作任务新套件）上评估性能，并演示了向真实SO-101机器人的仿真到真实迁移。在单张RTX 3090 GPU上训练15分钟，多数任务可在6分钟内收敛。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for Squint is to overcome the inefficiencies in visual reinforcement learning for robotics, where off-policy methods are sample-efficient but slow, while on-policy methods parallelize well but waste samples. The method introduces Squint, a visual Soft Actor Critic approach that accelerates wall-clock training through parallel simulation, a distributional critic, resolution squinting, layer normalization, a tuned update-to-data ratio, and an optimized implementation. Main experimental results show that Squint trains policies in 15 minutes on a single RTX 3090 GPU, with most tasks converging in under 6 minutes on the SO-101 Task Set, and successfully demonstrates sim-to-real transfer to a real robot.</div>
<div class="mono" style="margin-top:8px">Squint的动机是解决机器人视觉强化学习中的效率问题，其中离策略方法样本效率高但速度慢，而在策略方法可并行化但浪费样本。该方法提出了Squint，一种视觉软演员-评论家方法，通过并行模拟、分布评论家、分辨率眯眼、层归一化、调整的更新数据比和优化实现，加速了实际训练时间。主要实验结果表明，Squint在单个RTX 3090 GPU上15分钟内训练策略，在SO-101任务集中大多数任务在6分钟内收敛，并成功实现了到真实机器人的仿真到现实迁移。</div>
</details>
</div>
<div class="card">
<div class="title">SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards</div>
<div class="meta-line">Authors: Dengjia Zhang, Xiaoou Liu, Lu Cheng, Yaqing Wang, Kenton Murray, Hua Wei</div>
<div class="meta-line">First: 2026-02-24T18:04:54+00:00 · Latest: 2026-02-24T18:04:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21158v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21158v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly deployed as multi-step decision-making agents, where effective reward design is essential for guiding learning. Although recent work explores various forms of reward shaping and step-level credit assignment, a key signal remains largely overlooked: the intrinsic uncertainty of LLMs. Uncertainty reflects model confidence, reveals where exploration is needed, and offers valuable learning cues even in failed trajectories. We introduce SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards, a reinforcement learning framework that incorporates uncertainty directly into the reward design. SELAUR integrates entropy-, least-confidence-, and margin-based metrics into a combined token-level uncertainty estimate, providing dense confidence-aligned supervision, and employs a failure-aware reward reshaping mechanism that injects these uncertainty signals into step- and trajectory-level rewards to improve exploration efficiency and learning stability. Experiments on two benchmarks, ALFWorld and WebShop, show that our method consistently improves success rates over strong baselines. Ablation studies further demonstrate how uncertainty signals enhance exploration and robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SELAUR：基于不确定性感知奖励的自演进大语言模型智能体</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）正日益被部署为多步决策智能体，其中有效的奖励设计对引导学习至关重要。尽管近期研究探索了多种形式的奖励塑形和步级信用分配，但一个关键信号仍被普遍忽视：LLMs的内在不确定性。不确定性反映模型置信度，揭示需要探索的环节，即使在失败轨迹中也能提供有价值的学习线索。我们提出SELAUR：基于不确定性感知奖励的自演进LLM智能体，这是一个将不确定性直接纳入奖励设计的强化学习框架。SELAUR融合基于熵、最小置信度和间隔的度量指标，形成组合式的词元级不确定性估计，提供密集的置信度对齐监督，并采用失败感知的奖励重塑机制，将这些不确定性信号注入步级和轨迹级奖励，以提升探索效率和学习稳定性。在ALFWorld和WebShop两个基准测试上的实验表明，本方法在强基线模型上持续提升成功率。消融研究进一步验证了不确定性信号如何增强探索能力和鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the observation that the intrinsic uncertainty of large language models (LLMs), which reflects model confidence and indicates where exploration is needed, is an overlooked but valuable signal for guiding multi-step decision-making agents. The method, SELAUR, introduces a reinforcement learning framework that directly incorporates uncertainty into reward design by combining token-level uncertainty estimates from entropy, least-confidence, and margin metrics, and uses a failure-aware mechanism to reshape step- and trajectory-level rewards with these signals. Experimental results on ALFWorld and WebShop benchmarks show consistent improvements in success rates over strong baselines, with ablation studies confirming that the uncertainty signals enhance exploration efficiency and learning robustness.</div>
<div class="mono" style="margin-top:8px">本文的动机是观察到大型语言模型（LLM）的内在不确定性——它反映了模型置信度并指示了需要探索的方向——是一个被忽视但对指导多步决策智能体具有价值的信号。所提出的方法SELAUR是一个强化学习框架，通过整合基于熵、最小置信度和间隔的标记级不确定性估计，将不确定性直接纳入奖励设计，并采用一个失败感知的奖励重塑机制，将这些不确定性信号注入到步骤级和轨迹级奖励中。在ALFWorld和WebShop基准测试上的实验结果表明，该方法相较于强基线模型持续提高了成功率，消融研究进一步证实了不确定性信号增强了探索效率和学习的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Cooperative-Competitive Team Play of Real-World Craft Robots</div>
<div class="meta-line">Authors: Rui Zhao, Xihui Li, Yizheng Zhang, Yuzhen Liu, Zhong Zhang, Yufeng Zhang, Cheng Zhou, Zhengyou Zhang, Lei Han</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-24T17:15:37+00:00 · Latest: 2026-02-24T17:15:37+00:00</div>
<div class="meta-line">Comments: Accepted by 2026 IEEE International Conference on Robotics and Automation (ICRA 2026), Vienna, Austria</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21119v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21119v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent deep Reinforcement Learning (RL) has made significant progress in developing intelligent game-playing agents in recent years. However, the efficient training of collective robots using multi-agent RL and the transfer of learned policies to real-world applications remain open research questions. In this work, we first develop a comprehensive robotic system, including simulation, distributed learning framework, and physical robot components. We then propose and evaluate reinforcement learning techniques designed for efficient training of cooperative and competitive policies on this platform. To address the challenges of multi-agent sim-to-real transfer, we introduce Out of Distribution State Initialization (OODSI) to mitigate the impact of the sim-to-real gap. In the experiments, OODSI improves the Sim2Real performance by 20%. We demonstrate the effectiveness of our approach through experiments with a multi-robot car competitive game and a cooperative task in real-world settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>现实世界手工机器人的合作-竞争团队博弈</div>
<div class="mono" style="margin-top:8px">近年来，多智能体深度强化学习在开发智能游戏代理方面取得了显著进展。然而，如何利用多智能体强化学习高效训练集体机器人，并将习得策略迁移至现实应用，仍是待解决的研究问题。本研究首先构建了一套完整的机器人系统，包括仿真环境、分布式学习框架与实体机器人组件。随后，我们提出并评估了专为该平台设计的强化学习技术，以实现合作与竞争策略的高效训练。为应对多智能体仿真到现实迁移的挑战，我们引入分布外状态初始化方法以缓解仿真与现实间的差距。实验表明，该方法将仿真到现实的性能提升了20%。我们通过多机器人车辆竞争游戏和现实场景中的合作任务实验，验证了所提方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work is motivated by the challenges of efficiently training collective robots with multi-agent reinforcement learning (RL) and transferring learned policies to the real world. The method involves developing a full robotic system with simulation, a distributed learning framework, and hardware, and proposing RL techniques for training cooperative-competitive policies, specifically introducing Out of Distribution State Initialization (OODSI) to address the sim-to-real gap. The main experimental results show that OODSI improves Sim2Real performance by 20%, validated through real-world multi-robot car games and cooperative tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决使用多智能体强化学习高效训练集体机器人以及将学习到的策略迁移到现实世界中的挑战。方法包括开发一个包含仿真、分布式学习框架和物理机器人的完整机器人系统，并提出了用于训练合作-竞争策略的强化学习技术，特别引入了分布外状态初始化（OODSI）来缓解仿真到现实的差距。主要实验结果表明，OODSI将仿真到现实的性能提升了20%，并通过现实世界中的多机器人汽车竞争游戏和合作任务验证了其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">AceGRPO: Adaptive Curriculum Enhanced Group Relative Policy Optimization for Autonomous Machine Learning Engineering</div>
<div class="meta-line">Authors: Yuzhu Cai, Zexi Liu, Xinyu Zhu, Cheng Wang, Siheng Chen</div>
<div class="meta-line">First: 2026-02-08T10:55:03+00:00 · Latest: 2026-02-24T17:14:22+00:00</div>
<div class="meta-line">Comments: 17 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07906v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.07906v2">PDF</a> · <a href="https://github.com/yuzhu-cai/AceGRPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous Machine Learning Engineering (MLE) requires agents to perform sustained, iterative optimization over long horizons. While recent LLM-based agents show promise, current prompt-based agents for MLE suffer from behavioral stagnation due to frozen parameters. Although Reinforcement Learning (RL) offers a remedy, applying it to MLE is hindered by prohibitive execution latency and inefficient data selection. Recognizing these challenges, we propose AceGRPO with two core components: (1) Evolving Data Buffer that continuously repurposes execution traces into reusable training tasks, and (2) Adaptive Sampling guided by a Learnability Potential function, which dynamically prioritizes tasks at the agent&#x27;s learning frontier to maximize learning efficiency. Leveraging AceGRPO, our trained Ace-30B model achieves a 100% valid submission rate on MLE-Bench-Lite, approaches the performance of proprietary frontier models, and outperforms larger open-source baselines (e.g., DeepSeek-V3.2), demonstrating robust capability for sustained iterative optimization. Code is available at https://github.com/yuzhu-cai/AceGRPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AceGRPO：面向自主机器学习工程的自适应课程增强型群体相对策略优化</div>
<div class="mono" style="margin-top:8px">自主机器学习工程（MLE）要求智能体在长周期内执行持续迭代优化。尽管当前基于大语言模型的智能体展现出潜力，但现有基于提示的MLE智能体因参数冻结而存在行为停滞问题。强化学习虽能提供解决方案，但其在MLE中的应用受限于高昂的执行延迟和低效的数据选择。针对这些挑战，我们提出AceGRPO框架，其包含两大核心组件：（1）演化数据缓冲池——持续将执行轨迹转化为可复用的训练任务；（2）基于可学习性潜力函数的自适应采样机制——动态聚焦于智能体学习前沿的任务以最大化学习效率。基于AceGRPO训练的Ace-30B模型在MLE-Bench-Lite基准上实现100%有效提交率，性能接近前沿专有模型，并超越更大规模的开源基线模型（如DeepSeek-V3.2），展现出持续迭代优化的强大能力。代码已开源：https://github.com/yuzhu-cai/AceGRPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of behavioral stagnation in prompt-based agents for Autonomous Machine Learning Engineering (MLE), where frozen parameters limit sustained iterative optimization. To overcome this, the authors propose AceGRPO, a method featuring an Evolving Data Buffer that repurposes execution traces into reusable tasks and an Adaptive Sampling mechanism guided by a Learnability Potential function to prioritize tasks at the learning frontier for efficiency. Experimental results show that the trained Ace-30B model achieves a 100% valid submission rate on MLE-Bench-Lite, matches the performance of proprietary frontier models, and surpasses larger open-source baselines like DeepSeek-V3.2, demonstrating robust capabilities for long-horizon optimization.</div>
<div class="mono" style="margin-top:8px">本文针对自主机器学习工程中基于提示的智能体因参数冻结导致行为停滞、难以持续迭代优化的问题，提出AceGRPO方法。该方法包含两个核心组件：一是演化数据缓冲区，将执行轨迹转化为可重用的训练任务；二是基于可学习性潜力函数的自适应采样机制，动态优先处理智能体学习前沿的任务以最大化学习效率。实验结果表明，训练后的Ace-30B模型在MLE-Bench-Lite上实现了100%的有效提交率，性能接近专有前沿模型，并超越了如DeepSeek-V3.2等更大的开源基线，展现出强大的持续迭代优化能力。</div>
</details>
</div>
<div class="card">
<div class="title">SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation</div>
<div class="meta-line">Authors: Kushal Kedia, Tyler Ga Wei Lum, Jeannette Bohg, C. Karen Liu</div>
<div class="meta-line">First: 2026-02-18T20:42:39+00:00 · Latest: 2026-02-24T17:10:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16863v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.16863v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ability to manipulate tools significantly expands the set of tasks a robot can perform. Yet, tool manipulation represents a challenging class of dexterity, requiring grasping thin objects, in-hand object rotations, and forceful interactions. Since collecting teleoperation data for these behaviors is challenging, sim-to-real reinforcement learning (RL) is a promising alternative. However, prior approaches typically require substantial engineering effort to model objects and tune reward functions for each task. In this work, we propose SimToolReal, taking a step towards generalizing sim-to-real RL policies for tool manipulation. Instead of focusing on a single object and task, we procedurally generate a large variety of tool-like object primitives in simulation and train a single RL policy with the universal goal of manipulating each object to random goal poses. This approach enables SimToolReal to perform general dexterous tool manipulation at test-time without any object or task-specific training. We demonstrate that SimToolReal outperforms prior retargeting and fixed-grasp methods by 37% while matching the performance of specialist RL policies trained on specific target objects and tasks. Finally, we show that SimToolReal generalizes across a diverse set of everyday tools, achieving strong zero-shot performance over 120 real-world rollouts spanning 24 tasks, 12 object instances, and 6 tool categories.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SimToolReal：一种面向零样本灵巧工具操作的物体中心策略</div>
<div class="mono" style="margin-top:8px">操作工具的能力显著扩展了机器人可执行的任务范围。然而，工具操作代表了一类具有挑战性的灵巧性任务，需要抓握细薄物体、进行手内物体旋转以及强力的交互。由于收集这些行为的遥操作数据较为困难，仿真到现实的强化学习成为一种有前景的替代方案。但以往方法通常需要大量工程努力来建模物体并为每个任务调整奖励函数。本研究提出SimToolReal，旨在推动仿真到现实强化学习策略在工具操作中的泛化。该方法不再聚焦于单一物体和任务，而是在仿真中程序化生成大量多样化的类工具物体基元，并训练一个统一的强化学习策略，其通用目标是将每个物体操控至随机目标姿态。这一方法使SimToolReal在测试时无需任何物体或任务特定训练即可执行通用的灵巧工具操作。实验表明，SimToolReal在性能上超越先前的重定向和固定抓取方法37%，同时达到针对特定目标物体和任务训练的专用强化学习策略的水平。最后，我们证明SimToolReal能泛化至多种日常工具，在涵盖24项任务、12个物体实例和6种工具类别的120次现实世界测试中实现了强大的零样本性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the difficulty of collecting real-world teleoperation data for dexterous tool manipulation, this work introduces SimToolReal, a sim-to-real reinforcement learning approach that aims for generalization. The method involves procedurally generating a diverse set of tool-like object primitives in simulation and training a single policy to manipulate any of these objects to random goal poses, avoiding task-specific engineering. Experimental results show that this object-centric policy outperforms prior retargeting and fixed-grasp methods by 37%, matches the performance of specialist policies trained on specific objects, and demonstrates strong zero-shot generalization across 24 real-world tasks involving 12 tools from 6 categories.</div>
<div class="mono" style="margin-top:8px">本研究针对灵巧工具操作中真实遥操作数据收集困难的问题，提出了SimToolReal这一旨在实现泛化的仿真到现实强化学习方法。该方法通过在仿真中程序化生成多样化的类工具物体基元，并训练一个单一策略来将任何此类物体操控至随机目标姿态，从而避免了针对特定任务的设计。实验结果表明，这种以物体为中心的策略在性能上优于先前的重定向和固定抓取方法37%，与针对特定物体训练的专用策略性能相当，并在涉及6个类别、12个具体工具的24个现实世界任务中展现了强大的零样本泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Localized Dynamics-Aware Domain Adaption for Off-Dynamics Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Zhangjie Xia, Yu Yang, Pan Xu</div>
<div class="meta-line">First: 2026-02-24T16:32:50+00:00 · Latest: 2026-02-24T16:32:50+00:00</div>
<div class="meta-line">Comments: 33 pages, 9 figures, 11 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21072v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21072v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Off-dynamics offline reinforcement learning (RL) aims to learn a policy for a target domain using limited target data and abundant source data collected under different transition dynamics. Existing methods typically address dynamics mismatch either globally over the state space or via pointwise data filtering; these approaches can miss localized cross-domain similarities or incur high computational cost. We propose Localized Dynamics-Aware Domain Adaptation (LoDADA), which exploits localized dynamics mismatch to better reuse source data. LoDADA clusters transitions from source and target datasets and estimates cluster-level dynamics discrepancy via domain discrimination. Source transitions from clusters with small discrepancy are retained, while those from clusters with large discrepancy are filtered out. This yields a fine-grained and scalable data selection strategy that avoids overly coarse global assumptions and expensive per-sample filtering. We provide theoretical insights and extensive experiments across environments with diverse global and local dynamics shifts. Results show that LoDADA consistently outperforms state-of-the-art off-dynamics offline RL methods by better leveraging localized distribution mismatch.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向离动态离线强化学习的局部动态感知域适应方法</div>
<div class="mono" style="margin-top:8px">离动态离线强化学习旨在利用有限的目标域数据和在不同转移动态下收集的丰富源域数据，为目标域学习策略。现有方法通常在整个状态空间上全局处理动态失配，或通过逐点数据过滤；这些方法可能忽略局部跨域相似性或计算成本高昂。我们提出局部动态感知域适应（LoDADA），利用局部动态失配以更好地重用源数据。LoDADA对源和目标数据集的转移进行聚类，并通过域判别估计聚类级动态差异。保留差异较小的源转移聚类，过滤差异较大的聚类。这产生了一种细粒度且可扩展的数据选择策略，避免了过于粗略的全局假设和昂贵的逐样本过滤。我们提供了理论分析，并在具有不同全局与局部动态变化的环境中进行广泛实验。结果表明，LoDADA通过更好地利用局部分布失配，持续优于最先进的离动态离线强化学习方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of off-dynamics offline reinforcement learning, where a policy must be learned for a target domain using limited target data and abundant source data collected under different transition dynamics. The motivation is that existing methods, which handle dynamics mismatch either globally or via pointwise filtering, can overlook localized cross-domain similarities or be computationally expensive. The proposed method, Localized Dynamics-Aware Domain Adaptation (LoDADA), clusters transitions from both datasets and estimates cluster-level dynamics discrepancy through domain discrimination, selectively retaining source data from clusters with small discrepancy while filtering out others. Experimental results across environments with diverse dynamics shifts demonstrate that LoDADA consistently outperforms state-of-the-art methods by more effectively leveraging localized distribution mismatch.</div>
<div class="mono" style="margin-top:8px">本文针对离动态离线强化学习中的挑战，即必须使用有限的目标域数据和大量在不同转移动态下收集的源域数据来学习目标域策略。其动机在于，现有方法通过全局处理或逐点过滤来处理动态不匹配，可能忽略局部跨域相似性或计算成本高昂。所提出的方法——局部动态感知域适应（LoDADA），对来自两个数据集的转移进行聚类，并通过域判别估计聚类级别的动态差异，选择性保留差异较小的聚类中的源数据，同时过滤掉其他数据。在具有多样化动态变化的环境中的实验结果表明，LoDADA通过更有效地利用局部分布不匹配，持续优于最先进的离动态离线强化学习方法。</div>
</details>
</div>
<div class="card">
<div class="title">Safe Reinforcement Learning for Real-World Engine Control</div>
<div class="meta-line">Authors: Julian Bedei, Lucas Koch, Kevin Badalian, Alexander Winkler, Patrick Schaber, Jakob Andert</div>
<div class="meta-line">First: 2025-01-28T01:19:05+00:00 · Latest: 2026-02-24T15:50:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.16613v2">Abs</a> · <a href="https://arxiv.org/pdf/2501.16613v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work introduces a toolchain for applying Reinforcement Learning (RL), specifically the Deep Deterministic Policy Gradient (DDPG) algorithm, in safety-critical real-world environments. As an exemplary application, transient load control is demonstrated on a single-cylinder internal combustion engine testbench in Homogeneous Charge Compression Ignition (HCCI) mode, that offers high thermal efficiency and low emissions. However, HCCI poses challenges for traditional control methods due to its nonlinear, autoregressive, and stochastic nature. RL provides a viable solution, however, safety concerns, such as excessive pressure rise rates, must be addressed when applying to HCCI. A single unsuitable control input can severely damage the engine or cause misfiring and shut down. Additionally, operating limits are not known a priori and must be determined experimentally. To mitigate these risks, real-time safety monitoring based on the k-nearest neighbor algorithm is implemented, enabling safe interaction with the testbench. The feasibility of this approach is demonstrated as the RL agent learns a control policy through interaction with the testbench. A root mean square error of 0.1374 bar is achieved for the indicated mean effective pressure, comparable to neural network-based controllers from the literature. The toolchain&#x27;s flexibility is further demonstrated by adapting the agent&#x27;s policy to increase ethanol energy shares, promoting renewable fuel use while maintaining safety. This RL approach addresses the longstanding challenge of applying RL to safety-critical real-world environments. The developed toolchain, with its adaptability and safety mechanisms, paves the way for future applicability of RL in engine testbenches and other safety-critical settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向真实世界发动机控制的安全强化学习</div>
<div class="mono" style="margin-top:8px">本研究提出了一种在安全关键的真实世界环境中应用强化学习（RL）的工具链，特别采用深度确定性策略梯度（DDPG）算法。以均质充量压燃（HCCI）模式下单缸内燃机试验台的瞬态负荷控制为例，该模式具有高热效率和低排放优势，但其非线性、自回归和随机特性对传统控制方法构成挑战。RL为此提供了可行方案，但在应用于HCCI时需解决安全性问题，如过高的压力上升率。单一不当控制输入可能导致发动机严重损坏、失火或停机，且运行极限需通过实验确定。为降低风险，研究基于k近邻算法实现了实时安全监控，确保与试验台的安全交互。RL智能体通过与试验台交互学习控制策略，证明了该方法的可行性：指示平均有效压力的均方根误差达到0.1374巴，与文献中基于神经网络的控制器性能相当。通过调整智能体策略以提高乙醇能量占比，进一步展示了工具链的灵活性，在保障安全的同时促进可再生燃料使用。该RL方法攻克了将RL应用于安全关键真实环境的长期难题，其工具链的适应性与安全机制为RL在发动机试验台及其他安全关键场景的未来应用铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work is motivated by the need to apply Reinforcement Learning (RL) in safety-critical real-world systems, exemplified by the challenge of controlling a Homogeneous Charge Compression Ignition (HCCI) engine, which is nonlinear and stochastic. The method employs the Deep Deterministic Policy Gradient (DDPG) algorithm within a toolchain that integrates a real-time safety monitor using the k-nearest neighbor algorithm to prevent damaging engine conditions like excessive pressure. The main experimental results demonstrate the RL agent successfully learning a control policy on a physical engine testbench, achieving a low root mean square error of 0.1374 bar for indicated mean effective pressure, comparable to existing neural network controllers, and safely adapting to increased ethanol fuel shares.</div>
<div class="mono" style="margin-top:8px">本工作的动机是将强化学习应用于安全关键的真实世界系统，以控制非线性、随机性的均质充量压燃发动机为例。方法上，该研究在工具链中采用深度确定性策略梯度算法，并集成基于k近邻算法的实时安全监控器，以防止发动机出现过高压力等危险状况。主要实验结果表明，强化学习智能体在物理发动机试验台上成功学习了控制策略，实现了0.1374 bar的指示平均有效压力均方根误差，与现有神经网络控制器性能相当，并能安全适应乙醇燃料比例的提高。</div>
</details>
</div>
<div class="card">
<div class="title">Do We Need Adam? Surprisingly Strong and Sparse Reinforcement Learning with SGD in LLMs</div>
<div class="meta-line">Authors: Sagnik Mukherjee, Lifan Yuan, Pavan Jayasinha, Dilek Hakkani-Tür, Hao Peng</div>
<div class="meta-line">First: 2026-02-07T23:25:26+00:00 · Latest: 2026-02-24T15:43:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07729v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.07729v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL), particularly RL from verifiable reward (RLVR), has become a crucial phase of training large language models (LLMs) and a key focus of current scaling efforts. However, optimization practices in RL largely follow those of next-token prediction stages (e.g., pretraining and supervised fine-tuning), despite fundamental differences between RL and these stages highlighted by recent work. One such practice is the use of the AdamW optimizer, which is widely adopted for training large-scale transformers despite its high memory overhead. Our analysis shows that both momentum and adaptive learning rates in AdamW are less influential in RL than in SFT, leading us to hypothesize that RL benefits less from Adam-style per-parameter adaptive learning rates and momentum. Confirming this hypothesis, our experiments demonstrate that the substantially more memory-efficient SGD, which is known to perform poorly in supervised learning of large-scale transformers, matches or even outperforms AdamW in RL for LLMs. Remarkably, full fine-tuning with SGD updates fewer than 0.02% of model parameters without any sparsity-promoting regularization, more than 1000 times fewer than AdamW. Our analysis offers potential reasons for this update sparsity. These findings provide new insights into the optimization dynamics of RL in LLMs and show that RL can be substantially more parameter-efficient than previously recognized.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>我们真的需要Adam吗？LLM中SGD实现惊人强大且稀疏的强化学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL），特别是基于可验证奖励的强化学习（RLVR），已成为训练大型语言模型（LLM）的关键阶段，也是当前规模化研究的重点。然而，尽管近期研究强调了RL与预训练、监督微调等阶段存在本质差异，RL的优化实践仍主要沿用下一词预测阶段的方案。例如广泛用于大规模Transformer训练的AdamW优化器，虽内存开销巨大，却仍被普遍采用。我们的分析表明，AdamW中的动量与自适应学习率在RL中的作用远小于在监督微调中，由此推测RL从Adam式逐参数自适应学习率与动量中获益更少。实验证实了这一假设：在LLM的RL任务中，内存效率显著更高的SGD（已知在大规模Transformer监督学习中表现欠佳）达到甚至超越了AdamW的性能。值得注意的是，使用SGD进行全参数微调时，仅更新不足0.02%的模型参数（无需任何稀疏化正则化），更新量比AdamW减少超1000倍。我们进一步分析了导致这种更新稀疏性的潜在原因。这些发现为理解LLM中RL的优化动态提供了新视角，并揭示RL可能具有远超预期的参数效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates whether the AdamW optimizer, standard in large language model (LLM) training, is necessary for the reinforcement learning (RL) phase, given its high memory cost. The authors hypothesize that RL benefits less from Adam&#x27;s adaptive learning rates and momentum compared to supervised fine-tuning, and they test this by replacing AdamW with the simpler, memory-efficient SGD optimizer. Their experiments show that SGD matches or exceeds AdamW&#x27;s performance in RL for LLMs and, remarkably, achieves this by updating fewer than 0.02% of the model parameters without explicit sparsity techniques, indicating RL can be far more parameter-efficient than previously thought.</div>
<div class="mono" style="margin-top:8px">本文探讨了在大语言模型（LLM）训练中，强化学习（RL）阶段是否仍需使用内存开销高的标准AdamW优化器。作者假设相比于监督微调，RL从Adam的自适应学习率和动量中获益较少，因此测试了用更简单、内存高效的SGD优化器替代AdamW。实验结果表明，在LLM的RL中，SGD达到甚至超越了AdamW的性能，且显著的是，它在没有显式稀疏化正则的情况下，更新的模型参数少于0.02%，这表明RL的参数效率可以远高于先前的认知。</div>
</details>
</div>
<div class="card">
<div class="title">A Survey on the Optimization of Large Language Model-based Agents</div>
<div class="meta-line">Authors: Shangheng Du, Jiabao Zhao, Jinxin Shi, Zhentao Xie, Xin Jiang, Yanhong Bai, Liang He</div>
<div class="meta-line">Venue: ACM Computing Surveys 58(9), Article 223, July 2026</div>
<div class="meta-line">First: 2025-03-16T10:09:10+00:00 · Latest: 2026-02-24T15:31:52+00:00</div>
<div class="meta-line">Comments: Published in ACM Computing Surveys, Vol. 58, No. 9, Article 223, July 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.12434v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.12434v2">PDF</a> · <a href="https://github.com/YoungDubbyDu/LLM-Agent-Optimization">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rapid development of Large Language Models (LLMs), LLM-based agents have been widely adopted in various fields, becoming essential for autonomous decision-making and interactive tasks. However, current work typically relies on prompt design or fine-tuning strategies applied to vanilla LLMs, which often leads to limited effectiveness or suboptimal performance in complex agent-related environments. Although LLM optimization techniques can improve model performance across many general tasks, they lack specialized optimization towards critical agent functionalities such as long-term planning, dynamic environmental interaction, and complex decision-making. Although numerous recent studies have explored various strategies to optimize LLM-based agents for complex agent tasks, a systematic review summarizing and comparing these methods from a holistic perspective is still lacking. In this survey, we provide a comprehensive review of LLM-based agent optimization approaches, categorizing them into parameter-driven and parameter-free methods. We first focus on parameter-driven optimization, covering fine-tuning-based optimization, reinforcement learning-based optimization, and hybrid strategies, analyzing key aspects such as trajectory data construction, fine-tuning techniques, reward function design, and optimization algorithms. Additionally, we briefly discuss parameter-free strategies that optimize agent behavior through prompt engineering and external knowledge retrieval. Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions. Our repository for related references is available at https://github.com/YoungDubbyDu/LLM-Agent-Optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型的智能体优化方法综述</div>
<div class="mono" style="margin-top:8px">随着大语言模型（LLMs）的快速发展，基于LLM的智能体已在多个领域得到广泛应用，成为自主决策与交互任务的关键技术。然而，现有研究多依赖针对基础LLM的提示设计或微调策略，在复杂的智能体应用场景中常面临效果有限或性能欠佳的问题。尽管LLM优化技术能提升模型在通用任务中的表现，但缺乏对智能体关键功能（如长期规划、动态环境交互、复杂决策）的针对性优化。近年来虽涌现出许多优化LLM智能体应对复杂任务的研究策略，但仍缺乏从整体视角系统梳理与比较这些方法的综述性工作。本文全面综述了基于LLM的智能体优化方法，将其划分为参数驱动与无参数两类。首先聚焦参数驱动优化，涵盖基于微调的优化、基于强化学习的优化及混合策略，深入分析轨迹数据构建、微调技术、奖励函数设计和优化算法等关键环节；同时简要探讨通过提示工程和外部知识检索实现智能体行为优化的无参数策略。最后，系统总结了评估与调优所用的数据集和基准测试，回顾了LLM智能体的主要应用场景，并探讨了当前面临的挑战与未来发展方向。相关参考文献库详见https://github.com/YoungDubbyDu/LLM-Agent-Optimization。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the widespread adoption yet suboptimal performance of Large Language Model (LLM)-based agents in complex environments, this survey systematically reviews optimization methods to enhance their capabilities in tasks like long-term planning and decision-making. The method categorizes approaches into parameter-driven strategies, including fine-tuning and reinforcement learning, and parameter-free techniques like prompt engineering, analyzing key components such as data construction and reward design. The main experimental results are synthesized from various studies, summarizing the datasets, benchmarks, and applications used for evaluation, while highlighting ongoing challenges and future directions in the field.</div>
<div class="mono" style="margin-top:8px">本研究针对基于大语言模型（LLM）的智能体在复杂环境中性能不足的问题，系统综述了优化方法以提升其在长期规划、动态交互等任务中的能力。方法将优化策略分为参数驱动方法（如微调和强化学习）和无参数方法（如提示工程），并分析了轨迹数据构建、奖励函数设计等关键方面。主要实验结果综合了多项研究，总结了用于评估和调优的数据集、基准测试及应用案例，同时指出了该领域当前面临的挑战和未来发展方向。</div>
</details>
</div>
<div class="card">
<div class="title">UI-Venus-1.5 Technical Report</div>
<div class="meta-line">Authors: Venus Team, Changlong Gao, Zhangxuan Gu, Yulin Liu, Xinyu Qiu, Shuheng Shen, Yue Wen, Tianyu Xia, Zhenyu Xu, Zhengwen Zeng, Beitong Zhou, Xingran Zhou, Weizhi Chen, Sunhao Dai, Jingya Dou, Yichen Gong, Yuan Guo, Zhenlin Guo, Feng Li, Qian Li, Jinzhen Lin, Yuqi Zhou, Linchao Zhu, Liang Chen, Zhenyu Guo, Changhua Meng, Weiqiang Wang</div>
<div class="meta-line">First: 2026-02-09T18:43:40+00:00 · Latest: 2026-02-24T15:17:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09082v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.09082v2">PDF</a> · <a href="https://github.com/inclusionAI/UI-Venus">Code1</a> · <a href="https://huggingface.co/collections/inclusionAI/ui-venus">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging. In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications. The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios. Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UI-Venus-1.5技术报告</div>
<div class="mono" style="margin-top:8px">GUI智能体已成为自动化数字环境交互的强大范式，但实现广泛通用性与持续强劲的任务性能仍具挑战。本报告介绍了UI-Venus-1.5——一个为鲁棒现实应用设计的统一端到端GUI智能体。该模型系列包含两个密集变体（2B与8B）及一个专家混合变体（30B-A3B），以适应多样化下游应用场景。相较于前代版本，UI-Venus-1.5引入三项关键技术突破：（1）基于30余个数据集、百亿级语料的中期训练阶段，构建基础GUI语义理解；（2）采用全轨迹推演的在线强化学习，使训练目标与大规模环境中的长程动态导航对齐；（3）通过模型融合构建统一GUI智能体，将领域专用模型（基础交互、网页与移动端）整合为协调统一的检查点。大量实验表明，UI-Venus-1.5在ScreenSpot-Pro（69.6%）、VenusBench-GD（75.0%）及AndroidWorld（77.6%）等基准测试中创下性能新高，显著超越先前强基线模型。此外，该模型在各类中文移动应用中展现出鲁棒的导航能力，能在真实场景中有效执行用户指令。代码：https://github.com/inclusionAI/UI-Venus；模型：https://huggingface.co/collections/inclusionAI/ui-venus</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of creating GUI agents that are both broadly generalizable and highly performant in real-world tasks, this paper introduces UI-Venus-1.5, a unified end-to-end agent family with dense and mixture-of-experts variants. The method employs a three-part technical advance: mid-training on extensive GUI data to build foundational semantics, online reinforcement learning with full-trajectory rollouts for long-horizon navigation, and model merging to unify domain-specific agents into a single checkpoint. Experimental results show state-of-the-art performance on benchmarks like ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), along with robust navigation capabilities in diverse Chinese mobile applications.</div>
<div class="mono" style="margin-top:8px">本文旨在解决图形用户界面（GUI）智能体在广泛通用性和强大任务性能方面难以兼顾的挑战，提出了UI-Venus-1.5，一个统一的端到端智能体系列，包含密集型和混合专家型变体。其方法基于三项关键技术：利用海量GUI数据进行中期训练以建立基础语义理解，采用全轨迹展开的在线强化学习来适应大规模环境中的长程动态导航，以及通过模型融合将领域专用模型整合为单一检查点。主要实验结果表明，该模型在ScreenSpot-Pro（69.6%）、VenusBench-GD（75.0%）和AndroidWorld（77.6%）等基准测试中取得了最先进的性能，并在多种中国移动应用场景中展现出强大的导航执行能力。</div>
</details>
</div>
<div class="card">
<div class="title">Wasserstein Barycenter Soft Actor-Critic</div>
<div class="meta-line">Authors: Zahra Shahrooei, Ali Baheri</div>
<div class="meta-line">First: 2025-06-11T20:39:50+00:00 · Latest: 2026-02-24T14:53:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.10167v4">Abs</a> · <a href="https://arxiv.org/pdf/2506.10167v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep off-policy actor-critic algorithms have emerged as the leading framework for reinforcement learning in continuous control domains. However, most of these algorithms suffer from poor sample efficiency, especially in environments with sparse rewards. In this paper, we take a step towards addressing this issue by providing a principled directed exploration strategy. We propose Wasserstein Barycenter Soft Actor-Critic (WBSAC) algorithm, which benefits from a pessimistic actor for temporal difference learning and an optimistic actor to promote exploration. This is achieved by using the Wasserstein barycenter of the pessimistic and optimistic policies as the exploration policy and adjusting the degree of exploration throughout the learning process. We compare WBSAC with state-of-the-art off-policy actor-critic algorithms and show that WBSAC is more sample-efficient on MuJoCo continuous control tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Wasserstein重心软演员-评论家算法</div>
<div class="mono" style="margin-top:8px">深度离策略演员-评论家算法已成为连续控制领域强化学习的主流框架，但多数算法样本效率低下，尤其在稀疏奖励环境中。本文通过提出一种基于原理的定向探索策略以解决该问题，引入Wasserstein重心软演员-评论家算法。该算法融合悲观演员的时间差分学习与乐观演员的探索促进机制，以悲观策略和乐观策略的Wasserstein重心作为探索策略，并在学习过程中动态调整探索强度。实验表明，在MuJoCo连续控制任务中，本算法较现有离策略演员-评论家算法具有更高的样本效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the poor sample efficiency of deep off-policy actor-critic algorithms, especially in sparse-reward environments, this paper introduces the Wasserstein Barycenter Soft Actor-Critic (WBSAC) algorithm to provide a principled directed exploration strategy. The method employs a dual-actor framework: a pessimistic actor for temporal difference learning and an optimistic actor to encourage exploration, with the exploration policy derived as the Wasserstein barycenter of these two policies, dynamically adjusting exploration intensity during learning. Experimental results on MuJoCo continuous control tasks demonstrate that WBSAC achieves superior sample efficiency compared to state-of-the-art off-policy actor-critic algorithms.</div>
<div class="mono" style="margin-top:8px">针对深度离策略演员-评论家算法样本效率低下，尤其在稀疏奖励环境中的问题，本文提出了Wasserstein重心软演员-评论家（WBSAC）算法，以提供一种有原则的定向探索策略。该方法采用双演员框架：一个悲观演员用于时序差分学习，一个乐观演员促进探索，探索策略通过计算这两个策略的Wasserstein重心得到，并在学习过程中动态调整探索程度。在MuJoCo连续控制任务上的实验结果表明，WBSAC相比先进的离策略演员-评论家算法具有更高的样本效率。</div>
</details>
</div>
<div class="card">
<div class="title">PMG: Parameterized Motion Generator for Human-like Locomotion Control</div>
<div class="meta-line">Authors: Chenxi Han, Yuheng Min, Zihao Huang, Ao Hong, Hang Liu, Yi Cheng, Houde Liu</div>
<div class="meta-line">First: 2026-02-13T06:38:04+00:00 · Latest: 2026-02-24T14:34:42+00:00</div>
<div class="meta-line">Comments: Website: https://pmg-icra26.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12656v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.12656v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://pmg-icra26.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in data-driven reinforcement learning and motion tracking have substantially improved humanoid locomotion, yet critical practical challenges remain. In particular, while low-level motion tracking and trajectory-following controllers are mature, whole-body reference-guided methods are difficult to adapt to higher-level command interfaces and diverse task contexts: they require large, high-quality datasets, are brittle across speed and pose regimes, and are sensitive to robot-specific calibration. To address these limitations, we propose the Parameterized Motion Generator (PMG), a real-time motion generator grounded in an analysis of human motion structure that synthesizes reference trajectories using only a compact set of parameterized motion data together with high-dimensional control commands. Combined with an imitation-learning pipeline and an optimization-based sim-to-real motor parameter identification module, we validate the complete approach on our humanoid prototype ZERITH Z1 and show that, within a single integrated system, PMG produces natural, human-like locomotion, responds precisely to high-dimensional control inputs-including VR-based teleoperation-and enables efficient, verifiable sim-to-real transfer. Together, these results establish a practical, experimentally validated pathway toward natural and deployable humanoid control. Website: https://pmg-icra26.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PMG：参数化运动生成器——面向类人步态控制的参数化运动生成器</div>
<div class="mono" style="margin-top:8px">数据驱动的强化学习与运动追踪技术近期显著提升了人形机器人的步态控制能力，但关键的实际挑战依然存在。具体而言，尽管底层运动追踪与轨迹跟踪控制器已较为成熟，全身参考引导方法却难以适配高层指令接口与多样化任务场景：它们需要大规模高质量数据集，在不同速度与姿态区间表现脆弱，且对机器人特定校准极为敏感。为突破这些局限，我们提出参数化运动生成器（PMG）。该实时运动生成器基于人体运动结构分析，仅需紧凑的参数化运动数据集配合高维控制指令即可合成参考轨迹。结合模仿学习流程与基于优化的仿真-现实电机参数辨识模块，我们在人形机器人原型ZERITH Z1上验证了完整方案。实验表明，PMG在单一集成系统中能生成自然类人步态，精准响应高维控制输入（包括基于VR的遥操作），并实现高效可验证的仿真-现实迁移。这些成果共同为自然化、可部署的人形机器人控制建立了经实验验证的实用路径。项目网站：https://pmg-icra26.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing whole-body reference-guided locomotion controllers, which are difficult to adapt to high-level commands and diverse tasks, this paper introduces the Parameterized Motion Generator (PMG). The method synthesizes reference trajectories using a compact set of parameterized motion data and high-dimensional control commands, grounded in an analysis of human motion structure, and integrates it with an imitation-learning pipeline and a sim-to-real motor parameter identification module. Experimental validation on the ZERITH Z1 humanoid shows that PMG produces natural, human-like locomotion, responds precisely to high-dimensional inputs like VR teleoperation, and enables efficient sim-to-real transfer, establishing a practical pathway toward deployable humanoid control.</div>
<div class="mono" style="margin-top:8px">针对现有全身参考引导的步态控制器难以适应高层指令和多样化任务的局限性，本文提出了参数化运动生成器（PMG）。该方法基于对人体运动结构的分析，仅使用紧凑的参数化运动数据和高维控制指令来合成参考轨迹，并结合模仿学习流程与基于优化的仿真到现实电机参数识别模块。在ZERITH Z1人形机器人上的实验验证表明，PMG能生成自然、类人的步态，精确响应如VR遥操作等高维输入，并实现高效的仿真到现实迁移，为可部署的人形机器人控制建立了一条实用途径。</div>
</details>
</div>
<div class="card">
<div class="title">The Art of Efficient Reasoning: Data, Reward, and Optimization</div>
<div class="meta-line">Authors: Taiqiang Wu, Zenan Zu, Bo Zhou, Ngai Wong</div>
<div class="meta-line">First: 2026-02-24T14:28:16+00:00 · Latest: 2026-02-24T14:28:16+00:00</div>
<div class="meta-line">Comments: Tech Report, Insights on Efficient Reasoning via Reward Shaping</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20945v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20945v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高效推理的艺术：数据、奖励与优化</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）始终受益于规模化的思维链（CoT）推理，但也承受着沉重的计算开销。为解决此问题，高效推理旨在激励简短而准确的思维轨迹，通常通过强化学习（RL）的奖励塑形实现。本文系统研究了LLMs高效推理的机制。为进行全面评估，我们倡导采用更细粒度的指标，包括基于正确性的长度分布以及在2k至32k广泛令牌预算范围内的性能表现。首先，我们揭示训练过程遵循两阶段范式：长度适应与推理精炼。随后，我们在统一协议下开展大量实验（约20万GPU小时），解构训练提示与推演、奖励塑形及优化策略。关键发现之一是：在相对简单的提示上进行训练，可确保正向奖励信号的密度，从而避免长度塌缩。同时，习得的长度偏差具备跨领域泛化能力。我们将所有发现提炼为有价值的洞见与实践指南，并在Qwen3系列（0.6B至30B）中进一步验证，证明了其鲁棒性与泛化性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the computational inefficiency of scaled Chain-of-Thought reasoning in Large Language Models, this paper systematically investigates efficient reasoning methods that incentivize short yet accurate thinking trajectories through reward shaping with Reinforcement Learning. The method involves a unified experimental protocol deconstructing training prompts, rollouts, reward shaping, and optimization strategies, with a key finding that training on relatively easier prompts ensures positive reward signal density and prevents length collapse. Main experimental results, based on extensive tests (about 0.2 million GPU hours) and fine-grained metrics including length distribution and performance across token budgets, reveal a two-stage training paradigm of length adaptation and reasoning refinement, where the learned length bias generalizes across domains, as validated across the Qwen3 model series from 0.6B to 30B parameters.</div>
<div class="mono" style="margin-top:8px">本研究针对大语言模型中链式思维推理计算开销大的问题，旨在通过强化学习的奖励塑造激励简短而准确的推理轨迹，以提升推理效率。方法上采用统一实验协议，解构训练提示、策略展开、奖励塑造和优化策略，关键发现是通过在相对简单的提示上训练来确保正向奖励信号密度，避免长度崩溃。主要实验结果基于大量实验（约20万GPU小时）和细粒度指标（如长度分布和不同令牌预算下的性能），揭示了训练遵循长度适应和推理精炼的两阶段范式，且习得的长度偏差可跨领域泛化，这在Qwen3系列模型（0.6B至30B参数）上得到了验证。</div>
</details>
</div>
<div class="card">
<div class="title">Statistical Inference for Temporal Difference Learning with Linear Function Approximation</div>
<div class="meta-line">Authors: Weichen Wu, Gen Li, Yuting Wei, Alessandro Rinaldo</div>
<div class="meta-line">First: 2024-10-21T15:34:44+00:00 · Latest: 2026-02-24T12:51:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.16106v5">Abs</a> · <a href="https://arxiv.org/pdf/2410.16106v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate the statistical properties of Temporal Difference (TD) learning with Polyak-Ruppert averaging, arguably one of the most widely used algorithms in reinforcement learning, for the task of estimating the parameters of the optimal linear approximation to the value function. Assuming independent samples, we make three theoretical contributions that improve upon the current state-of-the-art results: (i) we establish refined high-dimensional Berry-Esseen bounds over the class of convex sets, achieving faster rates than the best known results, and (ii) we propose and analyze a novel, computationally efficient online plug-in estimator of the asymptotic covariance matrix; (iii) we derive sharper high probability convergence guarantees that depend explicitly on the asymptotic variance and hold under weaker conditions than those adopted in the literature. These results enable the construction of confidence regions and simultaneous confidence intervals for the linear parameters of the value function approximation, with guaranteed finite-sample coverage. We demonstrate the applicability of our theoretical findings through numerical experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于线性函数逼近的时间差分学习的统计推断</div>
<div class="mono" style="margin-top:8px">本文研究了强化学习中应用最广泛的算法之一——采用Polyak-Ruppert平均的时间差分学习在估计价值函数最优线性逼近参数时的统计特性。基于独立样本假设，我们在三个理论层面改进了现有最优成果：(i) 建立了凸集类上更精细的高维Berry-Esseen界，获得了优于已知结果的收敛速率；(ii) 提出并分析了一种计算高效的新型在线渐近协方差矩阵插件估计器；(iii) 推导出更敏锐的高概率收敛保证，其显式依赖于渐近方差且所需条件弱于文献要求。这些成果使得为价值函数逼近的线性参数构建置信域与同步置信区间成为可能，并确保有限样本覆盖性。我们通过数值实验验证了理论发现的实际适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need for reliable statistical inference in Temporal Difference (TD) learning, a core reinforcement learning algorithm, when it is used with linear function approximation to estimate value function parameters. The method involves analyzing TD learning with Polyak-Ruppert averaging, establishing refined high-dimensional Berry-Esseen bounds for faster convergence rates, proposing a novel online plug-in estimator for the asymptotic covariance matrix, and deriving sharper high-probability guarantees. The main experimental results, supported by numerical experiments, demonstrate that these theoretical advances enable the construction of confidence regions and intervals for the linear parameters with guaranteed finite-sample coverage, improving upon prior state-of-the-art statistical guarantees.</div>
<div class="mono" style="margin-top:8px">本文的动机是，当使用线性函数逼近来估计价值函数参数时，需要为时序差分学习这一核心强化学习算法提供可靠的统计推断。方法上，该研究分析了采用Polyak-Ruppert平均的时序差分学习，建立了更精细的高维Berry-Esseen界以获得更快的收敛速率，提出了一种新颖的在线插件估计器用于渐近协方差矩阵，并推导了更尖锐的高概率收敛保证。主要的实验结果，通过数值实验验证，表明这些理论进展能够构建线性参数的置信区域和同时置信区间，并保证有限样本覆盖，改进了现有最先进的统计保证。</div>
</details>
</div>
<div class="card">
<div class="title">From Parameters to Behaviors: Unsupervised Compression of the Policy Space</div>
<div class="meta-line">Authors: Davide Tenedini, Riccardo Zamboni, Mirco Mutti, Marcello Restelli</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-09-26T16:42:52+00:00 · Latest: 2026-02-24T12:23:51+00:00</div>
<div class="meta-line">Comments: ICLR 2026 camera ready version. Changed typo in the title</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.22566v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.22566v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite its recent successes, Deep Reinforcement Learning (DRL) is notoriously sample-inefficient. We argue that this inefficiency stems from the standard practice of optimizing policies directly in the high-dimensional and highly redundant parameter space $Θ$. This challenge is greatly compounded in multi-task settings. In this work, we develop a novel, unsupervised approach that compresses the policy parameter space $Θ$ into a low-dimensional latent space $\mathcal{Z}$. We train a generative model $g:\mathcal{Z}\toΘ$ by optimizing a behavioral reconstruction loss, which ensures that the latent space is organized by functional similarity rather than proximity in parameterization. We conjecture that the inherent dimensionality of this manifold is a function of the environment&#x27;s complexity, rather than the size of the policy network. We validate our approach in continuous control domains, showing that the parameterization of standard policy networks can be compressed up to five orders of magnitude while retaining most of its expressivity. As a byproduct, we show that the learned manifold enables task-specific adaptation via Policy Gradient operating in the latent space $\mathcal{Z}$.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从参数到行为：策略空间的无监督压缩</div>
<div class="mono" style="margin-top:8px">尽管深度强化学习（DRL）近期取得了成功，但其样本效率低下是众所周知的。我们认为这种低效源于直接在高度冗余的高维参数空间$Θ$中优化策略的标准做法。这一挑战在多任务设置中尤为突出。本研究提出了一种新颖的无监督方法，将策略参数空间$Θ$压缩至低维潜在空间$\mathcal{Z}$。我们通过优化行为重构损失训练生成模型$g:\mathcal{Z}\toΘ$，确保潜在空间按功能相似性而非参数邻近性组织。我们推测该流形的内在维度取决于环境复杂度，而非策略网络规模。我们在连续控制领域验证了该方法，证明标准策略网络的参数化可被压缩高达五个数量级，同时保留大部分表达能力。作为副产品，我们展示了学习得到的流形支持通过在潜在空间$\mathcal{Z}$中运行的策略梯度实现任务特定适应。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the sample inefficiency of Deep Reinforcement Learning (DRL), which stems from optimizing policies directly in a high-dimensional, redundant parameter space, this paper introduces an unsupervised method to compress the policy parameter space into a low-dimensional latent space. The method trains a generative model using a behavioral reconstruction loss, organizing the latent space by functional similarity rather than parameter proximity, and conjectures that the manifold&#x27;s dimensionality depends on environmental complexity, not network size. Experimental results in continuous control domains demonstrate compression of standard policy networks by up to five orders of magnitude while retaining expressivity, with the learned manifold enabling task-specific adaptation via Policy Gradient in the latent space.</div>
<div class="mono" style="margin-top:8px">本文针对深度强化学习（DRL）因在高维冗余参数空间中直接优化策略而导致的样本效率低下问题，提出了一种无监督方法，将策略参数空间压缩到低维潜在空间。该方法通过行为重构损失训练生成模型，使潜在空间按功能相似性而非参数邻近性组织，并推测流形维度取决于环境复杂性而非网络规模。在连续控制领域的实验结果表明，标准策略网络的参数化可压缩高达五个数量级同时保持表达能力，且学习到的流形支持在潜在空间中进行策略梯度以实现任务特定适应。</div>
</details>
</div>
<div class="card">
<div class="title">Regret-Guided Search Control for Efficient Learning in AlphaZero</div>
<div class="meta-line">Authors: Yun-Jui Tsai, Wei-Yu Chen, Yan-Ru Ju, Yu-Hung Chang, Ti-Rong Wu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-24T11:49:59+00:00 · Latest: 2026-02-24T11:49:59+00:00</div>
<div class="meta-line">Comments: Accepted by the Fourteenth International Conference on Learning Representations (ICLR 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20809v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20809v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://rlg.iis.sinica.edu.tw/papers/rgsc">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) agents achieve remarkable performance but remain far less learning-efficient than humans. While RL agents require extensive self-play games to extract useful signals, humans often need only a few games, improving rapidly by repeatedly revisiting states where mistakes occurred. This idea, known as search control, aims to restart from valuable states rather than always from the initial state. In AlphaZero, prior work Go-Exploit applies this idea by sampling past states from self-play or search trees, but it treats all states equally, regardless of their learning potential. We propose Regret-Guided Search Control (RGSC), which extends AlphaZero with a regret network that learns to identify high-regret states, where the agent&#x27;s evaluation diverges most from the actual outcome. These states are collected from both self-play trajectories and MCTS nodes, stored in a prioritized regret buffer, and reused as new starting positions. Across 9x9 Go, 10x10 Othello, and 11x11 Hex, RGSC outperforms AlphaZero and Go-Exploit by an average of 77 and 89 Elo, respectively. When training on a well-trained 9x9 Go model, RGSC further improves the win rate against KataGo from 69.3% to 78.2%, while both baselines show no improvement. These results demonstrate that RGSC provides an effective mechanism for search control, improving both efficiency and robustness of AlphaZero training. Our code is available at https://rlg.iis.sinica.edu.tw/papers/rgsc.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AlphaZero中基于遗憾引导的搜索控制以实现高效学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL）智能体虽能取得卓越性能，但其学习效率远低于人类。RL智能体需大量自我对弈以提取有效信号，而人类通常仅需少量对局，通过反复回溯错误发生的状态实现快速提升。这一思路称为搜索控制，旨在从有价值的状态而非始终从初始状态重启学习。在AlphaZero中，先前工作Go-Exploit通过从自我对弈或搜索树中采样历史状态应用此思路，但其平等对待所有状态，忽略了状态的学习潜力。我们提出遗憾引导搜索控制（RGSC），通过引入遗憾网络扩展AlphaZero，该网络学习识别高遗憾状态——即智能体评估与实际结果偏差最大的状态。这些状态从自我对弈轨迹和MCTS节点中收集，存储于优先遗憾缓冲区，并作为新起始位置重复利用。在9x9围棋、10x10黑白棋和11x11六边形棋中，RGSC分别以平均77和89 Elo的优势超越AlphaZero和Go-Exploit。在已充分训练的9x9围棋模型上进一步训练时，RGSC将对KataGo的胜率从69.3%提升至78.2%，而两种基线方法均无改进。结果表明，RGSC为搜索控制提供了有效机制，提升了AlphaZero训练的效率和鲁棒性。代码发布于https://rlg.iis.sinica.edu.tw/papers/rgsc。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the inefficiency of standard reinforcement learning agents compared to humans, who learn rapidly by revisiting mistakes, this paper introduces Regret-Guided Search Control (RGSC) to enhance AlphaZero. The method employs a regret network to identify high-regret states where the agent&#x27;s evaluation diverges from actual outcomes, collecting these from self-play and MCTS nodes into a prioritized buffer for reuse as starting positions. Experimental results across 9x9 Go, 10x10 Othello, and 11x11 Hex show RGSC outperforms AlphaZero and Go-Exploit by an average of 77 and 89 Elo, respectively, and further improves a well-trained 9x9 Go model&#x27;s win rate against KataGo from 69.3% to 78.2%, demonstrating enhanced training efficiency and robustness.</div>
<div class="mono" style="margin-top:8px">针对强化学习智能体学习效率远低于人类的问题，本文提出了后悔引导搜索控制（RGSC）来改进AlphaZero。该方法通过一个后悔网络识别高后悔状态，即智能体评估与实际结果差异最大的状态，从自我对弈和蒙特卡洛树搜索节点中收集这些状态，并存储于优先缓冲池中作为新的起始位置重用。在9x9围棋、10x10黑白棋和11x11六边形棋上的实验结果表明，RGSC平均分别比AlphaZero和Go-Exploit高出77和89 Elo，并将训练有素的9x9围棋模型对KataGo的胜率从69.3%提升至78.2%，证明了其在提升训练效率和鲁棒性方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Probing Dec-POMDP Reasoning in Cooperative MARL</div>
<div class="meta-line">Authors: Kale-ab Tessera, Leonard Hinckeldey, Riccardo Zamboni, David Abel, Amos Storkey</div>
<div class="meta-line">Venue: AAMAS 2026</div>
<div class="meta-line">First: 2026-02-24T11:44:46+00:00 · Latest: 2026-02-24T11:44:46+00:00</div>
<div class="meta-line">Comments: To appear at the 25th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20804v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20804v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cooperative multi-agent reinforcement learning (MARL) is typically framed as a decentralised partially observable Markov decision process (Dec-POMDP), a setting whose hardness stems from two key challenges: partial observability and decentralised coordination. Genuinely solving such tasks requires Dec-POMDP reasoning, where agents use history to infer hidden states and coordinate based on local information. Yet it remains unclear whether popular benchmarks actually demand this reasoning or permit success via simpler strategies. We introduce a diagnostic suite combining statistically grounded performance comparisons and information-theoretic probes to audit the behavioural complexity of baseline policies (IPPO and MAPPO) across 37 scenarios spanning MPE, SMAX, Overcooked, Hanabi, and MaBrax. Our diagnostics reveal that success on these benchmarks rarely requires genuine Dec-POMDP reasoning. Reactive policies match the performance of memory-based agents in over half the scenarios, and emergent coordination frequently relies on brittle, synchronous action coupling rather than robust temporal influence. These findings suggest that some widely used benchmarks may not adequately test core Dec-POMDP assumptions under current training paradigms, potentially leading to over-optimistic assessments of progress. We release our diagnostic tooling to support more rigorous environment design and evaluation in cooperative MARL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探究合作多智能体强化学习中的Dec-POMDP推理</div>
<div class="mono" style="margin-top:8px">合作多智能体强化学习通常被建模为分散式部分可观测马尔可夫决策过程，其难点主要源于部分可观测性与分散式协调两大挑战。真正解决此类任务需要Dec-POMDP推理，即智能体利用历史信息推断隐藏状态并基于局部信息进行协调。然而，现有主流基准测试是否真正需要这种推理，还是允许通过简单策略获得成功，仍不明确。我们开发了一套诊断工具，结合基于统计的性能比较与信息论探针，对IPPO和MAPPO基线策略在涵盖MPE、SMAX、Overcooked、Hanabi和MaBrax的37个场景中的行为复杂度进行审计。诊断结果表明，这些基准测试的成功很少需要真正的Dec-POMDP推理：超过半数的场景中反应式策略与基于记忆的智能体表现相当，且涌现的协调常依赖脆弱同步动作耦合而非稳健的时序影响。这些发现暗示当前训练范式下，部分广泛使用的基准可能未能充分检验Dec-POMDP核心假设，可能导致对研究进展的过度乐观评估。我们公开诊断工具以支持合作多智能体强化学习中更严谨的环境设计与评估。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study investigates whether cooperative multi-agent reinforcement learning (MARL) benchmarks genuinely require Dec-POMDP reasoning, which involves agents using history to infer hidden states and coordinate locally, or if simpler strategies suffice. The authors develop a diagnostic suite combining performance comparisons and information-theoretic probes to analyze baseline policies like IPPO and MAPPO across 37 scenarios from environments such as MPE, SMAX, Overcooked, Hanabi, and MaBrax. Experimental results show that success on these benchmarks rarely demands true Dec-POMDP reasoning, with reactive policies matching memory-based agents in over half the scenarios, and coordination often relying on brittle synchronous action coupling rather than robust temporal influence, suggesting current benchmarks may lead to over-optimistic progress assessments.</div>
<div class="mono" style="margin-top:8px">本研究探讨了合作多智能体强化学习（MARL）基准测试是否真正需要分散式部分可观测马尔可夫决策过程（Dec-POMDP）推理，即智能体利用历史推断隐藏状态并进行局部协调，还是更简单的策略就足够。作者开发了一个结合性能比较和信息论探针的诊断套件，分析了IPPO和MAPPO等基线策略在MPE、SMAX、Overcooked、Hanabi和MaBrax等环境的37个场景中的表现。实验结果表明，这些基准测试的成功很少需要真正的Dec-POMDP推理，反应式策略在一半以上场景中与基于记忆的智能体表现相当，且协调常依赖于脆弱的同步动作耦合而非稳健的时间影响，这表明当前基准测试可能导致对进展的过度乐观评估。</div>
</details>
</div>
<div class="card">
<div class="title">GLM-5: from Vibe Coding to Agentic Engineering</div>
<div class="meta-line">Authors: GLM-5-Team, :, Aohan Zeng, Xin Lv, Zhenyu Hou, Zhengxiao Du, Qinkai Zheng, Bin Chen, Da Yin, Chendi Ge, Chenghua Huang, Chengxing Xie, Chenzheng Zhu, Congfeng Yin, Cunxiang Wang, Gengzheng Pan, Hao Zeng, Haoke Zhang, Haoran Wang, Huilong Chen, Jiajie Zhang, Jian Jiao, Jiaqi Guo, Jingsen Wang, Jingzhao Du, Jinzhu Wu, Kedong Wang, Lei Li, Lin Fan, Lucen Zhong, Mingdao Liu, Mingming Zhao, Pengfan Du, Qian Dong, Rui Lu, Shuang-Li, Shulin Cao, Song Liu, Ting Jiang, Xiaodong Chen, Xiaohan Zhang, Xuancheng Huang, Xuezhen Dong, Yabo Xu, Yao Wei, Yifan An, Yilin Niu, Yitong Zhu, Yuanhao Wen, Yukuo Cen, Yushi Bai, Zhongpei Qiao, Zihan Wang, Zikang Wang, Zilin Zhu, Ziqiang Liu, Zixuan Li, Bojie Wang, Bosi Wen, Can Huang, Changpeng Cai, Chao Yu, Chen Li, Chengwei Hu, Chenhui Zhang, Dan Zhang, Daoyan Lin, Dayong Yang, Di Wang, Ding Ai, Erle Zhu, Fangzhou Yi, Feiyu Chen, Guohong Wen, Hailong Sun, Haisha Zhao, Haiyi Hu, Hanchen Zhang, Hanrui Liu, Hanyu Zhang, Hao Peng, Hao Tai, Haobo Zhang, He Liu, Hongwei Wang, Hongxi Yan, Hongyu Ge, Huan Liu, Huanpeng Chu, Jia&#x27;ni Zhao, Jiachen Wang, Jiajing Zhao, Jiamin Ren, Jiapeng Wang, Jiaxin Zhang, Jiayi Gui, Jiayue Zhao, Jijie Li, Jing An, Jing Li, Jingwei Yuan, Jinhua Du, Jinxin Liu, Junkai Zhi, Junwen Duan, Kaiyue Zhou, Kangjian Wei, Ke Wang, Keyun Luo, Laiqiang Zhang, Leigang Sha, Liang Xu, Lindong Wu, Lintao Ding, Lu Chen, Minghao Li, Nianyi Lin, Pan Ta, Qiang Zou, Rongjun Song, Ruiqi Yang, Shangqing Tu, Shangtong Yang, Shaoxiang Wu, Shengyan Zhang, Shijie Li, Shuang Li, Shuyi Fan, Wei Qin, Wei Tian, Weining Zhang, Wenbo Yu, Wenjie Liang, Xiang Kuang, Xiangmeng Cheng, Xiangyang Li, Xiaoquan Yan, Xiaowei Hu, Xiaoying Ling, Xing Fan, Xingye Xia, Xinyuan Zhang, Xinze Zhang, Xirui Pan, Xu Zou, Xunkai Zhang, Yadi Liu, Yandong Wu, Yanfu Li, Yidong Wang, Yifan Zhu, Yijun Tan, Yilin Zhou, Yiming Pan, Ying Zhang, Yinpei Su, Yipeng Geng, Yong Yan, Yonglin Tan, Yuean Bi, Yuhan Shen, Yuhao Yang, Yujiang Li, Yunan Liu, Yunqing Wang, Yuntao Li, Yurong Wu, Yutao Zhang, Yuxi Duan, Yuxuan Zhang, Zezhen Liu, Zhengtao Jiang, Zhenhe Yan, Zheyu Zhang, Zhixiang Wei, Zhuo Chen, Zhuoer Feng, Zijun Yao, Ziwei Chai, Ziyuan Wang, Zuzhou Zhang, Bin Xu, Minlie Huang, Hongning Wang, Juanzi Li, Yuxiao Dong, Jie Tang</div>
<div class="meta-line">First: 2026-02-17T17:50:56+00:00 · Latest: 2026-02-24T10:44:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15763v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.15763v2">PDF</a> · <a href="https://github.com/zai-org/GLM-5">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GLM-5：从氛围编码到智能体工程</div>
<div class="mono" style="margin-top:8px">我们推出新一代基础模型GLM-5，旨在实现从氛围编码到智能体工程的范式转变。该模型在其前代智能体、推理与编码能力基础上，采用DSA架构显著降低训练与推理成本，同时保持长上下文保真度。为提升模型对齐与自主性，我们构建了新型异步强化学习基础设施，通过解耦生成与训练大幅提升后训练效率。此外，我们提出创新的异步智能体强化学习算法，进一步提升强化学习质量，使模型能更有效地从复杂长程交互中学习。通过这些创新，GLM-5在主流开放基准测试中达到最先进性能。尤为关键的是，GLM-5在实际编程任务中展现出前所未有的能力，在处理端到端软件工程挑战方面超越以往基线。代码、模型及更多信息详见https://github.com/zai-org/GLM-5。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces GLM-5, a next-generation foundation model motivated by advancing from vibe coding to agentic engineering, aiming to enhance model autonomy and real-world coding capabilities. The method employs DSA to reduce training and inference costs while preserving long-context fidelity, alongside a novel asynchronous reinforcement learning infrastructure and algorithms that decouple generation from training to improve post-training efficiency and learning from complex interactions. Experimentally, GLM-5 achieves state-of-the-art results on major open benchmarks and demonstrates unprecedented performance in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges.</div>
<div class="mono" style="margin-top:8px">本文介绍了GLM-5，这是一个新一代基础模型，其动机是从氛围编码转向智能体工程，旨在提升模型自主性和现实世界编码能力。方法上采用DSA以降低训练和推理成本同时保持长上下文保真度，并引入新的异步强化学习基础设施和算法，通过解耦生成与训练来提高后训练效率及从复杂长期交互中学习的效果。实验结果表明，GLM-5在主要开放基准测试中取得了最先进的性能，并在现实世界编码任务中展现出前所未有的能力，在处理端到端软件工程挑战方面超越了先前基线。</div>
</details>
</div>
<div class="card">
<div class="title">One-Step Flow Q-Learning: Addressing the Diffusion Policy Bottleneck in Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Thanh Nguyen, Chang D. Yoo</div>
<div class="meta-line">First: 2025-08-19T15:05:55+00:00 · Latest: 2026-02-24T10:29:51+00:00</div>
<div class="meta-line">Comments: 10 pages, ICLR2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.13904v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.13904v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Q-Learning (DQL) has established diffusion policies as a high-performing paradigm for offline reinforcement learning, but its reliance on multi-step denoising for action generation renders both training and inference slow and fragile. Existing efforts to accelerate DQL toward one-step denoising typically rely on auxiliary modules or policy distillation, sacrificing either simplicity or performance. It remains unclear whether a one-step policy can be trained directly without such trade-offs. To this end, we introduce One-Step Flow Q-Learning (OFQL), a novel framework that enables effective one-step action generation during both training and inference, without auxiliary modules or distillation. OFQL reformulates the DQL policy within the Flow Matching (FM) paradigm but departs from conventional FM by learning an average velocity field that directly supports accurate one-step action generation. This design removes the need for multi-step denoising and backpropagation-through-time updates, resulting in substantially faster and more robust learning. Extensive experiments on the D4RL benchmark show that OFQL, despite generating actions in a single step, not only significantly reduces computation during both training and inference but also outperforms multi-step DQL by a large margin. Furthermore, OFQL surpasses all other baselines, achieving state-of-the-art performance in D4RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一步流Q学习：解决离线强化学习中的扩散策略瓶颈</div>
<div class="mono" style="margin-top:8px">扩散Q学习（DQL）已确立扩散策略作为离线强化学习的高性能范式，但其依赖多步去噪生成动作，导致训练和推断过程缓慢且脆弱。现有加速DQL实现一步去噪的方法通常依赖辅助模块或策略蒸馏，牺牲了简洁性或性能。目前尚不清楚能否直接训练一步策略而无需此类权衡。为此，我们提出一步流Q学习（OFQL），这是一种新颖框架，能在训练和推断期间实现高效的一步动作生成，无需辅助模块或蒸馏。OFQL在流匹配（FM）范式内重构DQL策略，但通过直接学习支持精确一步动作生成的平均速度场，突破了传统FM的限制。该设计消除了多步去噪和时序反向传播更新的需求，实现了显著更快、更鲁棒的学习。在D4RL基准上的大量实验表明，尽管OFQL仅用单步生成动作，不仅大幅降低了训练和推断的计算开销，还显著超越了多步DQL的性能。此外，OFQL超越了所有其他基线方法，在D4RL中达到了最先进的性能水平。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the computational inefficiency and fragility of Diffusion Q-Learning (DQL) in offline reinforcement learning, which relies on slow multi-step denoising for action generation. The authors propose One-Step Flow Q-Learning (OFQL), a novel framework that reformulates the policy within the Flow Matching paradigm to learn an average velocity field, enabling direct and accurate one-step action generation during both training and inference without auxiliary modules or distillation. Experimental results on the D4RL benchmark demonstrate that OFQL not only drastically reduces computational cost but also outperforms multi-step DQL and other baselines, achieving state-of-the-art performance.</div>
<div class="mono" style="margin-top:8px">本文针对离线强化学习中扩散Q学习依赖多步去噪导致动作生成效率低且脆弱的问题，提出了一步流Q学习新框架。该方法在流匹配范式下重构策略，学习一个平均速度场，从而在训练和推理中无需辅助模块或蒸馏即可直接实现准确的一步动作生成。在D4RL基准上的大量实验表明，该框架不仅显著降低了计算成本，而且性能大幅超越多步扩散Q学习及其他基线，达到了最先进水平。</div>
</details>
</div>
<div class="card">
<div class="title">PyVision-RL: Forging Open Agentic Vision Models via RL</div>
<div class="meta-line">Authors: Shitian Zhao, Shaoheng Lin, Ming Li, Haoquan Zhang, Wenshuo Peng, Kaipeng Zhang, Chen Wei</div>
<div class="meta-line">First: 2026-02-24T10:08:33+00:00 · Latest: 2026-02-24T10:08:33+00:00</div>
<div class="meta-line">Comments: preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20739v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20739v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PyVision-RL：通过强化学习锻造开放智能体视觉模型</div>
<div class="mono" style="margin-top:8px">面向智能体多模态模型的强化学习常面临交互崩溃问题，即模型倾向于减少工具使用和多轮推理，限制了智能体行为的优势。我们提出PyVision-RL——一个用于开放权重多模态模型的强化学习框架，能稳定训练并维持交互持续性。该方法结合过采样-过滤-排序的轨迹生成策略与累积工具奖励机制，有效防止崩溃并促进多轮工具调用。通过统一训练流程，我们开发了面向图像理解的PyVision-Image和面向视频理解的PyVision-Video。在视频推理任务中，PyVision-Video采用按需上下文构建技术，在推理过程中选择性采样任务相关帧，显著降低视觉令牌消耗。实验表明，该方法在保持优异性能的同时提升了效率，证明持续交互与按需视觉处理对可扩展多模态智能体至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind PyVision-RL is to address interaction collapse in reinforcement learning for agentic multimodal models, where models reduce tool usage and multi-turn reasoning, thereby limiting agentic benefits. The method introduces a framework combining an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to stabilize training and sustain interaction, applied via a unified pipeline to develop PyVision-Image and PyVision-Video models; for video, it uses on-demand context construction to selectively sample relevant frames, reducing visual token usage. Experimental results demonstrate strong performance and improved efficiency, highlighting that sustained interaction and on-demand visual processing are key for scalable multimodal agents.</div>
<div class="mono" style="margin-top:8px">PyVision-RL的动机是解决智能多模态模型在强化学习中出现的交互崩溃问题，即模型减少工具使用和多轮推理，从而限制了智能行为的优势。该方法引入了一个框架，结合过采样-过滤-排序的滚动策略与累积工具奖励，以稳定训练并维持交互，通过统一训练流程开发了PyVision-Image和PyVision-Video模型；针对视频理解，采用按需上下文构建，在推理过程中选择性采样任务相关帧，显著减少视觉令牌使用。实验结果显示其性能强劲且效率提升，表明持续交互和按需视觉处理对于可扩展多模态智能体至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">Fuz-RL: A Fuzzy-Guided Robust Framework for Safe Reinforcement Learning under Uncertainty</div>
<div class="meta-line">Authors: Xu Wan, Chao Yang, Cheng Yang, Jie Song, Mingyang Sun</div>
<div class="meta-line">First: 2026-02-24T09:50:17+00:00 · Latest: 2026-02-24T09:50:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20729v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20729v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safe Reinforcement Learning (RL) is crucial for achieving high performance while ensuring safety in real-world applications. However, the complex interplay of multiple uncertainty sources in real environments poses significant challenges for interpretable risk assessment and robust decision-making. To address these challenges, we propose Fuz-RL, a fuzzy measure-guided robust framework for safe RL. Specifically, our framework develops a novel fuzzy Bellman operator for estimating robust value functions using Choquet integrals. Theoretically, we prove that solving the Fuz-RL problem (in Constrained Markov Decision Process (CMDP) form) is equivalent to solving distributionally robust safe RL problems (in robust CMDP form), effectively avoiding min-max optimization. Empirical analyses on safe-control-gym and safety-gymnasium scenarios demonstrate that Fuz-RL effectively integrates with existing safe RL baselines in a model-free manner, significantly improving both safety and control performance under various types of uncertainties in observation, action, and dynamics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Fuz-RL：一种模糊引导的鲁棒框架，用于不确定性下的安全强化学习</div>
<div class="mono" style="margin-top:8px">安全强化学习（RL）对于在现实应用中确保安全的同时实现高性能至关重要。然而，真实环境中多种不确定性源的复杂交互，为可解释的风险评估和鲁棒决策带来了重大挑战。为应对这些挑战，我们提出了Fuz-RL，一种基于模糊测度引导的鲁棒框架，用于安全RL。具体而言，该框架通过Choquet积分构建了一种新颖的模糊贝尔曼算子，用于估计鲁棒值函数。理论上，我们证明求解Fuz-RL问题（以约束马尔可夫决策过程（CMDP）形式）等价于求解分布鲁棒的安全RL问题（以鲁棒CMDP形式），从而有效避免了极小极大优化。在safe-control-gym和safety-gymnasium场景上的实证分析表明，Fuz-RL能以无模型方式有效整合现有安全RL基线方法，在观测、动作和动力学等多种不确定性条件下，显著提升安全性和控制性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Fuz-RL, a fuzzy-guided robust framework designed to enhance safe reinforcement learning (RL) by addressing the challenges of interpretable risk assessment and robust decision-making under multiple sources of uncertainty in real-world environments. The method develops a novel fuzzy Bellman operator that estimates robust value functions using Choquet integrals, theoretically linking the framework to distributionally robust safe RL problems and avoiding complex min-max optimization. Experimental results on safe-control-gym and safety-gymnasium scenarios show that Fuz-RL integrates model-free with existing safe RL baselines, significantly improving both safety and control performance under uncertainties in observation, action, and dynamics.</div>
<div class="mono" style="margin-top:8px">本文提出了Fuz-RL，一个模糊引导的鲁棒框架，旨在解决现实环境中多源不确定性带来的可解释风险评估和鲁棒决策挑战，以增强安全强化学习。该方法通过使用Choquet积分开发了一种新颖的模糊贝尔曼算子来估计鲁棒值函数，从理论上将框架与分布鲁棒安全强化学习问题等价，避免了复杂的极小极大优化。在safe-control-gym和safety-gymnasium场景上的实验结果表明，Fuz-RL以无模型方式与现有安全强化学习基线有效集成，在观测、动作和动力学等多种不确定性下显著提升了安全性和控制性能。</div>
</details>
</div>
<div class="card">
<div class="title">Balancing Multiple Objectives in Urban Traffic Control with Reinforcement Learning from AI Feedback</div>
<div class="meta-line">Authors: Chenyang Zhao, Vinny Cahill, Ivana Dusparic</div>
<div class="meta-line">First: 2026-02-24T09:47:25+00:00 · Latest: 2026-02-24T09:47:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20728v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20728v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward design has been one of the central challenges for real world reinforcement learning (RL) deployment, especially in settings with multiple objectives. Preference-based RL offers an appealing alternative by learning from human preferences over pairs of behavioural outcomes. More recently, RL from AI feedback (RLAIF) has demonstrated that large language models (LLMs) can generate preference labels at scale, mitigating the reliance on human annotators. However, existing RLAIF work typically focuses only on single-objective tasks, leaving the open question of how RLAIF handles systems that involve multiple objectives. In such systems trade-offs among conflicting objectives are difficult to specify, and policies risk collapsing into optimizing for a dominant goal. In this paper, we explore the extension of the RLAIF paradigm to multi-objective self-adaptive systems. We show that multi-objective RLAIF can produce policies that yield balanced trade-offs reflecting different user priorities without laborious reward engineering. We argue that integrating RLAIF into multi-objective RL offers a scalable path toward user-aligned policy learning in domains with inherently conflicting objectives.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于人工智能反馈强化学习的多目标城市交通控制平衡策略</div>
<div class="mono" style="margin-top:8px">奖励设计一直是现实世界强化学习部署的核心挑战之一，尤其是在多目标场景中。基于偏好的强化学习通过从人类对行为结果对的偏好中学习，提供了一种有吸引力的替代方案。最近，基于人工智能反馈的强化学习证明大型语言模型能够大规模生成偏好标签，从而减少对人类标注者的依赖。然而，现有RLAIF研究通常仅关注单目标任务，对于RLAIF如何处理涉及多目标的系统仍存在开放性问题。在此类系统中，相互冲突目标间的权衡难以明确界定，策略可能陷入仅优化主导目标的困境。本文探索将RLAIF范式扩展至多目标自适应系统，证明多目标RLAIF能够生成反映不同用户优先级的平衡权衡策略，无需繁琐的奖励工程。我们认为，将RLAIF整合到多目标强化学习中，为具有内在冲突目标的领域提供了一条可扩展的用户对齐策略学习路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of designing balanced policies for multi-objective systems, such as urban traffic control, where specifying trade-offs among conflicting goals is difficult and traditional reinforcement learning risks over-optimizing a single objective. The method extends Reinforcement Learning from AI Feedback (RLAIF) to multi-objective settings, using large language models to generate preference labels at scale instead of relying on human annotators, thereby avoiding laborious reward engineering. Experimental results demonstrate that multi-objective RLAIF can produce policies that achieve balanced trade-offs aligned with different user priorities, offering a scalable approach for user-aligned policy learning in domains with inherently conflicting objectives.</div>
<div class="mono" style="margin-top:8px">本文针对多目标系统（如城市交通控制）中平衡策略设计的挑战展开研究，在这些系统中，冲突目标间的权衡难以明确指定，且传统强化学习易陷入对单一目标的过度优化。方法上，该工作将人工智能反馈强化学习（RLAIF）扩展至多目标场景，利用大语言模型规模化生成偏好标签以替代人工标注，从而避免了繁琐的奖励工程。实验结果表明，多目标RLAIF能够生成反映不同用户优先级的平衡权衡策略，为具有内在冲突目标的领域提供了一条可扩展的用户对齐策略学习路径。</div>
</details>
</div>
<div class="card">
<div class="title">Buffer Matters: Unleashing the Power of Off-Policy Reinforcement Learning in Large Language Model Reasoning</div>
<div class="meta-line">Authors: Xu Wan, Yansheng Wang, Wenqi Huang, Mingyang Sun</div>
<div class="meta-line">First: 2026-02-24T09:35:43+00:00 · Latest: 2026-02-24T09:35:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20722v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20722v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional on-policy Reinforcement Learning with Verifiable Rewards (RLVR) frameworks suffer from experience waste and reward homogeneity, which directly hinders learning efficiency on difficult samples during large language models post-training. In this paper, we introduce Batch Adaptation Policy Optimization (BAPO), an off-policy RLVR framework to improve the data efficiency in large language models post-training. It dynamically selects training batches by re-evaluating historically difficult samples and reusing high-quality ones, while holding a lower bound guarantee for policy improvement. Extensive experiments further demonstrate that BAPO achieves an average 12.5% improvement over GRPO across mathematics, planning, and visual reasoning tasks. Crucially, BAPO successfully resolves 40.7% of problems that base models consistently fail to solve.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>缓冲区的重要性：释放离线策略强化学习在大语言模型推理中的潜力</div>
<div class="mono" style="margin-top:8px">传统的基于可验证奖励的在线策略强化学习框架存在经验浪费和奖励同质化问题，这直接阻碍了大语言模型后训练阶段在困难样本上的学习效率。本文提出批量自适应策略优化，一种离线策略的RLVR框架，旨在提升大语言模型后训练的数据效率。该框架通过重新评估历史困难样本并复用高质量样本，动态选择训练批次，同时为策略改进提供下界保证。大量实验进一步表明，在数学、规划和视觉推理任务上，BAPO相比GRPO平均提升12.5%。关键的是，BAPO成功解决了基础模型持续无法解决的40.7%的问题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the inefficiencies of on-policy reinforcement learning with verifiable rewards (RLVR) in large language model post-training, which suffers from experience waste and reward homogeneity, this paper introduces Batch Adaptation Policy Optimization (BAPO), an off-policy RLVR framework designed to enhance data efficiency. The method dynamically selects training batches by re-evaluating historically difficult samples and reusing high-quality ones, all while maintaining a theoretical lower bound guarantee for policy improvement. Experimental results show that BAPO achieves an average 12.5% performance improvement over GRPO across mathematics, planning, and visual reasoning tasks, and crucially resolves 40.7% of problems that base models consistently fail to solve.</div>
<div class="mono" style="margin-top:8px">针对传统基于策略的、带有可验证奖励的强化学习在大语言模型后训练中存在经验浪费和奖励同质化、阻碍困难样本学习效率的问题，本文提出了批量自适应策略优化（BAPO），这是一种离策略的强化学习框架，旨在提高后训练的数据效率。该方法通过重新评估历史困难样本并重用高质量样本，动态选择训练批次，同时保证了策略改进的理论下界。主要实验结果表明，BAPO在数学、规划和视觉推理任务上平均性能比GRPO提升了12.5%，并且关键地解决了基础模型持续无法解决的40.7%的问题。</div>
</details>
</div>
<div class="card">
<div class="title">Polychromic Objectives for Reinforcement Learning</div>
<div class="meta-line">Authors: Jubayer Ibn Hamid, Ifdita Hasan Orney, Ellen Xu, Chelsea Finn, Dorsa Sadigh</div>
<div class="meta-line">First: 2025-09-29T19:32:11+00:00 · Latest: 2026-02-24T09:06:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.25424v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.25424v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning fine-tuning (RLFT) is a dominant paradigm for improving pretrained policies for downstream tasks. These pretrained policies, trained on large datasets, produce generations with a broad range of promising but unrefined behaviors. Often, a critical failure mode of RLFT arises when policies lose this diversity and collapse into a handful of easily exploitable outputs. This convergence hinders exploration, which is essential for expanding the capabilities of the pretrained policy and for amplifying the benefits of test-time compute scaling. To address this, we introduce an objective for policy gradient methods that explicitly enforces the exploration and refinement of diverse generations, which we call a polychromic objective. We then show how proximal policy optimization (PPO) can be adapted to optimize this objective. Our method (1) employs vine sampling to collect on-policy rollouts and (2) modifies the advantage function to reflect the advantage under our new objective. Experiments on BabyAI, Minigrid, and Algorithmic Creativity show that our method improves success rates by reliably solving a larger set of environment configurations and generalizes better under large perturbations. Moreover, when given multiple attempts in pass@$k$ experiments, the policy achieves substantially higher coverage, demonstrating its ability to maintain and exploit a diverse repertoire of strategies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习的多色目标</div>
<div class="mono" style="margin-top:8px">强化学习微调（RLFT）是改进下游任务预训练策略的主导范式。这些基于大规模数据集训练的预训练策略能生成广泛但未经优化的行为。RLFT的一个关键失败模式常出现在策略失去多样性、坍缩为少数易被利用的输出时，这种收敛阻碍了探索——这对扩展预训练策略能力及放大测试时计算扩展的效益至关重要。为此，我们提出一种策略梯度方法目标，明确强制对多样化生成进行探索与优化，称为多色目标。我们随后展示如何调整近端策略优化（PPO）以优化此目标。我们的方法（1）采用藤蔓采样收集同策略轨迹，（2）修改优势函数以反映新目标下的优势。在BabyAI、Minigrid和算法创造力上的实验表明，该方法通过可靠解决更多环境配置提升成功率，并在强扰动下表现更优泛化能力。此外，在pass@$k$实验中给予多次尝试时，策略实现了显著更高的覆盖率，证明其保持并利用多样化策略库的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the problem of policy collapse during reinforcement learning fine-tuning (RLFT), where pretrained policies lose behavioral diversity and become exploitable, hindering exploration and the benefits of test-time compute scaling. To address this, the authors introduce a polychromic objective that explicitly enforces the exploration and refinement of diverse generations, adapting proximal policy optimization (PPO) with vine sampling for on-policy rollouts and a modified advantage function. Experimental results on BabyAI, Minigrid, and Algorithmic Creativity demonstrate that the method improves success rates by solving more environment configurations, generalizes better under perturbations, and achieves higher coverage in pass@k experiments, showing its ability to maintain and exploit diverse strategies.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决强化学习微调（RLFT）中的策略崩溃问题，即预训练策略失去行为多样性并变得易于利用，从而阻碍探索和测试时计算扩展的收益。为此，作者提出了一种多色目标，明确强制探索和优化多样化的生成，通过藤蔓采样收集在线轨迹并修改优势函数来适配近端策略优化（PPO）。在BabyAI、Minigrid和Algorithmic Creativity上的实验结果表明，该方法通过解决更多环境配置提高了成功率，在扰动下具有更好的泛化能力，并在pass@k实验中实现了更高的覆盖率，证明了其保持和利用多样化策略的能力。</div>
</details>
</div>
<div class="card">
<div class="title">MASPO: Unifying Gradient Utilization, Probability Mass, and Signal Reliability for Robust and Sample-Efficient LLM Reasoning</div>
<div class="meta-line">Authors: Xiaoliang Fu, Jiaye Lin, Yangyi Fang, Binbin Zheng, Chaowen Hu, Zekai Shao, Cong Qin, Lu Pan, Ke Zeng, Xunliang Cai</div>
<div class="meta-line">First: 2026-02-19T17:05:20+00:00 · Latest: 2026-02-24T08:43:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17550v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.17550v2">PDF</a> · <a href="https://github.com/VenomRose-Juri/MASPO-RL}{https://github.com/VenomRose-Juri/MASPO-RL">Code1</a> · <a href="https://github.com/VenomRose-Juri/MASPO-RL">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing Reinforcement Learning with Verifiable Rewards (RLVR) algorithms, such as GRPO, rely on rigid, uniform, and symmetric trust region mechanisms that are fundamentally misaligned with the complex optimization dynamics of Large Language Models (LLMs). In this paper, we identify three critical challenges in these methods: (1) inefficient gradient utilization caused by the binary cutoff of hard clipping, (2) insensitive probability mass arising from uniform ratio constraints that ignore the token distribution, and (3) asymmetric signal reliability stemming from the disparate credit assignment ambiguity between positive and negative samples. To bridge these gaps, we propose Mass-Adaptive Soft Policy Optimization (MASPO), a unified framework designed to harmonize these three dimensions. MASPO integrates a differentiable soft Gaussian gating to maximize gradient utility, a mass-adaptive limiter to balance exploration across the probability spectrum, and an asymmetric risk controller to align update magnitudes with signal confidence. Extensive evaluations demonstrate that MASPO serves as a robust, all-in-one RLVR solution, significantly outperforming baselines. Our code is at: \href{https://github.com/VenomRose-Juri/MASPO-RL}{https://github.com/VenomRose-Juri/MASPO-RL}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MASPO：统一梯度利用、概率质量与信号可靠性以实现鲁棒且样本高效的LLM推理</div>
<div class="mono" style="margin-top:8px">现有的可验证奖励强化学习（RLVR）算法（如GRPO）依赖于僵化、均匀且对称的信任区域机制，这与大型语言模型（LLM）的复杂优化动态存在根本性错配。本文指出这些方法存在三个关键挑战：（1）硬截断的二元截止导致梯度利用效率低下；（2）均匀比率约束忽略词元分布，引发概率质量不敏感问题；（3）正负样本间信用分配模糊性差异导致信号可靠性不对称。为弥合这些差距，我们提出质量自适应软策略优化（MASPO），这是一个统一框架，旨在协调上述三个维度。MASPO集成了可微分软高斯门控以最大化梯度效用，质量自适应限幅器以平衡概率谱上的探索，以及非对称风险控制器使更新幅度与信号置信度对齐。大量实验表明，MASPO作为一种鲁棒的全能RLVR解决方案，显著优于基线方法。代码发布于：\href{https://github.com/VenomRose-Juri/MASPO-RL}{https://github.com/VenomRose-Juri/MASPO-RL}。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the misalignment between rigid trust region mechanisms in existing RLVR methods and the complex optimization dynamics of LLMs, this paper identifies three key challenges: inefficient gradient utilization, insensitive probability mass, and asymmetric signal reliability. To address these, the authors propose MASPO, a unified framework that integrates a differentiable soft Gaussian gating for gradient utility, a mass-adaptive limiter for balanced exploration, and an asymmetric risk controller for confidence-aligned updates. Experimental results show that MASPO significantly outperforms baseline methods, establishing it as a robust and sample-efficient RLVR solution for LLM reasoning.</div>
<div class="mono" style="margin-top:8px">本文的动机在于现有RLVR方法中僵化的信任区域机制与大型语言模型复杂优化动态之间的不匹配，识别了三个关键挑战：梯度利用效率低、概率质量不敏感以及信号可靠性不对称。为解决这些问题，作者提出了MASPO统一框架，它集成了可微软高斯门控以最大化梯度效用、质量自适应限制器以平衡概率谱上的探索，以及非对称风险控制器以实现与信号置信度一致的更新。大量实验评估表明，MASPO显著优于基线方法，成为一个鲁棒且样本高效的LLM推理RLVR解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">CAMEL: Confidence-Gated Reflection for Reward Modeling</div>
<div class="meta-line">Authors: Zirui Zhu, Hailun Xu, Yang Luo, Yong Liu, Kanchan Sarkar, Kun Xu, Yang You</div>
<div class="meta-line">First: 2026-02-24T08:20:08+00:00 · Latest: 2026-02-24T08:20:08+00:00</div>
<div class="meta-line">Comments: Preprint. 13 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20670v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20670v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward models play a fundamental role in aligning large language models with human preferences. Existing methods predominantly follow two paradigms: scalar discriminative preference models, which are efficient but lack interpretability, and generative judging models, which offer richer reasoning at the cost of higher computational overhead. We observe that the log-probability margin between verdict tokens strongly correlates with prediction correctness, providing a reliable proxy for instance difficulty without additional inference cost. Building on this insight, we propose CAMEL, a confidence-gated reflection framework that performs a lightweight single-token preference decision first and selectively invokes reflection only for low-confidence instances. To induce effective self-correction, we train the model via reinforcement learning with counterfactual prefix augmentation, which exposes the model to diverse initial verdicts and encourages genuine revision. Empirically, CAMEL achieves state-of-the-art performance on three widely used reward-model benchmarks with 82.9% average accuracy, surpassing the best prior model by 3.2% and outperforming 70B-parameter models using only 14B parameters, while establishing a strictly better accuracy-efficiency Pareto frontier.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CAMEL：基于置信度门控反思的奖励建模方法</div>
<div class="mono" style="margin-top:8px">奖励模型在使大语言模型与人类偏好对齐方面起着基础性作用。现有方法主要遵循两种范式：标量判别式偏好模型（效率高但缺乏可解释性）和生成式评判模型（提供更丰富的推理但计算开销较大）。我们观察到，判定词元之间的对数概率差值与预测正确性高度相关，可在无需额外推理成本的情况下为实例难度提供可靠代理。基于此发现，我们提出CAMEL——一种置信度门控反思框架，先执行轻量级单词元偏好决策，仅对低置信度实例选择性触发反思。为引导有效的自我修正，我们通过强化学习结合反事实前缀增强训练模型，使模型接触多样化的初始判定并促进实质性修正。实验表明，CAMEL在三个广泛使用的奖励模型基准测试中以82.9%的平均准确率取得最优性能，较先前最佳模型提升3.2%，仅用140亿参数即超越700亿参数模型，同时建立了严格更优的准确率-效率帕累托边界。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the trade-off between efficiency and interpretability in existing reward modeling paradigms for aligning large language models with human preferences. It introduces CAMEL, a confidence-gated reflection framework that first makes a lightweight single-token preference decision and then selectively invokes reflection only for low-confidence instances, using the log-probability margin between verdict tokens as a proxy for instance difficulty without extra cost. To enable effective self-correction, the method employs reinforcement learning with counterfactual prefix augmentation to expose the model to diverse initial verdicts. Experimentally, CAMEL achieves state-of-the-art performance with 82.9% average accuracy on three benchmarks, surpassing prior models by 3.2% and outperforming larger 70B-parameter models using only 14B parameters, while establishing a superior accuracy-efficiency Pareto frontier.</div>
<div class="mono" style="margin-top:8px">本文的动机源于现有奖励建模方法在效率与可解释性之间的权衡，这些方法用于将大语言模型与人类偏好对齐。它提出了CAMEL，一个置信度门控反思框架，先进行轻量级的单令牌偏好决策，然后仅对低置信度实例选择性触发反思，利用裁决令牌间的对数概率边际作为实例难度的代理，无需额外成本。为实现有效自我纠正，该方法采用带反事实前缀增强的强化学习，使模型接触多样化的初始裁决。实验结果表明，CAMEL在三个广泛使用的奖励建模基准上取得了最先进的性能，平均准确率达82.9%，比先前最佳模型提升3.2%，仅用140亿参数就超越了700亿参数模型，同时建立了更优的准确率-效率帕累托前沿。</div>
</details>
</div>
<div class="card">
<div class="title">TrajGPT-R: Generating Urban Mobility Trajectory with Reinforcement Learning-Enhanced Generative Pre-trained Transformer</div>
<div class="meta-line">Authors: Jiawei Wang, Chuang Yang, Jiawei Yong, Xiaohang Xu, Hongjun Wang, Noboru Koshizuka, Shintaro Fukushima, Ryosuke Shibasaki, Renhe Jiang</div>
<div class="meta-line">First: 2026-02-24T07:44:19+00:00 · Latest: 2026-02-24T07:44:19+00:00</div>
<div class="meta-line">Comments: TrajGPT-R is a Reinforcement Learning-Enhanced Generative Pre-trained Transformer for Mobility Trajectory Generation</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20643v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20643v1">PDF</a> · <a href="https://github.com/Wangjw6/TrajGPT_R">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mobility trajectories are essential for understanding urban dynamics and enhancing urban planning, yet access to such data is frequently hindered by privacy concerns. This research introduces a transformative framework for generating large-scale urban mobility trajectories, employing a novel application of a transformer-based model pre-trained and fine-tuned through a two-phase process. Initially, trajectory generation is conceptualized as an offline reinforcement learning (RL) problem, with a significant reduction in vocabulary space achieved during tokenization. The integration of Inverse Reinforcement Learning (IRL) allows for the capture of trajectory-wise reward signals, leveraging historical data to infer individual mobility preferences. Subsequently, the pre-trained model is fine-tuned using the constructed reward model, effectively addressing the challenges inherent in traditional RL-based autoregressive methods, such as long-term credit assignment and handling of sparse reward environments. Comprehensive evaluations on multiple datasets illustrate that our framework markedly surpasses existing models in terms of reliability and diversity. Our findings not only advance the field of urban mobility modeling but also provide a robust methodology for simulating urban data, with significant implications for traffic management and urban development planning. The implementation is publicly available at https://github.com/Wangjw6/TrajGPT_R.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TrajGPT-R：基于强化学习增强的生成式预训练Transformer的城市移动轨迹生成</div>
<div class="mono" style="margin-top:8px">移动轨迹对于理解城市动态和优化城市规划至关重要，但此类数据的获取常受隐私问题限制。本研究提出了一种生成大规模城市移动轨迹的创新框架，采用基于Transformer的模型，通过两阶段预训练与微调实现突破性应用。首先，将轨迹生成建模为离线强化学习问题，在词元化过程中显著压缩词汇空间。通过逆强化学习的整合，该框架能捕获轨迹级奖励信号，利用历史数据推断个体移动偏好。随后，使用构建的奖励模型对预训练模型进行微调，有效解决了传统基于强化学习的自回归方法中存在的长期信用分配和稀疏奖励环境处理等固有挑战。在多个数据集上的综合评估表明，本框架在可靠性和多样性方面显著超越现有模型。研究成果不仅推动了城市移动建模领域的发展，还为城市数据模拟提供了稳健的方法论，对交通管理和城市规划具有重要启示。代码已在https://github.com/Wangjw6/TrajGPT_R公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for urban mobility data that is often restricted due to privacy issues, this paper proposes TrajGPT-R, a framework that generates large-scale urban mobility trajectories using a transformer-based model enhanced with reinforcement learning. The method formulates trajectory generation as an offline reinforcement learning problem, employing tokenization to reduce vocabulary size and integrating inverse reinforcement learning to infer individual mobility preferences from historical data as reward signals; the pre-trained model is then fine-tuned with these rewards to overcome challenges like long-term credit assignment and sparse rewards. Experimental results on multiple datasets demonstrate that the framework significantly outperforms existing models in reliability and diversity, advancing urban mobility modeling and offering a robust tool for traffic management and urban planning.</div>
<div class="mono" style="margin-top:8px">本研究针对城市移动轨迹数据因隐私问题难以获取的挑战，提出了TrajGPT-R框架，利用强化学习增强的生成式预训练Transformer来生成大规模城市移动轨迹。方法将轨迹生成构建为离线强化学习问题，通过词元化减少词汇空间，并结合逆强化学习从历史数据中推断个体移动偏好作为奖励信号；随后使用构建的奖励模型对预训练模型进行微调，以解决传统基于强化学习的自回归方法中长期信用分配和稀疏奖励等难题。在多个数据集上的综合评估表明，该框架在可靠性和多样性方面显著优于现有模型，不仅推动了城市移动建模领域的发展，也为交通管理和城市规划提供了强大的数据模拟方法。</div>
</details>
</div>
<div class="card">
<div class="title">Enjoying Non-linearity in Multinomial Logistic Bandits: A Minimax-Optimal Algorithm</div>
<div class="meta-line">Authors: Pierre Boudart, Pierre Gaillard, Alessandro Rudi</div>
<div class="meta-line">First: 2025-07-07T08:18:25+00:00 · Latest: 2026-02-24T07:43:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.05306v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.05306v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider the multinomial logistic bandit problem in which a learner interacts with an environment by selecting actions to maximize expected rewards based on probabilistic feedback from multiple possible outcomes. In the binary setting, recent work has focused on understanding the impact of the non-linearity of the logistic model (Faury et al., 2020; Abeille et al., 2021). They introduced a problem-dependent constant $κ_* \geq 1$ that may be exponentially large in some problem parameters and which is captured by the derivative of the sigmoid function. It encapsulates the non-linearity and improves existing regret guarantees over $T$ rounds from $\smash{O(d\sqrt{T})}$ to $\smash{O(d\sqrt{T/κ_*})}$, where $d$ is the dimension of the parameter space. We extend their analysis to the multinomial logistic bandit framework with a finite action space, making it suitable for complex applications with more than two choices, such as reinforcement learning or recommender systems. To achieve this, we extend the definition of $ κ_* $ to the multinomial setting and propose an efficient algorithm that leverages the problem&#x27;s non-linearity. Our method yields a problem-dependent regret bound of order $ \smash{\widetilde{\mathcal{O}}( R d \sqrt{ {KT}/{κ_*}} ) } $, where $R$ denotes the norm of the vector of rewards and $K$ is the number of outcomes. This improves upon the best existing guarantees of order $ \smash{\widetilde{\mathcal{O}}( RdK \sqrt{T} )}$. Moreover, we provide a matching $\smash{ Ω(dR\sqrt{KT/κ_*})}$ lower-bound, showing that our algorithm is minimax-optimal and that our definition of $κ_*$ is optimal.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>享受多项逻辑老虎机中的非线性：一种极小极大最优算法</div>
<div class="mono" style="margin-top:8px">我们研究多项逻辑老虎机问题，其中学习者通过选择动作与环境交互，基于多个可能结果的概率反馈来最大化期望奖励。在二元设置中，近期研究聚焦于理解逻辑模型非线性的影响（Faury等人，2020；Abeille等人，2021）。他们引入了一个问题相关常数$κ_* \geq 1$，该常数在某些问题参数下可能呈指数级增长，并由sigmoid函数的导数捕捉。它封装了非线性特性，并将$T$轮内的现有遗憾保证从$\smash{O(d\sqrt{T})}$改进为$\smash{O(d\sqrt{T/κ_*})}$，其中$d$为参数空间维度。我们将此分析扩展到具有有限动作空间的多项逻辑老虎机框架，使其适用于超过两个选择的复杂应用（如强化学习或推荐系统）。为此，我们将$κ_*$的定义扩展至多项设置，并提出一种利用问题非线性的高效算法。我们的方法实现了阶为$\smash{\widetilde{\mathcal{O}}( R d \sqrt{ {KT}/{κ_*}} )}$的问题依赖遗憾界，其中$R$表示奖励向量的范数，$K$为结果数量。这优于现有最优阶$\smash{\widetilde{\mathcal{O}}( RdK \sqrt{T} )}$的保证。此外，我们提供了匹配的$\smash{ Ω(dR\sqrt{KT/κ_*})}$下界，证明我们的算法是极小极大最优的，且$κ_*$的定义是最优的。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the multinomial logistic bandit problem, motivated by the need to extend recent advances in binary logistic bandits to more complex scenarios with multiple outcomes, such as in reinforcement learning and recommender systems. The authors propose a new algorithm that generalizes the problem-dependent constant κ_* to the multinomial setting, leveraging the non-linearity of the logistic model to design an efficient method. Experimental results demonstrate a regret bound of order Õ(Rd√(KT/κ_*)), which improves upon prior Õ(RdK√T) bounds and is shown to be minimax-optimal via a matching lower bound, confirming the optimality of their κ_* definition.</div>
<div class="mono" style="margin-top:8px">本文研究了多项逻辑老虎机问题，其动机是将二元逻辑老虎机的最新进展扩展到具有多个结果的更复杂场景，如强化学习和推荐系统。作者提出了一种新算法，将问题依赖常数κ_*推广到多项设置，利用逻辑模型的非线性设计高效方法。实验结果表明，该算法实现了Õ(Rd√(KT/κ_*))的遗憾界，优于现有的Õ(RdK√T)界，并通过匹配的下界证明其极小极大最优性，从而验证了κ_*定义的最优性。</div>
</details>
</div>
<div class="card">
<div class="title">Performance Asymmetry in Model-Based Reinforcement Learning</div>
<div class="meta-line">Authors: Jing Yu Lim, Rushi Shah, Zarif Ikram, Samson Yu, Haozhe Ma, Tze-Yun Leong, Dianbo Liu</div>
<div class="meta-line">First: 2025-05-26T08:52:45+00:00 · Latest: 2026-02-24T06:45:28+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.19698v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.19698v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, Model-Based Reinforcement Learning (MBRL) have achieved super-human level performance on the Atari100k benchmark on average. However, we discover that conventional aggregates mask a major problem, Performance Asymmetry: MBRL agents dramatically outperform humans in certain tasks (Agent-Optimal tasks) while drastically underperform humans in other tasks (Human-Optimal tasks). Indeed, despite achieving SOTA in the overall mean Human-Normalized Scores (HNS), the SOTA agent scored the worst among baselines on Human-Optimal tasks, with a striking 21X performance gap between the Human-Optimal and Agent-Optimal subsets. To address this, we partition Atari100k evenly into Human-Optimal and Agent-Optimal subsets, and introduce a more balanced aggregate, Sym-HNS. Furthermore, we trace the striking Performance Asymmetry in the SOTA pixel diffusion world model to the curse of dimensionality and its prowess on high visual detail tasks (e.g. Breakout). To this end, we propose a novel latent end-to-end Joint Embedding DIffusion (JEDI) world model that achieves SOTA results in Sym-HNS, Human-Optimal tasks, and Breakout -- thus reversing the worsening Performance Asymmetry trend while improving computational efficiency and remaining competitive on the full Atari100k.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于模型的强化学习中的性能不对称性</div>
<div class="mono" style="margin-top:8px">近期，基于模型的强化学习（MBRL）在Atari100k基准测试中平均表现已超越人类水平。然而，我们发现传统聚合指标掩盖了一个重大问题——性能不对称性：MBRL智能体在某些任务（智能体优势任务）中显著超越人类，而在其他任务（人类优势任务）中却大幅落后于人类。尽管当前最优（SOTA）智能体在整体人类标准化分数（HNS）均值上达到最高水平，但在人类优势任务中的表现却是基线方法中最差的，其人类优势与智能体优势任务子集间存在高达21倍的性能差距。为解决此问题，我们将Atari100k均匀划分为人类优势与智能体优势子集，并引入更平衡的聚合指标——对称HNS（Sym-HNS）。进一步地，我们将SOTA像素扩散世界模型中显著的性能不对称性归因于维度灾难及其在高视觉细节任务（如《打砖块》）中的优势表现。为此，我们提出了一种新颖的潜在端到端联合嵌入扩散（JEDI）世界模型，该模型在Sym-HNS、人类优势任务及《打砖块》中均取得SOTA结果——不仅逆转了日益加剧的性能不对称趋势，同时提升了计算效率，并在完整Atari100k基准上保持竞争力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates a performance asymmetry issue in model-based reinforcement learning (MBRL), where state-of-the-art agents achieve super-human average scores on the Atari100k benchmark but mask a severe imbalance: they dramatically outperform humans on some tasks (Agent-Optimal) while drastically underperforming on others (Human-Optimal). To address this, the authors partition the benchmark into these two subsets and propose a more balanced evaluation metric called Sym-HNS. They trace the root cause in a leading pixel diffusion world model to the curse of dimensionality and its strength in high-detail tasks like Breakout. Consequently, they introduce a novel latent world model named Joint Embedding DIffusion (JEDI), which achieves state-of-the-art results on Sym-HNS and Human-Optimal tasks, reverses the asymmetry trend, improves computational efficiency, and remains competitive on the full benchmark.</div>
<div class="mono" style="margin-top:8px">本文研究了基于模型的强化学习（MBRL）中的性能不对称问题：当前最先进的智能体在Atari100k基准测试中平均表现超越人类，但掩盖了严重的不平衡——它们在部分任务（智能体优势任务）上远超人类，却在其他任务（人类优势任务）上大幅落后。为解决此问题，作者将基准测试均匀划分为这两类子集，并提出了一个更平衡的评估指标Sym-HNS。他们将顶尖像素扩散世界模型中的这种不对称根源归因于维度诅咒及其在如Breakout等高视觉细节任务上的优势。为此，他们提出了一种新颖的潜在端到端联合嵌入扩散（JEDI）世界模型，该模型在Sym-HNS和人类优势任务上取得了最先进的结果，扭转了性能不对称恶化的趋势，提高了计算效率，并在完整基准测试中保持竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">OptiLeak: Efficient Prompt Reconstruction via Reinforcement Learning in Multi-tenant LLM Services</div>
<div class="meta-line">Authors: Longxiang Wang, Xiang Zheng, Xuhao Zhang, Yao Zhang, Ye Wu, Cong Wang</div>
<div class="meta-line">First: 2026-02-24T06:35:22+00:00 · Latest: 2026-02-24T06:35:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20595v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20595v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-tenant LLM serving frameworks widely adopt shared Key-Value caches to enhance efficiency. However, this creates side-channel vulnerabilities enabling prompt leakage attacks. Prior studies identified these attack surfaces yet focused on expanding attack vectors rather than optimizing attack performance, reporting impractically high attack costs that underestimate the true privacy risk. We propose OptiLeak, a reinforcement learning-enhanced framework that maximizes prompt reconstruction efficiency through two-stage fine-tuning. Our key insight is that domain-specific ``hard tokens&#x27;&#x27; -- terms difficult to predict yet carrying sensitive information -- can be automatically identified via likelihood ranking and used to construct preference pairs for Direct Preference Optimization, eliminating manual annotation. This enables effective preference alignment while avoiding the overfitting issues of extended supervised fine-tuning. Evaluated on three benchmarks spanning medical and financial domains, OptiLeak achieves up to $12.48\times$ reduction in average requests per token compared to baseline approaches, with consistent improvements across model scales from 3B to 14B parameters. Our findings demonstrate that cache-based prompt leakage poses a more severe threat than previously reported, underscoring the need for robust cache isolation in production deployments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OptiLeak：基于强化学习的多租户大语言模型服务高效提示词重构方法</div>
<div class="mono" style="margin-top:8px">多租户大语言模型服务框架广泛采用共享键值缓存以提升效率，但这会引发侧信道漏洞，导致提示词泄露攻击。现有研究虽识别了此类攻击面，却侧重于扩展攻击向量而非优化攻击性能，所报告的不切实际的高攻击成本低估了真实隐私风险。本文提出OptiLeak——一种通过两阶段微调实现提示词重构效率最大化的强化学习增强框架。核心洞见在于：领域特定的&#x27;困难词元&#x27;（难以预测但携带敏感信息的术语）可通过似然排序自动识别，并用于构建直接偏好优化的偏好对，从而免除人工标注。该方法在实现有效偏好对齐的同时，避免了扩展监督微调导致的过拟合问题。在涵盖医疗与金融领域的三个基准测试中，OptiLeak相比基线方法实现了最高12.48倍的每词元平均请求数降低，且在3B至14B参数规模的模型上均保持稳定改进。研究结果表明，基于缓存的提示词泄露威胁比既往报道更为严重，凸显了生产部署中强化缓存隔离的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the security risks introduced by shared Key-Value caches in multi-tenant LLM services, which enable prompt leakage attacks; prior research underestimated the practical threat by focusing on attack vectors rather than optimizing attack efficiency. The proposed method, OptiLeak, is a reinforcement learning-enhanced framework that maximizes prompt reconstruction efficiency through a two-stage fine-tuning process, which automatically identifies domain-specific &#x27;hard tokens&#x27; via likelihood ranking and uses them to construct preference pairs for Direct Preference Optimization, thereby avoiding manual annotation and overfitting. The main experimental results, evaluated on three benchmarks in medical and financial domains, show that OptiLeak achieves up to a 12.48x reduction in average requests per token compared to baselines, with consistent improvements across model scales from 3B to 14B parameters, demonstrating that cache-based prompt leakage is a more severe threat than previously reported.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于多租户大语言模型服务中共享键值缓存带来的安全风险，该风险可导致提示词泄露攻击；先前研究侧重于扩展攻击面而非优化攻击性能，从而低估了实际威胁。所提出的方法OptiLeak是一个基于强化学习的框架，通过两阶段微调来最大化提示词重建效率，其核心是自动通过似然排序识别领域特定的&#x27;困难词元&#x27;，并利用它们构建偏好对进行直接偏好优化，从而避免了人工标注和过拟合问题。在医疗和金融领域的三个基准测试上的主要实验结果表明，与基线方法相比，OptiLeak实现了平均每词元请求数最高达12.48倍的降低，且在3B到14B参数规模的模型上均有一致的性能提升，这证明基于缓存的提示词泄露威胁比以往报道的更为严重。</div>
</details>
</div>
<div class="card">
<div class="title">VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training</div>
<div class="meta-line">Authors: Guobin Shen, Chenxiao Zhao, Xiang Cheng, Lei Huang, Xing Yu</div>
<div class="meta-line">First: 2026-02-11T09:48:08+00:00 · Latest: 2026-02-24T06:30:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10693v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.10693v2">PDF</a> · <a href="https://github.com/FloyedShen/VESPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VESPO：面向稳定离策略大语言模型训练的变分序列级软策略优化</div>
<div class="mono" style="margin-top:8px">训练稳定性始终是大语言模型强化学习的核心挑战。策略陈旧、异步训练以及训练与推理引擎间的失配，均会导致行为策略偏离当前策略，引发训练崩溃风险。重要性采样为这种分布偏移提供了理论修正，但存在高方差缺陷；现有解决方案（如词元级截断和序列级归一化）缺乏统一的理论基础。本文提出变分序列级软策略优化方法。通过将方差缩减融入提议分布的变分框架，VESPO推导出可直接作用于序列级重要性权重（无需长度归一化）的闭式重塑核。数学推理基准实验表明：VESPO能在高达64倍陈旧率及完全异步执行环境下保持训练稳定，并在稠密模型与专家混合模型中均取得持续性能提升。代码已开源：https://github.com/FloyedShen/VESPO</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of training instability in reinforcement learning for large language models, which arises from policy staleness, asynchronous training, and mismatches between training and inference engines. The authors propose VESPO, a method that integrates variance reduction into a variational framework over proposal distributions to derive a closed-form reshaping kernel for sequence-level importance weights, avoiding length normalization. Experimental results on mathematical reasoning benchmarks demonstrate that VESPO ensures stable training under high staleness ratios and asynchronous conditions, while consistently improving performance across both dense and Mixture-of-Experts models.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型强化学习中因策略陈旧、异步训练以及训练与推理引擎不匹配导致的训练不稳定问题展开研究。作者提出了VESPO方法，该方法将方差减少整合到关于提议分布的变分框架中，推导出一个直接作用于序列级重要性权重的闭式重塑核，无需长度归一化。在数学推理基准测试上的实验结果表明，VESPO能在高陈旧比和完全异步执行条件下保持训练稳定，并在稠密模型与专家混合模型上均取得一致的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">From Logs to Language: Learning Optimal Verbalization for LLM-Based Recommendation in Production</div>
<div class="meta-line">Authors: Yucheng Shi, Ying Li, Yu Wang, Yesu Feng, Arjun Rao, Rein Houthooft, Shradha Sehgal, Jin Wang, Hao Zhen, Ninghao Liu, Linas Baltrunas</div>
<div class="meta-line">First: 2026-02-24T05:15:24+00:00 · Latest: 2026-02-24T05:15:24+00:00</div>
<div class="meta-line">Comments: Work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20558v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20558v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are promising backbones for generative recommender systems, yet a key challenge remains underexplored: verbalization, i.e., converting structured user interaction logs into effective natural language inputs. Existing methods rely on rigid templates that simply concatenate fields, yielding suboptimal representations for recommendation. We propose a data-centric framework that learns verbalization for LLM-based recommendation. Using reinforcement learning, a verbalization agent transforms raw interaction histories into optimized textual contexts, with recommendation accuracy as the training signal. This agent learns to filter noise, incorporate relevant metadata, and reorganize information to improve downstream predictions. Experiments on a large-scale industrial streaming dataset show that learned verbalization delivers up to 93% relative improvement in discovery item recommendation accuracy over template-based baselines. Further analysis reveals emergent strategies such as user interest summarization, noise removal, and syntax normalization, offering insights into effective context construction for LLM-based recommender systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从日志到语言：学习基于LLM的生产推荐系统最优文本化方法</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）作为生成式推荐系统的核心架构前景广阔，但一个关键挑战尚未充分探索：文本化，即将结构化用户交互日志转化为有效的自然语言输入。现有方法依赖僵化的模板简单拼接字段，导致推荐表示次优。我们提出一种以数据为中心的框架，学习基于LLM推荐的文本化方法。通过强化学习，文本化代理将原始交互历史转化为优化的文本上下文，以推荐准确性作为训练信号。该代理学习过滤噪声、整合相关元数据并重组信息以提升下游预测性能。在大型工业流式数据集上的实验表明，学习型文本化方法在发现项推荐准确率上较基于模板的基线获得最高93%的相对提升。进一步分析揭示了用户兴趣摘要、噪声消除和句法规范化等涌现策略，为基于LLM的推荐系统有效上下文构建提供了新见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that while large language models (LLMs) hold promise for generative recommender systems, the process of verbalization—converting structured user interaction logs into natural language—remains a key underexplored challenge, with existing template-based methods being suboptimal. The proposed method is a data-centric framework where a reinforcement learning agent learns to transform raw interaction histories into optimized textual contexts, using recommendation accuracy as the training signal to filter noise and reorganize information. The main experimental results, from tests on a large-scale industrial streaming dataset, show that this learned verbalization delivers up to a 93% relative improvement in discovery item recommendation accuracy over template-based baselines, with analysis revealing emergent strategies like interest summarization and noise removal.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，尽管大语言模型（LLM）在生成式推荐系统中前景广阔，但如何将结构化的用户交互日志转化为有效的自然语言输入（即言语化）这一关键挑战尚未被充分探索，现有基于模板的方法效果欠佳。所提出的方法是一个以数据为中心的框架，其中通过强化学习训练一个言语化智能体，以推荐准确性为训练信号，将原始交互历史转化为优化的文本上下文，从而学习过滤噪声并重组信息。在大型工业流式数据集上的主要实验结果表明，这种学习得到的言语化方法相比基于模板的基线，在发现项推荐准确性上实现了高达93%的相对提升，分析还揭示了如用户兴趣总结、噪声去除等新兴策略。</div>
</details>
</div>
<div class="card">
<div class="title">PCPO: Proportionate Credit Policy Optimization for Aligning Image Generation Models</div>
<div class="meta-line">Authors: Jeongjae Lee, Jong Chul Ye</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-09-30T04:43:58+00:00 · Latest: 2026-02-24T05:01:33+00:00</div>
<div class="meta-line">Comments: 35 pages, 20 figures. ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.25774v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.25774v3">PDF</a> · <a href="https://github.com/jaylee2000/pcpo/">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While reinforcement learning has advanced the alignment of text-to-image (T2I) models, state-of-the-art policy gradient methods are still hampered by training instability and high variance, hindering convergence speed and compromising image quality. Our analysis identifies a key cause of this instability: disproportionate credit assignment, in which the mathematical structure of the generative sampler produces volatile and non-proportional feedback across timesteps. To address this, we introduce Proportionate Credit Policy Optimization (PCPO), a framework that enforces proportional credit assignment through a stable objective reformulation and a principled reweighting of timesteps. This correction stabilizes the training process, leading to significantly accelerated convergence and superior image quality. The improvement in quality is a direct result of mitigating model collapse, a common failure mode in recursive training. PCPO substantially outperforms existing policy gradient baselines on all fronts, including the state-of-the-art DanceGRPO. Code is available at https://github.com/jaylee2000/pcpo/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PCPO：面向图像生成模型对齐的比例信用策略优化</div>
<div class="mono" style="margin-top:8px">尽管强化学习推动了文本到图像（T2I）模型的对齐，但最先进的策略梯度方法仍受限于训练不稳定性和高方差，阻碍收敛速度并影响图像质量。我们的分析发现其关键原因在于不成比例的信用分配：生成采样器的数学结构导致跨时间步的反馈波动剧烈且不成比例。为此，我们提出比例信用策略优化（PCPO），该框架通过稳定的目标重构和时间步的加权调整，实现比例化信用分配。这一修正稳定了训练过程，显著加速收敛并提升图像质量。质量改善直接源于缓解了递归训练中常见的模型坍塌问题。PCPO在包括最先进的DanceGRPO在内的所有基线方法上均表现优异。代码发布于https://github.com/jaylee2000/pcpo/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses training instability and high variance in reinforcement learning for aligning text-to-image models, attributing these issues to disproportionate credit assignment caused by the generative sampler&#x27;s structure. It introduces Proportionate Credit Policy Optimization (PCPO), a framework that reformulates the objective and reweights timesteps to enforce proportional credit, thereby stabilizing training. Experimental results show that PCPO accelerates convergence, improves image quality by mitigating model collapse, and outperforms state-of-the-art baselines like DanceGRPO across all metrics.</div>
<div class="mono" style="margin-top:8px">该论文针对文本到图像模型对齐中强化学习的训练不稳定性和高方差问题，将其归因于生成采样器结构导致的不成比例信用分配。它引入了比例信用策略优化（PCPO）框架，通过重构目标和重新加权时间步来强制实施比例信用分配，从而稳定训练过程。实验结果表明，PCPO加速了收敛，通过缓解模型崩溃提高了图像质量，并在所有指标上超越了如DanceGRPO等最先进的基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Actor-Curator: Co-adaptive Curriculum Learning via Policy-Improvement Bandits for RL Post-Training</div>
<div class="meta-line">Authors: Zhengyao Gu, Jonathan Light, Raul Astudillo, Ziyu Ye, Langzhou He, Henry Peng Zou, Wei Cheng, Santiago Paternain, Philip S. Yu, Yisong Yue</div>
<div class="meta-line">First: 2026-02-24T04:19:48+00:00 · Latest: 2026-02-24T04:19:48+00:00</div>
<div class="meta-line">Comments: 37 pages, 8 figures, 1 table. Preprint under review. Equal contribution by first two authors</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20532v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20532v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Post-training large foundation models with reinforcement learning typically relies on massive and heterogeneous datasets, making effective curriculum learning both critical and challenging. In this work, we propose ACTOR-CURATOR, a scalable and fully automated curriculum learning framework for reinforcement learning post-training of large language models (LLMs). ACTOR-CURATOR learns a neural curator that dynamically selects training problems from large problem banks by directly optimizing for expected policy performance improvement. We formulate problem selection as a non-stationary stochastic bandit problem, derive a principled loss function based on online stochastic mirror descent, and establish regret guarantees under partial feedback. Empirically, ACTOR-CURATOR consistently outperforms uniform sampling and strong curriculum baselines across a wide range of challenging reasoning benchmarks, demonstrating improved training stability and efficiency. Notably, it achieves relative gains of 28.6% on AIME2024 and 30.5% on ARC-1D over the strongest baseline and up to 80% speedup. These results suggest that ACTOR-CURATOR is a powerful and practical approach for scalable LLM post-training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Actor-Curator：基于策略改进多臂赌博机的协同自适应课程学习用于强化学习后训练</div>
<div class="mono" style="margin-top:8px">基于强化学习对大型基础模型进行后训练通常依赖海量异构数据集，这使得有效的课程学习既关键又具挑战性。本研究提出ACTOR-CURATOR——一个可扩展、全自动的课程学习框架，专为大型语言模型（LLM）的强化学习后训练设计。该框架通过神经网络策展器动态从大型问题库中选择训练问题，直接以策略性能提升期望值为优化目标。我们将问题选择建模为非平稳随机多臂赌博机问题，基于在线随机镜像下降法推导出理论完备的损失函数，并在部分反馈条件下建立遗憾值保证。实验表明，在多种复杂推理基准测试中，ACTOR-CURATOR持续优于均匀采样和现有课程学习方法，展现出更优的训练稳定性与效率。特别在AIME2024和ARC-1D基准上分别取得28.6%和30.5%的相对性能提升，训练速度最高提升80%。这些结果表明ACTOR-CURATOR是可扩展LLM后训练的强大实用方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that post-training large foundation models with reinforcement learning often depends on massive, heterogeneous datasets, making effective curriculum learning both crucial and difficult. The method introduces ACTOR-CURATOR, a scalable and automated curriculum learning framework for reinforcement learning post-training of large language models, which learns a neural curator that dynamically selects training problems by directly optimizing for expected policy improvement, formulated as a non-stationary stochastic bandit problem with a principled loss function derived from online stochastic mirror descent and regret guarantees. The main experimental results show that ACTOR-CURATOR consistently outperforms uniform sampling and strong curriculum baselines across various challenging reasoning benchmarks, achieving relative gains of 28.6% on AIME2024 and 30.5% on ARC-1D over the strongest baseline, with up to 80% speedup, demonstrating improved training stability and efficiency.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，使用强化学习对大型基础模型进行后训练通常依赖于海量异构数据集，这使得有效的课程学习既关键又具有挑战性。方法上提出了ACTOR-CURATOR，这是一个可扩展且全自动的课程学习框架，用于大语言模型的强化学习后训练，它通过学习一个神经策展器来动态选择训练问题，通过直接优化期望策略改进来实现，该问题被表述为非平稳随机赌博机问题，并基于在线随机镜像下降推导出原则性损失函数且提供了遗憾保证。主要实验结果表明，ACTOR-CURATOR在多种具有挑战性的推理基准测试中持续优于均匀采样和强课程基线，在AIME2024上相对最强基线取得了28.6%的提升，在ARC-1D上提升了30.5%，并实现了高达80%的加速，显示出更好的训练稳定性和效率。</div>
</details>
</div>
<div class="card">
<div class="title">A Generalized Apprenticeship Learning Framework for Capturing Evolving Student Pedagogical Strategies</div>
<div class="meta-line">Authors: Md Mirajul Islam, Xi Yang, Adittya Soukarjya Saha, Rajesh Debnath, Min Chi</div>
<div class="meta-line">Venue: AIED 2025, LNCS 15879, Springer, pp. 393-408</div>
<div class="meta-line">First: 2026-02-24T04:08:31+00:00 · Latest: 2026-02-24T04:08:31+00:00</div>
<div class="meta-line">Comments: 16 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20527v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20527v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) and Deep Reinforcement Learning (DRL) have advanced rapidly in recent years and have been successfully applied to e-learning environments like intelligent tutoring systems (ITSs). Despite great success, the broader application of DRL to educational technologies has been limited due to major challenges such as sample inefficiency and difficulty designing the reward function. In contrast, Apprenticeship Learning (AL) uses a few expert demonstrations to infer the expert&#x27;s underlying reward functions and derive decision-making policies that generalize and replicate optimal behavior. In this work, we leverage a generalized AL framework, THEMES, to induce effective pedagogical policies by capturing the complexities of the expert student learning process, where multiple reward functions may dynamically evolve over time. We evaluate the effectiveness of THEMES against six state-of-the-art baselines, demonstrating its superior performance and highlighting its potential as a powerful alternative for inducing effective pedagogical policies and show that it can achieve high performance, with an AUC of 0.899 and a Jaccard of 0.653, using only 18 trajectories of a previous semester to predict student pedagogical decisions in a later semester.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种用于捕捉演化学生教学策略的广义学徒学习框架</div>
<div class="mono" style="margin-top:8px">近年来，强化学习（RL）与深度强化学习（DRL）发展迅速，已成功应用于智能导学系统等电子学习环境。尽管取得显著成果，但DRL在教育技术中的广泛应用仍受限于样本效率低下和奖励函数设计困难等主要挑战。相比之下，学徒学习（AL）通过少量专家示范数据，即可推断专家潜在的奖励函数，并推导出能够泛化和复现最优行为的决策策略。本研究采用广义AL框架THEMES，通过捕捉专家学生学习过程中可能随时间动态演化的多重奖励函数复杂性，推导出有效的教学策略。我们将THEMES与六种先进基线方法进行比较评估，结果表明其性能优越（AUC达0.899，Jaccard系数为0.653），仅需前一学期的18条轨迹即可预测后续学期的学生教学决策，凸显其作为推导有效教学策略的强大替代方案的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenges of applying deep reinforcement learning directly to intelligent tutoring systems, such as sample inefficiency and reward design difficulty, this work proposes a generalized apprenticeship learning framework called THEMES to capture evolving student pedagogical strategies. The method infers potentially dynamic expert reward functions from a small set of demonstrations to derive pedagogical policies that replicate optimal teaching behavior. Experimental results show THEMES outperforms six state-of-the-art baselines, achieving an AUC of 0.899 and a Jaccard score of 0.653 using only 18 expert trajectories to predict student decisions across semesters.</div>
<div class="mono" style="margin-top:8px">针对深度强化学习在智能导学系统中直接应用存在的样本效率低和奖励函数设计困难等挑战，本研究提出了一个名为THEMES的广义学徒学习框架，旨在捕捉不断演化的学生教学策略。该方法通过少量专家示范数据推断可能动态变化的专家奖励函数，从而推导出能复现最优教学行为的教学策略。实验结果表明，THEMES在六个先进基线方法中表现最优，仅使用18条专家轨迹预测跨学期学生决策，就取得了0.899的AUC和0.653的Jaccard分数。</div>
</details>
</div>
<div class="card">
<div class="title">KairosVL: Orchestrating Time Series and Semantics for Unified Reasoning</div>
<div class="meta-line">Authors: Haotian Si, Changhua Pei, Xiao He, Zeyan Li, Zhe Xie, Zexin Wang, Jiyao Hu, Zhaoyang Yu, Tieying Zhang, Dan Pei, Jianhui Li, Gaogang Xie</div>
<div class="meta-line">First: 2026-02-24T02:50:38+00:00 · Latest: 2026-02-24T02:50:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20494v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20494v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Driven by the increasingly complex and decision-oriented demands of time series analysis, we introduce the Semantic-Conditional Time Series Reasoning task, which extends conventional time series analysis beyond purely numerical modeling to incorporate contextual and semantic understanding. To further enhance the mode&#x27;s reasoning capabilities on complex time series problems, we propose a two-round reinforcement learning framework: the first round strengthens the mode&#x27;s perception of fundamental temporal primitives, while the second focuses on semantic-conditioned reasoning. The resulting model, KairosVL, achieves competitive performance across both synthetic and real-world tasks. Extensive experiments and ablation studies demonstrate that our framework not only boosts performance but also preserves intrinsic reasoning ability and significantly improves generalization to unseen scenarios. To summarize, our work highlights the potential of combining semantic reasoning with temporal modeling and provides a practical framework for real-world time series intelligence, which is in urgent demand.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KairosVL：时序与语义协同的统一推理框架</div>
<div class="mono" style="margin-top:8px">为应对时序分析日益复杂和决策导向的需求，我们提出了语义条件时序推理任务，将传统时序分析从纯数值建模扩展至融合上下文与语义理解。为提升模型在复杂时序问题上的推理能力，我们设计了一种两轮强化学习框架：首轮强化模型对基础时序基元的感知，次轮聚焦于语义条件推理。所构建的模型KairosVL在合成与真实任务中均取得优异性能。大量实验与消融研究表明，该框架不仅能提升性能，还能保持内在推理能力，并显著增强对未见场景的泛化性。本研究凸显了语义推理与时序建模结合的潜力，为当前亟需的现实世界时序智能提供了实用框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to move beyond purely numerical modeling in time series analysis and incorporate contextual semantics for complex decision-making, this paper introduces the Semantic-Conditional Time Series Reasoning task. The method employs a two-round reinforcement learning framework: the first round enhances perception of fundamental temporal primitives, and the second focuses on semantic-conditioned reasoning, resulting in the KairosVL model. Experimental results on synthetic and real-world tasks show that KairosVL achieves competitive performance, improves generalization to unseen scenarios, and preserves intrinsic reasoning ability, highlighting the potential of integrating semantic reasoning with temporal modeling for practical time series intelligence.</div>
<div class="mono" style="margin-top:8px">本文的动机是超越传统时间序列分析的纯数值建模，融入上下文语义以应对复杂的决策需求，为此提出了语义条件时间序列推理任务。方法上采用了两轮强化学习框架：第一轮强化模型对基本时间原语的感知，第二轮专注于语义条件推理，由此构建了KairosVL模型。在合成和真实世界任务上的实验结果表明，KairosVL取得了有竞争力的性能，提升了对未见场景的泛化能力，并保持了内在推理能力，凸显了将语义推理与时间建模结合用于实际时间序列智能的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion Modulation via Environment Mechanism Modeling for Planning</div>
<div class="meta-line">Authors: Hanping Zhang, Yuhong Guo</div>
<div class="meta-line">First: 2026-02-23T23:41:22+00:00 · Latest: 2026-02-23T23:41:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20422v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20422v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models have shown promising capabilities in trajectory generation for planning in offline reinforcement learning (RL). However, conventional diffusion-based planning methods often fail to account for the fact that generating trajectories in RL requires unique consistency between transitions to ensure coherence in real environments. This oversight can result in considerable discrepancies between the generated trajectories and the underlying mechanisms of a real environment. To address this problem, we propose a novel diffusion-based planning method, termed as Diffusion Modulation via Environment Mechanism Modeling (DMEMM). DMEMM modulates diffusion model training by incorporating key RL environment mechanisms, particularly transition dynamics and reward functions. Experimental results demonstrate that DMEMM achieves state-of-the-art performance for planning with offline reinforcement learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于环境机制建模的扩散调制规划方法</div>
<div class="mono" style="margin-top:8px">扩散模型在离线强化学习规划中的轨迹生成方面展现出潜力，但传统基于扩散的规划方法常忽略强化学习轨迹生成需保持状态转移间的一致性，以确保真实环境中的连贯性。这一疏漏可能导致生成轨迹与真实环境机制存在显著偏差。为此，我们提出一种新颖的基于扩散的规划方法——基于环境机制建模的扩散调制（DMEMM）。该方法通过融入关键强化学习环境机制（特别是状态转移动态和奖励函数）来调制扩散模型的训练。实验结果表明，DMEMM在离线强化学习规划任务中实现了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the observation that conventional diffusion-based planning methods in offline reinforcement learning often generate trajectories that lack consistency with real environment dynamics, leading to discrepancies. To address this, the authors propose DMEMM, a method that modulates diffusion model training by explicitly incorporating environment mechanisms such as transition dynamics and reward functions. Experimental results show that DMEMM achieves state-of-the-art performance in planning tasks, demonstrating its effectiveness in aligning generated trajectories with real-world constraints.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，传统的基于扩散模型的离线强化学习规划方法常因忽略环境动态一致性，导致生成轨迹与真实机制存在偏差。为此，作者提出了DMEMM方法，通过将环境机制（如转移动态和奖励函数）融入扩散模型训练来调制生成过程。实验结果表明，DMEMM在规划任务中取得了最先进的性能，有效提升了生成轨迹与真实环境的一致性。</div>
</details>
</div>
<div class="card">
<div class="title">Gap-Dependent Bounds for Nearly Minimax Optimal Reinforcement Learning with Linear Function Approximation</div>
<div class="meta-line">Authors: Haochen Zhang, Zhong Zheng, Lingzhou Xue</div>
<div class="meta-line">First: 2026-02-23T19:25:46+00:00 · Latest: 2026-02-23T19:25:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20297v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20297v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study gap-dependent performance guarantees for nearly minimax-optimal algorithms in reinforcement learning with linear function approximation. While prior works have established gap-dependent regret bounds in this setting, existing analyses do not apply to algorithms that achieve the nearly minimax-optimal worst-case regret bound $\tilde{O}(d\sqrt{H^3K})$, where $d$ is the feature dimension, $H$ is the horizon length, and $K$ is the number of episodes. We bridge this gap by providing the first gap-dependent regret bound for the nearly minimax-optimal algorithm LSVI-UCB++ (He et al., 2023). Our analysis yields improved dependencies on both $d$ and $H$ compared to previous gap-dependent results. Moreover, leveraging the low policy-switching property of LSVI-UCB++, we introduce a concurrent variant that enables efficient parallel exploration across multiple agents and establish the first gap-dependent sample complexity upper bound for online multi-agent RL with linear function approximation, achieving linear speedup with respect to the number of agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于线性函数逼近的近极小极大最优强化学习的间隙依赖界</div>
<div class="mono" style="margin-top:8px">我们研究了基于线性函数逼近的近极小极大最优强化学习算法的间隙依赖性能保证。尽管先前研究已在该设定下建立了间隙依赖的遗憾界，但现有分析不适用于达到近极小极大最优最坏情况遗憾界$\tilde{O}(d\sqrt{H^3K})$的算法（其中$d$为特征维度，$H$为时间跨度，$K$为回合数）。我们通过为近极小极大最优算法LSVI-UCB++（He等人，2023）提供首个间隙依赖遗憾界来弥合这一差距。与先前间隙依赖结果相比，我们的分析在$d$和$H$的依赖关系上均获得改进。此外，利用LSVI-UCB++的低策略切换特性，我们提出了一种支持多智能体高效并行探索的并发变体，并首次建立了线性函数逼近下在线多智能体强化学习的间隙依赖样本复杂度上界，实现了关于智能体数量的线性加速。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the gap in theoretical guarantees for nearly minimax-optimal reinforcement learning algorithms with linear function approximation, as prior gap-dependent regret analyses did not apply to algorithms achieving the optimal worst-case bound. The method focuses on analyzing the LSVI-UCB++ algorithm, providing the first gap-dependent regret bound for it, which improves dependencies on feature dimension and horizon length. Experimental results demonstrate that this analysis enables a concurrent variant for efficient parallel exploration in multi-agent settings, establishing the first gap-dependent sample complexity bound with linear speedup relative to the number of agents.</div>
<div class="mono" style="margin-top:8px">本文针对具有线性函数逼近的近乎极小极大最优强化学习算法，解决了其理论保证中的空白，因为先前的间隙依赖遗憾分析不适用于达到最优最坏情况界限的算法。方法集中于分析LSVI-UCB++算法，首次为其提供了间隙依赖遗憾界限，并改善了特征维度和时间范围的依赖关系。实验结果表明，该分析支持开发用于多智能体高效并行探索的并发变体，首次建立了在线多智能体强化学习中具有线性加速的间隙依赖样本复杂度上界。</div>
</details>
</div>
<div class="card">
<div class="title">TROLL: Trust Regions improve Reinforcement Learning for Large Language Models</div>
<div class="meta-line">Authors: Philipp Becker, Niklas Freymuth, Serge Thilges, Fabian Otto, Gerhard Neumann</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-04T14:14:20+00:00 · Latest: 2026-02-23T18:54:13+00:00</div>
<div class="meta-line">Comments: Published as a conference paper at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.03817v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.03817v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) with PPO-like clip objectives has become the standard choice for reward-based fine-tuning of large language models (LLMs). Although recent work has explored improved estimators of advantages and normalization, the clipping mechanism itself has remained untouched. Originally introduced as a proxy for principled KL-based trust regions, clipping is a crude approximation that often causes unstable updates and suboptimal performance. We replace the clip objective with a novel discrete differentiable trust region projection, which provides principled token-level KL constraints. The projection operates on a sparse subset of the model&#x27;s most important token logits to balance computational cost and projection effectiveness. Our approach, Trust Region Optimization for Large Language models (TROLL), serves as a direct replacement for PPO-like clipping during training and does not alter the model&#x27;s inference behavior. Across mathematical reasoning and code generation tasks, model families, as well as advantage-estimation methods, TROLL consistently outperforms PPO-like clipping in terms of training speed, stability, and final success rates.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TROLL：信任区域提升大语言模型的强化学习效果</div>
<div class="mono" style="margin-top:8px">采用类PPO裁剪目标的强化学习已成为基于奖励的大语言模型微调的标准方法。尽管近期研究探索了优势函数估计与归一化的改进方案，但裁剪机制本身始终未被触及。作为基于KL散度信任区域原理的代理方法，裁剪是一种粗略近似，常导致更新不稳定与次优性能。我们提出用新型离散可微信任区域投影替代裁剪目标，该投影能提供理论完备的词元级KL约束。该投影作用于模型最重要词元logits的稀疏子集，以平衡计算成本与投影效果。我们的方法——大语言模型信任区域优化，可直接替代训练过程中的类PPO裁剪，且不改变模型推理行为。在数学推理、代码生成任务、模型家族及优势估计方法的多维度测试中，该方法在训练速度、稳定性与最终成功率方面均持续优于类PPO裁剪。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the limitations of the standard PPO clip objective used in reinforcement learning fine-tuning of large language models, which is a crude approximation of KL-based trust regions and often leads to unstable training and suboptimal performance. The method introduces TROLL, which replaces the clip objective with a novel discrete differentiable trust region projection that enforces principled token-level KL constraints by operating on a sparse subset of the model&#x27;s most important token logits to balance efficiency and effectiveness. Experimental results across mathematical reasoning and code generation tasks show that TROLL consistently outperforms PPO-like clipping in training speed, stability, and final success rates, without altering inference behavior.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于大型语言模型强化学习微调中使用的标准PPO裁剪目标的局限性，该目标是对基于KL散度的信任区域的粗略近似，常导致训练不稳定和性能欠佳。方法上提出了TROLL，用新颖的离散可微信任区域投影替代裁剪目标，通过对模型最重要词元logits的稀疏子集进行操作来施加原则性的词元级KL约束，以平衡计算成本与投影效果。在数学推理和代码生成任务上的实验结果表明，TROLL在训练速度、稳定性和最终成功率方面均一致优于PPO类裁剪方法，且不改变模型推理行为。</div>
</details>
</div>
<div class="card">
<div class="title">Find the Fruit: Zero-Shot Sim2Real RL for Occlusion-Aware Plant Manipulation</div>
<div class="meta-line">Authors: Nitesh Subedi, Hsin-Jung Yang, Devesh K. Jha, Soumik Sarkar</div>
<div class="meta-line">First: 2025-05-22T11:37:39+00:00 · Latest: 2026-02-23T18:46:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.16547v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.16547v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous harvesting in the open presents a complex manipulation problem. In most scenarios, an autonomous system has to deal with significant occlusion and require interaction in the presence of large structural uncertainties (every plant is different). Perceptual and modeling uncertainty make design of reliable manipulation controllers for harvesting challenging, resulting in poor performance during deployment. We present a sim2real reinforcement learning (RL) framework for occlusion-aware plant manipulation, where a policy is learned entirely in simulation to reposition stems and leaves to reveal target fruit(s). In our proposed approach, we decouple high-level kinematic planning from low-level compliant control which simplifies the sim2real transfer. This decomposition allows the learned policy to generalize across multiple plants with different stiffness and morphology. In experiments with multiple real-world plant setups, our system achieves up to 86.7% success in exposing target fruits, demonstrating robustness to occlusion variation and structural uncertainty.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>寻找果实：用于遮挡感知植物操作的零样本仿真到真实强化学习</div>
<div class="mono" style="margin-top:8px">开放环境下的自主采摘是一个复杂的操作问题。在多数场景中，自主系统需应对严重遮挡，并在存在巨大结构不确定性（每株植物各异）的情况下进行交互。感知与建模的不确定性使得设计可靠的采摘操作控制器极具挑战，导致实际部署时性能不佳。本文提出一种用于遮挡感知植物操作的仿真到真实强化学习框架，策略完全在仿真中学习，通过重新定位茎叶以显露目标果实。该方法将高层运动规划与底层柔顺控制解耦，简化了仿真到真实的迁移。这种分解使学习策略能泛化至不同刚度和形态的多种植物。在多种真实植物设置的实验中，该系统成功暴露目标果实的比率最高达86.7%，展现出对遮挡变化和结构不确定性的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenges of autonomous harvesting in unstructured environments, where occlusion and plant variability hinder reliable manipulation, this paper introduces a sim2real reinforcement learning framework for occlusion-aware plant manipulation. The method decouples high-level kinematic planning from low-level compliant control, enabling policies learned entirely in simulation to generalize to real plants with differing stiffness and morphology. Experimental results on real-world plant setups show the system achieves up to 86.7% success in exposing target fruits, demonstrating robustness to occlusion and structural uncertainty.</div>
<div class="mono" style="margin-top:8px">针对非结构化环境中自主采摘面临的遮挡和植物形态多变导致操控不可靠的挑战，本文提出了一种用于遮挡感知植物操作的模拟到真实强化学习框架。该方法将高层运动规划与底层柔顺控制解耦，使得完全在模拟中学习的策略能够泛化到具有不同刚度和形态的真实植物上。在真实植物设置上的实验结果表明，该系统在暴露目标果实方面成功率高达86.7%，展现了对遮挡变化和结构不确定性的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">LAD: Learning Advantage Distribution for Reasoning</div>
<div class="meta-line">Authors: Wendi Li, Sharon Li</div>
<div class="meta-line">First: 2026-02-23T18:44:10+00:00 · Latest: 2026-02-23T18:44:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20132v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20132v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current reinforcement learning objectives for large-model reasoning primarily focus on maximizing expected rewards. This paradigm can lead to overfitting to dominant reward signals, while neglecting alternative yet valid reasoning trajectories, thereby limiting diversity and exploration. To address this issue, we introduce Learning Advantage Distributions (LAD), a distribution-matching framework that replaces advantage maximization with learning the advantage-induced distribution. By establishing the equivalence between the optimal policy update and an advantage-based target distribution, we derive a practical LAD objective formulated as minimizing an $f$-divergence between the policy-induced and advantage-induced distributions. This yields a gradient update that increases likelihood for high-advantage responses while suppressing over-confident probability growth, preventing collapse without requiring auxiliary entropy regularization. LAD incurs no extra training cost compared to GRPO and scales naturally to LLM post-training. In a controlled bandit setting, LAD faithfully recovers the multimodal advantage distribution, validating the theoretical formulation. Experiments on math and code reasoning tasks across several LLM backbones show that LAD reliably improves both accuracy and generative diversity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LAD：面向推理的优势分布学习</div>
<div class="mono" style="margin-top:8px">当前面向大模型推理的强化学习目标主要聚焦于最大化期望奖励。这种范式可能导致对主导奖励信号的过拟合，同时忽略其他有效但非主流的推理轨迹，从而限制多样性与探索性。为解决该问题，我们提出学习优势分布（LAD）——一种分布匹配框架，通过优势诱导分布的学习替代传统的优势最大化方法。通过建立最优策略更新与基于优势的目标分布之间的等价关系，我们推导出以最小化策略诱导分布与优势诱导分布间$f$散度为形式的实用LAD目标。该方法产生的梯度更新在提升高优势响应似然的同时，抑制过度自信的概率增长，无需辅助熵正则化即可避免分布坍缩。相比GRPO，LAD未引入额外训练成本，并可自然扩展至大语言模型后训练阶段。在受控赌博机环境中，LAD能准确复现多模态优势分布，验证了理论框架的有效性。在多个大语言模型骨干网络上的数学与代码推理实验表明，LAD能稳定提升准确率与生成多样性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the limitation of standard reinforcement learning objectives in large-model reasoning, which maximize expected rewards but risk overfitting to dominant signals and neglecting valid alternative reasoning paths, thereby reducing diversity and exploration. To address this, the authors propose Learning Advantage Distributions (LAD), a distribution-matching framework that replaces advantage maximization with learning an advantage-induced distribution; this is achieved by deriving a practical objective that minimizes an f-divergence between policy-induced and advantage-induced distributions, yielding gradient updates that promote high-advantage responses while preventing over-confident probability collapse without extra entropy regularization. Experimental results in a controlled bandit setting confirm LAD&#x27;s ability to recover multimodal advantage distributions, and tests on math and code reasoning tasks across multiple LLM backbones demonstrate reliable improvements in both accuracy and generative diversity, with no additional training cost compared to methods like GRPO.</div>
<div class="mono" style="margin-top:8px">本文的动机在于标准强化学习目标在大模型推理中的局限性，即最大化期望奖励可能导致过拟合于主导奖励信号，忽视其他有效推理路径，从而限制多样性和探索。为解决此问题，作者提出了学习优势分布（LAD），这是一个分布匹配框架，用学习优势诱导分布替代优势最大化；其方法是通过最小化策略诱导分布与优势诱导分布之间的f散度，推导出实用目标，产生梯度更新以增加高优势响应的可能性，同时抑制过度自信的概率增长，无需额外熵正则化即可防止崩溃。实验结果表明，在受控赌博机设置中，LAD能准确恢复多模态优势分布，验证了理论框架；在数学和代码推理任务上对多个LLM骨干的测试显示，LAD可靠地提升了准确性和生成多样性，且相比GRPO等方法无额外训练成本。</div>
</details>
</div>
<div class="card">
<div class="title">ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models</div>
<div class="meta-line">Authors: Andre He, Nathaniel Weir, Kaj Bostrom, Allen Nie, Darion Cassel, Sam Bayless, Huzefa Rangwala</div>
<div class="meta-line">First: 2026-02-23T18:34:29+00:00 · Latest: 2026-02-23T18:34:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20117v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20117v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising approach for training reasoning language models (RLMs) by leveraging supervision from verifiers. Although verifier implementation is easier than solution annotation for many tasks, existing synthetic data generation methods remain largely solution-centric, while verifier-based methods rely on a few hand-crafted procedural environments. In this work, we scale RLVR by introducing ReSyn, a pipeline that generates diverse reasoning environments equipped with instance generators and verifiers, covering tasks such as constraint satisfaction, algorithmic puzzles, and spatial reasoning. A Qwen2.5-7B-Instruct model trained with RL on ReSyn data achieves consistent gains across reasoning benchmarks and out-of-domain math benchmarks, including a 27\% relative improvement on the challenging BBEH benchmark. Ablations show that verifier-based supervision and increased task diversity both contribute significantly, providing empirical evidence that generating reasoning environments at scale can enhance reasoning abilities in RLMs</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReSyn：面向推理模型的可自主扩展合成环境</div>
<div class="mono" style="margin-top:8px">基于可验证奖励的强化学习（RLVR）通过利用验证器的监督，已成为训练推理语言模型（RLM）的有效方法。尽管在许多任务中验证器实现比解决方案标注更简便，但现有合成数据生成方法仍主要围绕解决方案展开，而基于验证器的方法则依赖少量人工设计的程序化环境。本研究通过引入ReSyn实现RLVR的规模化扩展——该流程能生成配备实例生成器和验证器的多样化推理环境，涵盖约束满足、算法谜题和空间推理等任务。使用ReSyn数据进行强化学习训练的Qwen2.5-7B-Instruct模型，在推理基准测试和跨领域数学基准测试中均取得稳定提升，其中在挑战性BBEH基准上实现27%的相对改进。消融实验表明，基于验证器的监督与增强的任务多样性均具有显著贡献，这为大规模生成推理环境可提升RLM推理能力提供了实证依据。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of scaling reinforcement learning with verifiable rewards (RLVR) for training reasoning language models, as existing methods are either solution-centric or limited to a few hand-crafted environments. It introduces ReSyn, a pipeline that autonomously generates diverse synthetic reasoning environments—including constraint satisfaction, algorithmic puzzles, and spatial reasoning tasks—each equipped with instance generators and verifiers to provide supervision. Experimental results show that a Qwen2.5-7B-Instruct model trained with reinforcement learning on ReSyn data achieves consistent improvements across reasoning benchmarks, with a 27% relative gain on the challenging BBEH benchmark, and ablations confirm the importance of verifier-based supervision and task diversity in enhancing reasoning abilities.</div>
<div class="mono" style="margin-top:8px">本文针对使用可验证奖励的强化学习在训练推理语言模型时难以扩展的问题，现有方法要么以解决方案为中心，要么局限于少量手工构建的环境。研究提出了ReSyn，一种能够自主生成多样化合成推理环境的流程，涵盖约束满足、算法谜题和空间推理等任务，每个环境都配备实例生成器和验证器以提供监督。实验结果表明，基于ReSyn数据通过强化学习训练的Qwen2.5-7B-Instruct模型在多个推理基准测试中取得一致提升，在具有挑战性的BBEH基准上相对提高了27%，消融研究证实了基于验证器的监督和任务多样性对增强推理能力的重要贡献。</div>
</details>
</div>
<div class="card">
<div class="title">AbstRaL: Augmenting LLMs&#x27; Reasoning by Reinforcing Abstract Thinking</div>
<div class="meta-line">Authors: Silin Gao, Antoine Bosselut, Samy Bengio, Emmanuel Abbe</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-06-09T13:34:50+00:00 · Latest: 2026-02-23T18:25:13+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.07751v4">Abs</a> · <a href="https://arxiv.org/pdf/2506.07751v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent studies have shown that large language models (LLMs), especially smaller ones, often lack robustness in grade school math (GSM) reasoning. In particular, they tend to experience performance drops when faced with distribution shifts, such as changes to numerical or nominal variables, or insertions of distracting clauses. A possible strategy to address this involves generating synthetic data to further &quot;instantiate&quot; reasoning problems on potential variations. In this work, we instead focus on the strategy of &quot;abstracting&quot; reasoning problems. This not only helps counteract distribution shifts but also facilitates the connection to symbolic tools for deriving solutions. Focusing on GSM, we find that this abstraction process is better acquired through reinforcement learning (RL) than just supervised fine-tuning, which often fails to produce faithful abstractions. Our method, AbstRaL -- which promotes abstract reasoning in LLMs using RL on granular abstraction data -- significantly mitigates performance degradation on recent GSM perturbation benchmarks. Besides, improving GSM robustness via AbstRaL is shown to also implicitly benefit LLMs&#x27; capabilities on OOD mathematical and general reasoning tasks, indicating that abstract thinking broadly enables better generalizability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AbstRaL：通过强化抽象思维增强大语言模型的推理能力</div>
<div class="mono" style="margin-top:8px">近期研究表明，大语言模型（LLMs），尤其是较小规模的模型，在小学数学（GSM）推理中常缺乏鲁棒性。具体而言，当面临分布偏移（如数值或名义变量的变化，或干扰性从句的插入）时，其性能易出现下降。一种可能的解决策略是生成合成数据，以进一步“实例化”针对潜在变体的推理问题。本研究则聚焦于“抽象化”推理问题的策略，这不仅有助于抵消分布偏移，还能促进与符号化工具连接以推导解决方案。针对GSM任务，我们发现通过强化学习（RL）比仅用监督微调更能有效掌握抽象化过程，后者常难以生成准确的抽象表示。我们的方法AbstRaL——通过在细粒度抽象数据上应用RL来增强LLMs的抽象推理能力——显著减轻了在近期GSM扰动基准测试中的性能下降。此外，通过AbstRaL提升GSM鲁棒性也被证明能隐式增强LLMs在分布外数学任务及通用推理任务上的能力，表明抽象思维广泛提升了模型的泛化性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the vulnerability of large language models, particularly smaller ones, to performance drops in grade school math reasoning when encountering distribution shifts like numerical changes or distracting clauses. Instead of generating synthetic data for instantiation, the method AbstRaL reinforces abstract thinking by using reinforcement learning on granular abstraction data, which proves more effective than supervised fine-tuning for producing faithful abstractions. Experimental results show that AbstRaL significantly mitigates performance degradation on GSM perturbation benchmarks and also implicitly improves out-of-distribution mathematical and general reasoning tasks, indicating that abstract thinking enhances overall generalizability.</div>
<div class="mono" style="margin-top:8px">该论文针对大型语言模型（尤其是较小模型）在小学数学推理中遇到数值变化或干扰子句等分布偏移时性能下降的问题，提出了一种新方法。不同于通过生成合成数据进行实例化的策略，AbstRaL方法采用强化学习在细粒度抽象数据上增强抽象思维，这比监督微调更能产生可靠的抽象表示。实验结果表明，AbstRaL显著减轻了在GSM扰动基准上的性能下降，并隐式提升了模型在分布外数学和一般推理任务上的能力，表明抽象思维能广泛促进更好的泛化性。</div>
</details>
</div>
<div class="card">
<div class="title">EBPO: Empirical Bayes Shrinkage for Stabilizing Group-Relative Policy Optimization</div>
<div class="meta-line">Authors: Kevin Han, Yuhang Zhou, Mingze Gao, Gedi Zhou, Serena Li, Abhishek Kumar, Xiangjun Fan, Weiwei Li, Lizhu Zhang</div>
<div class="meta-line">First: 2026-02-05T00:33:02+00:00 · Latest: 2026-02-23T18:23:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05165v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.05165v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for enhancing the reasoning capabilities of Large Language Models (LLMs). However, dominant approaches like Group Relative Policy Optimization (GRPO) face critical stability challenges: they suffer from high estimator variance under computational constraints (small group sizes) and vanishing gradient signals in saturated failure regimes where all responses yield identical zero rewards. To address this, we propose Empirical Bayes Policy Optimization (EBPO), a novel framework that regularizes local group-based baselines by borrowing strength from the policy&#x27;s accumulated global statistics. Instead of estimating baselines in isolation, EBPO employs a shrinkage estimator that dynamically balances local group statistics with a global prior updated via Welford&#x27;s online algorithm. Theoretically, we demonstrate that EBPO guarantees strictly lower Mean Squared Error (MSE), bounded entropy decay, and non-vanishing penalty signals in failure scenarios compared to GRPO. Empirically, EBPO consistently outperforms GRPO and other established baselines across diverse benchmarks, including AIME and OlympiadBench. Notably, EBPO exhibits superior training stability, achieving high-performance gains even with small group sizes, and benefits significantly from difficulty-stratified curriculum learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EBPO：基于经验贝叶斯收缩的组相对策略优化稳定方法</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）已被证明能有效提升大语言模型（LLMs）的推理能力。然而，主流方法如组相对策略优化（GRPO）面临关键的稳定性挑战：在计算受限（小组规模较小）时估计器方差较高，且在饱和失效场景（所有响应均产生相同的零奖励）中梯度信号趋于消失。为此，我们提出经验贝叶斯策略优化（EBPO），这是一种通过利用策略累积的全局统计量来正则化局部组基线的新框架。EBPO采用收缩估计器动态平衡局部组统计量与通过Welford在线算法更新的全局先验，而非孤立估计基线。理论上，我们证明相比GRPO，EBPO能严格保证更低的均方误差（MSE）、有界的熵衰减，并在失效场景中提供非消失的惩罚信号。实证中，EBPO在包括AIME和OlympiadBench在内的多种基准测试中持续优于GRPO及其他成熟基线。值得注意的是，EBPO展现出卓越的训练稳定性，即使在小规模组设置下仍能实现高性能提升，并能显著受益于难度分层课程学习。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Empirical Bayes Policy Optimization (EBPO) to address stability issues in Group Relative Policy Optimization (GRPO) for reinforcement learning with verifiable rewards, where GRPO suffers from high variance with small group sizes and vanishing gradients when all responses yield zero rewards. The method regularizes local group baselines by dynamically shrinking them toward a global prior updated online, theoretically ensuring lower mean squared error, bounded entropy decay, and non-vanishing penalty signals. Experimental results show EBPO outperforms GRPO and other baselines on benchmarks like AIME and OlympiadBench, offering greater training stability and performance gains even with small groups, and benefits from difficulty-stratified curriculum learning.</div>
<div class="mono" style="margin-top:8px">本文提出经验贝叶斯策略优化（EBPO），以解决可验证奖励强化学习中组相对策略优化（GRPO）的稳定性问题，其中GRPO在小组规模下存在高方差且在响应奖励全为零时梯度消失。该方法通过将局部组基线动态收缩至在线更新的全局先验进行正则化，从理论上保证了更低的均方误差、有界的熵衰减和非消失的惩罚信号。实验结果表明，EBPO在AIME和OlympiadBench等基准测试中优于GRPO及其他基线方法，即使小组规模较小也能提供更高的训练稳定性和性能提升，并受益于难度分层课程学习。</div>
</details>
</div>
<div class="card">
<div class="title">Descent-Guided Policy Gradient for Scalable Cooperative Multi-Agent Learning</div>
<div class="meta-line">Authors: Shan Yang, Yang Liu</div>
<div class="meta-line">First: 2026-02-23T17:45:08+00:00 · Latest: 2026-02-23T17:45:08+00:00</div>
<div class="meta-line">Comments: 10 pages, 5 figures, 5 tables; plus 16 pages of appendices</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20078v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20078v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scaling cooperative multi-agent reinforcement learning (MARL) is fundamentally limited by cross-agent noise: when agents share a common reward, the actions of all $N$ agents jointly determine each agent&#x27;s learning signal, so cross-agent noise grows with $N$. In the policy gradient setting, per-agent gradient estimate variance scales as $Θ(N)$, yielding sample complexity $\mathcal{O}(N/ε)$. We observe that many domains -- cloud computing, transportation, power systems -- have differentiable analytical models that prescribe efficient system states. In this work, we propose Descent-Guided Policy Gradient (DG-PG), a framework that constructs noise-free per-agent guidance gradients from these analytical models, decoupling each agent&#x27;s gradient from the actions of all others. We prove that DG-PG reduces gradient variance from $Θ(N)$ to $\mathcal{O}(1)$, preserves the equilibria of the cooperative game, and achieves agent-independent sample complexity $\mathcal{O}(1/ε)$. On a heterogeneous cloud scheduling task with up to 200 agents, DG-PG converges within 10 episodes at every tested scale -- from $N=5$ to $N=200$ -- directly confirming the predicted scale-invariant complexity, while MAPPO and IPPO fail to converge under identical architectures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于下降引导策略梯度的可扩展协作多智能体学习</div>
<div class="mono" style="margin-top:8px">协作多智能体强化学习（MARL）的可扩展性受限于跨智能体噪声：当智能体共享共同奖励时，所有N个智能体的行为共同决定每个智能体的学习信号，导致跨智能体噪声随N增长。在策略梯度框架下，单智能体梯度估计方差为Θ(N)，样本复杂度为𝒪(N/ε)。本文观察到，云计算、交通、电力系统等领域存在可微分的解析模型，可描述高效系统状态。为此，我们提出下降引导策略梯度（DG-PG）框架，利用这些解析模型构建无噪声的单智能体引导梯度，使每个智能体的梯度与其他智能体行为解耦。理论证明DG-PG将梯度方差从Θ(N)降至𝒪(1)，保持协作博弈的均衡性，并实现与智能体数量无关的样本复杂度𝒪(1/ε)。在最多200个智能体的异构云调度任务中，DG-PG在N=5至N=200的所有测试规模下均能在10个训练周期内收敛，直接验证了其规模不变的复杂度特性；而相同架构下的MAPPO和IPPO方法均未能收敛。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the scalability challenge in cooperative multi-agent reinforcement learning (MARL), where cross-agent noise causes per-agent gradient variance to scale linearly with the number of agents, leading to high sample complexity. To overcome this, the authors propose Descent-Guided Policy Gradient (DG-PG), a framework that leverages differentiable analytical models from domains like cloud computing to generate noise-free per-agent guidance gradients, thereby decoupling each agent&#x27;s gradient from others&#x27; actions. Theoretically, DG-PG reduces gradient variance from Θ(N) to O(1), preserves game equilibria, and achieves agent-independent sample complexity O(1/ε); experimentally, on a heterogeneous cloud scheduling task with up to 200 agents, it converges within 10 episodes across all scales, outperforming baselines like MAPPO and IPPO which fail to converge.</div>
<div class="mono" style="margin-top:8px">本文针对协作多智能体强化学习中的可扩展性问题，即跨智能体噪声导致每个智能体的梯度方差随智能体数量线性增长，从而带来高样本复杂度。为解决此问题，作者提出了下降引导策略梯度（DG-PG）框架，利用来自云计算等领域的可微分分析模型生成无噪声的个体引导梯度，从而将每个智能体的梯度与其他智能体的动作解耦。理论上，DG-PG将梯度方差从Θ(N)降低至O(1)，保持博弈均衡，并实现与智能体数量无关的样本复杂度O(1/ε)；实验上，在多达200个智能体的异构云调度任务中，该方法在所有规模下均在10个回合内收敛，而MAPPO和IPPO等基线方法在相同架构下无法收敛。</div>
</details>
</div>
<div class="card">
<div class="title">KINESIS: Motion Imitation for Human Musculoskeletal Locomotion</div>
<div class="meta-line">Authors: Merkourios Simos, Alberto Silvio Chiappa, Alexander Mathis</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2025-03-18T18:37:49+00:00 · Latest: 2026-02-23T17:30:07+00:00</div>
<div class="meta-line">Comments: Accepted to ICRA. Here we include an appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.14637v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.14637v2">PDF</a> · <a href="https://github.com/amathislab/Kinesis">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How do humans move? Advances in reinforcement learning (RL) have produced impressive results in capturing human motion using physics-based humanoid control. However, torque-controlled humanoids fail to model key aspects of human motor control such as biomechanical joint constraints \&amp; non-linear and overactuated musculotendon control. We present KINESIS, a model-free motion imitation framework that tackles these challenges. KINESIS is trained on 1.8 hours of locomotion data and achieves strong motion imitation performance on unseen trajectories. Through a negative mining approach, KINESIS learns robust locomotion priors that we leverage to deploy the policy on several downstream tasks such as text-to-control, target point reaching, and football penalty kicks. Importantly, KINESIS learns to generate muscle activity patterns that correlate well with human EMG activity. We show that these results scale seamlessly across biomechanical model complexity, demonstrating control of up to 290 muscles. Overall, the physiological plausibility makes KINESIS a promising model for tackling challenging problems in human motor control. Code, videos and benchmarks are available at https://github.com/amathislab/Kinesis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KINESIS：面向人体肌肉骨骼运动的动作模仿</div>
<div class="mono" style="margin-top:8px">人类如何运动？强化学习（RL）的进展在基于物理的人形控制捕捉人体运动方面取得了显著成果。然而，扭矩控制的人形模型未能模拟人类运动控制的关键方面，如生物力学关节约束、非线性及过驱动的肌肉肌腱控制。我们提出KINESIS，一种无模型动作模仿框架，以应对这些挑战。KINESIS基于1.8小时的运动数据训练，在未见轨迹上实现了强大的动作模仿性能。通过负样本挖掘方法，KINESIS学习了鲁棒的运动先验知识，我们利用这些知识将策略部署于多个下游任务，如文本到控制、目标点到达和足球点球射门。重要的是，KINESIS学会了生成与人类肌电活动高度相关的肌肉活动模式。我们证明这些结果可无缝扩展至不同复杂度的生物力学模型，实现了对多达290块肌肉的控制。总体而言，其生理合理性使KINESIS成为解决人类运动控制中挑战性问题的潜力模型。代码、视频和基准测试发布于https://github.com/amathislab/Kinesis。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for KINESIS is to overcome the limitations of torque-controlled humanoids in modeling human motor control, such as biomechanical joint constraints and complex musculotendon dynamics. The method introduces a model-free motion imitation framework trained on 1.8 hours of locomotion data, utilizing a negative mining approach to learn robust locomotion priors. Experimental results show strong imitation performance on unseen trajectories, successful deployment in downstream tasks like text-to-control and target reaching, and the generation of muscle activity patterns that correlate with human EMG, scaling to control up to 290 muscles across varying biomechanical complexities.</div>
<div class="mono" style="margin-top:8px">KINESIS的动机是克服扭矩控制人形机器人在模拟人类运动控制方面的局限，如生物力学关节约束和复杂的肌腱控制。该方法提出了一种基于1.8小时运动数据的无模型运动模仿框架，通过负样本挖掘学习鲁棒的运动先验。实验结果表明，该框架在未见轨迹上实现了强大的模仿性能，成功应用于文本控制、目标点到达等下游任务，并能生成与人类肌电活动相关的肌肉激活模式，可扩展至控制多达290块肌肉，适应不同生物力学模型复杂度。</div>
</details>
</div>
<div class="card">
<div class="title">Analysis of approximate linear programming solution to Markov decision problem with log barrier function</div>
<div class="meta-line">Authors: Donghwan Lee, Hyukjun Yang, Bum Geun Park</div>
<div class="meta-line">First: 2025-09-24T06:36:11+00:00 · Latest: 2026-02-23T16:58:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.19800v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.19800v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">There are two primary approaches to solving Markov decision problems (MDPs): dynamic programming based on the Bellman equation and linear programming (LP). Dynamic programming methods are the most widely used and form the foundation of both classical and modern reinforcement learning (RL). By contrast, LP-based methods have been less commonly employed, although they have recently gained attention in contexts such as offline RL. The relative underuse of the LP-based methods stems from the fact that it leads to an inequality-constrained optimization problem, which is generally more challenging to solve effectively compared with Bellman-equation-based methods. The purpose of this paper is to establish a theoretical foundation for solving LP-based MDPs in a more effective and practical manner. Our key idea is to leverage the log-barrier function, widely used in inequality-constrained optimization, to transform the LP formulation of the MDP into an unconstrained optimization problem. This reformulation enables approximate solutions to be obtained easily via gradient descent. While the method may appear simple, to the best of our knowledge, a thorough theoretical interpretation of this approach has not yet been developed. This paper aims to bridge this gap.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于对数障碍函数的马尔可夫决策问题近似线性规划解分析</div>
<div class="mono" style="margin-top:8px">求解马尔可夫决策问题（MDP）主要有两种方法：基于贝尔曼方程的动态规划和线性规划（LP）。动态规划方法应用最广泛，是经典及现代强化学习（RL）的基础。相比之下，基于LP的方法较少使用，尽管近期在离线RL等场景中受到关注。这类方法相对未被充分利用，是因为其会转化为不等式约束优化问题，通常比基于贝尔曼方程的方法更难高效求解。本文旨在为更高效、实用地求解基于LP的MDP建立理论基础。核心思路是利用不等式约束优化中广泛使用的对数障碍函数，将MDP的LP形式转化为无约束优化问题，从而可通过梯度下降轻松获得近似解。尽管方法看似简单，但据我们所知，目前尚未有对此方法的完整理论阐释。本文旨在填补这一空白。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the underutilization of linear programming (LP) methods for solving Markov decision problems (MDPs), which are typically constrained by inequalities and thus more challenging than dynamic programming approaches. The authors propose transforming the LP formulation into an unconstrained optimization problem using a log-barrier function, enabling approximate solutions via gradient descent. Their main contribution is establishing a theoretical foundation for this method, which, while simple in practice, had lacked rigorous interpretation, thereby bridging a gap in the literature on LP-based MDP solutions.</div>
<div class="mono" style="margin-top:8px">本文针对马尔可夫决策问题中线性规划方法因受不等式约束而比动态规划更难求解、使用较少的问题，提出利用对数障碍函数将线性规划转化为无约束优化问题，从而通过梯度下降获得近似解。该方法虽然在实践中简单，但此前缺乏严格的理论解释，本文的主要贡献正是为此建立了理论基础，弥补了线性规划求解马尔可夫决策问题研究中的空白。</div>
</details>
</div>
<div class="card">
<div class="title">A Secure and Private Distributed Bayesian Federated Learning Design</div>
<div class="meta-line">Authors: Nuocheng Yang, Sihua Wang, Zhaohui Yang, Mingzhe Chen, Changchuan Yin, Kaibin Huang</div>
<div class="meta-line">First: 2026-02-23T16:12:02+00:00 · Latest: 2026-02-23T16:12:02+00:00</div>
<div class="meta-line">Comments: 14 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20003v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20003v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Distributed Federated Learning (DFL) enables decentralized model training across large-scale systems without a central parameter server. However, DFL faces three critical challenges: privacy leakage from honest-but-curious neighbors, slow convergence due to the lack of central coordination, and vulnerability to Byzantine adversaries aiming to degrade model accuracy. To address these issues, we propose a novel DFL framework that integrates Byzantine robustness, privacy preservation, and convergence acceleration. Within this framework, each device trains a local model using a Bayesian approach and independently selects an optimal subset of neighbors for posterior exchange. We formulate this neighbor selection as an optimization problem to minimize the global loss function under security and privacy constraints. Solving this problem is challenging because devices only possess partial network information, and the complex coupling between topology, security, and convergence remains unclear. To bridge this gap, we first analytically characterize the trade-offs between dynamic connectivity, Byzantine detection, privacy levels, and convergence speed. Leveraging these insights, we develop a fully distributed Graph Neural Network (GNN)-based Reinforcement Learning (RL) algorithm. This approach enables devices to make autonomous connection decisions based on local observations. Simulation results demonstrate that our method achieves superior robustness and efficiency with significantly lower overhead compared to traditional security and privacy schemes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种安全私密的分布式贝叶斯联邦学习设计方案</div>
<div class="mono" style="margin-top:8px">分布式联邦学习（DFL）支持在没有中央参数服务器的大规模系统中进行去中心化模型训练。然而，DFL面临三大关键挑战：诚实但好奇的邻居导致的隐私泄露、缺乏中央协调导致的收敛缓慢，以及旨在降低模型准确性的拜占庭攻击脆弱性。为解决这些问题，我们提出了一种新型DFL框架，集成了拜占庭鲁棒性、隐私保护和收敛加速。在该框架中，每个设备采用贝叶斯方法训练本地模型，并独立选择最优邻居子集进行后验交换。我们将邻居选择建模为在安全和隐私约束下最小化全局损失函数的优化问题。由于设备仅掌握部分网络信息，且拓扑结构、安全性与收敛性之间存在复杂耦合关系，解决该问题具有挑战性。为弥合这一差距，我们首先通过理论分析刻画了动态连接性、拜占庭检测、隐私级别与收敛速度之间的权衡关系。基于这些发现，我们开发了一种完全分布式的基于图神经网络（GNN）的强化学习（RL）算法。该方法使设备能够根据本地观测自主做出连接决策。仿真结果表明，与传统安全和隐私方案相比，我们的方法以显著更低的开销实现了更优的鲁棒性和效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses critical challenges in Distributed Federated Learning (DFL), including privacy leakage from honest-but-curious neighbors, slow convergence without central coordination, and vulnerability to Byzantine attacks. The proposed solution integrates Byzantine robustness, privacy preservation, and convergence acceleration into a novel DFL framework, where each device employs a Bayesian approach for local training and independently selects an optimal neighbor subset for posterior exchange, formulated as an optimization problem under security and privacy constraints. To tackle the complexity of partial network information and coupled trade-offs, the authors first analytically characterize relationships between connectivity, Byzantine detection, privacy, and convergence, then develop a fully distributed Graph Neural Network-based Reinforcement Learning algorithm for autonomous device decisions. Experimental simulations show the method achieves superior robustness and efficiency with significantly lower overhead compared to traditional schemes.</div>
<div class="mono" style="margin-top:8px">本文针对分布式联邦学习（DFL）中的关键挑战，包括诚实但好奇的邻居导致的隐私泄露、缺乏中心协调导致的收敛缓慢，以及旨在降低模型准确性的拜占庭攻击脆弱性。提出的解决方案将拜占庭鲁棒性、隐私保护和收敛加速集成到一个新颖的DFL框架中，其中每个设备采用贝叶斯方法进行本地训练，并独立选择最优邻居子集进行后验交换，该过程被形式化为安全与隐私约束下的优化问题。为解决部分网络信息和复杂耦合权衡的难题，作者首先分析了连通性、拜占庭检测、隐私和收敛之间的权衡关系，然后开发了一种完全分布式的基于图神经网络的强化学习算法，使设备能基于本地观察自主决策。仿真实验结果表明，与传统安全和隐私方案相比，该方法以显著更低的开销实现了更优的鲁棒性和效率。</div>
</details>
</div>
<div class="card">
<div class="title">Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters</div>
<div class="meta-line">Authors: Ailin Huang, Ang Li, Aobo Kong, Bin Wang, Binxing Jiao, Bo Dong, Bojun Wang, Boyu Chen, Brian Li, Buyun Ma, Chang Su, Changxin Miao, Changyi Wan, Chao Lou, Chen Hu, Chen Xu, Chenfeng Yu, Chengting Feng, Chengyuan Yao, Chunrui Han, Dan Ma, Dapeng Shi, Daxin Jiang, Dehua Ma, Deshan Sun, Di Qi, Enle Liu, Fajie Zhang, Fanqi Wan, Guanzhe Huang, Gulin Yan, Guoliang Cao, Guopeng Li, Han Cheng, Hangyu Guo, Hanshan Zhang, Hao Nie, Haonan Jia, Haoran Lv, Hebin Zhou, Hekun Lv, Heng Wang, Heung-Yeung Shum, Hongbo Huang, Hongbo Peng, Hongyu Zhou, Hongyuan Wang, Houyong Chen, Huangxi Zhu, Huimin Wu, Huiyong Guo, Jia Wang, Jian Zhou, Jianjian Sun, Jiaoren Wu, Jiaran Zhang, Jiashu Lv, Jiashuo Liu, Jiayi Fu, Jiayu Liu, Jie Cheng, Jie Luo, Jie Yang, Jie Zhou, Jieyi Hou, Jing Bai, Jingcheng Hu, Jingjing Xie, Jingwei Wu, Jingyang Zhang, Jishi Zhou, Junfeng Liu, Junzhe Lin, Ka Man Lo, Kai Liang, Kaibo Liu, Kaijun Tan, Kaiwen Yan, Kaixiang Li, Kang An, Kangheng Lin, Lei Yang, Liang Lv, Liang Zhao, Liangyu Chen, Lieyu Shi, Liguo Tan, Lin Lin, Lina Chen, Luck Ma, Mengqiang Ren, Michael Li, Ming Li, Mingliang Li, Mingming Zhang, Mingrui Chen, Mitt Huang, Na Wang, Peng Liu, Qi Han, Qian Zhao, Qinglin He, Qinxin Du, Qiuping Wu, Quan Sun, Rongqiu Yang, Ruihang Miao, Ruixin Han, Ruosi Wan, Ruyan Guo, Shan Wang, Shaoliang Pang, Shaowen Yang, Shengjie Fan, Shijie Shang, Shiliang Yang, Shiwei Li, Shuangshuang Tian, Siqi Liu, Siye Wu, Siyu Chen, Song Yuan, Tiancheng Cao, Tianchi Yue, Tianhao Cheng, Tianning Li, Tingdan Luo, Wang You, Wei Ji, Wei Yuan, Wei Zhang, Weibo Wu, Weihao Xie, Wen Sun, Wenjin Deng, Wenzhen Zheng, Wuxun Xie, Xiangfeng Wang, Xiangwen Kong, Xiangyu Liu, Xiangyu Zhang, Xiaobo Yang, Xiaojia Liu, Xiaolan Yuan, Xiaoran Jiao, Xiaoxiao Ren, Xiaoyun Zhang, Xin Li, Xin Liu, Xin Wu, Xing Chen, Xingping Yang, Xinran Wang, Xu Zhao, Xuan He, Xuanti Feng, Xuedan Cai, Xuqiang Zhou, Yanbo Yu, Yang Li, Yang Xu, Yanlin Lai, Yanming Xu, Yaoyu Wang, Yeqing Shen, Yibo Zhu, Yichen Lv, Yicheng Cao, Yifeng Gong, Yijing Yang, Yikun Yang, Yin Zhao, Yingxiu Zhao, Yinmin Zhang, Yitong Zhang, Yixuan Zhang, Yiyang Chen, Yongchi Zhao, Yongshen Long, Yongyao Wang, Yousong Guan, Yu Zhou, Yuang Peng, Yuanhao Ding, Yuantao Fan, Yuanwei Lu, Yuanzhen Yang, Yuchu Luo, Yudi Zhao, Yue Peng, Yueqiang Lin, Yufan Lu, Yuling Zhao, Yunzhou Ju, Yurong Zhang, Yusheng Li, Yuxiang Yang, Yuyang Chen, Yuzhu Cai, Zejia Weng, Zetao Hong, Zexi Li, Zhe Xie, Zheng Ge, Zheng Gong, Zheng Zeng, Zhenyi Lu, Zhewei Huang, Zhichao Chang, Zhiguo Huang, Zhiheng Hu, Zidong Yang, Zili Wang, Ziqi Ren, Zixin Zhang, Zixuan Wang</div>
<div class="meta-line">First: 2026-02-11T07:53:51+00:00 · Latest: 2026-02-23T16:07:40+00:00</div>
<div class="meta-line">Comments: Technical report for Step 3.5 Flash</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10604v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.10604v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Step 3.5 Flash：以110亿活跃参数开启前沿级智能</div>
<div class="mono" style="margin-top:8px">我们推出Step 3.5 Flash，这是一个稀疏专家混合模型，旨在连接前沿级智能体智能与计算效率。我们聚焦于构建智能体最关键的要素：敏锐的推理能力与快速可靠的执行能力。该模型采用1960亿参数基础架构配合110亿活跃参数以实现高效推理，并通过3:1交错滑动窗口/全局注意力机制与多令牌预测技术优化，显著降低多轮智能体交互的延迟与成本。为达到前沿智能水平，我们设计了可扩展的强化学习框架，将可验证信号与偏好反馈相结合，同时确保在大规模离线策略训练下的稳定性，从而在数学、代码和工具使用领域实现持续自我改进。Step 3.5 Flash在智能体、编程和数学任务中表现卓越：IMO-AnswerBench达85.4%、LiveCodeBench-v6（2024.08-2025.05）达86.4%、tau2-Bench达88.2%、BrowseComp（含上下文管理）达69.0%、Terminal-Bench 2.0达51.0%，性能媲美GPT-5.2 xHigh与Gemini 3.0 Pro等前沿模型。通过重新定义效率边界，Step 3.5 Flash为在真实工业环境中部署复杂智能体提供了高密度基础架构。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Step 3.5 Flash, a sparse Mixture-of-Experts model motivated by the need to combine frontier-level agentic intelligence with computational efficiency for real-world deployment. Its method employs a 196B-parameter foundation with only 11B active parameters for inference, optimized through interleaved sliding-window/full attention and Multi-Token Prediction to reduce latency, and uses a scalable reinforcement learning framework that integrates verifiable signals and preference feedback for stable self-improvement. The main experimental results show strong performance across diverse benchmarks, achieving scores such as 85.4% on IMO-AnswerBench and 86.4% on LiveCodeBench-v6, which are comparable to frontier models like GPT-5.2 xHigh and Gemini 3.0 Pro.</div>
<div class="mono" style="margin-top:8px">本文介绍了Step 3.5 Flash，一个稀疏专家混合模型，其动机是为在实际工业环境中部署智能体，需将前沿水平的智能与计算效率相结合。方法上，它采用了一个1960亿参数的基础模型，但推理时仅激活110亿参数，并通过交错滑动窗口/全注意力机制与多令牌预测进行优化以降低延迟，同时设计了一个结合可验证信号与偏好反馈的可扩展强化学习框架，以实现稳定的自我改进。主要实验结果表明，该模型在多个智能体、编程和数学基准测试中表现强劲，如在IMO-AnswerBench上达到85.4%，在LiveCodeBench-v6上达到86.4%，性能与GPT-5.2 xHigh和Gemini 3.0 Pro等前沿模型相当。</div>
</details>
</div>
<div class="card">
<div class="title">Think2SQL: Reinforce LLM Reasoning Capabilities for Text2SQL</div>
<div class="meta-line">Authors: Simone Papicchio, Simone Rossi, Luca Cagliero, Paolo Papotti</div>
<div class="meta-line">First: 2025-04-21T13:05:26+00:00 · Latest: 2026-02-23T15:30:05+00:00</div>
<div class="meta-line">Comments: 26 pages, work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.15077v4">Abs</a> · <a href="https://arxiv.org/pdf/2504.15077v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Language Models (LLMs) have advanced the state-of-the-art in Text-to-SQL, robust reasoning in complex, multi-table environments remains a bottleneck for parameter-efficient models. This paper presents a systematic empirical study on injecting reasoning capabilities into Text-to-SQL through the lens of Reinforcement Learning with Verifiable Rewards (RLVR). We uncover a critical interplay between reward density, advantage scaling, and model capacity. Our analysis yields four primary insights. First, we propose a novel execution-guided dense reward function that significantly outperforms binary signals and existing state-of-the-art rewards by providing granular feedback at the instance level. Second, we analyze the mechanics of advantage calculation, demonstrating that while large models thrive on sparse signals with aggressive advantage scaling, smaller models require dense rewards and conservative scaling to improve Text-to-SQL performance. Third, we evaluate the impact of cold start, showing that distillation does not always improve RLVR performance and that supervised, fine-tuned models are prone to distributional mimicry. Fourth, we map the Pareto frontier of training efficiency, providing insights for optimizing Text-to-SQL reasoning under computational constraints. Our findings culminate in the Think2SQL family: our 4B-parameter model demonstrates reasoning capabilities competitive with state-of-the-art models such as o3. We release our models, datasets, and code to create a blueprint for RLVR optimization in Text-to-SQL at https://anonymous.4open.science/r/Think2SQL-3B7F.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Think2SQL：增强大型语言模型在Text2SQL中的推理能力</div>
<div class="mono" style="margin-top:8px">尽管大型语言模型（LLMs）在Text-to-SQL领域取得了先进成果，但在复杂多表环境中实现稳健推理仍是参数高效模型的一个瓶颈。本文通过可验证奖励的强化学习（RLVR）视角，对注入推理能力进行了系统性实证研究。我们揭示了奖励密度、优势缩放与模型容量之间的关键相互作用。分析得出四项主要见解：首先，提出一种新颖的执行引导密集奖励函数，通过在实例层面提供细粒度反馈，显著优于二元信号及现有先进奖励方法。其次，分析了优势计算机制，表明大模型能在稀疏信号和激进优势缩放下表现优异，而小模型则需要密集奖励和保守缩放以提升Text-to-SQL性能。第三，评估了冷启动的影响，显示知识蒸馏并不总能改善RLVR性能，且监督微调模型易陷入分布模仿。第四，绘制了训练效率的帕累托前沿，为计算约束下优化Text-to-SQL推理提供了见解。这些发现最终形成了Think2SQL系列：我们的40亿参数模型展现出与o3等先进模型相竞争的推理能力。我们在https://anonymous.4open.science/r/Think2SQL-3B7F公开了模型、数据集和代码，为Text-to-SQL中的RLVR优化提供了蓝图。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of enhancing reasoning capabilities in parameter-efficient Large Language Models (LLMs) for complex Text-to-SQL tasks, motivated by the bottleneck in robust reasoning within multi-table environments. The method involves a systematic empirical study using Reinforcement Learning with Verifiable Rewards (RLVR), focusing on the interplay between reward density, advantage scaling, and model capacity. Key experimental results include the proposal of a novel execution-guided dense reward function that outperforms existing rewards, an analysis showing that smaller models benefit from dense rewards with conservative scaling while larger models excel with sparse signals, and the introduction of the Think2SQL family, where a 4B-parameter model achieves reasoning capabilities competitive with state-of-the-art models like o3.</div>
<div class="mono" style="margin-top:8px">本文针对参数高效的大型语言模型在复杂文本到SQL任务中推理能力不足的问题展开研究，其动机在于多表环境下稳健推理的瓶颈。方法上，通过基于可验证奖励的强化学习进行系统性实证研究，重点关注奖励密度、优势缩放和模型容量之间的相互作用。主要实验结果包括：提出了一种新颖的执行引导密集奖励函数，其性能优于现有奖励；分析表明较小模型需要密集奖励和保守缩放以提升性能，而较大模型则能利用稀疏信号；并推出了Think2SQL模型系列，其中40亿参数模型展现出与o3等先进模型相竞争的推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Sparse Masked Attention Policies for Reliable Generalization</div>
<div class="meta-line">Authors: Caroline Horsch, Laurens Engwegen, Max Weltevrede, Matthijs T. J. Spaan, Wendelin Böhmer</div>
<div class="meta-line">First: 2026-02-23T15:23:17+00:00 · Latest: 2026-02-23T15:23:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19956v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19956v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In reinforcement learning, abstraction methods that remove unnecessary information from the observation are commonly used to learn policies which generalize better to unseen tasks. However, these methods often overlook a crucial weakness: the function which extracts the reduced-information representation has unknown generalization ability in unseen observations. In this paper, we address this problem by presenting an information removal method which more reliably generalizes to new states. We accomplish this by using a learned masking function which operates on, and is integrated with, the attention weights within an attention-based policy network. We demonstrate that our method significantly improves policy generalization to unseen tasks in the Procgen benchmark compared to standard PPO and masking approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>稀疏掩码注意力策略实现可靠泛化</div>
<div class="mono" style="margin-top:8px">在强化学习中，常采用从观测中剔除不必要信息的抽象方法，以学习能更好泛化至未见任务的策略。然而，这些方法常忽视一个关键缺陷：提取降维表征的函数在未见观测中的泛化能力未知。本文通过提出一种能更可靠地泛化至新状态的信息剔除方法来解决此问题。该方法利用一个学习得到的掩码函数，该函数在基于注意力的策略网络内部对注意力权重进行操作并与之集成。实验表明，在Procgen基准测试中，相较于标准PPO及掩码方法，我们的方法显著提升了策略对未见任务的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need to improve the generalization of reinforcement learning policies to unseen tasks, noting that existing abstraction methods for reducing observation information often fail because the representation extraction function itself may not generalize well to new states. To address this, the authors propose a method that integrates a learned masking function directly into the attention weights of an attention-based policy network, thereby selectively removing unnecessary information in a more reliable manner. Experimental results on the Procgen benchmark show that this approach significantly enhances policy generalization compared to standard PPO and other masking techniques.</div>
<div class="mono" style="margin-top:8px">本文的动机在于提升强化学习策略在未见任务上的泛化能力，指出现有通过抽象化减少观测信息的方法常因表征提取函数本身在新状态下泛化能力未知而存在缺陷。为此，作者提出一种方法，将学习的掩码函数直接集成到基于注意力的策略网络的注意力权重中，从而更可靠地选择性移除不必要信息。在Procgen基准测试中的实验结果表明，该方法相比标准PPO及其他掩码方法，显著改善了策略的泛化性能。</div>
</details>
</div>
<div class="card">
<div class="title">Janus-Q: End-to-End Event-Driven Trading via Hierarchical-Gated Reward Modeling</div>
<div class="meta-line">Authors: Xiang Li, Zikai Wei, Yiyan Qi, Wanyun Zhou, Xiang Liu, Penglei Sun, Yongqi Zhang, Xiaowen Chu</div>
<div class="meta-line">First: 2026-02-23T14:58:51+00:00 · Latest: 2026-02-23T14:58:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19919v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19919v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Financial market movements are often driven by discrete financial events conveyed through news, whose impacts are heterogeneous, abrupt, and difficult to capture under purely numerical prediction objectives. These limitations have motivated growing interest in using textual information as the primary source of trading signals in learning-based systems. Two key challenges hinder existing approaches: (1) the absence of large-scale, event-centric datasets that jointly model news semantics and statistically grounded market reactions, and (2) the misalignment between language model reasoning and financially valid trading behavior under dynamic market conditions. To address these challenges, we propose Janus-Q, an end-to-end event-driven trading framework that elevates financial news events from auxiliary signals to primary decision units. Janus-Q unifies event-centric data construction and model optimization under a two-stage paradigm. Stage I focuses on event-centric data construction, building a large-scale financial news event dataset comprising 62,400 articles annotated with 10 fine-grained event types, associated stocks, sentiment labels, and event-driven cumulative abnormal return (CAR). Stage II performs decision-oriented fine-tuning, combining supervised learning with reinforcement learning guided by a Hierarchical Gated Reward Model (HGRM), which explicitly captures trade-offs among multiple trading objectives. Extensive experiments demonstrate that Janus-Q achieves more consistent, interpretable, and profitable trading decisions than market indices and LLM baselines, improving the Sharpe Ratio by up to 102.0% while increasing direction accuracy by over 17.5% compared to the strongest competing strategies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Janus-Q：基于分层门控奖励建模的端到端事件驱动交易框架</div>
<div class="mono" style="margin-top:8px">金融市场波动常由新闻传递的离散金融事件驱动，其影响具有异质性、突发性，且难以通过纯数值预测目标捕捉。这些局限性促使学界日益关注将文本信息作为基于学习的交易系统中主要信号源。现有方法面临两大挑战：(1) 缺乏大规模、以事件为中心、能同时建模新闻语义与统计基础市场反应的数据集；(2) 语言模型推理与动态市场条件下金融有效交易行为之间的错位。为此，我们提出Janus-Q——一个端到端的事件驱动交易框架，将金融新闻事件从辅助信号提升为核心决策单元。Janus-Q通过两阶段范式统一事件中心数据构建与模型优化：第一阶段聚焦事件中心数据构建，创建包含62,400篇文章的大规模金融新闻事件数据集，标注10种细粒度事件类型、关联股票、情感标签及事件驱动的累积异常收益（CAR）；第二阶段进行决策导向微调，结合监督学习与由分层门控奖励模型（HGRM）引导的强化学习，显式捕捉多交易目标间的权衡。大量实验表明，Janus-Q相比市场指数和LLM基线能实现更一致、可解释且盈利的交易决策，相较于最强竞争策略，夏普比率最高提升102.0%，方向预测准确率提高超17.5%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the difficulty of capturing the heterogeneous and abrupt impacts of financial news events using purely numerical prediction, this paper proposes Janus-Q, an end-to-end event-driven trading framework that treats news events as primary decision units. The method involves a two-stage paradigm: first constructing a large-scale dataset of 62,400 news articles annotated with event types, stocks, sentiment, and event-driven abnormal returns, and then fine-tuning a model via supervised and reinforcement learning guided by a Hierarchical Gated Reward Model (HGRM) to align trading decisions with multiple financial objectives. Experimental results show that Janus-Q outperforms market indices and LLM baselines, improving the Sharpe Ratio by up to 102.0% and increasing direction accuracy by over 17.5%, leading to more consistent, interpretable, and profitable trading decisions.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，纯粹基于数值预测难以捕捉金融新闻事件对市场产生的异质性和突发性影响，因此提出了Janus-Q，一个端到端的事件驱动交易框架，将新闻事件作为主要决策单元。该方法采用两阶段范式：首先构建一个包含62,400篇新闻文章的大规模数据集，标注了事件类型、关联股票、情感标签和事件驱动的累计异常收益；然后通过监督学习和强化学习，在分层门控奖励模型（HGRM）的指导下对模型进行决策导向的微调，以平衡多个交易目标。主要实验结果表明，Janus-Q相比市场指数和大型语言模型基线策略，实现了更一致、可解释且盈利的交易决策，夏普比率最高提升102.0%，方向准确性提高超过17.5%。</div>
</details>
</div>
<div class="card">
<div class="title">Uncertainty-Aware Rank-One MIMO Q Network Framework for Accelerated Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Thanh Nguyen, Tung Luu, Tri Ton, Sungwoong Kim, Chang D. Yoo</div>
<div class="meta-line">Venue: IEEE Access, vol. 12, pp. 100972-100982, 2024</div>
<div class="meta-line">First: 2026-02-23T14:57:52+00:00 · Latest: 2026-02-23T14:57:52+00:00</div>
<div class="meta-line">Comments: 10 pages, 4 Figures, IEEE Access</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19917v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19917v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline reinforcement learning (RL) has garnered significant interest due to its safe and easily scalable paradigm. However, training under this paradigm presents its own challenge: the extrapolation error stemming from out-of-distribution (OOD) data. Existing methodologies have endeavored to address this issue through means like penalizing OOD Q-values or imposing similarity constraints on the learned policy and the behavior policy. Nonetheless, these approaches are often beset by limitations such as being overly conservative in utilizing OOD data, imprecise OOD data characterization, and significant computational overhead. To address these challenges, this paper introduces an Uncertainty-Aware Rank-One Multi-Input Multi-Output (MIMO) Q Network framework. The framework aims to enhance Offline Reinforcement Learning by fully leveraging the potential of OOD data while still ensuring efficiency in the learning process. Specifically, the framework quantifies data uncertainty and harnesses it in the training losses, aiming to train a policy that maximizes the lower confidence bound of the corresponding Q-function. Furthermore, a Rank-One MIMO architecture is introduced to model the uncertainty-aware Q-function, \TP{offering the same ability for uncertainty quantification as an ensemble of networks but with a cost nearly equivalent to that of a single network}. Consequently, this framework strikes a harmonious balance between precision, speed, and memory efficiency, culminating in improved overall performance. Extensive experimentation on the D4RL benchmark demonstrates that the framework attains state-of-the-art performance while remaining computationally efficient. By incorporating the concept of uncertainty quantification, our framework offers a promising avenue to alleviate extrapolation errors and enhance the efficiency of offline RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向加速离线强化学习的不确定性感知秩一MIMO Q网络框架</div>
<div class="mono" style="margin-top:8px">离线强化学习因其安全且易于扩展的特性受到广泛关注，但其训练面临分布外数据导致的泛化误差挑战。现有方法通过惩罚OOD Q值或约束策略相似性来应对，但常存在OOD数据利用过于保守、表征不精确、计算开销大等局限。本文提出一种不确定性感知秩一多输入多输出Q网络框架，旨在充分挖掘OOD数据潜力并保障学习效率。该框架通过量化数据不确定性并将其融入训练损失，以最大化Q函数置信下界为目标优化策略；同时采用秩一MIMO架构建模不确定性感知Q函数，在保持与集成网络相当的不确定性量化能力下，实现接近单网络的成本。实验表明，该框架在D4RL基准测试中达到最优性能，并在精度、速度与内存效率间取得平衡，为缓解泛化误差、提升离线强化学习效率提供了新途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of extrapolation error from out-of-distribution (OOD) data in offline reinforcement learning, where existing methods are often overly conservative, imprecise, or computationally expensive. The authors propose an Uncertainty-Aware Rank-One MIMO Q Network framework that quantifies data uncertainty and incorporates it into training losses to learn a policy maximizing the lower confidence bound of the Q-function, using a Rank-One MIMO architecture to model the uncertainty-aware Q-function with efficiency comparable to a single network. Experimental results on the D4RL benchmark show that the framework achieves state-of-the-art performance while maintaining computational efficiency, effectively balancing precision, speed, and memory usage.</div>
<div class="mono" style="margin-top:8px">本文针对离线强化学习中因分布外数据导致的泛化误差问题，现有方法常存在过于保守、表征不精确或计算开销大的局限。作者提出了一种不确定性感知的秩一多输入多输出Q网络框架，通过量化数据不确定性并将其融入训练损失，以学习最大化Q函数置信下界的策略，并采用秩一MIMO架构以接近单网络的成本建模不确定性感知的Q函数。在D4RL基准上的大量实验表明，该框架在保持计算高效的同时实现了最先进的性能，有效平衡了精度、速度和内存效率。</div>
</details>
</div>
<div class="card">
<div class="title">DSDR: Dual-Scale Diversity Regularization for Exploration in LLM Reasoning</div>
<div class="meta-line">Authors: Zhongwei Wan, Yun Shen, Zhihao Dou, Donghao Zhou, Yu Zhang, Xin Wang, Hui Shen, Jing Xiong, Chaofan Tao, Zixuan Zhong, Peizhou Huang, Mi Zhang</div>
<div class="meta-line">First: 2026-02-23T14:37:01+00:00 · Latest: 2026-02-23T14:37:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19895v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19895v1">PDF</a> · <a href="https://github.com/SUSTechBruce/DSDR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiers (RLVR) is a central paradigm for improving large language model (LLM) reasoning, yet existing methods often suffer from limited exploration. Policies tend to collapse onto a few reasoning patterns and prematurely stop deep exploration, while conventional entropy regularization introduces only local stochasticity and fails to induce meaningful path-level diversity, leading to weak and unstable learning signals in group-based policy optimization. We propose DSDR, a Dual-Scale Diversity Regularization reinforcement learning framework that decomposes diversity in LLM reasoning into global and coupling components. Globally, DSDR promotes diversity among correct reasoning trajectories to explore distinct solution modes. Locally, it applies a length-invariant, token-level entropy regularization restricted to correct trajectories, preventing entropy collapse within each mode while preserving correctness. The two scales are coupled through a global-to-local allocation mechanism that emphasizes local regularization for more distinctive correct trajectories. We provide theoretical support showing that DSDR preserves optimal correctness under bounded regularization, sustains informative learning signals in group-based optimization, and yields a principled global-to-local coupling rule. Experiments on multiple reasoning benchmarks demonstrate consistent improvements in accuracy and pass@k, highlighting the importance of dual-scale diversity for deep exploration in RLVR. Code is available at https://github.com/SUSTechBruce/DSDR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DSDR：面向大语言模型推理探索的双尺度多样性正则化</div>
<div class="mono" style="margin-top:8px">基于验证器的强化学习（RLVR）是提升大语言模型（LLM）推理能力的核心范式，但现有方法常受限于探索不足。策略易坍缩至少数推理模式并过早终止深度探索，而传统熵正则化仅引入局部随机性，无法实现有意义的路径级多样性，导致基于群体的策略优化信号弱且不稳定。本文提出DSDR，一种双尺度多样性正则化强化学习框架，将LLM推理中的多样性分解为全局与耦合分量。全局层面，DSDR促进正确推理轨迹间的多样性以探索不同求解模式；局部层面，它采用长度不变、限于正确轨迹的词元级熵正则化，防止各模式内熵坍缩同时保持正确性。两尺度通过全局到局部的分配机制耦合，对更具区分度的正确轨迹强化局部正则化。理论分析表明，DSDR在有界正则化下保持最优正确性，在群体优化中维持信息丰富的学习信号，并导出原则性的全局-局部耦合规则。在多项推理基准测试上的实验显示，该方法在准确率和pass@k指标上均取得稳定提升，印证了双尺度多样性对RLVR深度探索的重要性。代码发布于https://github.com/SUSTechBruce/DSDR。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limited exploration issue in reinforcement learning with verifiers (RLVR) for large language model reasoning, where existing methods often converge prematurely on few reasoning patterns and fail to achieve meaningful path-level diversity. To overcome this, the authors propose DSDR, a Dual-Scale Diversity Regularization framework that decomposes diversity into global and local components: globally, it encourages diversity among correct reasoning trajectories to explore distinct solution modes, and locally, it applies token-level entropy regularization restricted to correct trajectories to prevent entropy collapse within each mode while preserving correctness, with a coupling mechanism that allocates regularization emphasis based on trajectory distinctiveness. Theoretically, DSDR is shown to preserve optimal correctness under bounded regularization and sustain informative learning signals, and experiments on multiple reasoning benchmarks demonstrate consistent improvements in accuracy and pass@k metrics, highlighting the effectiveness of dual-scale diversity for deep exploration in RLVR.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型推理中基于验证器的强化学习方法存在的探索不足问题，即现有方法易过早收敛于少数推理模式而缺乏路径级多样性，提出了DSDR（双尺度多样性正则化）框架。该方法将多样性分解为全局和局部两个层面：全局层面促进正确推理轨迹间的多样性以探索不同的解决方案模式，局部层面则在正确轨迹上应用令牌级熵正则化以防止各模式内的熵崩溃并保持正确性，两者通过一个基于轨迹独特性分配正则化强度的耦合机制连接。理论分析表明DSDR能在有界正则化下保持最优正确性并提供稳定的学习信号，在多个推理基准测试上的实验结果显示其在准确率和pass@k指标上均取得一致提升，验证了双尺度多样性对深度探索的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Deep Learning: Our Miraculous Year 1990-1991</div>
<div class="meta-line">Authors: Juergen Schmidhuber</div>
<div class="meta-line">First: 2020-05-12T13:16:30+00:00 · Latest: 2026-02-23T14:21:58+00:00</div>
<div class="meta-line">Comments: 52 pages, over 300 references, 38 illustrations, extending v1 of 4 Oct 2019</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2005.05744v5">Abs</a> · <a href="https://arxiv.org/pdf/2005.05744v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Deep Learning Artificial Neural Networks (NNs) of our team have revolutionised Machine Learning &amp; AI. Many of the basic ideas behind this revolution were published within the 12 months of our &quot;Annus Mirabilis&quot; 1990-1991 at our lab in TU Munich. Back then, few people were interested. But a quarter century later, NNs based on our &quot;Miraculous Year&quot; were on over 3 billion devices, and used many billions of times per day, consuming a significant fraction of the world&#x27;s compute. In particular, in 1990-91, we laid foundations of Generative AI, publishing principles of (1) Generative Adversarial Networks for Artificial Curiosity and Creativity (now used for deepfakes), (2) Transformers (the T in ChatGPT - see the 1991 Unnormalized Linear Transformer), (3) Pre-training for deep NNs (see the P in ChatGPT), (4) NN distillation (key for DeepSeek), and (5) recurrent World Models for Reinforcement Learning and Planning in partially observable environments. The year 1991 also marks the emergence of the defining features of (6) LSTM, the most cited AI paper of the 20th century (based on deep residual learning and constant error flow through residual NN connections), and (7) the most cited paper of the 21st century, based on our LSTM-inspired Highway Net that was 10 times deeper than previous feedforward NNs. As of 2025, the two most frequently cited scientific articles of all time (with the most Google Scholar citations within 3 years - manuals excluded) are both directly based on our 1991 work.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度学习：我们的奇迹之年1990-1991</div>
<div class="mono" style="margin-top:8px">我们团队的深度学习人工神经网络（NNs）彻底改变了机器学习与人工智能。这场革命背后的许多核心思想，于1990-1991年我们在慕尼黑工业大学的实验室里，在短短12个月的“奇迹之年”期间发表。当时鲜有人关注。但四分之一个世纪后，基于我们“奇迹之年”成果的神经网络已部署在超过30亿台设备上，每日使用数百亿次，消耗了全球相当一部分的计算资源。具体而言，在1990-91年间，我们奠定了生成式AI的基础，发表了以下原理：（1）用于人工好奇心与创造力的生成对抗网络（现用于深度伪造），（2）Transformer（ChatGPT中的“T”——参见1991年的未归一化线性Transformer），（3）深度神经网络的预训练（参见ChatGPT中的“P”），（4）神经网络蒸馏（DeepSeek的关键技术），以及（5）用于部分可观测环境中强化学习与规划的循环世界模型。1991年还标志着（6）LSTM——20世纪被引次数最高的AI论文（基于深度残差学习及通过残差NN连接实现的恒定误差流）——决定性特征的诞生，以及（7）21世纪被引次数最高的论文，该论文基于我们受LSTM启发、深度比前馈神经网络深10倍的高速网络。截至2025年，有史以来被引频率最高的两篇科学文献（排除手册类，指三年内谷歌学术引用数最高）均直接基于我们1991年的工作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the unrecognized foundational work in neural networks during the early 1990s, this paper highlights the team&#x27;s pivotal contributions from that period. The method involves detailing the core principles established in 1990-1991, including Generative Adversarial Networks, Transformers, pre-training, neural network distillation, recurrent World Models, LSTM, and Highway Nets. The main experimental results, as evidenced by widespread adoption, show that these ideas now power over 3 billion devices daily and form the basis for the most cited AI papers of the 20th and 21st centuries, demonstrating their transformative impact on modern AI systems like ChatGPT and DeepSeek.</div>
<div class="mono" style="margin-top:8px">本文的动机源于团队在1990年代早期在神经网络领域未被充分认识的基础性工作。方法上，论文详细阐述了1990-1991年间确立的核心原理，包括生成对抗网络、Transformer、预训练、神经网络蒸馏、循环世界模型、LSTM和高速公路网络。主要的实验结果，通过广泛采用得以证明，显示这些理念如今每日驱动超过30亿台设备，并构成了20世纪和21世纪被引用最多的AI论文的基础，彰显了它们对ChatGPT和DeepSeek等现代AI系统的变革性影响。</div>
</details>
</div>
<div class="card">
<div class="title">Meta-Learning and Meta-Reinforcement Learning - Tracing the Path towards DeepMind&#x27;s Adaptive Agent</div>
<div class="meta-line">Authors: Björn Hoppmann, Christoph Scholz</div>
<div class="meta-line">First: 2026-02-23T13:39:58+00:00 · Latest: 2026-02-23T13:39:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19837v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19837v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans are highly effective at utilizing prior knowledge to adapt to novel tasks, a capability that standard machine learning models struggle to replicate due to their reliance on task-specific training. Meta-learning overcomes this limitation by allowing models to acquire transferable knowledge from various tasks, enabling rapid adaptation to new challenges with minimal data. This survey provides a rigorous, task-based formalization of meta-learning and meta-reinforcement learning and uses that paradigm to chronicle the landmark algorithms that paved the way for DeepMind&#x27;s Adaptive Agent, consolidating the essential concepts needed to understand the Adaptive Agent and other generalist approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>元学习与元强化学习——追溯通往DeepMind自适应智能体的演进路径</div>
<div class="mono" style="margin-top:8px">人类擅长运用先验知识适应新任务，而标准机器学习模型因依赖任务特定训练难以复现此能力。元学习通过让模型从多任务中获取可迁移知识，突破这一局限，使其能以极少数据快速适应新挑战。本文基于任务形式对元学习与元强化学习进行系统梳理，并以此框架追溯为DeepMind自适应智能体奠定基础的里程碑算法，整合理解该智能体及其他通用方法所需的核心概念。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to replicate human-like adaptability in AI, where standard models fail to transfer knowledge across tasks, this survey formalizes meta-learning and meta-reinforcement learning as frameworks for acquiring transferable skills from diverse tasks. The method involves a task-based paradigm to systematically trace key algorithms that enable rapid adaptation with minimal data, culminating in the development of DeepMind&#x27;s Adaptive Agent. The main experimental results highlighted are the consolidation of essential concepts from these landmark algorithms, demonstrating how they underpin generalist agents capable of efficient learning in novel scenarios.</div>
<div class="mono" style="margin-top:8px">受人类利用先验知识快速适应新任务的能力启发，针对标准机器学习模型依赖任务特定训练而难以迁移知识的局限，本文通过任务化形式化元学习和元强化学习框架，以系统追溯关键算法路径。方法上采用基于任务的范式，梳理了从多样任务中获取可迁移知识以实现小数据快速适应的算法演进，最终导向DeepMind自适应智能体的构建。主要实验成果集中于整合这些里程碑算法的核心概念，阐明了它们如何支撑通用型智能体在新环境中高效学习。</div>
</details>
</div>
<div class="card">
<div class="title">$O(1/k)$ Finite-Time Bound for Non-Linear Two-Time-Scale Stochastic Approximation</div>
<div class="meta-line">Authors: Siddharth Chandak</div>
<div class="meta-line">First: 2025-04-27T22:45:00+00:00 · Latest: 2026-02-23T13:35:08+00:00</div>
<div class="meta-line">Comments: Submitted to IEEE Transactions on Automatic Control</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.19375v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.19375v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Two-time-scale stochastic approximation (SA) is an algorithm with coupled iterations which has found broad applications in reinforcement learning, optimization and game control. In this work, we derive mean squared error bounds for non-linear two-time-scale iterations with contractive mappings. In the setting where both stepsizes are order $Θ(1/k)$, commonly referred to as single time-scale SA with multiple coupled sequences, we obtain the first $O(1/k)$ rate without imposing additional smoothness assumptions. In the setting with true time-scale separation, the previous best bound was $O(1/k^{2/3})$. We improve this to $O(1/k^a)$ for any $a&lt;1$ approaching the optimal $O(1/k)$ rate. The key step in our analysis involves rewriting the original iteration in terms of an averaged noise sequence whose variance decays sufficiently fast. Additionally, we use an induction-based approach to show that the iterates are bounded in expectation. Our results apply to Polyak averaging, as well as to algorithms from reinforcement learning, and optimization, including gradient descent-ascent and two-time-scale Lagrangian optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>非线性双时间尺度随机逼近的$O(1/k)$有限时间界</div>
<div class="mono" style="margin-top:8px">双时间尺度随机逼近（SA）是一种具有耦合迭代的算法，在强化学习、优化和博弈控制中具有广泛应用。本文推导了具有压缩映射的非线性双时间尺度迭代的均方误差界。在两种步长均为$Θ(1/k)$量级的设定下（通常称为多耦合序列的单时间尺度SA），我们在不附加光滑性假设的条件下首次获得$O(1/k)$收敛速率。在真实时间尺度分离的设定中，先前最佳界为$O(1/k^{2/3})$，我们将其改进为对任意$a&lt;1$的$O(1/k^a)$，逼近最优的$O(1/k)$速率。分析的关键步骤是将原始迭代重写为方差衰减足够快的平均噪声序列形式。此外，我们采用基于归纳法的方法证明迭代值在期望意义下有界。本结果适用于Polyak平均法，以及强化学习、优化领域的算法，包括梯度下降-上升法和双时间尺度拉格朗日优化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper aims to improve the convergence rate analysis for non-linear two-time-scale stochastic approximation (SA), a widely used algorithm in reinforcement learning and optimization. The key methodological innovation involves rewriting the iterative updates using an averaged noise sequence with fast-decaying variance and employing an induction-based proof to bound the iterates. The main results establish a mean squared error bound of O(1/k) for single time-scale settings with coupled sequences, and a rate of O(1/k^a) for any a&lt;1 in true two-time-scale settings, significantly improving upon the prior best bound of O(1/k^{2/3}) and approaching the optimal rate.</div>
<div class="mono" style="margin-top:8px">本文旨在改进非线性双时间尺度随机逼近算法的收敛速率分析，该算法广泛应用于强化学习和优化领域。方法上的核心创新在于使用方差快速衰减的平均噪声序列重写迭代更新，并采用基于归纳法的证明来约束迭代值。主要实验结果表明，在具有耦合序列的单时间尺度设置中，获得了O(1/k)的均方误差界；在真正的双时间尺度设置中，获得了任意a&lt;1的O(1/k^a)速率，显著优于先前最佳的O(1/k^{2/3})界，并接近最优速率。</div>
</details>
</div>
<div class="card">
<div class="title">Analysis of Off-Policy $n$-Step TD-Learning with Linear Function Approximation</div>
<div class="meta-line">Authors: Han-Dong Lim, Donghwan Lee</div>
<div class="meta-line">First: 2025-02-13T03:43:13+00:00 · Latest: 2026-02-23T11:04:45+00:00</div>
<div class="meta-line">Comments: Added experiments for n-step PVI and n-step TD convergence/divergence</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.08941v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.08941v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper analyzes multi-step temporal difference (TD)-learning algorithms within the ``deadly triad&#x27;&#x27; scenario, characterized by linear function approximation, off-policy learning, and bootstrapping. In particular, we prove that $n$-step TD-learning algorithms converge to a solution as the sampling horizon $n$ increases sufficiently. The paper is divided into two parts. In the first part, we comprehensively examine the fundamental properties of their model-based deterministic counterparts, including projected value iteration, gradient descent algorithms, which can be viewed as prototype deterministic algorithms whose analysis plays a pivotal role in understanding and developing their model-free reinforcement learning counterparts. In particular, we prove that these algorithms converge to meaningful solutions when $n$ is sufficiently large. Based on these findings, in the second part, two $n$-step TD-learning algorithms are proposed and analyzed, which can be seen as the model-free reinforcement learning counterparts of the model-based deterministic algorithms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于线性函数逼近的离策略$n$步时序差分学习分析</div>
<div class="mono" style="margin-top:8px">本文在由线性函数逼近、离策略学习和自举构成的“致命三要素”场景下，分析了多步时序差分学习算法。特别地，我们证明了当采样步长$n$充分增大时，$n$步时序差分学习算法会收敛至一个解。论文分为两部分：第一部分系统研究了其基于模型的确定性对应算法（包括投影值迭代、梯度下降算法）的基本性质，这些算法可视为原型确定性算法，其分析对理解和开发对应的无模型强化学习算法具有关键作用。我们证明了当$n$充分大时，这些算法会收敛至有意义的解。基于这些发现，第二部分提出并分析了两种$n$步时序差分学习算法，它们可视为前述基于模型的确定性算法在无模型强化学习中的对应实现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the challenge of ensuring convergence in reinforcement learning under the &#x27;deadly triad&#x27; of off-policy learning, linear function approximation, and bootstrapping. The method involves a two-part analysis: first, examining model-based deterministic algorithms like projected value iteration to establish convergence properties when the sampling horizon n is sufficiently large, and second, proposing and analyzing two corresponding model-free n-step TD-learning algorithms based on these insights. The main experimental results, as noted in the venue comments, demonstrate both convergence and divergence behaviors for n-step projected value iteration and n-step TD, validating the theoretical findings that increasing n can lead to stable solutions.</div>
<div class="mono" style="margin-top:8px">本文的研究动机源于在离策略学习、线性函数逼近和自举这&#x27;致命三要素&#x27;下确保强化学习收敛性的挑战。方法包括两部分分析：首先研究基于模型的确定性算法（如投影值迭代），以证明当采样步长n足够大时这些算法能收敛到有意义的解；其次，基于这些发现，提出并分析了两种对应的无模型n步时序差分学习算法。主要实验结果，如会议评论所示，展示了n步投影值迭代和n步时序差分算法的收敛与发散行为，验证了理论发现，即增加n可以导向稳定解。</div>
</details>
</div>
<div class="card">
<div class="title">What Matters for Simulation to Online Reinforcement Learning on Real Robots</div>
<div class="meta-line">Authors: Yarden As, Dhruva Tirumala, René Zurbrügg, Chenhao Li, Stelian Coros, Andreas Krause, Markus Wulfmeier</div>
<div class="meta-line">First: 2026-02-23T10:34:15+00:00 · Latest: 2026-02-23T10:34:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20220v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20220v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate what specific design choices enable successful online reinforcement learning (RL) on physical robots. Across 100 real-world training runs on three distinct robotic platforms, we systematically ablate algorithmic, systems, and experimental decisions that are typically left implicit in prior work. We find that some widely used defaults can be harmful, while a set of robust, readily adopted design choices within standard RL practice yield stable learning across tasks and hardware. These results provide the first large-sample empirical study of such design choices, enabling practitioners to deploy online RL with lower engineering effort.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>仿真对真实机器人线上强化学习的关键影响因素</div>
<div class="mono" style="margin-top:8px">本研究探讨了实现物理机器人成功线上强化学习（RL）的具体设计选择。通过在三个不同机器人平台上进行的100次真实世界训练实验，我们系统性地分析了以往研究中通常隐含的算法、系统及实验决策。研究发现，某些广泛采用的默认设置可能产生负面影响，而标准RL实践中一组稳健且易于实施的设计选择能在不同任务和硬件上实现稳定的学习效果。该成果首次对此类设计选择进行了大样本实证研究，有助于从业者以更低的工程成本部署线上强化学习系统。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the key design choices that enable successful online reinforcement learning (RL) on physical robots, motivated by the need to reduce the high engineering effort typically required for real-world deployment. The method involves systematically ablating algorithmic, systems, and experimental decisions across 100 real-world training runs on three distinct robotic platforms to identify robust practices. The main experimental results reveal that some widely used defaults can be harmful, while a specific set of design choices within standard RL practice yields stable learning across diverse tasks and hardware, providing a large-sample empirical guide for practitioners.</div>
<div class="mono" style="margin-top:8px">本文研究了在物理机器人上实现成功在线强化学习的关键设计选择，其动机在于降低现实世界部署通常所需的高昂工程成本。方法包括在三个不同机器人平台上进行100次真实世界训练，系统性地消融算法、系统和实验决策，以识别稳健的实践方案。主要实验结果表明，一些广泛使用的默认设置可能有害，而标准强化学习实践中的一组特定设计选择能在不同任务和硬件上实现稳定学习，为从业者提供了基于大样本实证的指导。</div>
</details>
</div>
<div class="card">
<div class="title">SpinGPT: A Large-Language-Model Approach to Playing Poker Correctly</div>
<div class="meta-line">Authors: Narada Maugin, Tristan Cazenave</div>
<div class="meta-line">First: 2025-09-26T14:15:44+00:00 · Latest: 2026-02-23T10:05:41+00:00</div>
<div class="meta-line">Comments: Accepted at Advances in Computer Games (ACG) 2025, LNCS (Springer)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.22387v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.22387v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Counterfactual Regret Minimization (CFR) algorithm and its variants have enabled the development of pokerbots capable of beating the best human players in heads-up (1v1) cash games and competing with them in six-player formats. However, CFR&#x27;s computational complexity rises exponentially with the number of players. Furthermore, in games with three or more players, following Nash equilibrium no longer guarantees a non-losing outcome. These limitations, along with others, significantly restrict the applicability of CFR to the most popular formats: tournaments. Motivated by the recent success of Large Language Models (LLM) in chess and Diplomacy, we present SpinGPT, the first LLM tailored to Spin &amp; Go, a popular three-player online poker format. SpinGPT is trained in two stages: (1) Supervised Fine-Tuning on 320k high-stakes expert decisions; (2) Reinforcement Learning on 270k solver-generated hands. Our results show that SpinGPT matches the solver&#x27;s actions in 78% of decisions (tolerant accuracy). With a simple deep-stack heuristic, it achieves 13.4 +/- 12.9 BB/100 versus Slumbot in heads-up over 30,000 hands (95% CI). These results suggest that LLMs could be a new way to deal with multi-player imperfect-information games like poker.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpinGPT：基于大语言模型的正确扑克玩法研究</div>
<div class="mono" style="margin-top:8px">反事实遗憾最小化（CFR）算法及其变体已推动开发出能在单挑现金局击败顶尖人类玩家、并在六人局与之抗衡的扑克机器人。然而，CFR的计算复杂度随玩家数量呈指数级增长，且在三人及以上对局中遵循纳什均衡不再保证非负收益。这些局限显著制约了CFR在最流行赛制——锦标赛中的应用。受大语言模型（LLM）近期在象棋与外交博弈中成功的启发，我们提出首个专为流行三人线上扑克模式“Spin &amp; Go”设计的LLM模型SpinGPT。该模型采用两阶段训练：（1）基于32万手高额专家决策的监督微调；（2）基于27万手求解器生成牌局的强化学习。结果显示：SpinGPT在容忍准确率下与求解器决策吻合度达78%；通过简单深筹码启发式策略，在与Slumbot的3万手单挑测试中取得13.4±12.9 BB/100的胜率（95%置信区间）。这表明LLM可能为扑克等多玩家非完全信息博弈提供新解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the computational limitations of Counterfactual Regret Minimization (CFR) in multi-player poker and the success of LLMs in strategic games, this paper introduces SpinGPT, a large language model designed for the three-player Spin &amp; Go poker format. The method involves a two-stage training process: first, supervised fine-tuning on expert decisions, followed by reinforcement learning on solver-generated data. Experimental results demonstrate that SpinGPT matches a solver&#x27;s actions with 78% tolerant accuracy and, using a deep-stack heuristic, achieves a win rate of 13.4 ± 12.9 BB/100 against Slumbot in heads-up play, indicating LLMs&#x27; potential for multi-player imperfect-information games.</div>
<div class="mono" style="margin-top:8px">本文的动机是应对反事实遗憾最小化算法在多玩家扑克中的计算局限性，并借鉴大型语言模型在策略游戏中的成功，提出了SpinGPT，一个专为三人制Spin &amp; Go扑克设计的大型语言模型。方法采用两阶段训练：首先在专家决策上进行监督微调，然后在求解器生成的手牌上进行强化学习。实验结果表明，SpinGPT以78%的容忍准确率匹配求解器动作，并通过简单的深筹码启发式方法，在单挑对决中对Slumbot取得了13.4 ± 12.9 BB/100的胜率，这显示了大型语言模型在处理多人非完美信息游戏方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle</div>
<div class="meta-line">Authors: Linghao Zhu, Yiran Guan, Dingkang Liang, Jianzhong Ju, Zhenbo Luo, Bin Qin, Jian Luan, Yuliang Liu, Xiang Bai</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-08-07T17:53:47+00:00 · Latest: 2026-02-23T09:33:32+00:00</div>
<div class="meta-line">Comments: This paper has been accepted by ICLR 2026 Project page at: https://xenozlh.github.io/Shuffle-R1/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.05612v5">Abs</a> · <a href="https://arxiv.org/pdf/2508.05612v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://xenozlh.github.io/Shuffle-R1/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has emerged as an effective post-training paradigm for enhancing the reasoning capabilities of multimodal large language model (MLLM). However, current RL pipelines often suffer from training inefficiencies caused by two underexplored issues: Advantage Collapsing, where most advantages in a batch concentrate near zero, and Rollout Silencing, where the proportion of rollouts contributing non-zero gradients diminishes over time. These issues lead to suboptimal gradient updates and hinder long-term learning efficiency. To address these issues, we propose Shuffle-R1, a simple yet principled framework that improves RL fine-tuning efficiency by dynamically restructuring trajectory sampling and batch composition. It introduces (1) Pairwise Trajectory Sampling, which selects high-contrast trajectories with large advantages to improve gradient signal quality, and (2) Advantage-based Trajectory Shuffle, which increases exposure of valuable rollouts through informed batch reshuffling. Experiments across multiple reasoning benchmarks show that our framework consistently outperforms strong RL baselines with minimal overhead. These results highlight the importance of data-centric adaptations for more efficient RL training in MLLM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Shuffle-R1：基于数据动态重排的多模态大语言模型高效强化学习框架</div>
<div class="mono" style="margin-top:8px">强化学习已成为提升多模态大语言模型推理能力的有效后训练范式。然而，当前强化学习流程常受两个未充分探讨问题导致的训练低效困扰：优势坍缩（批次中多数优势值趋近于零）与轨迹沉寂（贡献非零梯度的轨迹比例随时间递减）。这些问题导致梯度更新次优并阻碍长期学习效率。为此，我们提出Shuffle-R1——一种通过动态重组轨迹采样与批次构建来提升强化学习微调效率的简洁理论框架。该框架引入：（1）配对轨迹采样：选取具有显著优势差异的高对比度轨迹以提升梯度信号质量；（2）基于优势的轨迹重排：通过智能批次重组增加高价值轨迹的曝光度。在多推理基准测试中，本框架以最小开销持续超越主流强化学习基线方法，凸显了数据中心化适配对提升多模态大语言模型强化学习训练效率的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses inefficiencies in reinforcement learning (RL) fine-tuning for multimodal large language models (MLLMs), specifically the problems of Advantage Collapsing and Rollout Silencing, which lead to poor gradient updates and reduced learning efficiency. To solve this, the authors propose Shuffle-R1, a data-centric framework that dynamically restructures trajectory sampling and batch composition through Pairwise Trajectory Sampling to select high-contrast trajectories and Advantage-based Trajectory Shuffle to increase exposure of valuable rollouts. Experimental results across multiple reasoning benchmarks demonstrate that Shuffle-R1 consistently outperforms strong RL baselines with minimal overhead, highlighting the effectiveness of data-centric adaptations for efficient RL training in MLLMs.</div>
<div class="mono" style="margin-top:8px">本文针对多模态大语言模型（MLLM）强化学习（RL）微调中的效率低下问题，特别是优势坍缩和轨迹静默这两个导致梯度更新不佳和学习效率降低的未充分探索问题。为解决这些问题，作者提出了Shuffle-R1，这是一个以数据为中心的框架，通过成对轨迹采样选择高对比度轨迹，以及基于优势的轨迹重排来增加有价值轨迹的曝光，从而动态重组轨迹采样和批次组成。在多个推理基准测试上的实验结果表明，Shuffle-R1以最小开销持续优于强RL基线，凸显了以数据为中心的调整对于MLLM高效RL训练的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">QiMeng-CodeV-R1: Reasoning-Enhanced Verilog Generation</div>
<div class="meta-line">Authors: Yaoyu Zhu, Di Huang, Hanqi Lyu, Xiaoyun Zhang, Chongxiao Li, Wenxuan Shi, Yutong Wu, Jianan Mu, Jinghua Wang, Yang Zhao, Pengwei Jin, Shuyao Cheng, Shengwen Liang, Xishan Zhang, Rui Zhang, Zidong Du, Qi Guo, Xing Hu, Yunji Chen</div>
<div class="meta-line">First: 2025-05-30T03:51:06+00:00 · Latest: 2026-02-23T09:24:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.24183v5">Abs</a> · <a href="https://arxiv.org/pdf/2505.24183v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) trained via reinforcement learning with verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit, automatable verification, such as software programming and mathematical problems. Extending RLVR to electronic design automation (EDA), especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and accurate verification environments, the scarcity of high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To this end, we introduce CodeV-R1, an RLVR framework for training Verilog generation LLMs. First, we develop a rule-based testbench generator that performs robust equivalence checking against golden references. Second, we propose a round-trip data synthesis method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies code-NL-code consistency via the generated testbench, and filters out inequivalent examples to yield a high-quality dataset. Third, we employ a two-stage &quot;distill-then-RL&quot; training pipeline: distillation for the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior state-of-the-art by 12~20%, while even exceeding the performance of 671B DeepSeek-R1 on RTLLM. We have released our model, training code, and dataset to facilitate research in EDA and LLM communities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>启梦-CodeV-R1：推理增强的Verilog生成</div>
<div class="mono" style="margin-top:8px">通过可验证奖励强化学习（RLVR）训练的大语言模型（LLM）已在具有明确、可自动化验证的任务（如软件编程和数学问题）上取得突破。然而，将RLVR扩展到电子设计自动化（EDA）领域，特别是从自然语言（NL）规范自动生成硬件描述语言（HDL，如Verilog），面临三个关键挑战：缺乏自动化且准确的验证环境、高质量NL-代码对稀缺，以及RLVR的计算成本过高。为此，我们提出了CodeV-R1，一个用于训练Verilog生成LLM的RLVR框架。首先，我们开发了一个基于规则的测试平台生成器，能够对黄金参考进行鲁棒的等价性检查。其次，我们提出了一种往返数据合成方法，将开源Verilog代码片段与LLM生成的NL描述配对，通过生成的测试平台验证代码-NL-代码一致性，并过滤不等价样本以生成高质量数据集。第三，我们采用两阶段“蒸馏后强化学习”训练流程：通过蒸馏启动推理能力，随后采用自适应DAPO（我们提出的新型RLVR算法），该算法可通过自适应调整采样率降低训练成本。最终模型CodeV-R1-7B在VerilogEval v2和RTLLM v1.1上分别达到68.6%和72.9%的pass@1，较先前最优性能提升12~20%，甚至在RTLLM上超越了671B参数的DeepSeek-R1。我们已开源模型、训练代码和数据集，以促进EDA和LLM领域的研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of applying reinforcement learning with verifiable reward (RLVR) to electronic design automation, specifically for generating Verilog code from natural language specifications, which is hindered by the lack of automated verification, scarce high-quality data, and high computational costs. The authors propose CodeV-R1, a framework that includes a rule-based testbench generator for equivalence checking, a round-trip data synthesis method to create and filter high-quality NL-code pairs, and a two-stage training pipeline combining distillation with a novel adaptive RLVR algorithm called DAPO to reduce costs. Experimental results show that the resulting 7B model, CodeV-R1-7B, achieves 68.6% pass@1 on VerilogEval v2 and 72.9% on RTLLM v1.1, outperforming prior state-of-the-art by 12-20% and even exceeding a much larger 671B model on RTLLM.</div>
<div class="mono" style="margin-top:8px">本文针对将可验证奖励的强化学习应用于电子设计自动化，特别是从自然语言规范生成Verilog代码的挑战，该领域存在自动化验证环境缺乏、高质量数据稀缺和计算成本高昂三大难题。作者提出了CodeV-R1框架，包括基于规则的测试平台生成器用于等价性检查，一种往返数据合成方法以创建和筛选高质量的自然语言-代码对，以及结合蒸馏与新型自适应RLVR算法DAPO的两阶段训练流程以降低成本。实验结果表明，所得的7B模型CodeV-R1-7B在VerilogEval v2上达到68.6%的pass@1，在RTLLM v1.1上达到72.9%，比先前最优方法提升了12-20%，甚至在RTLLM上超越了更大的671B模型性能。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion Alignment as Variational Expectation-Maximization</div>
<div class="meta-line">Authors: Jaewoo Lee, Minsu Kim, Sanghyeok Choi, Inhyuck Song, Sujin Yun, Hyeongyu Kang, Woocheol Shin, Taeyoung Yun, Kiyoung Om, Jinkyoo Park</div>
<div class="meta-line">First: 2025-10-01T04:34:07+00:00 · Latest: 2026-02-23T08:58:39+00:00</div>
<div class="meta-line">Comments: 32 pages, 11 figures, 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.00502v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.00502v2">PDF</a> · <a href="https://github.com/Jaewoopudding/dav">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion alignment aims to optimize diffusion models for the downstream objective. While existing methods based on reinforcement learning or direct backpropagation achieve considerable success in maximizing rewards, they often suffer from reward over-optimization and mode collapse. We introduce Diffusion Alignment as Variational Expectation-Maximization (DAV), a framework that formulates diffusion alignment as an iterative process alternating between two complementary phases: the E-step and the M-step. In the E-step, we employ test-time search to generate diverse and reward-aligned samples. In the M-step, we refine the diffusion model using samples discovered by the E-step. We demonstrate that DAV can optimize reward while preserving diversity for both continuous and discrete tasks: text-to-image synthesis and DNA sequence design. Our code is available at https://github.com/Jaewoopudding/dav.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散对齐作为变分期望最大化</div>
<div class="mono" style="margin-top:8px">扩散对齐旨在针对下游目标优化扩散模型。尽管现有基于强化学习或直接反向传播的方法在最大化奖励方面取得显著成功，但它们常面临奖励过优化和模式坍塌问题。本文提出扩散对齐作为变分期望最大化（DAV）框架，将扩散对齐形式化为交替进行两个互补阶段的迭代过程：E步与M步。在E步中，我们采用测试时搜索生成多样化且与奖励对齐的样本；在M步中，利用E步发现的样本精调扩散模型。实验证明DAV能在连续与离散任务（文本到图像合成与DNA序列设计）中优化奖励的同时保持多样性。代码发布于https://github.com/Jaewoopudding/dav。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the problem of reward over-optimization and mode collapse in aligning diffusion models with downstream objectives. The authors propose Diffusion Alignment as Variational Expectation-Maximization (DAV), a framework that treats alignment as an iterative E-step and M-step process: the E-step uses test-time search to generate diverse, reward-aligned samples, and the M-step refines the diffusion model using these samples. Experimental results on text-to-image synthesis and DNA sequence design demonstrate that DAV effectively optimizes rewards while maintaining sample diversity for both continuous and discrete tasks.</div>
<div class="mono" style="margin-top:8px">本文针对扩散模型与下游目标对齐中存在的奖励过优化和模式崩溃问题，提出了一个名为DAV的变分期望最大化对齐框架。该方法将对齐过程构建为一个交替进行的E步和M步迭代：E步通过测试时搜索生成多样且与奖励对齐的样本，M步则利用这些样本优化扩散模型。在文本到图像合成和DNA序列设计任务上的实验结果表明，DAV能够在优化奖励的同时，有效保持连续和离散任务的样本多样性。</div>
</details>
</div>
<div class="card">
<div class="title">ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training</div>
<div class="meta-line">Authors: Rushuai Yang, Hecheng Wang, Chiming Liu, Xiaohan Yan, Yunlong Wang, Xuan Du, Shuoyu Yue, Yongcheng Liu, Chuheng Zhang, Lizhe Qi, Yi Chen, Wei Shan, Maoqing Yao</div>
<div class="meta-line">First: 2026-02-13T07:46:37+00:00 · Latest: 2026-02-23T08:56:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12691v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.12691v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study how to improve large foundation vision-language-action (VLA) systems through online reinforcement learning (RL) in real-world settings. Central to this process is the value function, which provides learning signals to guide VLA learning from experience. In practice, the value function is estimated from trajectory fragments collected from different data sources, including historical policies and intermittent human interventions. Estimating the value function of current behavior quality from the mixture data is inherently an off-policy evaluation problem. However, prior work often adopts conservative on-policy estimation for stability, which avoids direct evaluation of the current high-capacity policy and limits learning effectiveness. In this paper, we propose ALOE, an action-level off-policy evaluation framework for VLA post-training. ALOE applies chunking-based temporal-difference bootstrapping to evaluate individual action sequences instead of predicting final task outcomes. This design improves effective credit assignment to critical action chunks under sparse rewards and supports stable policy improvement. We evaluate our method on three real-world manipulation tasks, including smartphone packing as a high-precision task, laundry folding as a long-horizon deformable-object task, and bimanual pick-and-place involving multi-object perception. Across all tasks, ALOE improves learning efficiency without compromising execution speed, showing that off-policy RL can be reintroduced in a reliable manner for real-world VLA post-training. Videos and additional materials are available at our project website.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ALOE：面向视觉-语言-动作模型后训练的动作级离策略评估</div>
<div class="mono" style="margin-top:8px">本研究探讨如何在真实场景中通过在线强化学习提升大型基础视觉-语言-动作系统。价值函数作为核心组件，通过提供学习信号引导VLA从经验中学习。实践中，价值函数需从多源轨迹片段（包括历史策略与间歇性人工干预数据）中估算，这本质上是离策略评估问题。现有方法常为稳定性采用保守的同策略估计，避免直接评估当前高容量策略，限制了学习效能。本文提出ALOE——面向VLA后训练的动作级离策略评估框架，通过基于分块的时序差分自举法评估动作序列而非预测最终任务结果。该设计能在稀疏奖励下提升关键动作块的信用分配效率，支持稳定策略改进。我们在三项真实世界操作任务中验证方法：高精度智能手机封装、长周期可变形物体衣物折叠、以及需多目标感知的双臂抓放任务。所有任务均表明ALOE在保持执行速度的同时提升学习效率，证明离策略强化学习能以可靠方式重新应用于真实世界VLA后训练。演示视频与补充材料详见项目网站。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of improving large vision-language-action (VLA) models through online reinforcement learning in real-world settings, where learning signals from value functions are typically estimated conservatively using on-policy methods, limiting effectiveness. The authors propose ALOE, an action-level off-policy evaluation framework that employs chunking-based temporal-difference bootstrapping to assess individual action sequences rather than final task outcomes, enabling better credit assignment under sparse rewards and stable policy improvement. Experimental results on three real-world manipulation tasks—smartphone packing, laundry folding, and bimanual pick-and-place—demonstrate that ALOE enhances learning efficiency without sacrificing execution speed, reliably reintroducing off-policy RL for VLA post-training.</div>
<div class="mono" style="margin-top:8px">本文旨在解决在现实环境中通过在线强化学习改进大型视觉-语言-动作模型所面临的挑战，其中价值函数的学习信号通常采用保守的在线策略方法进行估计，从而限制了学习效果。作者提出了ALOE，一种动作级别的离线策略评估框架，它采用基于分块的时序差分自举来评估单个动作序列而非最终任务结果，从而在稀疏奖励下实现更好的信用分配和稳定的策略改进。在智能手机包装、衣物折叠和双手抓放三个现实世界操作任务上的实验结果表明，ALOE在不影响执行速度的情况下提高了学习效率，为VLA后训练可靠地重新引入了离线策略强化学习。</div>
</details>
</div>
<div class="card">
<div class="title">Unifying Evolutionary Prompt Search and Reinforcement Learning for LLM Self-Improvement</div>
<div class="meta-line">Authors: Lunjun Zhang, Ryan Chen, Bradly C. Stadie</div>
<div class="meta-line">First: 2026-02-16T12:34:27+00:00 · Latest: 2026-02-23T08:43:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14697v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.14697v2">PDF</a> · <a href="https://github.com/LunjunZhang/E-SPL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Building agentic systems that can autonomously self-improve from experience is a longstanding goal of AI. Large language models (LLMs) today primarily self-improve via two mechanisms: self-reflection for context updates, and reinforcement learning (RL) for weight updates. In this work, we propose Evolutionary System Prompt Learning (E-SPL), a method for jointly improving model contexts and model weights. In each RL iteration, E-SPL samples trajectories under multiple system prompts in parallel. It applies RL updates to LLM weights conditioned on system prompts, and evolutionary updates to system prompts via mutation and crossover, two genetic operators based on LLM self-reflection. Each system prompt is assigned a TrueSkill rating for evolutionary selection, updated from relative performance within each RL iteration. E-SPL encourages a natural division between declarative knowledge encoded in prompts and procedural knowledge encoded in weights, resulting in improved performance across reasoning and agentic tasks. For instance, in an easy-to-hard (AIME $\rightarrow$ BeyondAIME) generalization setting, E-SPL improves RL success rate from 38.8% $\rightarrow$ 45.1% while also outperforming reflective prompt evolution (40.0%). Overall, our results demonstrate that RL and evolutionary prompt search are deeply synergistic, and unifying the two yields consistent gains in sample efficiency and generalization. Code: https://github.com/LunjunZhang/E-SPL</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>统一进化提示搜索与强化学习实现大语言模型自我改进</div>
<div class="mono" style="margin-top:8px">构建能够从经验中自主进化的智能体系统是人工智能的长期目标。当前大语言模型主要通过两种机制实现自我改进：基于自我反思的上下文更新，以及基于强化学习的权重更新。本研究提出进化系统提示学习方法，通过协同优化模型上下文与模型权重实现改进。在每轮强化学习迭代中，该方法并行采样多组系统提示下的轨迹数据：基于系统提示条件对模型权重进行强化学习更新，同时利用大语言模型自我反思驱动的变异与交叉两种遗传算子对系统提示进行进化更新。每个系统提示通过每轮迭代内的相对表现更新其TrueSkill评级，以支持进化选择。该方法促使陈述性知识自然编码于提示中，而程序性知识编码于权重中，从而在推理与智能体任务中实现性能提升。例如在易到难的泛化场景中，该方法将强化学习成功率从38.8%提升至45.1%，同时优于反思式提示进化方法。研究结果表明强化学习与进化提示搜索具有深度协同效应，二者的统一能在样本效率与泛化能力方面获得持续增益。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the goal of creating autonomous AI systems that self-improve, this paper introduces Evolutionary System Prompt Learning (E-SPL), a method that unifies evolutionary prompt search and reinforcement learning (RL) to jointly optimize the context (via system prompts) and the model weights of a large language model. The method operates by sampling trajectories under multiple system prompts in parallel during RL iterations, applying RL updates to weights conditioned on prompts and using evolutionary operators like mutation and crossover—guided by LLM self-reflection—to update the prompts, with a TrueSkill rating system facilitating selection based on relative performance. Experimental results show that E-SPL improves performance on reasoning and agentic tasks, notably increasing RL success rates from 38.8% to 45.1% in an easy-to-hard generalization setting and outperforming reflective prompt evolution alone, demonstrating synergistic gains in sample efficiency and generalization.</div>
<div class="mono" style="margin-top:8px">本研究旨在构建能够自主从经验中自我改进的AI智能体系统，提出了进化系统提示学习（E-SPL）方法，该方法将进化提示搜索与强化学习（RL）相结合，以共同优化大语言模型的上下文（通过系统提示）和模型权重。该方法在每次RL迭代中并行采样多个系统提示下的轨迹，对基于提示的条件权重进行RL更新，并利用基于模型自反思的突变和交叉等进化算子来更新提示，同时采用TrueSkill评分系统根据相对表现进行选择。实验结果表明，E-SPL在推理和智能体任务上提升了性能，特别是在从易到难的泛化设置中，将RL成功率从38.8%提高至45.1%，并超越了单纯的反思提示进化方法，证明了二者在样本效率和泛化能力上的协同增益。</div>
</details>
</div>
<div class="card">
<div class="title">Advantage-based Temporal Attack in Reinforcement Learning</div>
<div class="meta-line">Authors: Shenghong He</div>
<div class="meta-line">First: 2026-02-23T08:08:23+00:00 · Latest: 2026-02-23T08:08:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19582v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19582v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Extensive research demonstrates that Deep Reinforcement Learning (DRL) models are susceptible to adversarially constructed inputs (i.e., adversarial examples), which can mislead the agent to take suboptimal or unsafe actions. Recent methods improve attack effectiveness by leveraging future rewards to guide adversarial perturbation generation over sequential time steps (i.e., reward-based attacks). However, these methods are unable to capture dependencies between different time steps in the perturbation generation process, resulting in a weak temporal correlation between the current perturbation and previous perturbations.In this paper, we propose a novel method called Advantage-based Adversarial Transformer (AAT), which can generate adversarial examples with stronger temporal correlations (i.e., time-correlated adversarial examples) to improve the attack performance. AAT employs a multi-scale causal self-attention (MSCSA) mechanism to dynamically capture dependencies between historical information from different time periods and the current state, thus enhancing the correlation between the current perturbation and the previous perturbation. Moreover, AAT introduces a weighted advantage mechanism, which quantifies the effectiveness of a perturbation in a given state and guides the generation process toward high-performance adversarial examples by sampling high-advantage regions. Extensive experiments demonstrate that the performance of AAT matches or surpasses mainstream adversarial attack baselines on Atari, DeepMind Control Suite and Google football tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习中的优势时序攻击</div>
<div class="mono" style="margin-top:8px">大量研究表明，深度强化学习（DRL）模型易受对抗性构造输入（即对抗样本）的影响，这些输入可能误导智能体采取次优或不安全的行动。近期方法通过利用未来奖励来指导序列时间步上的对抗扰动生成（即基于奖励的攻击），从而提升攻击效果。然而，这些方法在扰动生成过程中未能捕捉不同时间步之间的依赖关系，导致当前扰动与先前扰动之间的时序相关性较弱。本文提出一种名为基于优势的对抗变换器（AAT）的新方法，能够生成具有更强时序相关性（即时序相关对抗样本）的对抗样本，以提升攻击性能。AAT采用多尺度因果自注意力（MSCSA）机制，动态捕捉不同时间段历史信息与当前状态之间的依赖关系，从而增强当前扰动与先前扰动的相关性。此外，AAT引入加权优势机制，量化扰动在给定状态下的有效性，并通过采样高优势区域引导生成过程朝向高性能对抗样本。大量实验表明，AAT在Atari、DeepMind Control Suite和Google足球任务上的性能匹配或超越了主流对抗攻击基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing reward-based adversarial attacks in reinforcement learning, which fail to capture temporal dependencies between perturbations across sequential time steps, this paper introduces the Advantage-based Adversarial Transformer (AAT) method. AAT employs a multi-scale causal self-attention mechanism to dynamically model historical dependencies, enhancing temporal correlation in perturbation generation, and incorporates a weighted advantage mechanism to prioritize high-impact perturbations. Experimental results on Atari, DeepMind Control Suite, and Google Football tasks show that AAT matches or surpasses mainstream adversarial attack baselines in performance.</div>
<div class="mono" style="margin-top:8px">针对现有基于奖励的强化学习对抗攻击方法无法捕捉时序扰动间依赖关系的局限性，本文提出了优势对抗变换器方法。该方法采用多尺度因果自注意力机制动态建模历史依赖，增强扰动生成的时序相关性，并引入加权优势机制以优先生成高效扰动。在Atari、DeepMind Control Suite和Google Football任务上的实验结果表明，该方法的性能达到或超越了主流对抗攻击基线。</div>
</details>
</div>
<div class="card">
<div class="title">Cross-lingual Collapse: How Language-Centric Foundation Models Shape Reasoning in Large Language Models</div>
<div class="meta-line">Authors: Cheonbok Park, Jeonghoon Kim, Joosung Lee, Sanghwan Bae, Jaegul Choo, Kang Min Yoo</div>
<div class="meta-line">First: 2025-06-06T08:08:48+00:00 · Latest: 2026-02-23T08:02:48+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.05850v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.05850v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable reward (RLVR) has been instrumental in eliciting strong reasoning capabilities from large language models (LLMs) via long chains of thought (CoT). During RLVR training, we formalize and systemically study an empirical phenomenon whereby a multilingual model&#x27;s CoT reverts to its dominant pre-training language (e.g., English) even when prompted in another language, which we term Cross-lingual Collapse. Because the long-CoT regime magnifies exposure to linguistic priors, the underlying trade-off between maximizing reasoning depth and preserving target-language fidelity has remained under-characterized. To examine this trade-off, we train LLMs with Group-Relative Policy Optimization (GRPO) on translated versions of math datasets widely used to elicit long-CoT reasoning. Throughout training, we track both task accuracy and the language consistency of reasoning chains. Our experiments yield three findings: (i) under RLVR, CoT in LLMs systematically drifts toward the pre-training dominant language as reasoning performance rises; (ii) English-centric priors, long-CoT GRPO optimization, task difficulty, and high-entropy decoding jointly amplify this drift, and the pattern persists beyond mathematics; and (iii) interventions that favor target-language traces--via a language-consistency reward, decoding-time controls, or more balanced backbones--mitigate collapse but reveal a persistent performance-fidelity trade-off.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>跨语言坍缩：语言中心化基础模型如何塑造大语言模型的推理能力</div>
<div class="mono" style="margin-top:8px">基于可验证奖励的强化学习（RLVR）通过长思维链（CoT）有效激发了大语言模型（LLM）的强推理能力。在RLVR训练中，我们形式化并系统研究了一个经验现象：多语言模型的CoT即使在被其他语言提示时，仍会回归到其主导预训练语言（如英语），我们称之为跨语言坍缩。由于长CoT机制放大了对语言先验的暴露，最大化推理深度与保持目标语言忠实度之间的权衡关系尚未得到充分刻画。为探究此权衡，我们使用组相对策略优化（GRPO）在广泛用于激发长CoT推理的数学数据集翻译版本上训练LLM。在整个训练过程中，我们同步追踪任务准确率与推理链的语言一致性。实验得出三项发现：（一）在RLVR下，随着推理性能提升，LLM的CoT会系统性漂向预训练主导语言；（二）英语中心先验、长CoT GRPO优化、任务难度及高熵解码共同加剧此漂移，且该模式在数学领域外依然存在；（三）通过语言一致性奖励、解码时控制或更均衡的骨干模型等倾向于目标语言痕迹的干预措施可缓解坍缩，但揭示了性能与忠实度间持久的权衡关系。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates a phenomenon termed &#x27;Cross-lingual Collapse,&#x27; where multilingual large language models (LLMs), when trained with reinforcement learning for verifiable reasoning (RLVR) to produce long chains of thought (CoT), tend to revert their reasoning steps to their dominant pre-training language (e.g., English) even when prompted in another language. The motivation stems from the need to understand the trade-off between maximizing reasoning depth and preserving target-language fidelity during such training. The method involves training LLMs using Group-Relative Policy Optimization (GRPO) on translated math datasets to elicit long-CoT reasoning while tracking both task accuracy and language consistency. The main experimental results show that CoT systematically drifts toward the pre-training language as reasoning performance improves, a drift amplified by English-centric priors, long-CoT optimization, task difficulty, and high-entropy decoding, with interventions like language-consistency rewards mitigating collapse but revealing a persistent performance-fidelity trade-off.</div>
<div class="mono" style="margin-top:8px">本文研究了一种称为&#x27;跨语言坍缩&#x27;的现象，即多语言大语言模型在使用强化学习进行可验证推理训练以产生长思维链时，即使被提示使用其他语言，其推理步骤也倾向于回退到其主导的预训练语言。研究动机源于需要理解在此类训练中最大化推理深度与保持目标语言保真度之间的权衡。方法包括使用组相对策略优化在翻译的数学数据集上训练大语言模型以引发长思维链推理，同时追踪任务准确性和语言一致性。主要实验结果表明，随着推理性能提升，思维链会系统性地向预训练语言漂移，这种漂移被以英语为中心的先验、长思维链优化、任务难度和高熵解码所放大，而通过语言一致性奖励等干预措施可以缓解坍缩，但揭示出性能与保真度之间持续的权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Goal-Oriented Influence-Maximizing Data Acquisition for Learning and Optimization</div>
<div class="meta-line">Authors: Weichi Yao, Bianca Dumitrascu, Bryan R. Goldsmith, Yixin Wang</div>
<div class="meta-line">First: 2026-02-23T07:57:11+00:00 · Latest: 2026-02-23T07:57:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19578v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19578v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Active data acquisition is central to many learning and optimization tasks in deep neural networks, yet remains challenging because most approaches rely on predictive uncertainty estimates that are difficult to obtain reliably. To this end, we propose Goal-Oriented Influence- Maximizing Data Acquisition (GOIMDA), an active acquisition algorithm that avoids explicit posterior inference while remaining uncertainty-aware through inverse curvature. GOIMDA selects inputs by maximizing their expected influence on a user-specified goal functional, such as test loss, predictive entropy, or the value of an optimizer-recommended design. Leveraging first-order influence functions, we derive a tractable acquisition rule that combines the goal gradient, training-loss curvature, and candidate sensitivity to model parameters. We show theoretically that, for generalized linear models, GOIMDA approximates predictive-entropy minimization up to a correction term accounting for goal alignment and prediction bias, thereby, yielding uncertainty-aware behavior without maintaining a Bayesian posterior. Empirically, across learning tasks (including image and text classification) and optimization tasks (including noisy global optimization benchmarks and neural-network hyperparameter tuning), GOIMDA consistently reaches target performance with substantially fewer labeled samples or function evaluations than uncertainty-based active learning and Gaussian-process Bayesian optimization baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向目标的影响力最大化数据采集用于学习与优化</div>
<div class="mono" style="margin-top:8px">主动数据采集是深度神经网络中许多学习与优化任务的核心，但由于大多数方法依赖于难以可靠获取的预测不确定性估计，该问题仍具挑战性。为此，我们提出面向目标的影响力最大化数据采集算法，该主动采集算法避免显式的后验推断，同时通过逆曲率保持对不确定性的感知。GOIMDA通过最大化用户指定目标函数（如测试损失、预测熵或优化器推荐设计的取值）的期望影响力来选择输入数据。借助一阶影响函数，我们推导出一个可处理的采集规则，该规则结合了目标梯度、训练损失曲率以及候选样本对模型参数的敏感性。理论上我们证明，对于广义线性模型，GOIMDA通过包含目标对齐和预测偏差的修正项，近似实现了预测熵最小化，从而在不维持贝叶斯后验的情况下产生不确定性感知行为。在包括图像与文本分类的学习任务，以及含噪声全局优化基准测试和神经网络超参数调优的优化任务中，实验表明GOIMDA持续以显著少于基于不确定性的主动学习和高斯过程贝叶斯优化基线所需的标注样本或函数评估次数，达到目标性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of active data acquisition in deep learning, where existing methods often depend on unreliable predictive uncertainty estimates. To overcome this, it introduces Goal-Oriented Influence-Maximizing Data Acquisition (GOIMDA), a method that selects data points by maximizing their expected influence on a user-defined goal—such as test loss or optimizer recommendations—using first-order influence functions and inverse curvature, thereby avoiding explicit posterior inference while remaining uncertainty-aware. Theoretically, GOIMDA approximates predictive-entropy minimization for generalized linear models with a correction for goal alignment and bias. Experimentally, it demonstrates superior efficiency across learning and optimization tasks, including image classification and hyperparameter tuning, achieving target performance with significantly fewer labeled samples or evaluations compared to uncertainty-based active learning and Bayesian optimization baselines.</div>
<div class="mono" style="margin-top:8px">本文针对深度学习中的主动数据获取挑战，现有方法常依赖不可靠的预测不确定性估计。为此，提出了目标导向影响最大化数据获取（GOIMDA）方法，该方法通过使用一阶影响函数和逆曲率，选择能最大化对用户定义目标（如测试损失或优化器推荐）预期影响的数据点，从而避免显式后验推断，同时保持不确定性感知。理论上，GOIMDA在广义线性模型中近似于预测熵最小化，并包含目标对齐和偏差的校正项。实验表明，在学习和优化任务（如图像分类和超参数调优）中，GOIMDA相比基于不确定性的主动学习和高斯过程贝叶斯优化基线，能以显著更少的标记样本或评估次数达到目标性能。</div>
</details>
</div>
<div class="card">
<div class="title">Transitive RL: Value Learning via Divide and Conquer</div>
<div class="meta-line">Authors: Seohong Park, Aditya Oberai, Pranav Atreya, Sergey Levine</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-26T03:32:31+00:00 · Latest: 2026-02-23T07:03:22+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.22512v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.22512v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work, we present Transitive Reinforcement Learning (TRL), a new value learning algorithm based on a divide-and-conquer paradigm. TRL is designed for offline goal-conditioned reinforcement learning (GCRL) problems, where the aim is to find a policy that can reach any state from any other state in the smallest number of steps. TRL converts a triangle inequality structure present in GCRL into a practical divide-and-conquer value update rule. This has several advantages compared to alternative value learning paradigms. Compared to temporal difference (TD) methods, TRL suffers less from bias accumulation, as in principle it only requires $O(\log T)$ recursions (as opposed to $O(T)$ in TD learning) to handle a length-$T$ trajectory. Unlike Monte Carlo methods, TRL suffers less from high variance as it performs dynamic programming. Experimentally, we show that TRL achieves the best performance in highly challenging, long-horizon benchmark tasks compared to previous offline GCRL algorithms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>传递式强化学习：基于分治策略的价值学习</div>
<div class="mono" style="margin-top:8px">本研究提出传递式强化学习（TRL），这是一种基于分治范式的新型价值学习算法。TRL专为离线目标条件强化学习（GCRL）问题设计，其目标在于寻找能在最少步数内从任意状态到达其他任意状态的策略。TRL将GCRL中存在的三角不等式结构转化为实用的分治价值更新规则。相较于其他价值学习范式，TRL具有多重优势：与时间差分（TD）方法相比，TRL受偏差累积影响更小——理论上仅需$O(\log T)$次递归（TD学习需$O(T)$次）即可处理长度为$T$的轨迹；与蒙特卡洛方法相比，TRL通过动态规划降低了高方差问题。实验表明，在极具挑战性的长时程基准任务中，TRL相较现有离线GCRL算法取得了最优性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Transitive Reinforcement Learning (TRL), a novel value learning algorithm motivated by the need to improve offline goal-conditioned reinforcement learning (GCRL), which aims to find policies that minimize steps between states. The method leverages a divide-and-conquer paradigm by converting a triangle inequality structure in GCRL into a practical value update rule, reducing bias accumulation compared to temporal difference methods and lowering variance relative to Monte Carlo approaches through dynamic programming. Experimental results demonstrate that TRL outperforms prior offline GCRL algorithms on challenging, long-horizon benchmark tasks.</div>
<div class="mono" style="margin-top:8px">本文提出了传递性强化学习（TRL），这是一种新颖的价值学习算法，其动机在于改进离线目标条件强化学习（GCRL），该领域旨在寻找最小化状态间步数的策略。该方法利用分治范式，将GCRL中的三角不等式结构转化为实用的价值更新规则，与时间差分方法相比减少了偏差累积，并通过动态规划降低了相对于蒙特卡洛方法的高方差。实验结果表明，在具有挑战性的长视野基准任务中，TRL优于先前的离线GCRL算法。</div>
</details>
</div>
<div class="card">
<div class="title">Mixed-Reality Digital Twins: Leveraging the Physical and Virtual Worlds for Hybrid Sim2Real Transition of Multi-Agent Reinforcement Learning Policies</div>
<div class="meta-line">Authors: Chinmay Vilas Samak, Tanmay Vilas Samak, Venkat Narayan Krovi</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2024-03-16T18:47:04+00:00 · Latest: 2026-02-23T06:49:35+00:00</div>
<div class="meta-line">Comments: Accepted in IEEE Robotics and Automation Letters (RA-L) and additionally accepted to be presented at IEEE International Conference on Robotics and Automation (ICRA) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2403.10996v8">Abs</a> · <a href="https://arxiv.org/pdf/2403.10996v8">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent reinforcement learning (MARL) for cyber-physical vehicle systems usually requires a significantly long training time due to their inherent complexity. Furthermore, deploying the trained policies in the real world demands a feature-rich environment along with multiple physical embodied agents, which may not be feasible due to monetary, physical, energy, or safety constraints. This work seeks to address these pain points by presenting a mixed-reality (MR) digital twin (DT) framework capable of: (i) boosting training speeds by selectively scaling parallelized simulation workloads on-demand, and (ii) immersing the MARL policies across hybrid simulation-to-reality (sim2real) experiments. The viability and performance of the proposed framework are highlighted through two representative use cases, which cover cooperative as well as competitive classes of MARL problems. We study the effect of: (i) agent and environment parallelization on training time, and (ii) systematic domain randomization on zero-shot sim2real transfer, across both case studies. Results indicate up to 76.3% reduction in training time with the proposed parallelization scheme and sim2real gap as low as 2.9% using the proposed deployment method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>混合现实数字孪生：融合物理与虚拟世界实现多智能体强化学习策略的混合仿真到现实迁移</div>
<div class="mono" style="margin-top:8px">信息物理车辆系统中的多智能体强化学习（MARL）因其固有复杂性通常需要极长的训练时间。此外，在现实世界中部署训练后的策略需要具备丰富特征的环境及多个物理实体智能体，这可能因成本、物理条件、能源或安全限制而难以实现。本研究提出一种混合现实数字孪生框架以解决这些痛点，该框架能够：（1）通过按需选择性扩展并行化仿真任务来提升训练速度；（2）在混合仿真到现实实验中实现MARL策略的沉浸式部署。通过两个涵盖合作型与竞争型MARL问题的典型用例，验证了所提框架的可行性与性能。我们在两个案例中研究了：（1）智能体与环境并行化对训练时间的影响；（2）系统性领域随机化对零样本仿真到现实迁移的影响。结果表明：采用所提并行化方案可减少高达76.3%的训练时间，而通过所提部署方法实现的仿真到现实性能差距可低至2.9%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenges of lengthy training times and costly real-world deployment for multi-agent reinforcement learning (MARL) in cyber-physical vehicle systems by proposing a mixed-reality digital twin framework. The method accelerates training through on-demand parallelization of simulations and facilitates hybrid simulation-to-reality transitions by immersing policies across virtual and physical domains. Experimental results from cooperative and competitive MARL use cases demonstrate a training time reduction of up to 76.3% with parallelization and a minimal sim2real performance gap of 2.9% using systematic domain randomization.</div>
<div class="mono" style="margin-top:8px">本文针对网络物理车辆系统中多智能体强化学习训练时间长、真实部署成本高的问题，提出了一个混合现实数字孪生框架。该方法通过按需并行化仿真来加速训练，并通过在虚拟与物理域中沉浸策略来实现混合仿真到现实的过渡。在合作与竞争两类多智能体强化学习用例中的实验结果表明，所提并行化方案可将训练时间减少高达76.3%，而采用系统化域随机化的部署方法仅产生2.9%的仿真到现实性能差距。</div>
</details>
</div>
<div class="card">
<div class="title">EnterpriseBench Corecraft: Training Generalizable Agents on High-Fidelity RL Environments</div>
<div class="meta-line">Authors: Sushant Mehta, Logan Ritchie, Suhaas Garre, Ian Niebres, Nick Heiner, Edwin Chen</div>
<div class="meta-line">First: 2026-02-18T04:35:46+00:00 · Latest: 2026-02-23T06:33:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16179v4">Abs</a> · <a href="https://arxiv.org/pdf/2602.16179v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We show that training AI agents on high-fidelity reinforcement learning environments produces capabilities that generalize beyond the training distribution. We introduce CoreCraft, the first environment in EnterpriseBench, Surge AI&#x27;s suite of agentic RL environments. CoreCraft is a fully operational enterprise simulation of a customer support organization, comprising over 2,500 entities across 14 entity types with 23 unique tools, designed to measure whether AI agents can perform the multi-step, domain-specific work that real jobs demand. Frontier models such as GPT-5.2 and Claude Opus 4.6 solve fewer than 30% of tasks when all expert-authored rubric criteria must be satisfied. Using this environment, we train GLM 4.6 with Group Relative Policy Optimization (GRPO) and adaptive clipping. After a single epoch of training, the model improves from 25.37% to 36.76% task pass rate on held-out evaluation tasks. More importantly, these gains transfer to out-of-distribution benchmarks: +4.5% on BFCL Parallel, +7.4% on Tau2-Bench Retail, and +6.8% on Tool Decathlon (Pass@1). We believe three environment properties are consistent with the observed transfer: task-centric world building that optimizes for diverse, challenging tasks; expert-authored rubrics enabling reliable reward computation; and enterprise workflows that reflect realistic professional patterns. Our results suggest that environment quality, diversity, and realism are key factors enabling generalizable agent capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EnterpriseBench Corecraft：在高保真强化学习环境中训练可泛化智能体</div>
<div class="mono" style="margin-top:8px">研究表明，在高保真强化学习环境中训练AI智能体可产生超越训练分布的泛化能力。我们推出CoreCraft——Surge AI智能体强化学习环境套件EnterpriseBench的首个环境。CoreCraft是一个完全可运行的客户支持组织企业仿真系统，包含14种实体类型超过2500个实体及23种独特工具，旨在评估AI智能体能否执行现实岗位所需的多步骤、领域特定工作。当需满足所有专家制定的评估标准时，GPT-5.2和Claude Opus 4.6等前沿模型的任务通过率不足30%。在此环境中，我们采用组相对策略优化（GRPO）与自适应剪裁技术训练GLM 4.6模型。经单轮训练后，模型在保留评估任务上的通过率从25.37%提升至36.76%。更重要的是，这些增益可迁移至分布外基准测试：BFCL Parallel提升4.5%、Tau2-Bench Retail提升7.4%、Tool Decathlon（Pass@1）提升6.8%。我们认为三个环境特性与观察到的迁移效应相符：以任务为中心的世界构建优化了多样性与挑战性；专家制定的评估准则实现了可靠奖励计算；企业工作流反映了真实专业模式。结果表明，环境质量、多样性和真实性是形成可泛化智能体能力的关键因素。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study is motivated by the need to develop AI agents that can generalize beyond their training distribution, particularly for complex enterprise tasks. The method involves training agents on CoreCraft, a high-fidelity reinforcement learning environment that simulates a customer support organization with over 2,500 entities and 23 tools, using GLM 4.6 with Group Relative Policy Optimization and adaptive clipping. Experimental results show that after one training epoch, task pass rates improved from 25.37% to 36.76% on held-out tasks, with significant transfer gains on out-of-distribution benchmarks, such as +4.5% on BFCL Parallel and +6.8% on Tool Decathlon, indicating that environment quality and realism are crucial for generalizable agent capabilities.</div>
<div class="mono" style="margin-top:8px">该研究的动机是开发能够超越训练分布进行泛化的AI智能体，以应对复杂的企业任务。方法包括在CoreCraft这一高保真强化学习环境中训练智能体，该环境模拟了一个拥有超过2500个实体和23种工具的客户支持组织，使用GLM 4.6模型结合组相对策略优化和自适应裁剪技术。实验结果表明，经过一轮训练后，智能体在保留任务上的通过率从25.37%提升至36.76%，并在分布外基准测试中展现出显著的迁移增益，如在BFCL Parallel上提升4.5%，在Tool Decathlon上提升6.8%，这表明环境的质量、多样性和真实性是培养智能体泛化能力的关键因素。</div>
</details>
</div>
<div class="card">
<div class="title">STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens</div>
<div class="meta-line">Authors: Shiqi Liu, Zeyu He, Guojian Zhan, Letian Tao, Zhilong Zheng, Jiang Wu, Yinuo Wang, Yang Guan, Kehua Sheng, Bo Zhang, Keqiang Li, Jingliang Duan, Shengbo Eben Li</div>
<div class="meta-line">First: 2026-02-17T14:46:48+00:00 · Latest: 2026-02-23T06:22:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15620v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.15620v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often suffer from late-stage performance collapse, leading to degraded reasoning quality and unstable training. Our analysis shows that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. We find that training instability can be caused by a tiny fraction of tokens, approximately 0.01%, which we term spurious tokens. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. To mitigate this instability, we design an S2T (silencing spurious tokens) mechanism to efficiently identify spurious tokens through characteristic signals with low probability, low entropy, and positive advantage, and then suppress their gradient perturbations during optimization. Incorporating this mechanism into a group-based objective, we propose Spurious-Token-Aware Policy Optimization (STAPO), which promotes stable and effective large-scale model refinement. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13% ($ρ_{\mathrm{T}}$=1.0, top-p=1.0) and 3.69% ($ρ_{\mathrm{T}}$=0.7, top-p=0.9) over GRPO, 20-Entropy, and JustRL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>STAPO：通过抑制罕见伪标记实现大语言模型强化学习的稳定化</div>
<div class="mono" style="margin-top:8px">强化学习显著提升了大语言模型的推理能力，但现有RL微调方法严重依赖熵正则化与权重调整等启发式技术来维持稳定性，实践中常出现后期性能崩溃，导致推理质量下降与训练不稳定。分析表明，RL中逐标记策略梯度的大小与标记概率及局部策略熵呈负相关。我们发现训练不稳定性可能由约0.01%的极少数标记引发，这类标记被称为伪标记。当伪标记出现在正确响应中时，它们对推理结果的贡献微乎其微，却继承了完整的序列级奖励，导致梯度更新异常放大。为缓解此问题，我们设计了S2T机制，通过低概率、低熵与正优势值的特征信号高效识别伪标记，并在优化过程中抑制其梯度扰动。将该机制融入分组目标函数，我们提出伪标记感知策略优化方法STAPO，可促进大规模模型稳定有效的精调。在基于Qwen 1.7B、8B和14B基础模型的六个数学推理基准测试中，STAPO始终展现出更优的熵稳定性，相比GRPO、20-Entropy和JustRL方法，平均性能分别提升7.13%（$ρ_{\mathrm{T}}$=1.0，top-p=1.0）和3.69%（$ρ_{\mathrm{T}}$=0.7，top-p=0.9）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the instability and performance collapse observed in existing reinforcement learning fine-tuning methods for large language models, which rely on heuristics like entropy regularization. To address this, the authors analyze token-wise policy gradients and identify that a tiny fraction of spurious tokens—characterized by low probability, low entropy, and positive advantage—cause instability by receiving amplified gradients from sequence-level rewards. They propose STAPO, a method that incorporates an S2T mechanism to silence these spurious tokens during optimization, thereby stabilizing training. Experimental results on six mathematical reasoning benchmarks with Qwen models show that STAPO improves entropy stability and achieves average performance gains of 7.13% and 3.69% over baseline methods like GRPO.</div>
<div class="mono" style="margin-top:8px">本文的动机是针对大型语言模型强化学习微调方法中存在的训练不稳定和后期性能崩溃问题，这些方法通常依赖熵正则化等启发式技术。为解决此问题，作者分析了词元级策略梯度，发现极少数的伪词元——具有低概率、低熵和正优势特征——会因继承序列级奖励而导致梯度异常放大，从而引发不稳定。他们提出了STAPO方法，通过集成S2T机制在优化过程中抑制这些伪词元的梯度扰动。在六个数学推理基准测试中使用Qwen模型进行的实验表明，STAPO显著提升了熵稳定性，并相比GRPO等基线方法平均性能提高了7.13%和3.69%。</div>
</details>
</div>
<div class="card">
<div class="title">Cost-Aware Diffusion Active Search</div>
<div class="meta-line">Authors: Arundhati Banerjee, Jeff Schneider</div>
<div class="meta-line">First: 2026-02-23T06:11:51+00:00 · Latest: 2026-02-23T06:11:51+00:00</div>
<div class="meta-line">Comments: In submission</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19538v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19538v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Active search for recovering objects of interest through online, adaptive decision making with autonomous agents requires trading off exploration of unknown environments with exploitation of prior observations in the search space. Prior work has proposed information gain and Thompson sampling based myopic, greedy approaches for agents to actively decide query or search locations when the number of targets is unknown. Decision making algorithms in such partially observable environments have also shown that agents capable of lookahead over a finite horizon outperform myopic policies for active search. Unfortunately, lookahead algorithms typically rely on building a computationally expensive search tree that is simulated and updated based on the agent&#x27;s observations and a model of the environment dynamics. Instead, in this work, we leverage the sequence modeling abilities of diffusion models to sample lookahead action sequences that balance the exploration-exploitation trade-off for active search without building an exhaustive search tree. We identify the optimism bias in prior diffusion based reinforcement learning approaches when applied to the active search setting and propose mitigating solutions for efficient cost-aware decision making with both single and multi-agent teams. Our proposed algorithm outperforms standard baselines in offline reinforcement learning in terms of full recovery rate and is computationally more efficient than tree search in cost-aware active decision making.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>成本感知扩散主动搜索</div>
<div class="mono" style="margin-top:8px">通过自主智能体进行在线自适应决策以恢复感兴趣对象的主动搜索，需要在未知环境的探索与搜索空间中先验观测的利用之间进行权衡。先前研究提出了基于信息增益和汤普森采样的短视贪婪方法，使智能体在目标数量未知时能主动决定查询或搜索位置。此类部分可观测环境中的决策算法也表明，具备有限前瞻能力的智能体在主动搜索中优于短视策略。然而，前瞻算法通常依赖于构建计算成本高昂的搜索树，该树需基于智能体观测和环境动态模型进行模拟更新。本研究转而利用扩散模型的序列建模能力，通过采样前瞻动作序列来平衡主动搜索中的探索-利用权衡，而无需构建穷举式搜索树。我们指出了现有基于扩散的强化学习方法在主动搜索场景中存在的乐观偏差，并提出了针对单智能体与多智能体团队的高效成本感知决策缓解方案。所提算法在离线强化学习中，以完全恢复率为指标优于标准基线方法，且在成本感知主动决策中比树搜索算法计算效率更高。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of active search, where autonomous agents must balance exploration and exploitation to locate targets in unknown environments, particularly when the number of targets is unknown. To overcome the computational inefficiency of traditional lookahead methods like search trees, the authors propose a diffusion model-based approach that samples lookahead action sequences, enabling cost-aware decision-making without exhaustive tree construction. Experimental results demonstrate that this method outperforms standard offline reinforcement learning baselines in full recovery rate and offers greater computational efficiency than tree search in both single and multi-agent settings.</div>
<div class="mono" style="margin-top:8px">本文针对主动搜索中的挑战展开研究，即自主智能体在未知环境中需平衡探索与利用以定位目标，尤其是在目标数量未知的情况下。为解决传统前瞻方法（如搜索树）计算效率低的问题，作者提出了一种基于扩散模型的方法，通过采样前瞻动作序列来实现成本感知的决策，无需构建详尽的搜索树。实验结果表明，该方法在完全恢复率上优于标准的离线强化学习基线，并且在单智能体与多智能体设置中，比树搜索具有更高的计算效率。</div>
</details>
</div>
<div class="card">
<div class="title">Does Your Reasoning Model Implicitly Know When to Stop Thinking?</div>
<div class="meta-line">Authors: Zixuan Huang, Xin Xia, Yuxi Ren, Jianbin Zheng, Xuanda Wang, Zhixia Zhang, Hongyan Xie, Songshi Liang, Zehao Chen, Xuefeng Xiao, Fuzhen Zhuang, Jianxin Li, Yikun Ban, Deqing Wang</div>
<div class="meta-line">First: 2026-02-09T07:38:22+00:00 · Latest: 2026-02-23T05:13:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08354v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08354v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>你的推理模型是否隐含知晓何时停止思考？</div>
<div class="mono" style="margin-top:8px">大型推理模型（LRMs）通过长链思维（CoTs）在复杂推理任务上的能力已显著提升，但该方法常导致大量冗余，损害计算效率并造成实时应用的显著延迟。近期研究表明，更长的推理链常与正确性无关，甚至可能损害准确性。在对这一现象的深入分析中，我们意外发现并实证验证了LRMs隐含知晓何时停止思考的能力，而这一能力被当前采样范式所掩盖。受此启发，我们提出了SAGE（自我感知引导高效推理），一种释放这种高效推理潜力的新型采样范式。此外，将SAGE作为混合采样整合到基于群体的强化学习（SAGE-RL）中，使SAGE-RL能够将SAGE发现的高效推理模式有效融入标准pass@1推理，在多个高难度数学基准测试中显著提升了LRMs的推理准确性和效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the inefficiency and potential inaccuracy of long reasoning chains in large reasoning models, this paper investigates whether models inherently know when to stop reasoning. The method introduces SAGE, a novel sampling paradigm that leverages this implicit capability to guide efficient reasoning, and further integrates it into reinforcement learning as SAGE-RL to enhance standard inference. Experimental results show that SAGE-RL significantly improves both reasoning accuracy and computational efficiency across multiple challenging mathematical benchmarks.</div>
<div class="mono" style="margin-top:8px">本文针对大型推理模型中长思维链导致的效率低下和准确性降低问题，探究了模型是否隐含知道何时停止推理。方法上提出了SAGE这一新颖采样范式，以释放这种高效推理潜力，并将其与强化学习结合为SAGE-RL来优化标准推理。实验结果表明，SAGE-RL在多个具有挑战性的数学基准测试中，显著提升了推理准确性和计算效率。</div>
</details>
</div>
<div class="card">
<div class="title">MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models</div>
<div class="meta-line">Authors: Chieh-Yun Chen, Zhonghao Wang, Qi Chen, Zhifan Ye, Min Shi, Yue Zhao, Yinan Zhao, Hui Qu, Wei-An Lin, Yiru Shen, Ajinkya Kale, Irfan Essa, Humphrey Shi</div>
<div class="meta-line">Venue: CVPR 2026</div>
<div class="meta-line">First: 2025-11-25T18:49:21+00:00 · Latest: 2026-02-23T05:04:31+00:00</div>
<div class="meta-line">Comments: CVPR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.20629v4">Abs</a> · <a href="https://arxiv.org/pdf/2511.20629v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning from human feedback (RLHF) with reward models has advanced alignment of generative models to human aesthetic and perceptual preferences. However, jointly optimizing multiple rewards often incurs an alignment tax, improving one dimension while degrading others. To address this, we introduce two complementary methods: MapReduce LoRA and Reward-aware Token Embedding (RaTE). MapReduce LoRA trains preference-specific LoRA experts in parallel and iteratively merges them to refine a shared base model; RaTE learns reward-specific token embeddings that compose at inference for flexible preference control. Experiments on Text-to-Image generation (Stable Diffusion 3.5 Medium and FLUX.1-dev) show improvements of 36.1%, 4.6%, and 55.7%, and 32.7%, 4.3%, and 67.1% on GenEval, PickScore, and OCR, respectively. On Text-to-Video generation (HunyuanVideo), visual and motion quality improve by 48.1% and 90.0%, respectively. On the language task, Helpful Assistant, with Llama-2 7B, helpful and harmless improve by 43.4% and 136.7%, respectively. Our framework sets a new state-of-the-art multi-preference alignment recipe across modalities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MapReduce LoRA：推进生成模型多偏好优化的帕累托前沿</div>
<div class="mono" style="margin-top:8px">基于奖励模型的人类反馈强化学习（RLHF）提升了生成模型与人类审美及感知偏好的对齐能力。然而，联合优化多个奖励常引发对齐代价，即提升某一维度时其他维度会退化。为此，我们提出两种互补方法：MapReduce LoRA与奖励感知词元嵌入（RaTE）。MapReduce LoRA并行训练针对特定偏好的LoRA专家模型，并通过迭代融合优化共享基础模型；RaTE则学习奖励特定的词元嵌入，在推理时组合以实现灵活的偏好控制。在文生图任务（Stable Diffusion 3.5 Medium与FLUX.1-dev）的实验中，GenEval、PickScore与OCR指标分别提升36.1%/4.6%/55.7%和32.7%/4.3%/67.1%。在文生视频任务（HunyuanVideo）中，视觉与运动质量分别提升48.1%与90.0%。在语言任务（基于Llama-2 7B的助理性测试）中，有益性与无害性分别提升43.4%与136.7%。本框架为跨模态多偏好对齐确立了新的技术标杆。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of multi-preference optimization in generative models, where aligning with multiple human preferences often leads to an alignment tax, improving some aspects while degrading others. To mitigate this, the authors propose two complementary methods: MapReduce LoRA, which trains preference-specific LoRA experts in parallel and iteratively merges them to refine a shared base model, and Reward-aware Token Embedding (RaTE), which learns reward-specific token embeddings that compose at inference for flexible preference control. Experimental results across text-to-image generation (Stable Diffusion 3.5 Medium and FLUX.1-dev), text-to-video generation (HunyuanVideo), and language tasks (Llama-2 7B) demonstrate significant improvements, such as up to 67.1% on OCR scores for images and 136.7% on harmless scores for language, establishing a new state-of-the-art in multi-preference alignment across modalities.</div>
<div class="mono" style="margin-top:8px">本文针对生成模型中的多偏好优化问题，指出对齐多种人类偏好常导致对齐税，即提升某些维度时损害其他维度。为解决此问题，作者提出了两种互补方法：MapReduce LoRA，通过并行训练偏好特定的LoRA专家并迭代合并以优化共享基础模型；以及奖励感知词嵌入（RaTE），学习奖励特定的词嵌入，在推理时组合以实现灵活的偏好控制。在文本到图像生成（Stable Diffusion 3.5 Medium和FLUX.1-dev）、文本到视频生成（HunyuanVideo）和语言任务（Llama-2 7B）上的实验结果显示显著提升，例如图像OCR分数最高提高67.1%，语言无害性分数最高提高136.7%，为跨模态多偏好对齐设定了新的最先进方案。</div>
</details>
</div>
<div class="card">
<div class="title">Scalable Multi-Objective and Meta Reinforcement Learning via Gradient Estimation</div>
<div class="meta-line">Authors: Zhenshuo Zhang, Minxuan Duan, Youran Ye, Hongyang R. Zhang</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-11-16T21:05:21+00:00 · Latest: 2026-02-23T03:07:02+00:00</div>
<div class="meta-line">Comments: 25 pages. Appeared in AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12779v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.12779v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the problem of efficiently estimating policies that simultaneously optimize multiple objectives in reinforcement learning (RL). Given $n$ objectives (or tasks), we seek the optimal partition of these objectives into $k \ll n$ groups, where each group comprises related objectives that can be trained together. This problem arises in applications such as robotics, control, and preference optimization in language models, where learning a single policy for all $n$ objectives is suboptimal as $n$ grows. We introduce a two-stage procedure -- meta-training followed by fine-tuning -- to address this problem. We first learn a meta-policy for all objectives using multitask learning. Then, we adapt the meta-policy to multiple randomly sampled subsets of objectives. The adaptation step leverages a first-order approximation property of well-trained policy networks, which is empirically verified to be accurate within a 2% error margin across various RL environments. The resulting algorithm, PolicyGradEx, efficiently estimates an aggregate task-affinity score matrix given a policy evaluation algorithm. Based on the estimated affinity score matrix, we cluster the $n$ objectives into $k$ groups by maximizing the intra-cluster affinity scores. Experiments on three robotic control and the Meta-World benchmarks demonstrate that our approach outperforms state-of-the-art baselines by 16% on average, while delivering up to $26\times$ faster speedup relative to performing full training to obtain the clusters. Ablation studies validate each component of our approach. For instance, compared with random grouping and gradient-similarity-based grouping, our loss-based clustering yields an improvement of 19%. Finally, we analyze the generalization error of policy networks by measuring the Hessian trace of the loss surface, which gives non-vacuous measures relative to the observed generalization errors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于梯度估计的可扩展多目标与元强化学习</div>
<div class="mono" style="margin-top:8px">本研究探讨在强化学习中高效估计能同时优化多个目标的策略问题。给定n个目标（或任务），我们寻求将这些目标最优划分为k（k &lt;&lt; n）个组，每组包含可共同训练的相关目标。该问题出现在机器人学、控制及语言模型偏好优化等应用中——当n增大时，为所有目标学习单一策略会变得次优。我们提出两阶段方法（元训练后微调）以解决此问题：首先通过多任务学习为所有目标训练元策略，随后将元策略适配至多个随机采样的目标子集。适配步骤利用了训练良好的策略网络的一阶近似特性，经实验验证在各类RL环境中误差范围可控制在2%以内。所得算法PolicyGradEx能基于策略评估算法高效估计聚合任务亲和度得分矩阵。根据估计的亲和度矩阵，我们通过最大化簇内亲和度得分将n个目标聚类为k组。在三个机器人控制任务及Meta-World基准测试上的实验表明，本方法平均优于现有最优基线16%，且相较于通过完整训练获取聚类的方法实现高达26倍的加速。消融实验验证了方法的各组件有效性：例如基于损失的聚类相比随机分组和基于梯度相似性的分组可提升19%性能。最后，我们通过测量损失曲面的Hessian迹分析策略网络的泛化误差，该指标相对于观测到的泛化误差具有非平凡的解释力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of efficiently learning policies for multiple objectives in reinforcement learning, where scaling to many tasks makes a single policy suboptimal. The proposed method, PolicyGradEx, uses a two-stage approach: first meta-training a policy across all objectives, then fine-tuning it on random subsets to estimate task affinities via gradient approximation, which is shown to be accurate within 2% error. Experimental results on robotic control and Meta-World benchmarks show the method outperforms state-of-the-art baselines by 16% on average and achieves up to 26× faster clustering, with ablation studies confirming a 19% improvement over random or gradient-based grouping.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中多目标策略学习效率低下的问题，提出了一种可扩展的方法，旨在通过分组相关目标来优化策略。所提出的PolicyGradEx方法采用两阶段流程：先通过元训练学习一个跨所有目标的元策略，再通过微调随机子集来估计任务亲和度，其梯度近似误差在2%以内。在机器人控制和Meta-World基准测试中，该方法平均性能优于现有基线16%，聚类速度提升高达26倍，消融实验表明基于损失的聚类比随机或基于梯度的分组性能提升19%。</div>
</details>
</div>
<div class="card">
<div class="title">SenTSR-Bench: Thinking with Injected Knowledge for Time-Series Reasoning</div>
<div class="meta-line">Authors: Zelin He, Boran Han, Xiyuan Zhang, Shuai Zhang, Haotian Lin, Qi Zhu, Haoyang Fang, Danielle C. Maddix, Abdul Fatir Ansari, Akash Chandrayan, Abhinav Pradhan, Bernie Wang, Matthew Reimherr</div>
<div class="meta-line">First: 2026-02-23T02:55:32+00:00 · Latest: 2026-02-23T02:55:32+00:00</div>
<div class="meta-line">Comments: Accepted by the 29th International Conference on Artificial Intelligence and Statistics (AISTATS 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19455v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19455v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Time-series diagnostic reasoning is essential for many applications, yet existing solutions face a persistent gap: general reasoning large language models (GRLMs) possess strong reasoning skills but lack the domain-specific knowledge to understand complex time-series patterns. Conversely, fine-tuned time-series LLMs (TSLMs) understand these patterns but lack the capacity to generalize reasoning for more complicated questions. To bridge this gap, we propose a hybrid knowledge-injection framework that injects TSLM-generated insights directly into GRLM&#x27;s reasoning trace, thereby achieving strong time-series reasoning with in-domain knowledge. As collecting data for knowledge injection fine-tuning is costly, we further leverage a reinforcement learning-based approach with verifiable rewards (RLVR) to elicit knowledge-rich traces without human supervision, then transfer such an in-domain thinking trace into GRLM for efficient knowledge injection. We further release SenTSR-Bench, a multivariate time-series-based diagnostic reasoning benchmark collected from real-world industrial operations. Across SenTSR-Bench and other public datasets, our method consistently surpasses TSLMs by 9.1%-26.1% and GRLMs by 7.9%-22.4%, delivering robust, context-aware time-series diagnostic insights.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SenTSR-Bench：基于知识注入的时间序列推理思考</div>
<div class="mono" style="margin-top:8px">时间序列诊断推理对众多应用至关重要，但现有解决方案面临一个持续存在的差距：通用推理大语言模型（GRLM）具备强大的推理能力，但缺乏理解复杂时间序列模式的领域知识；而经过微调的时间序列大语言模型（TSLM）虽能理解这些模式，却缺乏对更复杂问题进行泛化推理的能力。为弥合这一差距，我们提出一种混合知识注入框架，将TSLM生成的洞察直接注入GRLM的推理轨迹中，从而借助领域知识实现强大的时间序列推理。由于为知识注入微调收集数据成本高昂，我们进一步采用基于强化学习的可验证奖励方法（RLVR），在无需人工监督的情况下生成富含知识的推理轨迹，并将此类领域内思考轨迹迁移至GRLM以实现高效知识注入。我们还发布了SenTSR-Bench——一个基于多变量时间序列的诊断推理基准数据集，其数据采集自真实工业场景。在SenTSR-Bench及其他公开数据集上的实验表明，我们的方法始终优于TSLM 9.1%-26.1%，优于GRLM 7.9%-22.4%，能提供鲁棒且上下文感知的时间序列诊断洞察。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the persistent gap in time-series diagnostic reasoning where general reasoning large language models (GRLMs) lack domain-specific knowledge, while fine-tuned time-series LLMs (TSLMs) lack generalization capacity for complex questions. To bridge this, the authors propose a hybrid knowledge-injection framework that injects TSLM-generated insights directly into a GRLM&#x27;s reasoning trace, and they further employ a reinforcement learning-based approach with verifiable rewards (RLVR) to elicit knowledge-rich traces without costly human supervision for fine-tuning. Experimental results on the newly released SenTSR-Bench and other public datasets show that the method consistently outperforms TSLMs by 9.1%-26.1% and GRLMs by 7.9%-22.4%, delivering robust, context-aware diagnostic insights.</div>
<div class="mono" style="margin-top:8px">本文针对时间序列诊断推理中存在的持续差距：通用推理大语言模型缺乏领域专业知识，而微调后的时间序列大语言模型又缺乏对复杂问题的泛化推理能力。为弥补这一差距，作者提出了一种混合知识注入框架，将时间序列模型生成的洞察直接注入通用模型的推理轨迹中，并进一步采用基于强化学习的可验证奖励方法，在无需人工监督的情况下获取知识丰富的轨迹，从而避免昂贵的微调数据收集。在发布的SenTSR-Bench基准及其他公共数据集上的实验结果表明，该方法始终优于时间序列模型9.1%-26.1%，优于通用模型7.9%-22.4%，提供了鲁棒且上下文感知的诊断洞察。</div>
</details>
</div>
<div class="card">
<div class="title">RAmmStein: Regime Adaptation in Mean-reverting Markets with Stein Thresholds -- Optimal Impulse Control in Concentrated AMMs</div>
<div class="meta-line">Authors: Pranay Anchuri</div>
<div class="meta-line">Venue: www</div>
<div class="meta-line">First: 2026-02-23T01:25:16+00:00 · Latest: 2026-02-23T01:25:16+00:00</div>
<div class="meta-line">Comments: 12 pages, 1 figure, 4 tables, 1 algorithm; submitted to Designing DeFi workshop (https://www.designingdefi.xyz/)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19419v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19419v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Concentrated liquidity provision in decentralized exchanges presents a fundamental Impulse Control problem. Liquidity Providers (LPs) face a non-trivial trade-off between maximizing fee accrual through tight price-range concentration and minimizing the friction costs of rebalancing, including gas fees and swap slippage. Existing methods typically employ heuristic or threshold strategies that fail to account for market dynamics. This paper formulates liquidity management as an optimal control problem and derives the corresponding Hamilton-Jacobi-Bellman quasi-variational inequality (HJB-QVI). We present an approximate solution RAmmStein, a Deep Reinforcement Learning method that incorporates the mean-reversion speed (theta) of an Ornstein-Uhlenbeck process among other features as input to the model. We demonstrate that the agent learns to separate the state space into regions of action and inaction. We evaluate the framework using high-frequency 1Hz Coinbase trade data comprising over 6.8M trades. Experimental results show that RAmmStein achieves a superior net ROI of 0.72% compared to both passive and aggressive strategies. Notably, the agent reduces rebalancing frequency by 67% compared to a greedy rebalancing strategy while maintaining 88% active time. Our results demonstrate that regime-aware laziness can significantly improve capital efficiency by preserving the returns that would otherwise be eroded by the operational costs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RAmmStein：均值回归市场中的状态自适应与Stein阈值——集中流动性做市商的最优脉冲控制</div>
<div class="mono" style="margin-top:8px">去中心化交易所中的集中流动性供给构成一个本质的脉冲控制问题。流动性提供者面临两难权衡：既要通过紧密价格区间集中来最大化手续费收益，又要最小化再平衡的摩擦成本（包括Gas费和交易滑点）。现有方法多采用启发式或阈值策略，未能充分考虑市场动态。本文提出将流动性管理建模为最优控制问题，推导出对应的Hamilton-Jacobi-Bellman拟变分不等式（HJB-QVI）。我们提出近似解法RAmmStein——一种深度强化学习方法，其模型输入特征包含Ornstein-Uhlenbeck过程的均值回归速率（θ）。实验证明智能体能够学会将状态空间划分为行动区与静默区。我们使用Coinbase高频交易数据（1Hz频率，逾680万笔交易）评估该框架。实验结果显示：相较于被动与激进策略，RAmmStein实现了0.72%的卓越净投资回报率。值得注意的是，相比贪婪再平衡策略，该智能体将再平衡频率降低67%，同时保持88%的活跃时间。研究结果表明，基于市场状态的延迟决策能通过保留原本会被操作成本侵蚀的收益，显著提升资金效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of liquidity management in concentrated automated market makers, where liquidity providers must balance fee income from tight price ranges against the costs of frequent rebalancing. The authors formulate this as an optimal impulse control problem, solving it via a Hamilton-Jacobi-Bellman quasi-variational inequality and proposing RAmmStein, a deep reinforcement learning method that incorporates market regime features like mean-reversion speed. Using high-frequency Coinbase trade data, experiments show RAmmStein achieves a 0.72% net return on investment, reduces rebalancing frequency by 67% compared to greedy strategies, and maintains 88% active time, demonstrating that regime-aware laziness enhances capital efficiency by minimizing operational costs.</div>
<div class="mono" style="margin-top:8px">本文针对集中流动性自动做市商中的流动性管理难题展开研究，流动性提供者需要在通过狭窄价格区间赚取手续费与频繁再平衡产生的成本之间取得平衡。作者将其建模为最优脉冲控制问题，通过哈密顿-雅可比-贝尔曼拟变分不等式求解，并提出了RAmmStein这一融合市场状态特征（如均值回归速度）的深度强化学习方法。基于高频Coinbase交易数据的实验表明，RAmmStein实现了0.72%的净投资回报率，相比贪婪再平衡策略减少了67%的再平衡频率，同时保持88%的活跃时间，证明了基于市场状态的惰性策略能通过降低操作成本有效提升资金效率。</div>
</details>
</div>
<div class="card">
<div class="title">IR$^3$: Contrastive Inverse Reinforcement Learning for Interpretable Detection and Mitigation of Reward Hacking</div>
<div class="meta-line">Authors: Mohammad Beigi, Ming Jin, Junshan Zhang, Jiaxin Zhang, Qifan Wang, Lifu Huang</div>
<div class="meta-line">First: 2026-02-23T01:14:53+00:00 · Latest: 2026-02-23T01:14:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19416v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19416v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning from Human Feedback (RLHF) enables powerful LLM alignment but can introduce reward hacking - models exploit spurious correlations in proxy rewards without genuine alignment. Compounding this, the objectives internalized during RLHF remain opaque, making hacking behaviors difficult to detect or correct. We introduce IR3 (Interpretable Reward Reconstruction and Rectification), a framework that reverse-engineers, interprets, and surgically repairs the implicit objectives driving RLHF-tuned models. We propose Contrastive Inverse Reinforcement Learning (C-IRL), which reconstructs the implicit reward function by contrasting paired responses from post-alignment and baseline policies to explain behavioral shifts during RLHF. We then decompose the reconstructed reward via sparse autoencoders into interpretable features, enabling identification of hacking signatures through contribution analysis. Finally, we propose mitigation strategies - clean reward optimization, adversarial shaping, constrained optimization, and feature-guided distillation - that target problematic features while preserving beneficial alignment. Experiments across multiple reward model configurations show that IR3 achieves 0.89 correlation with ground-truth rewards, identifies hacking features with over 90% precision, and significantly reduces hacking behaviors while maintaining capabilities within 3% of the original model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IR$^3$：基于对比逆向强化学习的可解释奖励黑客检测与缓解方法</div>
<div class="mono" style="margin-top:8px">基于人类反馈的强化学习（RLHF）虽能实现强大的大语言模型对齐，但可能引发奖励黑客问题——模型利用代理奖励中的伪相关性实现虚假对齐。更复杂的是，RLHF过程中内化的目标具有不透明性，使得黑客行为难以检测或修正。本文提出IR3（可解释奖励重构与修正）框架，通过逆向工程解析、解释并精准修复驱动RLHF调优模型的隐式目标。我们设计对比逆向强化学习（C-IRL），通过对比对齐后策略与基线策略的成对响应来重构隐式奖励函数，从而解释RLHF期间的行为偏移。随后通过稀疏自编码器将重构奖励分解为可解释特征，借助贡献分析识别黑客特征标记。最后提出针对性缓解策略——清洁奖励优化、对抗性塑形、约束优化及特征引导蒸馏——在保留有益对齐特征的同时修正问题特征。在多奖励模型配置的实验中，IR3与真实奖励的相关系数达0.89，黑客特征识别精度超过90%，在保持原模型97%能力的前提下显著减少黑客行为。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the problem of reward hacking in Reinforcement Learning from Human Feedback (RLHF), where models exploit spurious correlations in proxy rewards, leading to misalignment that is difficult to detect due to opaque internalized objectives. The method introduces IR3, a framework that uses Contrastive Inverse Reinforcement Learning (C-IRL) to reconstruct implicit reward functions by contrasting responses from aligned and baseline policies, then decomposes these rewards via sparse autoencoders into interpretable features to identify hacking signatures. Main experimental results show that IR3 achieves a 0.89 correlation with ground-truth rewards, identifies hacking features with over 90% precision, and significantly reduces hacking behaviors while maintaining model capabilities within 3% of the original.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决人类反馈强化学习中的奖励破解问题，即模型利用代理奖励中的虚假相关性导致未对齐行为，且由于目标不透明而难以检测。方法上提出了IR3框架，采用对比逆向强化学习通过对比对齐策略与基线策略的响应来重建隐式奖励函数，并利用稀疏自编码器将其分解为可解释特征以识别破解特征。主要实验结果表明，IR3与真实奖励的相关系数达0.89，破解特征识别精度超过90%，能显著减少破解行为，同时将模型能力保持在原模型的3%以内。</div>
</details>
</div>
<div class="card">
<div class="title">Hilbert-Augmented Reinforcement Learning for Scalable Multi-Robot Coverage and Exploration</div>
<div class="meta-line">Authors: Tamil Selvan Gurunathan, Aryya Gangopadhyay</div>
<div class="meta-line">First: 2026-02-23T00:19:19+00:00 · Latest: 2026-02-23T00:19:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19400v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19400v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a coverage framework that integrates Hilbert space-filling priors into decentralized multi-robot learning and execution. We augment DQN and PPO with Hilbert-based spatial indices to structure exploration and reduce redundancy in sparse-reward environments, and we evaluate scalability in multi-robot grid coverage. We further describe a waypoint interface that converts Hilbert orderings into curvature-bounded, time-parameterized SE(2) trajectories (planar (x, y, θ)), enabling onboard feasibility on resource-constrained robots. Experiments show improvements in coverage efficiency, redundancy, and convergence speed over DQN/PPO baselines. In addition, we validate the approach on a Boston Dynamics Spot legged robot, executing the generated trajectories in indoor environments and observing reliable coverage with low redundancy. These results indicate that geometric priors improve autonomy and scalability for swarm and legged robotics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>希尔伯特增强型强化学习用于可扩展多机器人覆盖与探索</div>
<div class="mono" style="margin-top:8px">我们提出了一种覆盖框架，将希尔伯特空间填充先验知识集成到去中心化多机器人学习与执行中。我们通过基于希尔伯特的空间索引增强DQN和PPO算法，以结构化探索并减少稀疏奖励环境中的冗余，并在多机器人网格覆盖任务中评估其可扩展性。我们进一步描述了一种航点接口，可将希尔伯特排序转换为曲率有界、时间参数化的SE(2)轨迹（平面(x, y, θ)），从而在资源受限的机器人上实现机载可行性。实验表明，相较于DQN/PPO基线方法，本方法在覆盖效率、冗余度和收敛速度方面均有提升。此外，我们在波士顿动力Spot腿式机器人上验证了该方法，在室内环境中执行生成的轨迹，并观察到可靠的低冗余覆盖。这些结果表明，几何先验知识能提升集群机器人与腿式机器人的自主性与可扩展性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need to improve the efficiency and scalability of multi-robot coverage and exploration in sparse-reward environments. The method integrates Hilbert space-filling curve priors into decentralized reinforcement learning algorithms like DQN and PPO, using Hilbert-based spatial indices to structure exploration and reduce redundancy; it also introduces a waypoint interface to convert these orderings into feasible, curvature-bounded trajectories for resource-constrained robots. Experimental results on grid coverage tasks show improvements in coverage efficiency, reduced redundancy, and faster convergence compared to baseline DQN/PPO, with further validation on a Boston Dynamics Spot robot demonstrating reliable, low-redundancy coverage in indoor environments, indicating that geometric priors enhance autonomy and scalability for robotic swarms and legged platforms.</div>
<div class="mono" style="margin-top:8px">本文旨在提升稀疏奖励环境下多机器人覆盖与探索的效率和可扩展性。方法将希尔伯特空间填充曲线先验集成到去中心化强化学习算法（如DQN和PPO）中，利用基于希尔伯特的空间索引来结构化探索并减少冗余；同时引入航点接口，将这些排序转换为适用于资源受限机器人的可行、曲率有界的轨迹。在网格覆盖任务上的实验结果表明，相较于基线DQN/PPO，该方法提高了覆盖效率、减少了冗余并加快了收敛速度；在波士顿动力Spot腿式机器人上的进一步验证显示，其在室内环境中实现了可靠且低冗余的覆盖，表明几何先验能增强机器人集群和腿式平台的自主性与可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Information-Optimized Multi-Agent Path Finding: A Hybrid Framework with Reduced Inter-Agent Information Sharing</div>
<div class="meta-line">Authors: Bharath Muppasani, Ritirupa Dey, Biplav Srivastava, Vignesh Narayanan</div>
<div class="meta-line">First: 2025-10-10T15:25:40+00:00 · Latest: 2026-02-22T23:52:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.09469v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.09469v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent pathfinding (MAPF) remains a critical problem in robotics and autonomous systems, where agents must navigate shared spaces efficiently while avoiding conflicts. Traditional centralized algorithms with global information provide high-quality solutions but scale poorly in large-scale scenarios due to the combinatorial explosion of conflicts. Conversely, distributed approaches that have local information, particularly learning-based methods, offer better scalability by operating with relaxed information availability, yet often at the cost of solution quality. In realistic deployments, information is a constrained resource: broadcasting full agent states and goals can raise privacy concerns, strain limited bandwidth, and require extra sensing and communication hardware, increasing cost and energy use. We focus on the core question of how MAPF can be solved with minimal inter-agent information sharing while preserving solution feasibility. To this end, we present an information-centric formulation of the MAPF problem and introduce a hybrid framework, IO-MAPF, that integrates decentralized path planning with a lightweight centralized coordinator. In this framework, agents use reinforcement learning (RL) to plan independently, while the central coordinator provides minimal, targeted signals, such as static conflict-cell indicators or short conflict trajectories, that are dynamically shared to support efficient conflict resolution. We introduce an Information Units (IU) metric to quantify information use and show that our alert-driven design achieves 2x to 23x reduction in information sharing, compared to the state-of-the-art algorithms, while maintaining high success rates, demonstrating that reliable MAPF is achievable under strongly information-restricted, privacy-preserving conditions. We demonstrate the effectiveness of our algorithm using simulation and hardware experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向信息优化的多智能体路径规划：一种减少智能体间信息共享的混合框架</div>
<div class="mono" style="margin-top:8px">多智能体路径规划（MAPF）是机器人与自主系统中的关键问题，智能体需在共享空间中高效导航并避免冲突。依赖全局信息的传统集中式算法虽能提供高质量解，但在大规模场景中因冲突组合爆炸而难以扩展。相反，基于局部信息的分布式方法（尤其是学习型方法）通过放宽信息可获性实现更好扩展性，但常以牺牲解质量为代价。在实际部署中，信息是受限资源：广播完整的智能体状态与目标可能引发隐私担忧、挤占有限带宽，并需额外传感通信硬件，增加成本与能耗。本文聚焦于如何在保持解可行性的前提下，以最小化智能体间信息共享实现MAPF。为此，我们提出MAPF的信息中心化建模，并引入混合框架IO-MAPF，该框架将分散式路径规划与轻量级集中协调器相结合。在该框架中，智能体使用强化学习独立规划路径，中央协调器则动态提供极简的定向信号（如静态冲突单元指示器或短冲突轨迹）以支持高效冲突消解。我们引入信息单位（IU）度量来量化信息使用，实验表明：相较于前沿算法，本研究的警报驱动设计能将信息共享量降低2至23倍，同时保持高成功率，证明在强信息受限且保护隐私的条件下仍可实现可靠MAPF。我们通过仿真与硬件实验验证了算法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of multi-agent pathfinding (MAPF) under constrained information sharing, motivated by the need to balance solution quality with practical concerns like privacy, bandwidth, and hardware costs in real-world deployments. The authors propose a hybrid framework called IO-MAPF, which combines decentralized reinforcement learning for independent agent planning with a lightweight central coordinator that provides minimal, targeted signals—such as conflict indicators—to resolve conflicts efficiently. Experimental results demonstrate that this approach reduces inter-agent information sharing by 2x to 23x compared to state-of-the-art methods while maintaining high success rates, proving effective in both simulation and hardware tests under information-restricted conditions.</div>
<div class="mono" style="margin-top:8px">本文针对多智能体路径规划（MAPF）中信息共享受限的挑战展开研究，其动机是在实际部署中需平衡解决方案质量与隐私、带宽和硬件成本等实际问题。作者提出了一种名为IO-MAPF的混合框架，该框架结合了用于独立智能体规划的分散式强化学习，以及一个轻量级中央协调器，后者提供最小化、有针对性的信号（如冲突指示）以高效解决冲突。实验结果表明，与现有先进方法相比，该方法将智能体间信息共享减少了2至23倍，同时保持高成功率，在仿真和硬件测试中均证明了其在信息受限条件下的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Bootstrapping Task Spaces for Self-Improvement</div>
<div class="meta-line">Authors: Minqi Jiang, Andrei Lupu, Yoram Bachrach</div>
<div class="meta-line">First: 2025-09-04T18:01:00+00:00 · Latest: 2026-02-22T23:28:26+00:00</div>
<div class="meta-line">Comments: TMLR, February 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.04575v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.04575v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Progress in many task domains emerges from repeated revisions to previous solution attempts. Training agents that can reliably self-improve over such sequences at inference-time is a natural target for reinforcement learning (RL), yet the naive approach assumes a fixed maximum iteration depth, which can be both costly and arbitrary. We present Exploratory Iteration (ExIt), a family of autocurriculum RL methods that directly exploits the recurrent structure of self-improvement tasks to train LLMs to perform multi-step self-improvement at inference-time while only training on the most informative single-step iterations. ExIt grows a task space by selectively sampling the most informative intermediate, partial histories encountered during an episode for continued iteration, treating these starting points as new self-iteration task instances to train a self-improvement policy. ExIt can further pair with explicit exploration mechanisms to sustain greater task diversity. Across several domains, encompassing competition math, multi-turn tool-use, and machine learning engineering, we demonstrate that ExIt strategies, starting from either a single or many task instances, can produce policies exhibiting strong inference-time self-improvement on held-out task instances, and the ability to iterate towards higher performance over a step budget extending beyond the average iteration depth encountered during training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自改进任务空间的引导构建</div>
<div class="mono" style="margin-top:8px">许多任务领域的进展源于对先前解决方案尝试的反复修订。训练能够在推理时沿此类序列可靠自改进的智能体，是强化学习的自然目标，但朴素方法假设固定的最大迭代深度，这既昂贵又随意。我们提出探索性迭代——一类自动课程强化学习方法，直接利用自改进任务的循环结构，训练大语言模型在推理时执行多步自改进，同时仅针对信息量最大的单步迭代进行训练。该方法通过选择性采样在任务执行过程中遇到的最具信息量的中间部分历史记录进行持续迭代，将这些起点视为新的自迭代任务实例来训练自改进策略，并可结合显式探索机制维持更高任务多样性。在涵盖竞赛数学、多轮工具使用和机器学习工程的多个领域中，我们证明该策略从单个或多个任务实例出发，能产生在保留任务实例上展现强推理时自改进能力的策略，并能在超出训练期间平均迭代深度的步数预算内通过迭代实现更高性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for agents that can self-improve through iterative revisions without assuming a fixed, costly iteration depth, this paper introduces Exploratory Iteratory (ExIt), an autocurriculum reinforcement learning method. The method constructs a task space by selectively sampling informative intermediate histories from self-improvement episodes and treats them as new training instances, optionally paired with exploration mechanisms to maintain diversity. Experimental results across domains like competition math, tool-use, and ML engineering show that ExIt-trained policies achieve strong inference-time self-improvement on held-out tasks, iterating beyond the average depth seen in training.</div>
<div class="mono" style="margin-top:8px">本文的动机是训练能够通过迭代修订自我改进的智能体，避免固定且代价高昂的迭代深度假设，提出了探索性迭代（ExIt）这一自动课程强化学习方法。该方法通过选择性采样自我改进过程中信息丰富的中间历史来构建任务空间，将其作为新的训练实例，并可结合探索机制保持任务多样性。在竞赛数学、多轮工具使用和机器学习工程等领域的实验结果表明，ExIt训练的策略在未见任务上实现了强大的推理时自我改进能力，迭代步数超过了训练时的平均深度。</div>
</details>
</div>
<div class="card">
<div class="title">Stable Deep Reinforcement Learning via Isotropic Gaussian Representations</div>
<div class="meta-line">Authors: Ali Saheb, Johan Obando-Ceron, Aaron Courville, Pouya Bashivan, Pablo Samuel Castro</div>
<div class="meta-line">First: 2026-02-22T22:55:27+00:00 · Latest: 2026-02-22T22:55:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19373v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19373v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep reinforcement learning systems often suffer from unstable training dynamics due to non-stationarity, where learning objectives and data distributions evolve over time. We show that under non-stationary targets, isotropic Gaussian embeddings are provably advantageous. In particular, they induce stable tracking of time-varying targets for linear readouts, achieve maximal entropy under a fixed variance budget, and encourage a balanced use of all representational dimensions--all of which enable agents to be more adaptive and stable. Building on this insight, we propose the use of Sketched Isotropic Gaussian Regularization for shaping representations toward an isotropic Gaussian distribution during training. We demonstrate empirically, over a variety of domains, that this simple and computationally inexpensive method improves performance under non-stationarity while reducing representation collapse, neuron dormancy, and training instability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于各向同性高斯表示的稳定深度强化学习</div>
<div class="mono" style="margin-top:8px">深度强化学习系统常因非平稳性导致训练不稳定，即学习目标和数据分布随时间变化。我们证明，在非平稳目标下，各向同性高斯嵌入具有理论优势：它们能稳定跟踪线性读出器的时变目标，在固定方差约束下实现最大熵，并促进所有表示维度的均衡使用——这些特性使智能体更具适应性和稳定性。基于此，我们提出使用草图各向同性高斯正则化，在训练中将表示塑造成各向同性高斯分布。通过多领域实验验证，这种简单且计算成本低的方法能在非平稳环境下提升性能，同时减少表示塌缩、神经元休眠和训练不稳定性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the instability in deep reinforcement learning caused by non-stationary training dynamics, where objectives and data distributions shift over time. The authors demonstrate theoretically that isotropic Gaussian representations offer advantages by enabling stable tracking of moving targets for linear predictors, maximizing entropy for a given variance, and promoting balanced use of all representational dimensions. Motivated by this, they propose Sketched Isotropic Gaussian Regularization, a simple and efficient method to shape neural network representations toward an isotropic Gaussian distribution during training. Experimental results across various domains show that this regularization improves agent performance under non-stationary conditions while mitigating issues like representation collapse, neuron dormancy, and overall training instability.</div>
<div class="mono" style="margin-top:8px">本文针对深度强化学习中因目标与数据分布时变导致的非平稳性及训练不稳定问题展开研究。作者从理论上证明，各向同性高斯表示具有优势：它能使线性预测器稳定跟踪时变目标，在固定方差下实现最大熵，并促进所有表征维度的均衡使用。基于此，他们提出了草图化各向同性高斯正则化方法，这是一种简单高效的技术，用于在训练过程中将神经网络表征塑造为各向同性高斯分布。在多个领域的实验结果表明，该正则化方法能有效提升智能体在非平稳环境下的性能，同时缓解表征塌缩、神经元休眠和训练不稳定等问题。</div>
</details>
</div>
<div class="card">
<div class="title">Debate2Create: Robot Co-design via Multi-Agent LLM Debate</div>
<div class="meta-line">Authors: Kevin Qiu, Marek Cygan</div>
<div class="meta-line">First: 2025-10-29T18:00:16+00:00 · Latest: 2026-02-22T22:15:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.25850v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.25850v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Debate2Create (D2C), a multi-agent LLM framework that formulates robot co-design as structured, iterative debate grounded in physics-based evaluation. A design agent and control agent engage in a thesis-antithesis-synthesis loop, while pluralistic LLM judges provide multi-objective feedback to steer exploration. Across five MuJoCo locomotion benchmarks, D2C achieves up to $3.2\times$ the default Ant score and $\sim9\times$ on Swimmer, outperforming prior LLM-based methods and black-box optimization. Iterative debate yields 18--35% gains over compute-matched zero-shot generation, and D2C-generated rewards transfer to default morphologies in 4/5 tasks. Our results demonstrate that structured multi-agent debate offers an effective alternative to hand-designed objectives for joint morphology-reward optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Debate2Create：基于多智能体大语言模型辩论的机器人协同设计</div>
<div class="mono" style="margin-top:8px">我们提出Debate2Create（D2C），一种多智能体大语言模型框架，将机器人协同设计构建为基于物理评估的结构化迭代辩论。设计智能体与控制智能体通过“正题-反题-合题”循环进行交互，而多元大语言模型评委提供多目标反馈以引导探索。在五个MuJoCo运动基准测试中，D2C在Ant任务上达到默认分数的3.2倍，在Swimmer任务上达到约9倍，优于现有基于大语言模型的方法与黑盒优化。相比计算资源匹配的零样本生成，迭代辩论带来18-35%的性能提升，且D2C生成的奖励函数在4/5任务中可迁移至默认形态。结果表明，结构化多智能体辩论为形态-奖励联合优化提供了一种替代人工设计目标的有效方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Debate2Create (D2C), a framework motivated by the need to automate robot co-design without relying on hand-crafted objectives. The method employs a multi-agent LLM system where a design agent and a control agent engage in iterative, physics-grounded debates, guided by pluralistic LLM judges that provide multi-objective feedback to optimize both morphology and control. Experimental results on five MuJoCo locomotion benchmarks show D2C achieves up to 3.2 times the default Ant score and approximately 9 times on Swimmer, outperforming prior LLM-based methods and black-box optimization, with iterative debate yielding 18–35% gains over zero-shot generation and demonstrating successful reward transfer in most tasks.</div>
<div class="mono" style="margin-top:8px">本文提出了Debate2Create（D2C）框架，其动机在于无需依赖人工设计目标即可实现机器人协同设计的自动化。该方法采用多智能体大语言模型系统，其中设计智能体与控制智能体进行基于物理评估的迭代辩论，并由多元大语言模型法官提供多目标反馈，以共同优化形态与控制策略。在五个MuJoCo运动基准测试中的实验结果表明，D2C在Ant任务上达到默认分数的3.2倍，在Swimmer任务上约为9倍，优于先前基于大语言模型的方法和黑盒优化，迭代辩论相比零样本生成带来18–35%的性能提升，且在多数任务中生成的奖励能成功迁移到默认形态上。</div>
</details>
</div>
<div class="card">
<div class="title">LLMs Can Learn to Reason Via Off-Policy RL</div>
<div class="meta-line">Authors: Daniel Ritter, Owen Oertell, Bradley Guo, Jonathan Chang, Kianté Brantley, Wen Sun</div>
<div class="meta-line">First: 2026-02-22T22:12:51+00:00 · Latest: 2026-02-22T22:12:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19362v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19362v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) approaches for Large Language Models (LLMs) frequently use on-policy algorithms, such as PPO or GRPO. However, policy lag from distributed training architectures and differences between the training and inference policies break this assumption, making the data off-policy by design. To rectify this, prior work has focused on making this off-policy data appear more on-policy, either via importance sampling (IS), or by more closely aligning the training and inference policies by explicitly modifying the inference engine. In this work, we embrace off-policyness and propose a novel off-policy RL algorithm that does not require these modifications: Optimal Advantage-based Policy Optimization with Lagged Inference policy (OAPL). We show that OAPL outperforms GRPO with importance sampling on competition math benchmarks, and can match the performance of a publicly available coding model, DeepCoder, on LiveCodeBench, while using 3x fewer generations during training. We further empirically demonstrate that models trained via OAPL have improved test time scaling under the Pass@k metric. OAPL allows for efficient, effective post-training even with lags of more than 400 gradient steps between the training and inference policies, 100x more off-policy than prior approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型可通过离策略强化学习习得推理能力</div>
<div class="mono" style="margin-top:8px">针对大语言模型的强化学习方法常采用PPO或GRPO等同策略算法。然而，分布式训练架构导致的策略滞后及训练与推理策略间的差异，本质上使数据具有离策略特性。为修正此问题，先前研究主要通过重要性采样或显式修改推理引擎使策略对齐，以模拟同策略数据。本研究则直接利用离策略特性，提出一种无需上述调整的新型离策略强化学习算法——基于最优优势的滞后推理策略优化算法。实验表明，在数学竞赛基准测试中，该算法优于采用重要性采样的GRPO；在LiveCodeBench上，其性能可与公开编码模型DeepCoder持平，且训练时生成量减少三分之二。进一步实证显示，经该算法训练的模型在Pass@k指标下具有更优的测试时扩展性。即使训练与推理策略间存在超过400个梯度步的滞后（离策略程度达先前方法的100倍），该算法仍能实现高效有效的训练后优化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the inherent off-policy nature of data in distributed reinforcement learning (RL) for Large Language Models (LLMs), where traditional on-policy algorithms like PPO are compromised by policy lag and mismatches between training and inference. The authors propose a novel off-policy RL algorithm called Optimal Advantage-based Policy Optimization with Lagged Inference policy (OAPL), which directly embraces off-policyness without requiring modifications like importance sampling or inference engine adjustments. Experimental results show that OAPL outperforms GRPO with importance sampling on math benchmarks, matches the performance of the DeepCoder model on LiveCodeBench with three times fewer training generations, and demonstrates improved test-time scaling under the Pass@k metric, effectively handling policy lags over 400 gradient steps.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型分布式强化学习中数据固有的离策略性问题展开研究，传统策略上算法如PPO因策略滞后及训练与推理策略不匹配而失效。作者提出了一种名为基于最优优势的策略优化与滞后推理策略的新离策略RL算法，该方法直接接纳离策略性，无需重要性采样或修改推理引擎等调整。实验结果表明，该算法在数学基准上优于采用重要性采样的GRPO，在LiveCodeBench上以三倍更少的训练生成次数匹配了DeepCoder模型的性能，并在Pass@k指标下展现出更好的测试时扩展能力，能有效处理超过400步梯度更新的策略滞后。</div>
</details>
</div>
<div class="card">
<div class="title">Conformal Signal Temporal Logic for Robust Reinforcement Learning Control: A Case Study</div>
<div class="meta-line">Authors: Hani Beirami, M M Manjurul Islam</div>
<div class="meta-line">First: 2026-02-15T22:10:11+00:00 · Latest: 2026-02-22T22:08:26+00:00</div>
<div class="meta-line">Comments: 6 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14322v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.14322v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate how formal temporal logic specifications can enhance the safety and robustness of reinforcement learning (RL) control in aerospace applications. Using the open source AeroBench F-16 simulation benchmark, we train a Proximal Policy Optimization (PPO) agent to regulate engine throttle and track commanded airspeed. The control objective is encoded as a Signal Temporal Logic (STL) requirement to maintain airspeed within a prescribed band during the final seconds of each maneuver. To enforce this specification at run time, we introduce a conformal STL shield that filters the RL agent&#x27;s actions using online conformal prediction. We compare three settings: (i) PPO baseline, (ii) PPO with a classical rule-based STL shield, and (iii) PPO with the proposed conformal shield, under both nominal conditions and a severe stress scenario involving aerodynamic model mismatch, actuator rate limits, measurement noise, and mid-episode setpoint jumps. Experiments show that the conformal shield preserves STL satisfaction while maintaining near baseline performance and providing stronger robustness guarantees than the classical shield. These results demonstrate that combining formal specification monitoring with data driven RL control can substantially improve the reliability of autonomous flight control in challenging environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于共形信号时序逻辑的强化学习鲁棒控制：案例研究</div>
<div class="mono" style="margin-top:8px">本研究探讨形式化时序逻辑规约如何提升航空航天应用中强化学习控制的安全性与鲁棒性。基于开源AeroBench F-16仿真基准，我们训练近端策略优化智能体调节发动机油门并跟踪指令空速。控制目标被编码为信号时序逻辑要求：在每次机动最后阶段将空速维持在预设区间。为实现运行时规约执行，我们提出共形STL防护罩，通过在线共形预测过滤强化学习智能体的动作。我们在三种设置下进行对比：（1）PPO基线，（2）PPO+传统基于规则的STL防护罩，（3）PPO+所提共形防护罩，测试场景包含标称工况及涉及气动模型失配、执行器速率限制、测量噪声和任务中设定点突变的极端应力工况。实验表明共形防护罩在保持接近基线性能的同时确保STL满足度，且比传统防护罩提供更强的鲁棒性保证。这些结果证明，将形式化规约监测与数据驱动的强化学习控制相结合，能显著提升挑战性环境下自主飞行控制的可靠性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study aims to improve the safety and robustness of reinforcement learning (RL) control in aerospace by integrating formal temporal logic specifications. The method employs a Proximal Policy Optimization (PPO) agent trained in an F-16 simulation, with the control objective encoded as a Signal Temporal Logic (STL) requirement; a novel conformal STL shield is introduced to filter the agent&#x27;s actions using online conformal prediction for runtime enforcement. Experimental results under nominal and stressed conditions show that the proposed conformal shield maintains STL satisfaction and near-baseline performance while offering stronger robustness guarantees compared to a classical rule-based shield, demonstrating enhanced reliability for autonomous flight control.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过整合形式化时序逻辑规约，提升强化学习在航空航天控制中的安全性与鲁棒性。方法采用在F-16仿真中训练的近端策略优化（PPO）智能体，将控制目标编码为信号时序逻辑（STL）要求，并引入一种新颖的保形STL屏蔽器，利用在线保形预测在运行时过滤智能体的动作。在标称和压力场景下的实验结果表明，所提出的保形屏蔽器在保持STL满足度和接近基线性能的同时，相比传统的基于规则的屏蔽器提供了更强的鲁棒性保证，证明了其在挑战性环境中能显著提升自主飞行控制的可靠性。</div>
</details>
</div>
<div class="card">
<div class="title">Vid2Sid: Videos Can Help Close the Sim2Real Gap</div>
<div class="meta-line">Authors: Kevin Qiu, Yu Zhang, Marek Cygan, Josie Hughes</div>
<div class="meta-line">First: 2026-02-22T22:08:16+00:00 · Latest: 2026-02-22T22:08:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19359v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19359v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Calibrating a robot simulator&#x27;s physics parameters (friction, damping, material stiffness) to match real hardware is often done by hand or with black-box optimizers that reduce error but cannot explain which physical discrepancies drive the error. When sensing is limited to external cameras, the problem is further compounded by perception noise and the absence of direct force or state measurements. We present Vid2Sid, a video-driven system identification pipeline that couples foundation-model perception with a VLM-in-the-loop optimizer that analyzes paired sim-real videos, diagnoses concrete mismatches, and proposes physics parameter updates with natural language rationales. We evaluate our approach on a tendon-actuated finger (rigid-body dynamics in MuJoCo) and a deformable continuum tentacle (soft-body dynamics in PyElastica). On sim2real holdout controls unseen during training, Vid2Sid achieves the best average rank across all settings, matching or exceeding black-box optimizers while uniquely providing interpretable reasoning at each iteration. Sim2sim validation confirms that Vid2Sid recovers ground-truth parameters most accurately (mean relative error under 13\% vs. 28--98\%), and ablation analysis reveals three calibration regimes. VLM-guided optimization excels when perception is clean and the simulator is expressive, while model-class limitations bound performance in more challenging settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Vid2Sid：视频助力缩小仿真与现实差距</div>
<div class="mono" style="margin-top:8px">传统机器人仿真器的物理参数（摩擦、阻尼、材料刚度）标定通常依赖人工或黑盒优化器，虽能降低误差却无法解释误差的物理成因。当感知仅限于外部相机时，感知噪声与缺乏直接力/状态测量进一步加剧了问题。本文提出Vid2Sid——一种视频驱动的系统辨识流程，通过耦合基础模型感知与视觉语言模型闭环优化器，分析成对的仿真-现实视频，诊断具体失配点，并以自然语言论证提出物理参数更新方案。我们在肌腱驱动手指（MuJoCo刚体动力学）和可变形连续体触手（PyElastica软体动力学）上评估该方法。在训练未见的仿真-现实控制任务中，Vid2Sid在所有设定下均取得最佳平均排名，性能匹配或超越黑盒优化器，且能逐轮提供可解释的推理过程。仿真间验证证实Vid2Sid最精确地恢复了真实参数（平均相对误差&lt;13% vs. 28-98%），消融实验揭示了三种标定机制：在感知清晰且仿真器表达能力强的场景中，视觉语言模型引导的优化表现卓越；而在更具挑战性的场景中，模型类别限制成为性能瓶颈。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Vid2Sid, a video-driven system identification pipeline motivated by the challenge of calibrating robot simulators to match real hardware, especially when sensing is limited to external cameras and traditional black-box optimizers lack interpretability. The method couples foundation-model perception with a vision-language model (VLM)-in-the-loop optimizer that analyzes paired simulation-real videos, diagnoses physical mismatches, and proposes physics parameter updates with natural language rationales. Experimental results on a tendon-actuated finger and a deformable continuum tentacle show that Vid2Sid achieves the best average rank on sim2real holdout controls, matches or exceeds black-box optimizer performance, and provides interpretable reasoning, while sim2sim validation confirms it recovers ground-truth parameters most accurately with mean relative error under 13%.</div>
<div class="mono" style="margin-top:8px">本文提出了Vid2Sid，一种视频驱动的系统辨识流程，其动机在于解决机器人模拟器物理参数校准以匹配真实硬件的难题，特别是在传感仅限于外部相机且传统黑盒优化器缺乏可解释性的情况下。该方法结合基础模型感知与视觉语言模型（VLM）在环优化器，分析成对的模拟-真实视频，诊断物理不匹配，并以自然语言理由提出物理参数更新。在肌腱驱动手指和可变形连续体触手上的实验结果表明，Vid2Sid在训练未见过的sim2real控制任务中取得最佳平均排名，匹配或超越黑盒优化器性能，并提供可解释的推理，同时sim2sim验证证实其以低于13%的平均相对误差最准确地恢复真实参数。</div>
</details>
</div>
<div class="card">
<div class="title">Interpretable Failure Analysis in Multi-Agent Reinforcement Learning Systems</div>
<div class="meta-line">Authors: Risal Shahriar Shefin, Debashis Gupta, Thai Le, Sarra Alqahtani</div>
<div class="meta-line">First: 2026-02-08T19:55:26+00:00 · Latest: 2026-02-22T21:06:22+00:00</div>
<div class="meta-line">Comments: Accepted to the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08104v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08104v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-Agent Reinforcement Learning (MARL) is increasingly deployed in safety-critical domains, yet methods for interpretable failure detection and attribution remain underdeveloped. We introduce a two-stage gradient-based framework that provides interpretable diagnostics for three critical failure analysis tasks: (1) detecting the true initial failure source (Patient-0); (2) validating why non-attacked agents may be flagged first due to domino effects; and (3) tracing how failures propagate through learned coordination pathways. Stage 1 performs interpretable per-agent failure detection via Taylor-remainder analysis of policy-gradient costs, declaring an initial Patient-0 candidate at the first threshold crossing. Stage 2 provides validation through geometric analysis of critic derivatives-first-order sensitivity and directional second-order curvature aggregated over causal windows to construct interpretable contagion graphs. This approach explains &quot;downstream-first&quot; detection anomalies by revealing pathways that amplify upstream deviations. Evaluated across 500 episodes in Simple Spread (3 and 5 agents) and 100 episodes in StarCraft II using MADDPG and HATRPO, our method achieves 88.2-99.4% Patient-0 detection accuracy while providing interpretable geometric evidence for detection decisions. By moving beyond black-box detection to interpretable gradient-level forensics, this framework offers practical tools for diagnosing cascading failures in safety-critical MARL systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多智能体强化学习系统中的可解释故障分析</div>
<div class="mono" style="margin-top:8px">多智能体强化学习（MARL）正日益应用于安全关键领域，但可解释的故障检测与归因方法仍不成熟。本文提出一种基于梯度的两阶段框架，为三项关键故障分析任务提供可解释诊断：（1）检测真实初始故障源（零号患者）；（2）验证因连锁效应导致未受攻击智能体被率先标记的原因；（3）追踪故障如何通过习得的协作路径传播。第一阶段通过策略梯度成本的泰勒余项分析实现可解释的逐智能体故障检测，在首次阈值突破时声明初始零号患者候选。第二阶段通过评论家导数的几何分析——聚合因果窗口内的一阶敏感性与方向二阶曲率——构建可解释传播图进行验证。该方法通过揭示放大上游偏差的路径，解释了“下游优先”检测异常现象。在Simple Spread（3和5智能体）的500轮次及StarCraft II（使用MADDPG与HATRPO算法）的100轮次评估中，本方法实现88.2-99.4%的零号患者检测准确率，并为检测决策提供可解释的几何证据。该框架突破黑盒检测局限，提供梯度层面的可解释取证工具，为安全关键MARL系统的级联故障诊断提供实用方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is the need for interpretable failure analysis in safety-critical Multi-Agent Reinforcement Learning (MARL) systems, where current methods lack transparency. The proposed method is a two-stage gradient-based framework that performs interpretable diagnostics for failure source detection, validation of detection anomalies, and failure propagation tracing, using Taylor-remainder analysis and geometric analysis of critic derivatives. The main experimental results, from evaluations in Simple Spread and StarCraft II environments using algorithms like MADDPG and HATRPO, show that the method achieves 88.2-99.4% accuracy in identifying the initial failure source while providing interpretable geometric evidence for its decisions.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于安全关键多智能体强化学习系统对可解释故障分析的需求，现有方法缺乏透明度。所提出的方法是一个基于梯度的两阶段框架，通过泰勒余项分析和评论家导数的几何分析，对故障源检测、检测异常验证及故障传播追踪进行可解释诊断。主要实验结果基于在Simple Spread和星际争霸II环境中使用MADDPG和HATRPO等算法的评估，表明该方法在识别初始故障源时达到88.2-99.4%的准确率，同时为决策提供了可解释的几何证据。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Reasoning for Diffusion LLMs via Distribution Matching Policy Optimization</div>
<div class="meta-line">Authors: Yuchen Zhu, Wei Guo, Jaemoo Choi, Petr Molodyk, Bo Yuan, Molei Tao, Yongxin Chen</div>
<div class="meta-line">First: 2025-10-09T13:59:50+00:00 · Latest: 2026-02-22T20:36:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.08233v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.08233v2">PDF</a> · <a href="https://github.com/yuchen-zhu-zyc/DMPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion large language models (dLLMs) are promising alternatives to autoregressive large language models (AR-LLMs), as they potentially allow higher inference throughput. Reinforcement learning (RL) is a crucial component for dLLMs to achieve comparable performance with AR-LLMs on important tasks, such as reasoning. However, RL algorithms that are well-suited for dLLMs&#x27; unique characteristics have yet to be developed. This paper proposes Distribution Matching Policy Optimization (DMPO), a principled and theoretically grounded RL fine-tuning method specifically designed to enhance the reasoning capabilities of dLLMs by matching the dLLM policy distribution to the optimal, reward-tilted one through cross-entropy optimization. We identify a key challenge in the implementation with a small training batch size and propose several effective solutions through a novel weight baseline subtraction technique. DMPO exhibits superior performance on multiple reasoning benchmarks without supervised fine-tuning, with an accuracy improvement of up to $54.3\%$ over previously SOTA baselines and $66.41\%$ over the base model, underscoring the effectiveness of the distribution matching framework. Our code is available at https://github.com/yuchen-zhu-zyc/DMPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过分布匹配策略优化增强扩散大语言模型的推理能力</div>
<div class="mono" style="margin-top:8px">扩散大语言模型（dLLMs）作为自回归大语言模型（AR-LLMs）的有前景替代方案，因其可能实现更高的推理吞吐量而备受关注。强化学习（RL）是使dLLMs在推理等重要任务上达到与AR-LLMs相当性能的关键组成部分，但目前尚缺乏针对dLLMs独特特性设计的RL算法。本文提出分布匹配策略优化（DMPO），这是一种基于理论原则的RL微调方法，专门通过交叉熵优化将dLLM的策略分布与最优的奖励倾斜分布对齐，以增强其推理能力。我们指出了在小训练批量下实施的关键挑战，并通过新颖的权重基线消减技术提出了多种有效解决方案。DMPO在多个推理基准测试中展现出卓越性能，无需监督微调即可实现高达54.3%相对于先前SOTA基线的准确率提升，以及66.41%相对于基础模型的提升，充分证明了分布匹配框架的有效性。代码已开源：https://github.com/yuchen-zhu-zyc/DMPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for specialized reinforcement learning (RL) methods to enhance the reasoning capabilities of diffusion large language models (dLLMs), which offer potential inference throughput advantages over autoregressive LLMs but lack tailored optimization approaches. The authors propose Distribution Matching Policy Optimization (DMPO), a principled RL fine-tuning method that aligns the dLLM&#x27;s policy distribution with an optimal, reward-tilted distribution via cross-entropy optimization, overcoming implementation challenges related to small batch sizes through a novel weight baseline subtraction technique. Experimental results demonstrate that DMPO significantly improves reasoning performance without supervised fine-tuning, achieving accuracy gains of up to 54.3% over prior state-of-the-art baselines and 66.41% over the base model across multiple benchmarks.</div>
<div class="mono" style="margin-top:8px">本文针对扩散大语言模型（dLLMs）在推理任务中缺乏专门强化学习优化方法的问题展开研究，dLLMs相比自回归大语言模型具有潜在推理吞吐优势。作者提出了分布匹配策略优化（DMPO），这是一种基于理论的强化学习微调方法，通过交叉熵优化使dLLM的策略分布与最优的奖励倾斜分布对齐，并采用新颖的权重基线减法技术解决了小批量训练带来的实现挑战。实验结果表明，DMPO在无需监督微调的情况下显著提升了推理能力，在多个基准测试中准确率较先前最优基线提升最高达54.3%，较基础模型提升66.41%。</div>
</details>
</div>
<div class="card">
<div class="title">AFABench: A Generic Framework for Benchmarking Active Feature Acquisition</div>
<div class="meta-line">Authors: Valter Schütz, Han Wu, Reza Rezvan, Linus Aronsson, Morteza Haghir Chehreghani</div>
<div class="meta-line">First: 2025-08-20T14:29:16+00:00 · Latest: 2026-02-22T20:33:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.14734v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.14734v3">PDF</a> · <a href="https://github.com/Linusaronsson/AFA-Benchmark">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In many real-world scenarios, acquiring all features of a data instance can be expensive or impractical due to monetary cost, latency, or privacy concerns. Active Feature Acquisition (AFA) addresses this challenge by dynamically selecting a subset of informative features for each data instance, trading predictive performance against acquisition cost. While numerous methods have been proposed for AFA, ranging from myopic information-theoretic strategies to non-myopic reinforcement learning approaches, fair and systematic evaluation of these methods has been hindered by a lack of standardized benchmarks. In this paper, we introduce AFABench, the first benchmark framework for AFA. Our benchmark includes a diverse set of synthetic and real-world datasets, supports a wide range of acquisition policies, and provides a modular design that enables easy integration of new methods and tasks. We implement and evaluate representative algorithms from all major categories, including static, myopic, and reinforcement learning-based approaches. To test the lookahead capabilities of AFA policies, we introduce a novel synthetic dataset, CUBE-NM, designed to expose the limitations of myopic selection. Our results highlight key trade-offs between different AFA strategies and provide actionable insights for future research. The benchmark code is available at: https://github.com/Linusaronsson/AFA-Benchmark.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AFABench：一种用于基准测试主动特征获取的通用框架</div>
<div class="mono" style="margin-top:8px">在许多现实场景中，由于成本、延迟或隐私问题，获取数据实例的所有特征可能代价高昂或不切实际。主动特征获取（AFA）通过动态选择每个数据实例的信息特征子集来解决这一挑战，在预测性能与获取成本之间进行权衡。尽管已提出多种AFA方法，从短视的信息论策略到非短视的强化学习方法，但由于缺乏标准化基准，这些方法的公平系统评估一直受阻。本文介绍了首个AFA基准框架AFABench。该基准包含多样化的合成与真实数据集，支持广泛的获取策略，并提供模块化设计，便于新方法与任务的集成。我们实现并评估了所有主要类别的代表性算法，包括静态、短视及基于强化学习的方法。为测试AFA策略的前瞻能力，我们引入新型合成数据集CUBE-NM，旨在揭示短视选择的局限性。实验结果凸显了不同AFA策略间的关键权衡，并为未来研究提供了实用见解。基准代码发布于：https://github.com/Linusaronsson/AFA-Benchmark。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need for a standardized evaluation framework for Active Feature Acquisition (AFA) methods, which aim to reduce feature acquisition costs while maintaining predictive performance, as existing methods lack fair comparison. The authors introduce AFABench, a generic benchmark framework that includes diverse datasets, supports various acquisition policies, and offers modular design for easy integration of new methods; they also create a synthetic dataset, CUBE-NM, to test non-myopic capabilities. Experimental results from evaluating representative algorithms, including static, myopic, and reinforcement learning approaches, reveal key trade-offs between strategies and provide insights for future research.</div>
<div class="mono" style="margin-top:8px">本文的动机在于需要为主动特征获取方法建立一个标准化评估框架，因为这些方法旨在降低特征获取成本同时保持预测性能，但现有方法缺乏公平比较。作者提出了AFABench，这是一个通用基准框架，包含多样化数据集，支持多种获取策略，并采用模块化设计以便轻松集成新方法；他们还创建了一个合成数据集CUBE-NM来测试非短视能力。通过评估代表性算法（包括静态、短视和基于强化学习的方法），实验结果表明了不同策略之间的关键权衡，并为未来研究提供了实用见解。</div>
</details>
</div>
<div class="card">
<div class="title">Soft Sequence Policy Optimization: Bridging GMPO and SAPO</div>
<div class="meta-line">Authors: Svetlana Glazyrina, Maksim Kryzhanovskiy, Roman Ischenko</div>
<div class="meta-line">First: 2026-02-22T20:21:00+00:00 · Latest: 2026-02-22T20:21:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19327v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19327v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A significant portion of recent research on Large Language Model (LLM) alignment focuses on developing new policy optimization methods based on Group Relative Policy Optimization (GRPO). Two prominent directions have emerged: (i) a shift toward sequence-level importance sampling weights that better align with the sequence-level rewards used in many tasks, and (ii) alternatives to PPO-style clipping that aim to avoid the associated loss of training signal and entropy collapse. Recent work, such as Soft Adaptive Policy Optimization (SAPO), reformulates the Scopic objective within the GRPO framework and achieves both sequence coherence and token adaptivity. Geometric-Mean Policy Optimization (GMPO) leverages token-wise ratio clipping within sequence importance sampling weights. Building on these ideas, this work proposes a new objective that promotes effective policy exploration while maintaining training stability. Specifically, we introduce Soft Sequence Policy Optimization, an off-policy reinforcement learning objective that incorporates soft gating functions over token-level probability ratios within sequence-level importance weights.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>软序列策略优化：连接GMPO与SAPO</div>
<div class="mono" style="margin-top:8px">近期大语言模型对齐研究的重要方向是基于组相对策略优化框架开发新的策略优化方法。两个主要趋势包括：(1) 转向与任务中序列级奖励更匹配的序列级重要性采样权重；(2) 寻找替代PPO式裁剪的方法以避免训练信号丢失和熵崩溃。近期研究如软自适应策略优化在GRPO框架内重构了Scopic目标，同时实现了序列连贯性和词元自适应性。几何平均策略优化则在序列重要性采样权重中采用词元级比率裁剪。基于这些进展，本研究提出一种新目标，在保持训练稳定性的同时促进有效策略探索。具体而言，我们提出软序列策略优化——一种离策略强化学习目标，通过在序列级重要性权重中引入词元级概率比率的软门控函数来实现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to improve policy optimization for Large Language Model alignment, this work addresses limitations in existing methods like GRPO by bridging two recent approaches: sequence-level importance sampling for better reward alignment and alternatives to PPO-style clipping to prevent training signal loss. The method introduces Soft Sequence Policy Optimization, an off-policy reinforcement learning objective that integrates soft gating functions over token-level probability ratios within sequence-level importance weights, aiming to enhance policy exploration while ensuring stability. Experimental results demonstrate that this approach effectively balances sequence coherence and token adaptivity, promoting robust training without entropy collapse.</div>
<div class="mono" style="margin-top:8px">本研究旨在改进大语言模型对齐中的策略优化方法，针对现有方法如GRPO的局限性，结合了两种近期方向：使用序列级重要性采样以更好匹配任务奖励，以及替代PPO式裁剪以避免训练信号丢失和熵崩溃。方法上提出了软序列策略优化，这是一种离策略强化学习目标，通过在序列级重要性权重中引入对令牌级概率比的软门控函数，以在保持训练稳定性的同时促进有效的策略探索。实验结果表明，该方法能有效平衡序列连贯性和令牌自适应性，实现稳健训练且避免熵崩溃。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260224_0355.html">20260224_0355</a>
<a href="archive/20260223_0321.html">20260223_0321</a>
<a href="archive/20260222_0327.html">20260222_0327</a>
<a href="archive/20260221_0347.html">20260221_0347</a>
<a href="archive/20260220_0349.html">20260220_0349</a>
<a href="archive/20260219_0406.html">20260219_0406</a>
<a href="archive/20260218_0358.html">20260218_0358</a>
<a href="archive/20260217_0329.html">20260217_0329</a>
<a href="archive/20260216_0321.html">20260216_0321</a>
<a href="archive/20260215_0335.html">20260215_0335</a>
<a href="archive/20260213_0416.html">20260213_0416</a>
<a href="archive/20260212_0417.html">20260212_0417</a>
<a href="archive/20260211_0421.html">20260211_0421</a>
<a href="archive/20260210_0359.html">20260210_0359</a>
<a href="archive/20260209_0321.html">20260209_0321</a>
<a href="archive/20260208_0336.html">20260208_0336</a>
<a href="archive/20260207_0400.html">20260207_0400</a>
<a href="archive/20260206_0358.html">20260206_0358</a>
<a href="archive/20260205_0400.html">20260205_0400</a>
<a href="archive/20260204_0411.html">20260204_0411</a>
<a href="archive/20260202_0320.html">20260202_0320</a>
<a href="archive/20260201_0343.html">20260201_0343</a>
<a href="archive/20260131_0357.html">20260131_0357</a>
<a href="archive/20260130_0355.html">20260130_0355</a>
<a href="archive/20260129_0353.html">20260129_0353</a>
<a href="archive/20260128_0342.html">20260128_0342</a>
<a href="archive/20260127_0320.html">20260127_0320</a>
<a href="archive/20260126_0313.html">20260126_0313</a>
<a href="archive/20260125_0321.html">20260125_0321</a>
<a href="archive/20260124_0344.html">20260124_0344</a>
<a href="archive/20260123_0345.html">20260123_0345</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0408.html">20260121_0408</a>
<a href="archive/20260120_0316.html">20260120_0316</a>
<a href="archive/20260119_0312.html">20260119_0312</a>
<a href="archive/20260118_0323.html">20260118_0323</a>
<a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
