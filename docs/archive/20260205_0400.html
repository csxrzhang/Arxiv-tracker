<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-05 04:00</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260205_0400</div>
    <div class="row"><div class="card">
<div class="title">Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes</div>
<div class="meta-line">Authors: Amrith Setlur, Zijian Wang, Andrew Cohen, Paria Rashidinejad, Sang Michael Xie</div>
<div class="meta-line">First: 2026-01-26T18:57:00+00:00 · Latest: 2026-02-03T18:58:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18795v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18795v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. PrefixRL boosts the learning signal on hard problems by modulating the difficulty of the problem through the off-policy prefix length. We prove that the PrefixRL objective is not only consistent with the standard RL objective but also more sample efficient. Empirically, we discover back-generalization: training only on prefixed problems generalizes to out-of-distribution unprefixed performance, with learned strategies often differing from those in the prefix. In our experiments, we source the off-policy traces by rejection sampling with the base model, creating a self-improvement loop. On hard reasoning problems, PrefixRL reaches the same training reward 2x faster than the strongest baseline (SFT on off-policy data then RL), even after accounting for the compute spent on the initial rejection sampling, and increases the final reward by 3x. The gains transfer to held-out benchmarks, and PrefixRL is still effective when off-policy traces are derived from a different model family, validating its flexibility in practical settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>复用计算量：通过条件化极离策略前缀扩展强化学习在难题上的应用</div>
<div class="mono" style="margin-top:8px">传统用于大语言模型推理的强化学习方法在难题上浪费算力，因为正确的同策略轨迹稀少、策略梯度消失且学习停滞。为引导更高效的强化学习，我们考虑以离策略轨迹的形式复用旧采样计算量（来自先前的推理或强化学习训练）。标准离策略方法通过离策略数据进行监督，导致强化学习优化过程不稳定。我们提出PrefixRL方法：以成功离策略轨迹的前缀为条件，运行同策略强化学习来完成后续部分，从而规避离策略不稳定性。该方法通过调节离策略前缀长度来控制问题难度，从而增强难题上的学习信号。我们证明PrefixRL目标不仅与标准强化学习目标一致，且具有更高的样本效率。实证中发现反向泛化现象：仅在前缀化问题上训练可泛化至分布外无前缀任务，且习得策略常与前缀策略不同。实验中，我们通过基础模型的拒绝采样获取离策略轨迹，形成自我改进循环。在复杂推理问题上，即使计入初始拒绝采样的算力消耗，PrefixRL达到相同训练奖励的速度仍比最强基线（离策略数据监督微调后强化学习）快2倍，最终奖励提升3倍。该优势可迁移至保留基准测试，且当离策略轨迹源自不同模型族时PrefixRL依然有效，验证了其在实际场景中的灵活性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the inefficiency of standard reinforcement learning (RL) for large language models on hard reasoning problems, where on-policy learning stalls due to rare correct traces and vanishing gradients. To overcome this, the authors propose PrefixRL, a method that reuses off-policy traces from prior inference by conditioning on their prefixes and performing on-policy RL to complete them, thereby avoiding instabilities from direct off-policy supervision while modulating problem difficulty. Experimental results show that PrefixRL achieves training rewards 2x faster than strong baselines and improves final rewards by 3x on hard problems, with gains transferring to held-out benchmarks and demonstrating flexibility across model families.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在困难推理问题上标准强化学习效率低下的问题，提出了一种名为PrefixRL的方法，该方法通过重用先前推理中的离策略轨迹，以其前缀为条件进行在策略强化学习来完成轨迹，从而避免直接离策略监督的不稳定性并调节问题难度。实验结果表明，PrefixRL在困难问题上比最强基线快2倍达到相同训练奖励，并将最终奖励提高3倍，其增益可迁移到保留基准测试中，且在不同模型家族来源的离策略轨迹上仍有效，验证了其实用灵活性。</div>
</details>
</div>
<div class="card">
<div class="title">Understanding and Exploiting Weight Update Sparsity for Communication-Efficient Distributed RL</div>
<div class="meta-line">Authors: Erfan Miahi, Eugene Belilovsky</div>
<div class="meta-line">First: 2026-02-03T18:56:48+00:00 · Latest: 2026-02-03T18:56:48+00:00</div>
<div class="meta-line">Comments: 32 pages, 14 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03839v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03839v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is a critical component for post-training large language models (LLMs). However, in bandwidth-constrained distributed RL, scalability is often bottlenecked by the synchronization of policy weights from trainers to inference workers, particularly over commodity networks or in decentralized settings. While recent studies suggest that RL updates modify only a small fraction of model parameters, these observations are typically based on coarse checkpoint differences. We present a systematic empirical study of weight-update sparsity at both step-level and multi-step granularities, examining its evolution across training dynamics, off-policy delay, and model scale. We find that update sparsity is consistently high, frequently exceeding 99% across practically relevant settings. Leveraging this structure, we propose PULSE (Patch Updates via Lossless Sparse Encoding), a simple yet highly efficient lossless weight synchronization method that transmits only the indices and values of modified parameters. PULSE is robust to transmission errors and avoids floating-point drift inherent in additive delta schemes. In bandwidth-constrained decentralized environments, our approach achieves over 100x (14 GB to ~108 MB) communication reduction while maintaining bit-identical training dynamics and performance compared to full weight synchronization. By exploiting this structure, PULSE enables decentralized RL training to approach centralized throughput, reducing the bandwidth required for weight synchronization from 20 Gbit/s to 0.2 Gbit/s to maintain high GPU utilization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>理解并利用权重更新稀疏性实现通信高效的分布式强化学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL）是大型语言模型（LLM）后训练的关键组成部分。然而，在带宽受限的分布式RL中，可扩展性常受限于从训练器到推理工作器的策略权重同步，尤其是在商用网络或去中心化环境中。尽管近期研究表明RL更新仅修改模型参数的极小部分，但这些观察通常基于粗略的检查点差异。我们系统实证研究了步级与多步粒度下的权重更新稀疏性，分析了其在训练动态、离策略延迟和模型规模中的演变。我们发现更新稀疏性始终较高，在实际相关设置中常超过99%。利用此结构，我们提出PULSE（基于无损稀疏编码的补丁更新），这是一种简单高效的无损权重同步方法，仅传输修改参数的索引和数值。PULSE对传输错误具有鲁棒性，避免了加法增量方案固有的浮点漂移。在带宽受限的去中心化环境中，相比全权重同步，我们的方法实现了超过100倍（从14 GB降至约108 MB）的通信缩减，同时保持比特级一致的训练动态与性能。通过利用此结构，PULSE使去中心化RL训练接近中心化吞吐量，将维持高GPU利用率所需的权重同步带宽从20 Gbit/s降至0.2 Gbit/s。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the communication bottleneck in distributed reinforcement learning (RL) for large language models, where synchronizing full policy weights across networks limits scalability. The method involves a systematic study revealing that RL weight updates are extremely sparse (often &gt;99%), leading to the proposal of PULSE, a lossless synchronization technique that transmits only the indices and values of changed parameters. The main experimental results demonstrate that PULSE reduces communication by over 100x (from ~14 GB to ~108 MB) in bandwidth-constrained settings while preserving training performance and dynamics, effectively lowering required bandwidth from 20 Gbit/s to 0.2 Gbit/s to maintain high GPU utilization.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于分布式强化学习在大型语言模型训练中的通信瓶颈，即跨网络同步完整策略权重会限制可扩展性。方法上，通过系统实证研究发现强化学习权重更新具有极高稀疏性（常超过99%），据此提出了PULSE——一种仅传输修改参数索引和值的无损同步技术。主要实验结果表明，在带宽受限环境中，PULSE实现了超过100倍的通信压缩（从约14GB降至约108MB），同时保持训练性能与动态不变，将维持高GPU利用率所需的同步带宽从20Gbit/s降至0.2Gbit/s。</div>
</details>
</div>
<div class="card">
<div class="title">ME-IGM: Individual-Global-Max in Maximum Entropy Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Wen-Tse Chen, Yuxuan Li, Shiyu Huang, Jiayu Chen, Jeff Schneider</div>
<div class="meta-line">Venue: Proc. of the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026), Paphos, Cyprus, May 25 - 29, 2026, IFAAMAS, 19 pages</div>
<div class="meta-line">First: 2024-06-20T01:55:08+00:00 · Latest: 2026-02-03T18:35:29+00:00</div>
<div class="meta-line">Comments: Published in the Proceedings of the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.13930v4">Abs</a> · <a href="https://arxiv.org/pdf/2406.13930v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent credit assignment is a fundamental challenge for cooperative multi-agent reinforcement learning (MARL), where a team of agents learn from shared reward signals. The Individual-Global-Max (IGM) condition is a widely used principle for multi-agent credit assignment, requiring that the joint action determined by individual Q-functions maximizes the global Q-value. Meanwhile, the principle of maximum entropy has been leveraged to enhance exploration in MARL. However, we identify a critical limitation in existing maximum entropy MARL methods: a misalignment arises between local policies and the joint policy that maximizes the global Q-value, leading to violations of the IGM condition. To address this misalignment, we propose an order-preserving transformation. Building on it, we introduce ME-IGM, a novel maximum entropy MARL algorithm compatible with any credit assignment mechanism that satisfies the IGM condition while enjoying the benefits of maximum entropy exploration. We empirically evaluate two variants of ME-IGM: ME-QMIX and ME-QPLEX, in non-monotonic matrix games, and demonstrate their state-of-the-art performance across 17 scenarios in SMAC-v2 and Overcooked.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ME-IGM：最大熵多智能体强化学习中的个体-全局最大化</div>
<div class="mono" style="margin-top:8px">多智能体信用分配是合作式多智能体强化学习（MARL）的核心挑战，其中智能体团队从共享奖励信号中学习。个体-全局最大化（IGM）条件是广泛使用的多智能体信用分配原则，要求由个体Q函数确定的联合动作能最大化全局Q值。同时，最大熵原理已被用于增强MARL中的探索。然而，我们发现现有最大熵MARL方法存在一个关键局限：局部策略与最大化全局Q值的联合策略之间出现错位，导致违反IGM条件。为解决此问题，我们提出一种保序变换方法。在此基础上，我们引入ME-IGM——一种新颖的最大熵MARL算法，兼容任何满足IGM条件的信用分配机制，同时享有最大熵探索的优势。我们在非单调矩阵游戏中实证评估了ME-IGM的两个变体：ME-QMIX和ME-QPLEX，并在SMAC-v2和Overcooked的17个场景中展示了其领先性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses a misalignment between local policies and the joint policy in maximum entropy multi-agent reinforcement learning (MARL), which violates the Individual-Global-Max (IGM) condition crucial for credit assignment. To resolve this, the authors propose an order-preserving transformation and introduce ME-IGM, a novel algorithm that integrates maximum entropy exploration with any IGM-compliant credit assignment method. Experimental results show that its variants, ME-QMIX and ME-QPLEX, achieve state-of-the-art performance in non-monotonic matrix games and across 17 scenarios in SMAC-v2 and Overcooked benchmarks.</div>
<div class="mono" style="margin-top:8px">本文针对最大熵多智能体强化学习（MARL）中局部策略与联合策略之间的错位问题，该问题破坏了信用分配的关键原则——个体-全局最大值（IGM）条件。为解决此问题，作者提出了一种保序变换，并引入了ME-IGM算法，该算法将最大熵探索与任何符合IGM条件的信用分配机制相结合。实验结果表明，其变体ME-QMIX和ME-QPLEX在非单调矩阵游戏以及SMAC-v2和Overcooked的17个场景中均取得了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving</div>
<div class="meta-line">Authors: Yesom Park, Annie C. Lu, Shao-Ching Huang, Qiyang Hu, Y. Sungtaek Ju, Stanley Osher</div>
<div class="meta-line">First: 2026-02-03T18:18:30+00:00 · Latest: 2026-02-03T18:18:30+00:00</div>
<div class="meta-line">Comments: 27 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03816v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03816v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose SymPlex, a reinforcement learning framework for discovering analytical symbolic solutions to partial differential equations (PDEs) without access to ground-truth expressions. SymPlex formulates symbolic PDE solving as tree-structured decision-making and optimizes candidate solutions using only the PDE and its boundary conditions. At its core is SymFormer, a structure-aware Transformer that models hierarchical symbolic dependencies via tree-relative self-attention and enforces syntactic validity through grammar-constrained autoregressive decoding, overcoming the limited expressivity of sequence-based generators. Unlike numerical and neural approaches that approximate solutions in discretized or implicit function spaces, SymPlex operates directly in symbolic expression space, enabling interpretable and human-readable solutions that naturally represent non-smooth behavior and explicit parametric dependence. Empirical results demonstrate exact recovery of non-smooth and parametric PDE solutions using deep learning-based symbolic methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SymPlex：面向符号偏微分方程求解的结构感知Transformer</div>
<div class="mono" style="margin-top:8px">本文提出SymPlex——一种无需真实表达式即可发现偏微分方程解析符号解的强化学习框架。该框架将符号PDE求解建模为树结构决策过程，仅利用PDE及其边界条件优化候选解。其核心组件SymFormer是一种结构感知Transformer，通过树相对自注意力建模层次化符号依赖关系，并借助语法约束的自回归解码确保句法有效性，克服了基于序列生成器的表达能力局限。与在离散化或隐函数空间逼近解的传统数值/神经网络方法不同，SymPlex直接在符号表达式空间操作，生成可解释、人类可读的解，天然表征非光滑行为与显式参数依赖。实验结果表明，该基于深度学习的符号方法能精确还原非光滑及含参数PDE的解析解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces SymPlex, a reinforcement learning framework motivated by the need to discover analytical symbolic solutions for partial differential equations (PDEs) without relying on ground-truth expressions, aiming to produce interpretable, human-readable results. The method formulates symbolic PDE solving as tree-structured decision-making, utilizing SymFormer—a structure-aware Transformer that employs tree-relative self-attention and grammar-constrained autoregressive decoding to model hierarchical dependencies and ensure syntactic validity, thereby addressing limitations of sequence-based generators. Experimental results show that SymPlex successfully recovers exact non-smooth and parametric PDE solutions, demonstrating the effectiveness of deep learning-based symbolic methods in directly operating within symbolic expression spaces.</div>
<div class="mono" style="margin-top:8px">本文提出了SymPlex，一个强化学习框架，其动机是在无需真实表达式的情况下发现偏微分方程的解析符号解，以生成可解释、人类可读的结果。该方法将符号PDE求解构建为树结构决策过程，核心是SymFormer——一种结构感知的Transformer，通过树相对自注意力和语法约束的自回归解码来建模层次依赖关系并确保句法有效性，从而克服了基于序列生成器的表达限制。实验结果表明，SymPlex能够精确恢复非光滑和参数化的PDE解，证明了基于深度学习的符号方法在直接操作符号表达式空间中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation</div>
<div class="meta-line">Authors: Ziru Chen, Dongdong Chen, Ruinan Jin, Yingbin Liang, Yujia Xie, Huan Sun</div>
<div class="meta-line">First: 2026-02-03T18:08:41+00:00 · Latest: 2026-02-03T18:08:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03806v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03806v1">PDF</a> · <a href="https://github.com/OSU-NLP-Group/cobalt">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, there have been significant research interests in training large language models (LLMs) with reinforcement learning (RL) on real-world tasks, such as multi-turn code generation. While online RL tends to perform better than offline RL, its higher training cost and instability hinders wide adoption. In this paper, we build on the observation that multi-turn code generation can be formulated as a one-step recoverable Markov decision process and propose contextual bandit learning with offline trajectories (Cobalt), a new method that combines the benefits of online and offline RL. Cobalt first collects code generation trajectories using a reference LLM and divides them into partial trajectories as contextual prompts. Then, during online bandit learning, the LLM is trained to complete each partial trajectory prompt through single-step code generation. Cobalt outperforms two multi-turn online RL baselines based on GRPO and VeRPO, and substantially improves R1-Distill 8B and Qwen3 8B by up to 9.0 and 6.2 absolute Pass@1 scores on LiveCodeBench. Also, we analyze LLMs&#x27; in-context reward hacking behaviors and augment Cobalt training with perturbed trajectories to mitigate this issue. Overall, our results demonstrate Cobalt as a promising solution for iterative decision-making tasks like multi-turn code generation. Our code and data are available at https://github.com/OSU-NLP-Group/cobalt.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>连接在线与离线强化学习：面向多轮代码生成的上下文赌博机学习</div>
<div class="mono" style="margin-top:8px">近期，利用强化学习在真实世界任务（如多轮代码生成）上训练大语言模型的研究备受关注。在线强化学习虽通常优于离线强化学习，但其较高的训练成本和不稳定性限制了广泛应用。本文基于多轮代码生成可建模为一步可恢复马尔可夫决策过程的观察，提出了结合离线轨迹的上下文赌博机学习方法（Cobalt），该方法融合了在线与离线强化学习的优势。Cobalt首先使用参考大语言模型收集代码生成轨迹，并将其分割为部分轨迹作为上下文提示；随后在在线赌博机学习阶段，通过单步代码生成训练大语言模型完成每个部分轨迹提示。Cobalt在基于GRPO和VeRPO的两种多轮在线强化学习基线方法上表现更优，并将R1-Distill 8B和Qwen3 8B模型在LiveCodeBench上的绝对Pass@1分数分别提升高达9.0和6.2分。此外，本文分析了大语言模型的上下文奖励操纵行为，并通过扰动轨迹增强Cobalt训练以缓解该问题。总体而言，我们的结果表明Cobalt为多轮代码生成等迭代决策任务提供了有前景的解决方案。代码与数据已开源：https://github.com/OSU-NLP-Group/cobalt。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the high cost and instability of online reinforcement learning (RL) for training large language models on multi-turn code generation, this paper formulates the task as a one-step recoverable Markov decision process and proposes Contextual Bandit Learning with Offline Trajectories (Cobalt). The method first collects offline code generation trajectories from a reference model, splits them into partial trajectories as contextual prompts, and then performs online bandit learning where the model is trained to complete each prompt with a single-step generation. Experimentally, Cobalt outperforms online RL baselines like GRPO and VeRPO, substantially improving the Pass@1 scores of models such as R1-Distill 8B and Qwen3 8B by up to 9.0 and 6.2 points on LiveCodeBench, while also mitigating in-context reward hacking through trajectory perturbation.</div>
<div class="mono" style="margin-top:8px">针对在线强化学习训练大语言模型进行多轮代码生成时成本高、不稳定的问题，本文将该任务形式化为一步可恢复马尔可夫决策过程，提出了基于离线轨迹的上下文赌博机学习方法（Cobalt）。该方法首先收集参考模型的离线代码生成轨迹，将其分割为部分轨迹作为上下文提示，然后通过在线赌博机学习训练模型以单步生成完成每个提示。实验结果表明，Cobalt在LiveCodeBench上优于GRPO和VeRPO等在线强化学习基线，将R1-Distill 8B和Qwen3 8B模型的Pass@1分数分别提升了最高9.0和6.2分，并通过轨迹扰动缓解了上下文奖励黑客行为。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Estimation of Kernel Surrogate Models for Task Attribution</div>
<div class="meta-line">Authors: Zhenshuo Zhang, Minxuan Duan, Hongyang R. Zhang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-03T17:43:48+00:00 · Latest: 2026-02-03T17:43:48+00:00</div>
<div class="meta-line">Comments: 27 pages. To appear in ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03783v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03783v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern AI agents such as large language models are trained on diverse tasks -- translation, code generation, mathematical reasoning, and text prediction -- simultaneously. A key question is to quantify how each individual training task influences performance on a target task, a problem we refer to as task attribution. The direct approach, leave-one-out retraining, measures the effect of removing each task, but is computationally infeasible at scale. An alternative approach that builds surrogate models to predict a target task&#x27;s performance for any subset of training tasks has emerged in recent literature. Prior work focuses on linear surrogate models, which capture first-order relationships, but miss nonlinear interactions such as synergy, antagonism, or XOR-type effects. In this paper, we first consider a unified task weighting framework for analyzing task attribution methods, and show a new connection between linear surrogate models and influence functions through a second-order analysis. Then, we introduce kernel surrogate models, which more effectively represent second-order task interactions. To efficiently learn the kernel surrogate, we develop a gradient-based estimation procedure that leverages a first-order approximation of pretrained models; empirically, this yields accurate estimates with less than $2\%$ relative error without repeated retraining. Experiments across multiple domains -- including math reasoning in transformers, in-context learning, and multi-objective reinforcement learning -- demonstrate the effectiveness of kernel surrogate models. They achieve a $25\%$ higher correlation with the leave-one-out ground truth than linear surrogates and influence-function baselines. When used for downstream task selection, kernel surrogate models yield a $40\%$ improvement in demonstration selection for in-context learning and multi-objective reinforcement learning benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向任务归因的核代理模型高效估计方法</div>
<div class="mono" style="margin-top:8px">现代人工智能代理（如大语言模型）通常在翻译、代码生成、数学推理和文本预测等多样化任务上同时训练。核心问题在于量化每个独立训练任务对目标任务性能的影响，即任务归因问题。直接方法（留一重训练法）通过移除各任务来测量影响，但计算成本随规模增长而不可行。近期研究提出构建代理模型来预测任意训练任务子集对目标任务的性能影响。现有工作主要聚焦线性代理模型，这类模型虽能捕捉一阶关系，却无法表征协同、拮抗或异或型效应等非线性交互。本文首先提出统一的任务加权框架分析任务归因方法，并通过二阶分析揭示线性代理模型与影响函数的新关联。进而引入能更有效表征二阶任务交互的核代理模型。为高效学习核代理模型，我们开发了基于梯度的估计方法，利用预训练模型的一阶近似实现计算；实证表明该方法无需重复训练即可获得相对误差低于2%的精确估计。在数学推理、上下文学习及多目标强化学习等领域的实验验证了核代理模型的有效性：其与留一法基准真相的相关系数较线性代理模型和影响函数基线提升25%；应用于下游任务选择时，在上下文学习的示例选择和多目标强化学习基准测试中实现40%的性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the computational challenge of task attribution in modern AI systems, where directly measuring each training task&#x27;s influence via leave-one-out retraining is infeasible. To capture nonlinear task interactions like synergy or antagonism, the authors propose kernel surrogate models, extending beyond prior linear methods, and develop an efficient gradient-based estimation technique that avoids repeated retraining. Experimental results across domains such as math reasoning and in-context learning show that kernel surrogates achieve 25% higher correlation with ground-truth leave-one-out effects and yield 40% improvements in downstream task selection compared to linear baselines.</div>
<div class="mono" style="margin-top:8px">本文针对现代AI系统中任务归因的计算挑战，直接通过留一法重训练衡量每个训练任务的影响不可行。为捕捉任务间的非线性交互如协同或拮抗，作者提出了核代理模型，超越了先前的线性方法，并开发了一种高效的基于梯度的估计技术，避免了重复训练。在数学推理和上下文学习等多个领域的实验结果表明，核代理模型与留一法真实效果的相关性比线性基线和影响函数方法高25%，并在下游任务选择中实现了40%的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL</div>
<div class="meta-line">Authors: Ian Wu, Yuxiao Qu, Amrith Setlur, Aviral Kumar</div>
<div class="meta-line">First: 2026-02-03T17:34:04+00:00 · Latest: 2026-02-03T17:34:04+00:00</div>
<div class="meta-line">Comments: preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03773v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03773v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) that can continually improve beyond their training budgets are able to solve increasingly difficult problems by adapting at test time, a property we refer to as extrapolation. However, standard reinforcement learning (RL) operates over fixed problem distributions and training budgets, which limits extrapolation amidst distribution shift at test time. To address this, we introduce RC, an iterative decoding algorithm that replaces standard autoregressive decoding during both training and inference. RC exploits an asymmetry between the response generation and summarization capabilities of LLMs to construct reasoning chains that consistently improve across iterations. Models trained to use RC can extrapolate and continually improve over reasoning horizons more than an order of magnitude longer than those seen during training. Empirically, training a 4B model with RC using a 16k-token training budget improves performance on HMMT 2025 from 40% to nearly 70% with 0.5m tokens at test time, outperforming both comparably sized models and many larger reasoning LLMs. Finally, we also show that models trained with RC can more effectively leverage existing scaffolds to further scale test-time performance, due to the improved summary-conditioned generation abilities learned through training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>推理缓存：通过短视界强化学习实现长视界持续改进</div>
<div class="mono" style="margin-top:8px">能够突破训练预算持续改进的大语言模型（LLMs）可通过测试时自适应解决日益复杂的问题，这一特性我们称为外推能力。然而，标准强化学习（RL）在固定问题分布和训练预算下运行，限制了测试时分布漂移下的外推能力。为此，我们提出RC——一种在训练和推理阶段替代标准自回归解码的迭代解码算法。RC利用LLMs在响应生成与摘要能力间的不对称性，构建跨迭代持续改进的推理链。经RC训练的模型能够实现外推，并在超过训练所见一个数量级的长推理视界上持续改进。实证表明：使用16k词元训练预算对4B模型进行RC训练后，在测试时使用0.5百万词元可将HMMT 2025任务性能从40%提升至近70%，优于同规模模型及多数大型推理LLMs。最后，我们还证明RC训练模型能更有效地利用现有脚手架进一步扩展测试时性能，这得益于训练获得的改进型摘要条件生成能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of standard reinforcement learning (RL) in enabling Large Language Models (LLMs) to extrapolate and improve continually beyond their training budgets when faced with distribution shifts at test time. It introduces Reasoning Cache (RC), an iterative decoding algorithm that replaces autoregressive decoding by leveraging an asymmetry in LLMs&#x27; capabilities for response generation and summarization to build reasoning chains that improve across iterations. Experimental results show that a 4B model trained with RC using a 16k-token budget can extend reasoning horizons far beyond training, improving performance on HMMT 2025 from 40% to nearly 70% with 0.5 million test-time tokens, outperforming comparable and larger models, and also enhancing the use of existing scaffolds due to learned summary-conditioned generation abilities.</div>
<div class="mono" style="margin-top:8px">本文针对标准强化学习在大型语言模型面对测试时分布变化时难以持续改进和泛化的问题，提出了一种称为推理缓存（RC）的迭代解码算法，以替代自回归解码，利用模型在响应生成与总结能力上的不对称性构建跨迭代持续改进的推理链。实验结果表明，一个40亿参数的模型使用16k令牌的训练预算通过RC训练后，能在测试时用50万令牌将HMMT 2025任务上的性能从40%提升至近70%，推理范围远超训练所见，优于同类规模及更大模型，并因习得的总结条件生成能力而更有效地利用现有框架提升测试性能。</div>
</details>
</div>
<div class="card">
<div class="title">A Two-Timescale Primal-Dual Framework for Reinforcement Learning via Online Dual Variable Guidance</div>
<div class="meta-line">Authors: Axel Friedrich Wolter, Tobias Sutter</div>
<div class="meta-line">First: 2025-05-07T15:18:43+00:00 · Latest: 2026-02-03T16:19:43+00:00</div>
<div class="meta-line">Comments: 54 pages, 1 figure; Revised version with additional finite-time convergence results</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.04494v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.04494v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study reinforcement learning by combining recent advances in regularized linear programming formulations with the classical theory of stochastic approximation. Motivated by the challenge of designing algorithms that leverage off-policy data while maintaining on-policy exploration, we propose PGDA-RL, a novel primal-dual Projected Gradient Descent-Ascent algorithm for solving regularized Markov Decision Processes (MDPs). PGDA-RL integrates experience replay-based gradient estimation with a two-timescale decomposition of the underlying nested optimization problem. The algorithm operates asynchronously, interacts with the environment through a single trajectory of correlated data, and updates its policy online in response to the dual variable associated with the occupancy measure of the underlying MDP. We prove that PGDA-RL converges almost surely to the optimal value function and policy of the regularized MDP. Our convergence analysis relies on tools from stochastic approximation theory and holds under weaker assumptions than those required by existing primal-dual RL approaches, notably removing the need for a simulator or a fixed behavioral policy. Under a strengthened ergodicity assumption on the underlying Markov chain, we establish a last-iterate finite-time guarantee with $\tilde{O} (k^{-2/3})$ mean-square convergence, aligning with the best-known rates for two-timescale stochastic approximation methods under Markovian sampling and biased gradient estimates.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于在线对偶变量引导的双时间尺度原始-对偶强化学习框架</div>
<div class="mono" style="margin-top:8px">本研究结合正则化线性规划的最新进展与经典随机逼近理论，探索强化学习算法设计。针对如何利用离轨数据同时保持同轨探索的挑战，我们提出PGDA-RL算法——一种用于求解正则化马尔可夫决策过程的新型原始-对偶投影梯度下降-上升算法。该算法将基于经验回放的梯度估计与嵌套优化问题的双时间尺度分解相结合，通过单条相关数据轨迹与环境异步交互，并依据MDP占用测度的对偶变量在线更新策略。我们证明PGDA-RL几乎必然收敛至正则化MDP的最优值函数与策略，其收敛分析基于随机逼近理论工具，且所需假设弱于现有原始-对偶RL方法（尤其无需模拟器或固定行为策略）。在强化马尔可夫链遍历性假设下，我们建立了末次迭代有限时间保证，获得$\tilde{O}(k^{-2/3})$均方收敛速率，与马尔可夫采样及有偏梯度估计条件下双时间尺度随机逼近方法的最佳已知速率一致。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of leveraging off-policy data while maintaining on-policy exploration in reinforcement learning, this paper proposes PGDA-RL, a primal-dual algorithm based on a two-timescale projected gradient descent-ascent framework for solving regularized Markov Decision Processes. The method integrates experience replay for gradient estimation with an asynchronous online update scheme guided by dual variables, operating on a single trajectory of correlated data without requiring a simulator or fixed behavioral policy. Experimental results demonstrate almost sure convergence to the optimal policy and value function, with a finite-time mean-square convergence rate of $\tilde{O}(k^{-2/3})$ under ergodicity assumptions, matching state-of-the-art rates for two-timescale stochastic approximation under Markovian sampling.</div>
<div class="mono" style="margin-top:8px">本文旨在解决强化学习中利用离策略数据同时保持在线探索的挑战，提出了PGDA-RL算法，这是一种基于双时间尺度投影梯度下降-上升框架的原对偶方法，用于求解正则化马尔可夫决策过程。该方法结合了经验回放的梯度估计与由对偶变量引导的异步在线更新机制，仅使用单条相关数据轨迹运行，无需模拟器或固定行为策略。实验结果表明，该算法几乎必然收敛到最优策略和价值函数，在遍历性假设下实现了$\tilde{O}(k^{-2/3})$的有限时间均方收敛速率，与马尔可夫采样下双时间尺度随机逼近方法的最佳已知速率一致。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking the Reranker: Boundary-Aware Evidence Selection for Robust Retrieval-Augmented Generation</div>
<div class="meta-line">Authors: Jiashuo Sun, Pengcheng Jiang, Saizhuo Wang, Jiajun Fan, Heng Wang, Siru Ouyang, Ming Zhong, Yizhu Jiao, Chengsong Huang, Xueqiang Xu, Pengrui Han, Peiran Li, Jiaxin Huang, Ge Liu, Heng Ji, Jiawei Han</div>
<div class="meta-line">First: 2026-02-03T16:08:23+00:00 · Latest: 2026-02-03T16:08:23+00:00</div>
<div class="meta-line">Comments: 19 pages, 8 tables, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03689v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03689v1">PDF</a> · <a href="https://github.com/GasolSun36/BAR-RAG">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieval-Augmented Generation (RAG) systems remain brittle under realistic retrieval noise, even when the required evidence appears in the top-K results. A key reason is that retrievers and rerankers optimize solely for relevance, often selecting either trivial, answer-revealing passages or evidence that lacks the critical information required to answer the question, without considering whether the evidence is suitable for the generator. We propose BAR-RAG, which reframes the reranker as a boundary-aware evidence selector that targets the generator&#x27;s Goldilocks Zone -- evidence that is neither trivially easy nor fundamentally unanswerable for the generator, but is challenging yet sufficient for inference and thus provides the strongest learning signal. BAR-RAG trains the selector with reinforcement learning using generator feedback, and adopts a two-stage pipeline that fine-tunes the generator under the induced evidence distribution to mitigate the distribution mismatch between training and inference. Experiments on knowledge-intensive question answering benchmarks show that BAR-RAG consistently improves end-to-end performance under noisy retrieval, achieving an average gain of 10.3 percent over strong RAG and reranking baselines while substantially improving robustness. Code is publicly avaliable at https://github.com/GasolSun36/BAR-RAG.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重思重排序器：面向检索增强生成鲁棒性的边界感知证据选择</div>
<div class="mono" style="margin-top:8px">检索增强生成（RAG）系统在实际检索噪声下仍显脆弱，即使所需证据已出现在前K个结果中。关键原因在于检索器与重排序器仅针对相关性进行优化，往往选择要么是琐碎、直接揭示答案的段落，要么是缺乏回答问题所需关键信息的证据，而未考虑证据是否适合生成器。我们提出BAR-RAG，将重排序器重构为边界感知证据选择器，其目标是定位生成器的“黄金平衡区”——既非对生成器过于简单，也非本质上无法回答，而是具有挑战性却足以支撑推理的证据，从而提供最强的学习信号。BAR-RAG通过强化学习利用生成器反馈训练选择器，并采用两阶段流程在诱导的证据分布下微调生成器，以缓解训练与推理间的分布失配问题。在知识密集型问答基准上的实验表明，BAR-RAG在噪声检索环境下持续提升端到端性能，相较主流RAG与重排序基线平均获得10.3%的性能增益，同时显著增强系统鲁棒性。代码已公开于https://github.com/GasolSun36/BAR-RAG。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the brittleness of Retrieval-Augmented Generation (RAG) systems under noisy retrieval, where traditional retrievers and rerankers often select evidence that is either too trivial or insufficient for the generator. The proposed method, BAR-RAG, reframes the reranker as a boundary-aware evidence selector that targets a &#x27;Goldilocks Zone&#x27; of challenging yet sufficient evidence, training it with reinforcement learning based on generator feedback and fine-tuning the generator to align with this evidence distribution. Experimental results on knowledge-intensive QA benchmarks demonstrate that BAR-RAG consistently improves end-to-end performance under noisy retrieval, achieving an average gain of 10.3% over strong baselines while enhancing robustness.</div>
<div class="mono" style="margin-top:8px">本文针对检索增强生成（RAG）系统在噪声检索下的脆弱性问题，指出传统检索器和重排序器常选择过于简单或不足的证据。提出的BAR-RAG方法将重排序器重构为边界感知的证据选择器，以定位“适中区域”的证据——既具挑战性又足够生成答案，并通过强化学习基于生成器反馈进行训练，同时微调生成器以匹配证据分布。在知识密集型问答基准上的实验表明，BAR-RAG在噪声检索下持续提升端到端性能，相比强基线平均增益10.3%，并显著增强了鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration</div>
<div class="meta-line">Authors: Bowei He, Minda Hu, Zenan Xu, Hongru Wang, Licheng Zong, Yankai Chen, Chen Ma, Xue Liu, Pluto Zhou, Irwin King</div>
<div class="meta-line">First: 2026-02-03T15:32:09+00:00 · Latest: 2026-02-03T15:32:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03647v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03647v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a &#x27;cut-and-regenerate&#x27; mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Search-R2：通过执行器-精炼器协作增强搜索集成推理能力</div>
<div class="mono" style="margin-top:8px">搜索集成推理使语言智能体能够通过主动查询外部源来超越静态参数化知识。然而，通过强化学习训练这些智能体受到多尺度信用分配问题的阻碍：现有方法通常依赖稀疏的轨迹级奖励，无法区分高质量推理与偶然猜测，导致冗余或误导性搜索行为。为此，我们提出Search-R2——一种新颖的执行器-精炼器协作框架，通过定向干预增强推理能力，两个组件在训练期间联合优化。该方法将生成过程分解为：执行器生成初始推理轨迹，元精炼器通过&#x27;截断-再生&#x27;机制选择性诊断并修复缺陷步骤。为提供细粒度监督，我们设计了混合奖励机制，将结果正确性与量化检索证据信息密度的密集过程奖励相结合。理论上，我们将执行器-精炼器交互形式化为平滑混合策略，证明选择性校正相比强基线能带来严格的性能提升。在多种通用和多跳问答数据集上的大量实验表明，Search-R2在不同模型规模上始终优于基于RAG和强化学习的强基线，以最小开销实现更优的推理准确率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the multi-scale credit assignment problem in training search-integrated reasoning agents via reinforcement learning, where sparse rewards often fail to differentiate high-quality reasoning from lucky guesses, leading to inefficient search behaviors. To overcome this, the authors propose Search-R2, an Actor-Refiner collaboration framework that decomposes reasoning into an Actor generating initial trajectories and a Meta-Refiner selectively diagnosing and repairing flawed steps through a cut-and-regenerate mechanism, supported by a hybrid reward combining outcome correctness with dense process rewards based on retrieved evidence information density. Experimental results across various general and multi-hop QA datasets show that Search-R2 consistently outperforms strong RAG and RL-based baselines in reasoning accuracy with minimal overhead, validating its effectiveness across different model scales.</div>
<div class="mono" style="margin-top:8px">本文针对基于强化学习的搜索集成推理智能体训练中的多尺度信用分配问题，即稀疏奖励难以区分高质量推理与侥幸猜测，导致搜索行为低效。为此，作者提出Search-R2框架，采用执行者-精炼者协作机制，将推理分解为执行者生成初始轨迹和元精炼者通过剪裁-再生机制选择性诊断修复错误步骤，并引入结合结果正确性和检索证据信息密度的混合奖励进行细粒度监督。在多种通用和多跳问答数据集上的实验表明，Search-R2在不同模型规模下均优于强RAG和基于强化学习的基线方法，以最小开销实现了更优的推理准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Fine-Tuning for History-Aware Dense Retriever in RAG</div>
<div class="meta-line">Authors: Yicheng Zhang, Zhen Qin, Zhaomin Wu, Wenqi Zhang, Shuiguang Deng</div>
<div class="meta-line">First: 2026-02-03T15:30:14+00:00 · Latest: 2026-02-03T15:30:14+00:00</div>
<div class="meta-line">Comments: On going work. Codes are released at https://github.com/zyc140345/HARR</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03645v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03645v1">PDF</a> · <a href="https://github.com/zyc140345/HARR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieval-augmented generation (RAG) enables large language models (LLMs) to produce evidence-based responses, and its performance hinges on the matching between the retriever and LLMs. Retriever optimization has emerged as an efficient alternative to fine-tuning LLMs. However, existing solutions suffer from objective mismatch between retriever optimization and the goal of RAG pipeline. Reinforcement learning (RL) provides a promising solution to address this limitation, yet applying RL to retriever optimization introduces two fundamental challenges: 1) the deterministic retrieval is incompatible with RL formulations, and 2) state aliasing arises from query-only retrieval in multi-hop reasoning. To address these challenges, we replace deterministic retrieval with stochastic sampling and formulate RAG as a Markov decision process, making retriever optimizable by RL. Further, we incorporate retrieval history into the state at each retrieval step to mitigate state aliasing. Extensive experiments across diverse RAG pipelines, datasets, and retriever scales demonstrate consistent improvements of our approach in RAG performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向RAG中历史感知稠密检索器的强化微调</div>
<div class="mono" style="margin-top:8px">检索增强生成（RAG）使大语言模型（LLM）能够生成基于证据的响应，其性能取决于检索器与LLM的匹配程度。检索器优化已成为微调LLM的高效替代方案。然而，现有解决方案存在检索器优化目标与RAG流程目标不匹配的问题。强化学习（RL）为解决这一局限提供了可行方案，但将RL应用于检索器优化面临两个根本性挑战：1）确定性检索与RL框架不兼容；2）多跳推理中仅基于查询的检索会导致状态混淆。为应对这些挑战，我们采用随机采样替代确定性检索，并将RAG建模为马尔可夫决策过程，使检索器可通过RL进行优化。此外，我们在每个检索步骤中将检索历史纳入状态表示以缓解状态混淆问题。跨多种RAG流程、数据集和检索器规模的实验表明，该方法能持续提升RAG性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the objective mismatch in optimizing retrievers for retrieval-augmented generation (RAG) systems, where existing methods fail to align retriever training with the overall RAG pipeline goal. The authors propose a reinforcement learning (RL) approach that reformulates retrieval as a stochastic sampling process within a Markov decision process, enabling direct optimization of the retriever via RL, and they incorporate retrieval history into the state representation to resolve state aliasing issues in multi-hop reasoning. Experimental results across various RAG pipelines, datasets, and retriever scales show that this method consistently improves RAG performance.</div>
<div class="mono" style="margin-top:8px">本文针对检索增强生成（RAG）系统中检索器优化的目标不匹配问题，现有方法未能使检索器训练与RAG整体目标对齐。作者提出一种强化学习方法，将检索重新表述为马尔可夫决策过程中的随机采样，从而通过强化学习直接优化检索器，并引入检索历史到状态表示中以解决多跳推理中的状态混淆问题。在不同RAG流程、数据集和检索器规模上的实验结果表明，该方法能持续提升RAG性能。</div>
</details>
</div>
<div class="card">
<div class="title">TRE: Encouraging Exploration in the Trust Region</div>
<div class="meta-line">Authors: Chao Huang, Yujing Lu, Quangang Li, Shenghe Wang, Yan Wang, Yueyang Zhang, Long Xia, Jiashu Zhao, Zhiyuan Sun, Daiting Shi, Tingwen Liu</div>
<div class="meta-line">First: 2026-02-03T15:21:49+00:00 · Latest: 2026-02-03T15:21:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03635v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03635v1">PDF</a> · <a href="https://github.com/WhyChaos/TRE-Encouraging-Exploration-in-the-Trust-Region">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Entropy regularization is a standard technique in reinforcement learning (RL) to enhance exploration, yet it yields negligible effects or even degrades performance in Large Language Models (LLMs). We attribute this failure to the cumulative tail risk inherent to LLMs with massive vocabularies and long generation horizons. In such environments, standard global entropy maximization indiscriminately dilutes probability mass into the vast tail of invalid tokens rather than focusing on plausible candidates, thereby disrupting coherent reasoning. To address this, we propose Trust Region Entropy (TRE), a method that encourages exploration strictly within the model&#x27;s trust region. Extensive experiments across mathematical reasoning (MATH), combinatorial search (Countdown), and preference alignment (HH) tasks demonstrate that TRE consistently outperforms vanilla PPO, standard entropy regularization, and other exploration baselines. Our code is available at https://github.com/WhyChaos/TRE-Encouraging-Exploration-in-the-Trust-Region.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TRE：在信任区域内鼓励探索</div>
<div class="mono" style="margin-top:8px">熵正则化是强化学习中增强探索的标准技术，但在大语言模型中效果甚微甚至可能降低性能。我们将此归因于大语言模型因庞大词表和生成长度而固有的累积尾部风险。在此类环境中，标准的全局熵最大化会不加区分地将概率质量分散至大量无效词元的尾部，而非聚焦于合理候选，从而破坏连贯推理。为此，我们提出信任区域熵方法，严格在模型信任区域内鼓励探索。在数学推理、组合搜索和偏好对齐任务上的大量实验表明，TRE始终优于原始PPO、标准熵正则化及其他探索基线。代码已开源：https://github.com/WhyChaos/TRE-Encouraging-Exploration-in-the-Trust-Region。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that standard entropy regularization, a common technique for encouraging exploration in reinforcement learning, often fails or harms performance when fine-tuning large language models (LLMs) due to the models&#x27; large vocabularies and long generation sequences, which cause probability mass to be wasted on implausible tokens. The proposed method, Trust Region Entropy (TRE), addresses this by restricting exploration to a trust region where the model&#x27;s predictions remain reliable, thereby preserving coherent reasoning. The main experimental results, from tests on mathematical reasoning, combinatorial search, and preference alignment tasks, show that TRE consistently outperforms standard Proximal Policy Optimization (PPO), entropy regularization, and other baseline exploration methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，标准的熵正则化这一强化学习中常用的探索鼓励技术，在微调大语言模型时常常失效或损害性能，原因是模型词表巨大且生成长度较长，导致概率质量被分散到大量无效的令牌上。为解决此问题，本文提出了信任区域熵方法，该方法将探索严格限制在模型预测可靠的信任区域内，从而保持推理的连贯性。在数学推理、组合搜索和偏好对齐任务上的主要实验结果表明，该方法 consistently 优于标准的近端策略优化、熵正则化及其他探索基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamic Priors in Bayesian Optimization for Hyperparameter Optimization</div>
<div class="meta-line">Authors: Lukas Fehring, Marcel Wever, Maximilian Spliethöver, Leona Hennig, Henning Wachsmuth, Marius Lindauer</div>
<div class="meta-line">First: 2025-11-04T13:44:04+00:00 · Latest: 2026-02-03T15:19:02+00:00</div>
<div class="meta-line">Comments: 8 pages plus references and appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.02570v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.02570v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bayesian optimization (BO) is a widely used approach to hyperparameter optimization (HPO). However, most existing HPO methods only incorporate expert knowledge during initialization, limiting practitioners&#x27; ability to influence the optimization process as new insights emerge. This limits the applicability of BO in iterative machine learning development workflows. We propose DynaBO, a BO framework that enables continuous user control of the optimization process. Over time, DynaBO leverages provided user priors by augmenting the acquisition function with decaying, prior-weighted preferences while preserving asymptotic convergence guarantees. To reinforce robustness, we introduce a data-driven safeguard that detects and can be used to reject misleading priors. We prove theoretical results on near-certain convergence, robustness to adversarial priors, and accelerated convergence when informative priors are provided. Extensive experiments across various HPO benchmarks show that DynaBO consistently outperforms our state-of-the-art competitors across all benchmarks and for all prior kinds. Our results demonstrate that DynaBO enables reliable and efficient collaborative BO, bridging automated and manually controlled model development.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>贝叶斯优化中用于超参数优化的动态先验</div>
<div class="mono" style="margin-top:8px">贝叶斯优化（BO）是超参数优化（HPO）中广泛使用的方法。然而，现有大多数HPO方法仅在初始化阶段融入专家知识，限制了实践者在获得新见解时影响优化过程的能力，这制约了BO在迭代式机器学习开发工作流中的适用性。我们提出DynaBO框架，该框架支持用户持续控制优化过程。DynaBO通过向采集函数添加衰减型先验加权偏好来持续利用用户提供的先验知识，同时保持渐近收敛保证。为增强鲁棒性，我们引入数据驱动的安全机制，可检测并拒绝误导性先验。我们证明了该框架在近似确定性收敛、对抗性先验鲁棒性及信息性先验加速收敛方面的理论结果。在多种HPO基准测试中的大量实验表明，DynaBO在所有基准测试和各类先验条件下均持续优于当前最先进的对比方法。结果表明DynaBO能够实现可靠高效的协作式贝叶斯优化，在自动化与人工控制的模型开发之间架起桥梁。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation in standard Bayesian optimization for hyperparameter tuning, where expert knowledge is typically only used at initialization, hindering iterative refinement during machine learning workflows. The authors propose DynaBO, a framework that allows continuous integration of user priors by augmenting the acquisition function with decaying prior-weighted preferences, while maintaining asymptotic convergence and incorporating a data-driven safeguard to reject misleading priors. Experimental results across various benchmarks demonstrate that DynaBO consistently outperforms state-of-the-art methods, enabling more efficient and collaborative optimization.</div>
<div class="mono" style="margin-top:8px">本文针对标准贝叶斯优化在超参数调优中的局限性提出改进，传统方法通常仅在初始化阶段利用专家知识，限制了在机器学习工作流中的迭代优化能力。作者提出了DynaBO框架，通过将衰减的先验加权偏好融入采集函数，允许持续整合用户先验知识，同时保持渐近收敛性，并引入数据驱动的安全机制来排除误导性先验。在多个基准测试上的实验结果表明，DynaBO始终优于现有先进方法，实现了更高效、可协作的优化过程。</div>
</details>
</div>
<div class="card">
<div class="title">SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization</div>
<div class="meta-line">Authors: Maksim Afanasyev, Illarion Iov</div>
<div class="meta-line">First: 2026-02-02T17:46:06+00:00 · Latest: 2026-02-03T13:58:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02383v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02383v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Direct preference optimization methods have emerged as a computationally efficient alternative to Reinforcement Learning from Human Feedback (RLHF) for aligning Large Language Models (LLMs). Latest approaches have streamlined the alignment process by deriving implicit reward functions, yet they often suffer from a critical objective mismatch: optimizing the relative margin between chosen and rejected responses does not guarantee the preservation of the chosen response&#x27;s absolute likelihood. This can lead to unlearning, where the model degrades the probability of high-quality outputs to satisfy margin constraints, and formatting collapse caused by the over-penalization of rejected sequences. In this work, we introduce SLIME (Stabilized Likelihood Implicit Margin Enforcement), a reference-free alignment objective designed to decouple preference learning from generation quality. SLIME incorporates a three-pronged objective: (1) an anchoring term to maximize the likelihood of preferred responses; (2) a stabilizing penalty that prevents the probabilities of rejected tokens from collapsing to zero; and (3) a dual-margin mechanism that combines hard and soft constraints for precise boundary shaping. Our results demonstrate that SLIME achieves superior performance compared to state-of-the-art baselines while maintaining higher generation stability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SLIME：基于稳定似然的隐式边界强化偏好优化方法</div>
<div class="mono" style="margin-top:8px">直接偏好优化方法已成为替代基于人类反馈的强化学习（RLHF）对齐大语言模型（LLM）的高效计算方案。最新方法通过推导隐式奖励函数简化了对齐流程，但常存在关键目标失配问题：优化选定与拒绝回答间的相对边界并不能保证维持选定回答的绝对似然。这可能导致模型遗忘（为满足边界约束而降低高质量输出的概率）以及因过度惩罚拒绝序列引发的格式崩溃。本研究提出SLIME（基于稳定似然的隐式边界强化）——一种无需参考模型的对齐目标，旨在解耦偏好学习与生成质量。SLIME包含三重目标：（1）锚定项以最大化优选回答的似然；（2）稳定惩罚项防止拒绝标记的概率坍缩至零；（3）结合硬约束与软约束的双边界机制以实现精确边界塑造。实验表明，SLIME在保持更高生成稳定性的同时，性能优于当前最先进的基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for SLIME arises from the limitations of existing direct preference optimization methods, which often suffer from objective mismatch, leading to unlearning of high-quality outputs and formatting collapse due to over-penalization. To address this, the method introduces a reference-free alignment objective that decouples preference learning from generation quality through a three-pronged approach: an anchoring term to maximize the likelihood of preferred responses, a stabilizing penalty to prevent rejected token probabilities from collapsing, and a dual-margin mechanism combining hard and soft constraints for boundary shaping. Experimental results show that SLIME outperforms state-of-the-art baselines while maintaining higher generation stability.</div>
<div class="mono" style="margin-top:8px">SLIME的提出动机源于现有直接偏好优化方法的局限性，这些方法常因目标不匹配导致高质量输出被遗忘，以及因过度惩罚拒绝序列引发格式崩溃。为解决此问题，该方法引入了一种无参考的对齐目标，通过三部分设计将偏好学习与生成质量解耦：使用锚定项最大化偏好响应的似然，稳定惩罚项防止拒绝标记概率崩溃，以及结合硬约束和软约束的双重边界机制以精确塑造边界。实验结果表明，SLIME在保持更高生成稳定性的同时，性能优于现有先进基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Not All Negative Samples Are Equal: LLMs Learn Better from Plausible Reasoning</div>
<div class="meta-line">Authors: Zixiang Di, Jinyi Han, Shuo Zhang, Ying Liao, Zhi Li, Xiaofeng Ji, Yongqi Wang, Zheming Yang, Ming Gao, Bingdong Li, Jie Wang</div>
<div class="meta-line">First: 2026-02-03T13:32:02+00:00 · Latest: 2026-02-03T13:32:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03516v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03516v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning from negative samples holds great promise for improving Large Language Model (LLM) reasoning capability, yet existing methods treat all incorrect responses as equally informative, overlooking the crucial role of sample quality. To address this, we propose Plausible Negative Samples (PNS), a method that synthesizes high-quality negative samples exhibiting expected format and structural coherence while ultimately yielding incorrect answers. PNS trains a dedicated model via reverse reinforcement learning (RL) guided by a composite reward combining format compliance, accuracy inversion, reward model assessment, and chain-of-thought evaluation, generating responses nearly indistinguishable from correct solutions. We further validate PNS as a plug-and-play data source for preference optimization across three backbone models on seven mathematical reasoning benchmarks. Results demonstrate that PNS consistently outperforms other negative sample synthesis methods, achieving an average improvement of 2.03% over RL-trained models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>并非所有负样本都同等重要：大语言模型从合理推理中学习效果更佳</div>
<div class="mono" style="margin-top:8px">从负样本中学习对提升大语言模型（LLM）推理能力具有巨大潜力，但现有方法将所有错误答案视为同等信息量，忽视了样本质量的关键作用。为此，我们提出“合理负样本（PNS）”方法，通过合成高质量负样本，使其在格式与结构上保持连贯性，但最终得出错误答案。PNS通过逆向强化学习训练专用模型，结合格式合规性、答案反转、奖励模型评估和思维链评价的复合奖励机制，生成与正确答案几乎无法区分的响应。我们在七个数学推理基准上对三种骨干模型进行偏好优化的即插即用数据源验证，结果表明PNS持续优于其他负样本合成方法，相比强化学习训练模型平均提升2.03%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation in existing methods that treat all incorrect responses as equally informative for improving Large Language Model (LLM) reasoning, proposing that sample quality is crucial. The authors introduce Plausible Negative Samples (PNS), a method that synthesizes high-quality negative samples which maintain proper format and structural coherence but lead to incorrect answers, using reverse reinforcement learning guided by a composite reward for format compliance, accuracy inversion, reward model assessment, and chain-of-thought evaluation. Experimental validation across three backbone models on seven mathematical reasoning benchmarks shows that PNS consistently outperforms other negative sample synthesis methods, achieving an average improvement of 2.03% over RL-trained models, demonstrating its effectiveness as a plug-and-play data source for preference optimization.</div>
<div class="mono" style="margin-top:8px">本文针对现有方法在提升大语言模型推理能力时将所有错误回答视为同等信息量的局限性，提出样本质量至关重要。作者引入了合理负样本方法，该方法通过逆向强化学习合成高质量负样本，这些样本保持预期格式和结构连贯性但最终得出错误答案，其训练由结合格式合规性、准确性反转、奖励模型评估和思维链评估的复合奖励指导。在七个数学推理基准上对三个骨干模型的实验验证表明，该方法始终优于其他负样本合成方法，相比强化学习训练模型平均提升2.03%，证明了其作为即插即用偏好优化数据源的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">CMR: Contractive Mapping Embeddings for Robust Humanoid Locomotion on Unstructured Terrains</div>
<div class="meta-line">Authors: Qixin Zeng, Hongyin Zhang, Shangke Lyu, Junxi Jin, Donglin Wang, Chao Huang</div>
<div class="meta-line">First: 2026-02-03T13:30:18+00:00 · Latest: 2026-02-03T13:30:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03511v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03511v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robust disturbance rejection remains a longstanding challenge in humanoid locomotion, particularly on unstructured terrains where sensing is unreliable and model mismatch is pronounced. While perception information, such as height map, enhances terrain awareness, sensor noise and sim-to-real gaps can destabilize policies in practice. In this work, we provide theoretical analysis that bounds the return gap under observation noise, when the induced latent dynamics are contractive. Furthermore, we present Contractive Mapping for Robustness (CMR) framework that maps high-dimensional, disturbance-prone observations into a latent space, where local perturbations are attenuated over time. Specifically, this approach couples contrastive representation learning with Lipschitz regularization to preserve task-relevant geometry while explicitly controlling sensitivity. Notably, the formulation can be incorporated into modern deep reinforcement learning pipelines as an auxiliary loss term with minimal additional technical effort required. Further, our extensive humanoid experiments show that CMR potently outperforms other locomotion algorithms under increased noise.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CMR：基于压缩映射嵌入的仿人机器人非结构化地形鲁棒运动控制</div>
<div class="mono" style="margin-top:8px">仿人机器人在非结构化地形上的鲁棒抗干扰能力仍是长期挑战，尤其在感知不可靠、模型失配显著的环境中。虽然高度图等感知信息能增强地形认知，但传感器噪声与仿真-现实差距易导致策略失稳。本研究通过理论分析证明：当潜在动力学具有压缩性时，观测噪声下的回报差距存在上界。进而提出鲁棒性压缩映射框架，将高维易扰观测映射至潜在空间，使局部扰动随时间衰减。该方法结合对比表征学习与Lipschitz正则化，在显式控制敏感度的同时保持任务相关几何结构。该框架可作为辅助损失项无缝集成至现代深度强化学习流程，无需复杂额外技术投入。大量仿人机器人实验表明，在增强噪声环境下CMR显著优于其他运动控制算法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of robust humanoid locomotion on unstructured terrains, where sensor noise and model inaccuracies can destabilize control policies. The proposed method, Contractive Mapping for Robustness (CMR), maps noisy high-dimensional observations into a contractive latent space to attenuate disturbances over time, using contrastive learning with Lipschitz regularization to maintain task-relevant geometry while controlling sensitivity. Experimental results on humanoid locomotion demonstrate that CMR significantly outperforms other algorithms under conditions of increased observation noise, validating its effectiveness for robust disturbance rejection.</div>
<div class="mono" style="margin-top:8px">本文针对非结构化地形下人形机器人鲁棒运动的挑战，其中传感器噪声和模型不匹配易导致控制策略失稳。所提出的方法CMR（收缩映射鲁棒性框架）将易受干扰的高维观测映射到收缩的潜在空间，以随时间衰减局部扰动，该方法结合对比表征学习和Lipschitz正则化，在控制敏感性的同时保持任务相关几何结构。在人形机器人的大量实验中，CMR在增强噪声条件下显著优于其他运动算法，证实了其在鲁棒抗干扰方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Reparameterization Flow Policy Optimization</div>
<div class="meta-line">Authors: Hai Zhong, Zhuoran Li, Xun Wang, Longbo Huang</div>
<div class="meta-line">First: 2026-02-03T13:22:08+00:00 · Latest: 2026-02-03T13:22:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03501v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03501v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reparameterization Policy Gradient (RPG) has emerged as a powerful paradigm for model-based reinforcement learning, enabling high sample efficiency by backpropagating gradients through differentiable dynamics. However, prior RPG approaches have been predominantly restricted to Gaussian policies, limiting their performance and failing to leverage recent advances in generative models. In this work, we identify that flow policies, which generate actions via differentiable ODE integration, naturally align with the RPG framework, a connection not established in prior work. However, naively exploiting this synergy proves ineffective, often suffering from training instability and a lack of exploration. We propose Reparameterization Flow Policy Optimization (RFO). RFO computes policy gradients by backpropagating jointly through the flow generation process and system dynamics, unlocking high sample efficiency without requiring intractable log-likelihood calculations. RFO includes two tailored regularization terms for stability and exploration. We also propose a variant of RFO with action chunking. Extensive experiments on diverse locomotion and manipulation tasks, involving both rigid and soft bodies with state or visual inputs, demonstrate the effectiveness of RFO. Notably, on a challenging locomotion task controlling a soft-body quadruped, RFO achieves almost $2\times$ the reward of the state-of-the-art baseline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重参数化流策略优化</div>
<div class="mono" style="margin-top:8px">重参数化策略梯度（RPG）已成为基于模型的强化学习中一种强大的范式，通过可微动力学反向传播梯度实现高样本效率。然而，现有RPG方法主要局限于高斯策略，限制了其性能且未能利用生成模型的最新进展。本研究发现，通过可微ODE积分生成动作的流策略天然契合RPG框架——这一关联在先前工作中尚未建立。但简单利用这种协同效应效果有限，常存在训练不稳定和探索不足的问题。我们提出重参数化流策略优化（RFO），通过流生成过程与系统动力学的联合反向传播计算策略梯度，无需复杂对数似然计算即可实现高样本效率。RFO包含两个针对稳定性与探索的定制正则项，并提出带动作分块的变体。在涵盖刚体/软体 locomotion 与 manipulation 任务（含状态/视觉输入）的广泛实验中，RFO均展现卓越性能。在控制软体四足机器人的高难度 locomotion 任务中，RFO获得近乎最优基线2倍的奖励。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the limitations of prior Reparameterization Policy Gradient (RPG) methods, which are confined to Gaussian policies and fail to incorporate advanced generative models. The proposed method, Reparameterization Flow Policy Optimization (RFO), integrates flow policies—which generate actions via differentiable ODE integration—into the RPG framework, enabling joint backpropagation through both the flow generation and system dynamics for efficient gradient computation, while introducing regularization terms for stability and exploration. Main experimental results on diverse locomotion and manipulation tasks, including rigid and soft bodies with state or visual inputs, show that RFO significantly outperforms baselines, achieving nearly double the reward on a challenging soft-body quadruped locomotion task.</div>
<div class="mono" style="margin-top:8px">本文的动机源于现有重参数化策略梯度（RPG）方法的局限性，这些方法局限于高斯策略且未能利用先进的生成模型。所提出的方法——重参数化流策略优化（RFO）——将流策略（通过可微ODE积分生成动作）整合到RPG框架中，实现了通过流生成和系统动力学的联合反向传播以高效计算梯度，同时引入了正则化项以确保稳定性和探索性。在包括刚体和软体的多种运动与操作任务（使用状态或视觉输入）上的主要实验结果表明，RFO显著优于基线方法，在一个具有挑战性的软体四足机器人运动任务中获得了近乎两倍的奖励。</div>
</details>
</div>
<div class="card">
<div class="title">IntentRL: Training Proactive User-intent Agents for Open-ended Deep Research via Reinforcement Learning</div>
<div class="meta-line">Authors: Haohao Luo, Zexi Li, Yuexiang Xie, Wenhao Zhang, Yaliang Li, Ying Shen</div>
<div class="meta-line">First: 2026-02-03T12:43:09+00:00 · Latest: 2026-02-03T12:43:09+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03468v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03468v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep Research (DR) agents extend Large Language Models (LLMs) beyond parametric knowledge by autonomously retrieving and synthesizing evidence from large web corpora into long-form reports, enabling a long-horizon agentic paradigm. However, unlike real-time conversational assistants, DR is computationally expensive and time-consuming, creating an autonomy-interaction dilemma: high autonomy on ambiguous user queries often leads to prolonged execution with unsatisfactory outcomes. To address this, we propose IntentRL, a framework that trains proactive agents to clarify latent user intents before starting long-horizon research. To overcome the scarcity of open-ended research data, we introduce a scalable pipeline that expands a few seed samples into high-quality dialogue turns via a shallow-to-deep intent refinement graph. We further adopt a two-stage reinforcement learning (RL) strategy: Stage I applies RL on offline dialogues to efficiently learn general user-interaction behavior, while Stage II uses the trained agent and a user simulator for online rollouts to strengthen adaptation to diverse user feedback. Extensive experiments show that IntentRL significantly improves both intent hit rate and downstream task performance, outperforming the built-in clarify modules of closed-source DR agents and proactive LLM baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IntentRL：通过强化学习训练面向开放式深度研究的主动用户意图代理</div>
<div class="mono" style="margin-top:8px">深度研究（DR）代理通过自主从大规模网络语料库中检索并综合证据生成长篇报告，将大语言模型（LLM）的应用扩展至参数化知识之外，实现了一种长视野的自主范式。然而，与实时对话助手不同，DR 计算成本高且耗时，引发了自主性与交互性的矛盾：对模糊用户查询的高度自主性常导致执行时间过长且结果不尽人意。为此，我们提出 IntentRL 框架，训练主动代理在启动长视野研究前澄清用户的潜在意图。为克服开放式研究数据稀缺的问题，我们引入一种可扩展的流水线，通过浅层到深层的意图细化图将少量种子样本扩展为高质量对话轮次。进一步采用两阶段强化学习（RL）策略：第一阶段在离线对话上应用 RL 以高效学习通用用户交互行为；第二阶段利用训练后的代理与用户模拟器进行在线推演，以增强对多样化用户反馈的适应能力。大量实验表明，IntentRL 在意图命中率和下游任务性能上均显著提升，优于闭源 DR 代理的内置澄清模块及主动型 LLM 基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the autonomy-interaction dilemma in Deep Research (DR) agents, where high autonomy on ambiguous queries leads to inefficient, unsatisfactory outcomes. The method introduces IntentRL, a framework that trains proactive agents to clarify latent user intents before research begins, using a scalable pipeline to generate high-quality dialogue data via a shallow-to-deep intent refinement graph and a two-stage reinforcement learning strategy combining offline and online training. Main experimental results demonstrate that IntentRL significantly improves intent hit rate and downstream task performance, outperforming both closed-source DR agents&#x27; clarify modules and proactive LLM baselines.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决深度研究（DR）智能体中的自主性与交互性困境，即对模糊查询的高度自主性常导致低效且不理想的结果。方法上提出了IntentRL框架，通过训练主动智能体在研究开始前澄清用户的潜在意图，采用可扩展的流水线，利用浅层到深层的意图细化图生成高质量对话数据，并结合离线和在线训练的两阶段强化学习策略。主要实验结果表明，IntentRL显著提高了意图命中率和下游任务性能，优于闭源DR智能体的内置澄清模块以及主动型大语言模型基线。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Variance: Prompt-Efficient RLVR via Rare-Event Amplification and Bidirectional Pairing</div>
<div class="meta-line">Authors: Xin Sheng, Jiaxin Li, Yujuan Pang, Ran Peng, Yong Ma</div>
<div class="meta-line">First: 2026-02-03T12:17:25+00:00 · Latest: 2026-02-03T12:17:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03452v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03452v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) is effective for training large language models on deterministic outcome reasoning tasks. Prior work shows RLVR works with few prompts, but prompt selection is often based only on training-accuracy variance, leading to unstable optimization directions and weaker transfer. We revisit prompt selection from a mechanism-level view and argue that an effective minibatch should provide both (i) a reliable positive anchor and (ii) explicit negative learning signals from rare failures. Based on this principle, we propose \emph{positive--negative pairing}: at each update, we sample a hard-but-solvable $q^{+}$ and an easy-but-brittle prompt $q^{-}$(high success rate but not perfect), characterized by low and high empirical success rates under multiple rollouts. We further introduce Weighted GRPO, which reweights binary outcomes at the pair level and uses group-normalized advantages to amplify rare successes on $q^{+}$ into sharp positive guidance while turning rare failures on $q^{-}$ into strong negative penalties. This bidirectional signal provides informative learning feedback for both successes and failures, improving sample efficiency without suppressing exploration. On Qwen2.5-Math-7B, a single paired minibatch per update consistently outperforms a GRPO baseline that selects two prompts via commonly used variance-based selection heuristics: AIME~2025 Pass@8 improves from 16.8 to 22.2, and AMC23 Pass@64 from 94.0 to 97.0, while remaining competitive with large-scale RLVR trained from a pool of 1209 training prompts. Similar gains are observed on Qwen2.5-Math-7B-Instruct.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越方差：通过稀有事件放大与双向配对实现高效提示的RLVR</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）在确定性结果推理任务上训练大语言模型效果显著。先前研究表明RLVR仅需少量提示即可工作，但提示选择通常仅基于训练准确率方差，导致优化方向不稳定且泛化能力较弱。本文从机制层面重新审视提示选择，提出有效的微批次应同时提供：（i）可靠的正向锚点；（ii）来自稀有失败的显式负向学习信号。基于此原则，我们提出\emph{正负配对}方法：每次更新时，采样一个困难但可解的提示$q^{+}$（低经验成功率）与一个简单但脆弱的提示$q^{-}$（高成功率但不完美）。进一步提出加权GRPO，在配对层级重新加权二元结果，并利用组归一化优势将$q^{+}$上的稀有成功放大为明确正向指导，同时将$q^{-}$上的稀有失败转化为强负向惩罚。这种双向信号为成功与失败均提供信息丰富的学习反馈，在保持探索性的同时提升样本效率。在Qwen2.5-Math-7B上的实验表明：每次更新使用单个配对微批次，持续优于基于常用方差启发式选择双提示的GRPO基线——AIME~2025 Pass@8从16.8提升至22.2，AMC23 Pass@64从94.0提升至97.0，且与使用1209个训练提示的大规模RLVR保持竞争力。在Qwen2.5-Math-7B-Instruct模型上也观察到类似增益。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the inefficiency of prompt selection in reinforcement learning with verifiable rewards (RLVR) for language models, where prior variance-based methods lead to unstable optimization. The authors propose a mechanism-level approach called positive-negative pairing, which constructs minibatches by pairing a hard-but-solvable prompt with a high success rate but brittle prompt to provide both reliable positive anchors and explicit negative learning signals from rare failures. They further introduce Weighted GRPO, which reweights binary outcomes and uses group-normalized advantages to amplify rare successes into sharp positive guidance and rare failures into strong negative penalties, enhancing sample efficiency without suppressing exploration. Experimental results on Qwen2.5-Math-7B show that a single paired minibatch per update outperforms variance-based selection, improving AIME 2025 Pass@8 from 16.8 to 22.2 and AMC23 Pass@64 from 94.0 to 97.0, while remaining competitive with large-scale RLVR trained on many more prompts.</div>
<div class="mono" style="margin-top:8px">本文针对语言模型在可验证奖励强化学习（RLVR）中提示选择效率低下的问题，指出先前基于方差的方法会导致优化方向不稳定。作者从机制层面提出了一种正负配对方法，通过将难但可解的提示与高成功率但脆弱的提示配对构建小批量，以提供可靠的正向锚点和来自罕见失败的显式负向学习信号。进一步引入加权GRPO，通过对二元结果重新加权并使用组归一化优势，将罕见成功放大为尖锐的正向指导，并将罕见失败转化为强负向惩罚，从而在不抑制探索的情况下提高样本效率。在Qwen2.5-Math-7B上的实验结果表明，每次更新使用单个配对小批量优于基于方差的选择方法，将AIME 2025 Pass@8从16.8提升至22.2，AMC23 Pass@64从94.0提升至97.0，同时与使用大量提示训练的大规模RLVR保持竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">CRL-VLA: Continual Vision-Language-Action Learning</div>
<div class="meta-line">Authors: Qixin Zeng, Shuo Zhang, Hongyin Zhang, Renjie Wang, Han Zhao, Libang Zhao, Runze Li, Donglin Wang, Chao Huang</div>
<div class="meta-line">First: 2026-02-03T12:09:53+00:00 · Latest: 2026-02-03T12:09:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03445v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03445v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Lifelong learning is critical for embodied agents in open-world environments, where reinforcement learning fine-tuning has emerged as an important paradigm to enable Vision-Language-Action (VLA) models to master dexterous manipulation through environmental interaction. Thus, Continual Reinforcement Learning (CRL) is a promising pathway for deploying VLA models in lifelong robotic scenarios, yet balancing stability (retaining old skills) and plasticity (learning new ones) remains a formidable challenge for existing methods. We introduce CRL-VLA, a framework for continual post-training of VLA models with rigorous theoretical bounds. We derive a unified performance bound linking the stability-plasticity trade-off to goal-conditioned advantage magnitude, scaled by policy divergence. CRL-VLA resolves this dilemma via asymmetric regulation: constraining advantage magnitudes on prior tasks while enabling controlled growth on new tasks. This is realized through a simple but effective dual-critic architecture with novel Goal-Conditioned Value Formulation (GCVF), where a frozen critic anchors semantic consistency and a trainable estimator drives adaptation. Experiments on the LIBERO benchmark demonstrate that CRL-VLA effectively harmonizes these conflicting objectives, outperforming baselines in both anti-forgetting and forward adaptation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CRL-VLA：持续视觉-语言-动作学习</div>
<div class="mono" style="margin-top:8px">在开放世界环境中，终身学习对于具身智能体至关重要，其中强化学习微调已成为重要范式，使视觉-语言-动作模型能通过环境交互掌握灵巧操作。因此，持续强化学习是将VLA模型部署于终身机器人场景的可行路径，但平衡稳定性（保留旧技能）与可塑性（学习新技能）仍是现有方法的重大挑战。我们提出CRL-VLA框架，通过严格理论边界实现VLA模型的持续后训练。我们推导出统一性能边界，将稳定性-可塑性权衡与目标条件优势幅度相关联，并通过策略散度进行缩放。CRL-VLA通过非对称调控解决该困境：约束先前任务的优势幅度，同时允许新任务受控增长。这通过简单有效的双评论家架构实现，其中冻结评论家锚定语义一致性，可训练估计器驱动适应。在LIBERO基准测试中，CRL-VLA有效协调了这些冲突目标，在抗遗忘与前向适应方面均优于基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the challenge of balancing stability and plasticity in continual reinforcement learning for vision-language-action models deployed in lifelong robotic scenarios. The method introduces CRL-VLA, a framework with theoretical bounds that uses asymmetric regulation via a dual-critic architecture with Goal-Conditioned Value Formulation to constrain advantage on prior tasks while enabling controlled adaptation on new ones. Experimental results on the LIBERO benchmark show that CRL-VLA outperforms baselines in both retaining old skills and learning new ones, effectively harmonizing the stability-plasticity trade-off.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决视觉-语言-动作模型在终身机器人场景中持续强化学习时稳定性与可塑性平衡的挑战。方法上提出了CRL-VLA框架，该框架具有理论边界，通过采用目标条件价值公式的双评论家架构进行非对称调节，以约束旧任务的优势同时允许新任务受控适应。在LIBERO基准测试中的实验结果表明，CRL-VLA在保留旧技能和学习新技能方面均优于基线方法，有效协调了稳定性与可塑性之间的权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Chain-of-Goals Hierarchical Policy for Long-Horizon Offline Goal-Conditioned RL</div>
<div class="meta-line">Authors: Jinwoo Choi, Sang-Hyun Lee, Seung-Woo Seo</div>
<div class="meta-line">First: 2026-02-03T11:11:03+00:00 · Latest: 2026-02-03T11:11:03+00:00</div>
<div class="meta-line">Comments: 22 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03389v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03389v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline goal-conditioned reinforcement learning remains challenging for long-horizon tasks. While hierarchical approaches mitigate this issue by decomposing tasks, most existing methods rely on separate high- and low-level networks and generate only a single intermediate subgoal, making them inadequate for complex tasks that require coordinating multiple intermediate decisions. To address this limitation, we draw inspiration from the chain-of-thought paradigm and propose the Chain-of-Goals Hierarchical Policy (CoGHP), a novel framework that reformulates hierarchical decision-making as autoregressive sequence modeling within a unified architecture. Given a state and a final goal, CoGHP autoregressively generates a sequence of latent subgoals followed by the primitive action, where each latent subgoal acts as a reasoning step that conditions subsequent predictions. To implement this efficiently, we pioneer the use of an MLP-Mixer backbone, which supports cross-token communication and captures structural relationships among state, goal, latent subgoals, and action. Across challenging navigation and manipulation benchmarks, CoGHP consistently outperforms strong offline baselines, demonstrating improved performance on long-horizon tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向长时域离线目标条件强化学习的链式目标分层策略</div>
<div class="mono" style="margin-top:8px">离线目标条件强化学习在长时域任务中仍面临挑战。虽然分层方法通过任务分解缓解了这一问题，但现有方法大多依赖独立的高层与底层网络，且仅生成单一中间子目标，难以应对需要协调多重中间决策的复杂任务。为突破此局限，我们受思维链范式启发，提出链式目标分层策略——一种将分层决策重构为统一架构内自回归序列建模的新框架。给定状态与最终目标，该框架自回归地生成潜在子目标序列及原始动作，每个潜在子目标作为推理步骤制约后续预测。为实现高效计算，我们首次采用MLP-Mixer骨干网络，支持跨令牌通信并捕捉状态、目标、潜在子目标与动作间的结构关系。在导航与操作基准测试中，该框架持续超越现有离线基线，显著提升长时域任务性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of long-horizon tasks in offline goal-conditioned reinforcement learning, where existing hierarchical methods often rely on separate networks and generate only single subgoals, limiting their effectiveness for complex multi-step decisions. Inspired by chain-of-thought reasoning, the authors propose the Chain-of-Goals Hierarchical Policy (CoGHP), a unified framework that reformulates hierarchical decision-making as autoregressive sequence modeling to generate sequences of latent subgoals and actions. Using an MLP-Mixer backbone to enable cross-token communication and capture structural relationships, CoGHP is evaluated on navigation and manipulation benchmarks, where it consistently outperforms strong offline baselines, particularly in long-horizon scenarios.</div>
<div class="mono" style="margin-top:8px">本文针对离线目标条件强化学习中长时程任务的挑战，指出现有分层方法通常依赖分离的网络且仅生成单一子目标，难以处理复杂的多步决策问题。受思维链范式启发，作者提出了链式目标分层策略（CoGHP），这是一个统一框架，将分层决策重新表述为自回归序列建模，以生成潜在子目标序列和原始动作。通过采用MLP-Mixer骨干网络实现跨令牌通信并捕捉状态、目标、潜在子目标和动作之间的结构关系，CoGHP在导航和操作基准测试中进行了评估，结果表明其在长时程任务上持续优于现有离线基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">An Approximate Ascent Approach To Prove Convergence of PPO</div>
<div class="meta-line">Authors: Leif Doering, Daniel Schmidt, Moritz Melcher, Sebastian Kassing, Benedikt Wille, Tilman Aach, Simon Weissmann</div>
<div class="meta-line">First: 2026-02-03T11:10:22+00:00 · Latest: 2026-02-03T11:10:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03386v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03386v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Proximal Policy Optimization (PPO) is among the most widely used deep reinforcement learning algorithms, yet its theoretical foundations remain incomplete. Most importantly, convergence and understanding of fundamental PPO advantages remain widely open. Under standard theory assumptions we show how PPO&#x27;s policy update scheme (performing multiple epochs of minibatch updates on multi-use rollouts with a surrogate gradient) can be interpreted as approximated policy gradient ascent. We show how to control the bias accumulated by the surrogate gradients and use techniques from random reshuffling to prove a convergence theorem for PPO that sheds light on PPO&#x27;s success. Additionally, we identify a previously overlooked issue in truncated Generalized Advantage Estimation commonly used in PPO. The geometric weighting scheme induces infinite mass collapse onto the longest $k$-step advantage estimator at episode boundaries. Empirical evaluations show that a simple weight correction can yield substantial improvements in environments with strong terminal signal, such as Lunar Lander.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种证明PPO收敛性的近似上升方法</div>
<div class="mono" style="margin-top:8px">近端策略优化（PPO）是最广泛使用的深度强化学习算法之一，但其理论基础仍不完善。最关键的是，PPO的收敛性及其核心优势的理解仍存在广泛空白。在标准理论假设下，我们展示了PPO的策略更新方案（对复用轨迹进行多轮小批量更新，采用替代梯度）可被解释为近似策略梯度上升。我们阐明了如何控制替代梯度累积的偏差，并运用随机重排技术证明了PPO的收敛定理，从而揭示其成功机理。此外，我们发现了PPO常用的截断广义优势估计中一个被忽视的问题：几何加权方案在回合边界处会导致无限质量坍缩至最长的k步优势估计器。实证评估表明，在具有强终止信号的环境（如月球着陆器）中，简单的权重修正能带来显著性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the incomplete theoretical understanding of Proximal Policy Optimization (PPO), a widely used deep reinforcement learning algorithm, by providing a convergence proof and analyzing a practical issue. The motivation is to clarify PPO&#x27;s theoretical foundations, particularly its convergence properties and advantages. The method interprets PPO&#x27;s policy update as approximated policy gradient ascent, controls bias from surrogate gradients, and applies random reshuffling techniques to prove convergence. Experimental results demonstrate that the analysis sheds light on PPO&#x27;s success and identifies a problem in truncated Generalized Advantage Estimation where geometric weighting causes infinite mass collapse at episode boundaries; a simple weight correction is shown to yield substantial improvements in environments like Lunar Lander with strong terminal signals.</div>
<div class="mono" style="margin-top:8px">本文针对广泛使用的深度强化学习算法——近端策略优化（PPO）理论理解不完整的问题，提供了收敛性证明并分析了一个实际问题。其动机是阐明PPO的理论基础，特别是其收敛性和优势。方法上将PPO的策略更新解释为近似策略梯度上升，控制代理梯度带来的偏差，并应用随机重排技术证明收敛性。实验结果表明，该分析揭示了PPO的成功原因，并发现了截断广义优势估计中的一个问题：几何加权方案在回合边界会导致无限质量塌陷到最长的k步优势估计器上；在像月球着陆器这样具有强终端信号的环境中，简单的权重修正能带来显著改进。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards</div>
<div class="meta-line">Authors: Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She, Viet Anh Nguyen</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-02T03:50:01+00:00 · Latest: 2026-02-03T10:25:06+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01601v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.01601v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce VIP, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, VIP uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that VIP consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可验证奖励在线强化学习的自适应轨迹分配策略</div>
<div class="mono" style="margin-top:8px">采样效率是可验证奖励强化学习的关键瓶颈。现有基于分组的策略优化方法（如GRPO）为所有训练提示分配固定数量的轨迹。这种均匀分配隐含地将所有提示视为同等信息量，可能导致计算预算使用效率低下并阻碍训练进展。本文提出VIP（方差感知预测分配策略），将给定轨迹预算分配给当前批次中的提示，以最小化策略更新的期望梯度方差。在每次迭代中，VIP使用轻量级高斯过程模型基于近期轨迹预测每个提示的成功概率。这些概率预测被转化为方差估计，随后输入凸优化问题，在严格计算预算约束下确定最优轨迹分配。实验结果表明，VIP在多个基准测试中持续提升采样效率，并比均匀或启发式分配策略获得更高性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the sampling inefficiency in reinforcement learning with verifiable rewards, where fixed uniform rollout allocation treats all prompts as equally informative, this paper introduces VIP, a Variance-Informed Predictive allocation strategy. The method employs a lightweight Gaussian process model to predict per-prompt success probabilities from recent rollouts, translates these into variance estimates, and solves a convex optimization problem to optimally allocate a given rollout budget to minimize expected gradient variance. Experimental results across multiple benchmarks demonstrate that VIP consistently improves sampling efficiency and achieves higher performance compared to uniform or heuristic allocation strategies.</div>
<div class="mono" style="margin-top:8px">本文针对可验证奖励强化学习中采样效率低下的问题，提出了一种方差感知预测分配策略VIP，以解决现有均匀分配方法将所有提示视为同等信息量而导致的预算使用低效。该方法采用轻量级高斯过程模型，基于近期轨迹预测每个提示的成功概率，将其转化为方差估计，并通过凸优化在计算预算约束下确定最优的轨迹分配方案。在多个基准测试中的实验结果表明，VIP相比均匀或启发式分配策略，能持续提升采样效率并获得更高的性能表现。</div>
</details>
</div>
<div class="card">
<div class="title">MentalSeek-Dx: Towards Progressive Hypothetico-Deductive Reasoning for Real-world Psychiatric Diagnosis</div>
<div class="meta-line">Authors: Xiao Sun, Yuming Yang, Junnan Zhu, Jiang Zhong, Xinyu Zhou, Kaiwen Wei</div>
<div class="meta-line">First: 2026-02-03T10:03:35+00:00 · Latest: 2026-02-03T10:03:35+00:00</div>
<div class="meta-line">Comments: 36 pages, 27 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03340v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03340v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mental health disorders represent a burgeoning global public health challenge. While Large Language Models (LLMs) have demonstrated potential in psychiatric assessment, their clinical utility is severely constrained by benchmarks that lack ecological validity and fine-grained diagnostic supervision. To bridge this gap, we introduce \textbf{MentalDx Bench}, the first benchmark dedicated to disorder-level psychiatric diagnosis within real-world clinical settings. Comprising 712 de-identified electronic health records annotated by board-certified psychiatrists under ICD-11 guidelines, the benchmark covers 76 disorders across 16 diagnostic categories. Evaluation of 18 LLMs reveals a critical \textit{paradigm misalignment}: strong performance at coarse diagnostic categorization contrasts with systematic failure at disorder-level diagnosis, underscoring a gap between pattern-based modeling and clinical hypothetico-deductive reasoning. In response, we propose \textbf{MentalSeek-Dx}, a medical-specialized LLM trained to internalize this clinical reasoning process through supervised trajectory construction and curriculum-based reinforcement learning. Experiments on MentalDx Bench demonstrate that MentalSeek-Dx achieves state-of-the-art (SOTA) performance with only 14B parameters, establishing a clinically grounded framework for reliable psychiatric diagnosis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MentalSeek-Dx：面向真实世界精神疾病诊断的渐进式假设-演绎推理</div>
<div class="mono" style="margin-top:8px">精神健康障碍已成为日益严峻的全球公共卫生挑战。尽管大语言模型在精神疾病评估中展现出潜力，但其临床应用因缺乏生态效度和细粒度诊断监督的基准而严重受限。为弥合这一差距，我们推出首个专注于真实临床场景下疾病层级精神诊断的基准——\textbf{MentalDx Bench}。该基准包含712份经委员会认证精神科医生依据ICD-11标准标注的去标识化电子健康记录，涵盖16个诊断类别下的76种疾病。对18个大语言模型的评估揭示了关键的\textit{范式错位}：粗粒度诊断分类的优异表现与疾病层级诊断的系统性失败形成鲜明对比，凸显了基于模式建模与临床假设-演绎推理之间的鸿沟。为此，我们提出\textbf{MentalSeek-Dx}——一种通过监督轨迹构建和课程式强化学习内化临床推理过程的医学专用大语言模型。在MentalDx Bench上的实验表明，MentalSeek-Dx仅以140亿参数即达到最先进性能，为可靠的精神疾病诊断建立了临床基础框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the global mental health crisis and the limitations of existing LLM benchmarks lacking ecological validity for psychiatric diagnosis, this work introduces MentalDx Bench, a novel benchmark comprising 712 real-world clinical records annotated by psychiatrists. The method proposes MentalSeek-Dx, a specialized LLM trained via supervised trajectory construction and curriculum-based reinforcement learning to internalize clinical hypothetico-deductive reasoning. Experimental results show that while standard LLMs fail at fine-grained disorder-level diagnosis, MentalSeek-Dx achieves state-of-the-art performance on the benchmark with only 14B parameters, establishing a clinically grounded framework.</div>
<div class="mono" style="margin-top:8px">本研究针对全球心理健康危机以及现有大语言模型在精神病学诊断中缺乏生态效度的基准限制，提出了首个基于真实世界临床记录的精神障碍诊断基准MentalDx Bench。方法上，作者开发了MentalSeek-Dx这一专业大语言模型，通过监督轨迹构建和课程强化学习来内化临床假设演绎推理过程。实验结果表明，尽管标准大语言模型在细粒度障碍级别诊断上系统性失败，但仅具140亿参数的MentalSeek-Dx在该基准上取得了最先进的性能，建立了一个临床可靠的诊断框架。</div>
</details>
</div>
<div class="card">
<div class="title">MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates for Exponentially Stabilizing Control</div>
<div class="meta-line">Authors: Yongwei Zhang, Yuanzhe Xing, Quanyi Liang, Quan Quan, Zhikun She</div>
<div class="meta-line">First: 2025-12-31T16:36:44+00:00 · Latest: 2026-02-03T10:02:50+00:00</div>
<div class="meta-line">Comments: This work has been submitted to the IEEE for possible publication</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24955v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.24955v2">PDF</a> · <a href="https://github.com/YuanZhe-Xing/MSACL}{https://github.com/YuanZhe-Xing/MSACL">Code1</a> · <a href="https://github.com/YuanZhe-Xing/MSACL">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">For safety-critical applications, model-free reinforcement learning (RL) faces numerous challenges, particularly the difficulty of establishing verifiable stability guarantees while maintaining high exploration efficiency. To address these challenges, we present Multi-Step Actor-Critic Learning with Lyapunov Certificates (MSACL), a novel approach that seamlessly integrates exponential stability with maximum entropy reinforcement learning (MERL). In contrast to existing methods that rely on complex reward engineering and single-step constraints, MSACL utilizes intuitive rewards and multi-step data for actor-critic learning. Specifically, we first introduce Exponential Stability Labels (ESLs) to categorize samples and propose a $λ$-weighted aggregation mechanism to learn Lyapunov certificates. Leveraging these certificates, we then develop a stability-aware advantage function to guide policy optimization, thereby ensuring rapid Lyapunov descent and robust state convergence. We evaluate MSACL across six benchmarks, comprising four stabilization and two high-dimensional tracking tasks. Experimental results demonstrate its consistent superiority over both standard RL baselines and state-of-the-art Lyapunov-based RL algorithms. Beyond rapid convergence, MSACL exhibits significant robustness against environmental uncertainties and remarkable generalization to unseen reference signals. The source code and benchmarking environments are available at \href{https://github.com/YuanZhe-Xing/MSACL}{https://github.com/YuanZhe-Xing/MSACL}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MSACL：基于李雅普诺夫证书的多步演员-评论家学习算法及其在指数稳定控制中的应用</div>
<div class="mono" style="margin-top:8px">针对安全关键应用，无模型强化学习面临诸多挑战，尤其是在保持高探索效率的同时难以建立可验证的稳定性保证。为此，我们提出基于李雅普诺夫证书的多步演员-评论家学习算法（MSACL），该方法将指数稳定性与最大熵强化学习无缝集成。相较于依赖复杂奖励工程和单步约束的现有方法，MSACL采用直观奖励机制并利用多步数据进行演员-评论家学习。具体而言，我们首先引入指数稳定性标签对样本分类，并提出λ加权聚合机制学习李雅普诺夫证书。基于这些证书，我们进一步构建稳定性感知优势函数以指导策略优化，从而确保李雅普诺夫函数的快速下降和状态鲁棒收敛。我们在六个基准测试中评估MSACL，包括四个镇定任务和两个高维跟踪任务。实验结果表明，该方法在标准强化学习基线和最先进的基于李雅普诺夫的强化学习算法中均表现出持续优越性。除快速收敛外，MSACL对环境不确定性具有显著鲁棒性，并对未见参考信号展现出卓越泛化能力。源代码与基准测试环境已发布于https://github.com/YuanZhe-Xing/MSACL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for verifiable stability guarantees and high exploration efficiency in safety-critical reinforcement learning (RL), this paper introduces MSACL, a method that integrates exponential stability with maximum entropy RL. The approach uses intuitive rewards and multi-step data, employing Exponential Stability Labels and a λ-weighted aggregation to learn Lyapunov certificates, which then guide policy optimization via a stability-aware advantage function. Experimental results on six benchmarks, including stabilization and tracking tasks, show that MSACL outperforms standard RL and state-of-the-art Lyapunov-based methods, demonstrating rapid convergence, robustness to uncertainties, and strong generalization to unseen signals.</div>
<div class="mono" style="margin-top:8px">针对安全关键应用中无模型强化学习难以获得可验证稳定性保证且探索效率低的问题，本文提出了MSACL方法，将指数稳定性与最大熵强化学习相结合。该方法采用直观奖励和多步数据，通过指数稳定性标签和λ加权聚合学习李雅普诺夫证书，并利用稳定性感知的优势函数指导策略优化。在包含稳定化和高维跟踪任务的六个基准测试中，实验结果表明MSACL优于标准强化学习和先进的基于李雅普诺夫的方法，展现出快速收敛性、对环境不确定性的强鲁棒性以及对未见参考信号的出色泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Task-Centric Policy Optimization from Misaligned Motion Priors</div>
<div class="meta-line">Authors: Ziang Zheng, Kai Feng, Yi Nie, Shentao Qin</div>
<div class="meta-line">First: 2026-01-27T09:46:34+00:00 · Latest: 2026-02-03T09:56:39+00:00</div>
<div class="meta-line">Comments: Work requires further details and not complete yet</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19411v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.19411v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humanoid control often leverages motion priors from human demonstrations to encourage natural behaviors. However, such demonstrations are frequently suboptimal or misaligned with robotic tasks due to embodiment differences, retargeting errors, and task-irrelevant variations, causing naïve imitation to degrade task performance. Conversely, task-only reinforcement learning admits many task-optimal solutions, often resulting in unnatural or unstable motions. This exposes a fundamental limitation of linear reward mixing in adversarial imitation learning. We propose \emph{Task-Centric Motion Priors} (TCMP), a task-priority adversarial imitation framework that treats imitation as a conditional regularizer rather than a co-equal objective. TCMP maximizes task improvement while incorporating imitation signals only when they are compatible with task progress, yielding an adaptive, geometry-aware update that preserves task-feasible descent and suppresses harmful imitation under misalignment. We provide theoretical analysis of gradient conflict and task-priority stationary points, and validate our claims through humanoid control experiments demonstrating robust task performance with consistent motion style under noisy demonstrations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于未对齐运动先验的任务中心策略优化</div>
<div class="mono" style="margin-top:8px">人形机器人控制常利用人类演示的运动先验来促进行为自然性。然而，由于本体差异、重定向误差和任务无关的变异，此类演示常存在次优性或与机器人任务未对齐，导致简单模仿会降低任务性能。反之，纯任务强化学习虽能产生多种任务最优解，却常导致不自然或不稳定的运动。这揭示了对抗模仿学习中线性奖励混合的根本局限。我们提出\emph{任务中心运动先验}（TCMP），这是一种任务优先的对抗模仿框架，将模仿视为条件正则化器而非平等目标。TCMP在最大化任务改进的同时，仅当模仿信号与任务进展兼容时才纳入它们，从而产生一种自适应、几何感知的更新机制，既能保持任务可行的下降方向，又能在未对齐时抑制有害模仿。我们提供了梯度冲突与任务优先稳定点的理论分析，并通过人形控制实验验证了所提方法在噪声演示下能实现稳健的任务性能与一致的运动风格。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of using human motion priors for humanoid control, where direct imitation can degrade task performance due to misalignment from embodiment differences or suboptimal demonstrations, while task-only reinforcement learning often yields unnatural motions. The authors propose Task-Centric Motion Priors (TCMP), a task-priority adversarial imitation framework that treats imitation as a conditional regularizer, adaptively incorporating imitation signals only when they align with task progress to preserve task-feasible optimization and suppress harmful imitation. Experimental results on humanoid control demonstrate that TCMP achieves robust task performance while maintaining consistent motion style even with noisy demonstrations, supported by theoretical analysis of gradient conflicts and stationary points.</div>
<div class="mono" style="margin-top:8px">该论文针对人形机器人控制中使用人类运动先验的挑战，指出直接模仿会因本体差异或次优演示导致任务性能下降，而仅基于任务的强化学习常产生不自然的运动。作者提出任务中心运动先验（TCMP），这是一个任务优先的对抗模仿框架，将模仿视为条件正则化器，仅在模仿信号与任务进展兼容时自适应地纳入，以保持任务可行的优化并抑制有害模仿。在人形控制实验中，TCMP在噪声演示下实现了鲁棒的任务性能并保持了一致的运动风格，辅以对梯度冲突和驻点的理论分析。</div>
</details>
</div>
<div class="card">
<div class="title">From Classical to Quantum Reinforcement Learning and Its Applications in Quantum Control: A Beginner&#x27;s Tutorial</div>
<div class="meta-line">Authors: Abhijit Sen, Sonali Panda, Mahima Arya, Subhajit Patra, Zizhan Zheng, Denys I. Bondar</div>
<div class="meta-line">First: 2026-01-13T15:40:55+00:00 · Latest: 2026-02-03T09:52:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08662v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08662v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This tutorial is designed to make reinforcement learning (RL) more accessible to undergraduate students by offering clear, example-driven explanations. It focuses on bridging the gap between RL theory and practical coding applications, addressing common challenges that students face when transitioning from conceptual understanding to implementation. Through hands-on examples and approachable explanations, the tutorial aims to equip students with the foundational skills needed to confidently apply RL techniques in real-world scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从经典到量子强化学习及其在量子控制中的应用：初学者教程</div>
<div class="mono" style="margin-top:8px">本教程旨在通过清晰、示例驱动的讲解，使本科生更易理解强化学习（RL）。重点在于弥合RL理论与实际编程应用之间的差距，解决学生从概念理解转向实践时面临的常见挑战。通过动手示例和易于理解的解释，本教程致力于帮助学生掌握在现实场景中自信应用RL技术所需的基础技能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to make reinforcement learning (RL) more accessible to undergraduate students, this tutorial provides clear, example-driven explanations to bridge the gap between RL theory and practical coding applications. The method involves using hands-on examples and approachable explanations to address common challenges students face when moving from conceptual understanding to implementation. The main experimental outcome is that the tutorial equips students with foundational skills, enabling them to confidently apply RL techniques in real-world scenarios.</div>
<div class="mono" style="margin-top:8px">本教程旨在让本科生更容易理解强化学习，通过提供清晰、示例驱动的解释来弥合强化学习理论与实际编程应用之间的差距。其方法包括使用实践示例和易于理解的讲解，以解决学生在从概念理解转向实现过程中遇到的常见挑战。主要实验结果表明，该教程使学生掌握了基础技能，能够自信地在实际场景中应用强化学习技术。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Helpfulness-Harmlessness Alignment with Preference Vectors</div>
<div class="meta-line">Authors: Ren-Wei Liang, Chin-Ting Hsu, Chan-Hung Yu, Saransh Agrawal, Shih-Cheng Huang, Shang-Tse Chen, Kuan-Hao Huang, Shao-Hua Sun</div>
<div class="meta-line">First: 2025-04-27T12:16:51+00:00 · Latest: 2026-02-03T09:48:15+00:00</div>
<div class="meta-line">Comments: Accepted at The 19th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2026), Rabat, Morocco. 22 pages, 5 figures, 9 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.20106v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.20106v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ensuring that large language models (LLMs) are both helpful and harmless is a critical challenge, as overly strict constraints can lead to excessive refusals, while permissive models risk generating harmful content. Existing approaches, such as reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO), attempt to balance these trade-offs but suffer from performance conflicts, limited controllability, and poor extendability. To address these issues, we propose Preference Vector, a novel framework inspired by task arithmetic. Instead of optimizing multiple preferences within a single objective, we train separate models on individual preferences, extract behavior shifts as preference vectors, and dynamically merge them at test time. This modular approach enables fine-grained, user-controllable preference adjustments and facilitates seamless integration of new preferences without retraining. Experiments show that our proposed Preference Vector framework improves helpfulness without excessive conservatism, allows smooth control over preference trade-offs, and supports scalable multi-preference alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于偏好向量的自适应助益-无害对齐方法</div>
<div class="mono" style="margin-top:8px">确保大语言模型（LLMs）既助益又无害是一项关键挑战：约束过严会导致过度拒绝，而宽松模型则可能生成有害内容。现有方法（如基于人类反馈的强化学习RLHF和直接偏好优化DPO）试图平衡这些权衡，但存在性能冲突、可控性有限和扩展性差等问题。为解决这些问题，我们提出受任务算术启发的创新框架——偏好向量。该方法不在单一目标中优化多重偏好，而是针对个体偏好分别训练独立模型，将行为偏移提取为偏好向量，并在测试时动态融合。这种模块化方法支持细粒度、用户可控的偏好调节，并能无缝整合新偏好而无需重新训练。实验表明，偏好向量框架在提升助益性时避免过度保守，支持平滑的偏好权衡调控，并具备可扩展的多偏好对齐能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of aligning large language models to be both helpful and harmless, as existing methods like RLHF and DPO often create performance conflicts and lack controllability. The authors propose a novel framework called Preference Vector, inspired by task arithmetic, which trains separate models for individual preferences, extracts their behavioral shifts as vectors, and dynamically combines them during inference. Experimental results demonstrate that this approach improves helpfulness without excessive refusals, enables smooth user control over trade-offs, and supports scalable integration of new preferences without retraining.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在兼顾有用性和无害性对齐时面临的挑战，指出现有方法如RLHF和DPO存在性能冲突、可控性差等问题。作者受任务算术启发，提出了一种名为偏好向量的新框架，该方法为不同偏好单独训练模型，将其行为变化提取为向量，并在推理时动态融合。实验结果表明，该框架能在不过度保守的情况下提升模型的有用性，允许用户平滑控制偏好权衡，并支持无需重新训练即可扩展集成新偏好。</div>
</details>
</div>
<div class="card">
<div class="title">MedSAM-Agent: Empowering Interactive Medical Image Segmentation with Multi-turn Agentic Reinforcement Learning</div>
<div class="meta-line">Authors: Shengyuan Liu, Liuxin Bao, Qi Yang, Wanting Geng, Boyun Zheng, Chenxin Li, Wenting Chen, Houwen Peng, Yixuan Yuan</div>
<div class="meta-line">First: 2026-02-03T09:47:49+00:00 · Latest: 2026-02-03T09:47:49+00:00</div>
<div class="meta-line">Comments: 23 Pages, 4 Figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03320v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03320v1">PDF</a> · <a href="https://github.com/CUHK-AIM-Group/MedSAM-Agent}{here">Code1</a> · <a href="https://github.com/CUHK-AIM-Group/MedSAM-Agent">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Medical image segmentation is evolving from task-specific models toward generalizable frameworks. Recent research leverages Multi-modal Large Language Models (MLLMs) as autonomous agents, employing reinforcement learning with verifiable reward (RLVR) to orchestrate specialized tools like the Segment Anything Model (SAM). However, these approaches often rely on single-turn, rigid interaction strategies and lack process-level supervision during training, which hinders their ability to fully exploit the dynamic potential of interactive tools and leads to redundant actions. To bridge this gap, we propose MedSAM-Agent, a framework that reformulates interactive segmentation as a multi-step autonomous decision-making process. First, we introduce a hybrid prompting strategy for expert-curated trajectory generation, enabling the model to internalize human-like decision heuristics and adaptive refinement strategies. Furthermore, we develop a two-stage training pipeline that integrates multi-turn, end-to-end outcome verification with a clinical-fidelity process reward design to promote interaction parsimony and decision efficiency. Extensive experiments across 6 medical modalities and 21 datasets demonstrate that MedSAM-Agent achieves state-of-the-art performance, effectively unifying autonomous medical reasoning with robust, iterative optimization. Code is available \href{https://github.com/CUHK-AIM-Group/MedSAM-Agent}{here}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MedSAM-Agent：通过多轮智能体强化学习赋能交互式医学图像分割</div>
<div class="mono" style="margin-top:8px">医学图像分割正从任务专用模型向通用化框架演进。近期研究利用多模态大语言模型作为自主智能体，采用带可验证奖励的强化学习来协调如Segment Anything Model等专用工具。然而，这些方法通常依赖单轮、僵化的交互策略，且训练过程缺乏流程级监督，限制了其充分挖掘交互工具动态潜力的能力，并导致冗余操作。为弥补这一不足，我们提出MedSAM-Agent框架，将交互式分割重构为多步骤自主决策过程。首先，我们引入混合提示策略生成专家轨迹，使模型内化类人决策启发与自适应优化策略。此外，我们开发了包含多轮端到端结果验证与临床保真度流程奖励设计的两阶段训练流程，以提升交互简洁性与决策效率。在6种医学模态和21个数据集上的大量实验表明，MedSAM-Agent实现了最先进的性能，有效将自主医学推理与鲁棒的迭代优化相统一。代码已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses limitations in current interactive medical image segmentation, where single-turn strategies and insufficient process supervision lead to inefficient tool use. The authors propose MedSAM-Agent, a framework that reframes segmentation as a multi-step decision process, employing a hybrid prompting strategy to generate expert-like trajectories and a two-stage training pipeline with process-level rewards to enhance efficiency. Experiments across 6 modalities and 21 datasets show state-of-the-art performance, demonstrating effective integration of autonomous reasoning with iterative optimization.</div>
<div class="mono" style="margin-top:8px">本文针对当前交互式医学图像分割中单轮策略和过程监督不足导致工具使用效率低下的问题，提出了MedSAM-Agent框架，将分割重构为多步决策过程。该方法采用混合提示策略生成专家级轨迹，并通过包含过程奖励的两阶段训练流程提升交互效率。在6种模态和21个数据集上的实验表明，该框架取得了最先进的性能，有效实现了自主推理与迭代优化的结合。</div>
</details>
</div>
<div class="card">
<div class="title">Emergent time-keeping mechanisms in a deep reinforcement learning agent performing an interval timing task</div>
<div class="meta-line">Authors: Amrapali Pednekar, Alvaro Garrido, Pieter Simoens, Yara Khaluf</div>
<div class="meta-line">First: 2025-08-06T13:56:41+00:00 · Latest: 2026-02-03T09:46:37+00:00</div>
<div class="meta-line">Comments: Accepted at 2025 Artificial Life Conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.15784v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.15784v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Drawing parallels between Deep Artificial Neural Networks (DNNs) and biological systems can aid in understanding complex biological mechanisms that are difficult to disentangle. Temporal processing, an extensively researched topic, is one such example that lacks a coherent understanding of its underlying mechanisms. In this study, we investigate temporal processing in a Deep Reinforcement Learning (DRL) agent performing an interval timing task and explore potential biological counterparts to its emergent behavior. The agent was successfully trained to perform a duration production task, which involved marking successive occurrences of a target interval while viewing a video sequence. Analysis of the agent&#x27;s internal states revealed oscillatory neural activations, a ubiquitous pattern in biological systems. Interestingly, the agent&#x27;s actions were predominantly influenced by neurons exhibiting these oscillations with high amplitudes and frequencies corresponding to the target interval. Parallels are drawn between the agent&#x27;s time-keeping strategy and the Striatal Beat Frequency (SBF) model, a biologically plausible model of interval timing. Furthermore, the agent maintained its oscillatory representations and task performance when tested on different video sequences (including a blank video). Thus, once learned, the agent internalized its time-keeping mechanism and showed minimal reliance on its environment to perform the timing task. A hypothesis about the resemblance between this emergent behavior and certain aspects of the evolution of biological processes like circadian rhythms, has been discussed. This study aims to contribute to recent research efforts of utilizing DNNs to understand biological systems, with a particular emphasis on temporal processing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度强化学习智能体执行间隔计时任务中涌现的时间保持机制</div>
<div class="mono" style="margin-top:8px">通过类比深度人工神经网络与生物系统，有助于理解难以解析的复杂生物机制。时间处理作为广泛研究的课题，其底层机制尚缺乏统一理解。本研究探索了执行间隔计时任务的深度强化学习智能体中的时间处理，并对其涌现行为与潜在生物对应机制进行了分析。智能体经训练成功完成时长生成任务，即在观看视频序列时标记目标间隔的连续出现。对其内部状态的分析揭示了振荡性神经激活模式——一种生物系统中普遍存在的模式。值得注意的是，智能体的行为主要受那些呈现高振幅、频率与目标间隔对应的振荡神经元影响。研究将智能体的时间保持策略与生物合理的间隔计时模型——纹状体节拍频率模型进行了类比。此外，智能体在不同视频序列（包括空白视频）测试中仍保持振荡表征与任务性能，表明其一旦习得时间保持机制，执行计时任务时对环境依赖极低。文中还探讨了该涌现行为与昼夜节律等生物过程演化特征的相似性假说。本研究旨在推动利用深度神经网络理解生物系统的最新探索，尤其关注时间处理领域。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates temporal processing by training a deep reinforcement learning agent on an interval timing task, aiming to draw parallels between artificial and biological neural mechanisms. The method involved training the agent to mark target intervals while viewing video sequences, with analysis of its internal states revealing oscillatory neural activations similar to biological patterns, particularly where high-amplitude oscillations correlated with the target interval influenced actions. Experimental results showed the agent&#x27;s strategy resembled the biologically plausible Striatal Beat Frequency model, and it maintained performance and oscillatory representations even with different video inputs, including a blank video, indicating an internalized time-keeping mechanism with minimal environmental reliance, suggesting insights into the evolution of biological processes like circadian rhythms.</div>
<div class="mono" style="margin-top:8px">本研究通过训练深度强化学习智能体执行间隔计时任务，探究时间处理机制，旨在建立人工与生物神经机制之间的类比。方法涉及训练智能体在观看视频序列时标记目标间隔，并分析其内部状态，发现其神经激活呈现振荡模式，类似于生物系统，其中高振幅振荡与目标间隔相关并主导了行为。实验结果表明，智能体的计时策略与生物上可行的纹状体节拍频率模型相似，且在不同视频序列（包括空白视频）测试中仍保持性能和振荡表征，表明其内化了计时机制，对环境依赖极小，这为理解生物过程（如昼夜节律）的演化提供了启示。</div>
</details>
</div>
<div class="card">
<div class="title">Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time Recurrent RL</div>
<div class="meta-line">Authors: Julian Lemmel, Felix Resch, Mónika Farsang, Ramin Hasani, Daniela Rus, Radu Grosu</div>
<div class="meta-line">First: 2026-02-02T15:41:53+00:00 · Latest: 2026-02-03T09:41:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02236v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02236v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying pretrained policies in real-world applications presents substantial challenges that fundamentally limit the practical applicability of learning-based control systems. When autonomous systems encounter environmental changes in system dynamics, sensor drift, or task objectives, fixed policies rapidly degrade in performance. We show that employing Real-Time Recurrent Reinforcement Learning (RTRRL), a biologically plausible algorithm for online adaptation, can effectively fine-tune a pretrained policy to improve autonomous agents&#x27; performance on driving tasks. We further show that RTRRL synergizes with a recent biologically inspired recurrent network model, the Liquid-Resistance Liquid-Capacitance RNN. We demonstrate the effectiveness of this closed-loop approach in a simulated CarRacing environment and in a real-world line-following task with a RoboRacer car equipped with an event camera.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于实时循环强化学习的自动驾驶预训练控制器在线微调</div>
<div class="mono" style="margin-top:8px">在现实应用中部署预训练策略面临重大挑战，从根本上限制了基于学习的控制系统的实际适用性。当自主系统遭遇系统动力学变化、传感器漂移或任务目标改变时，固定策略的性能会迅速下降。研究表明，采用实时循环强化学习（RTRRL）——一种具有生物合理性的在线适应算法——能够有效微调预训练策略，提升自主智能体在驾驶任务中的表现。进一步研究发现，RTRRL可与近期受生物启发的循环网络模型（液阻-液容循环神经网络）产生协同效应。我们在模拟CarRacing环境及配备事件相机的RoboRacer实车循线任务中，验证了该闭环方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that pretrained policies for autonomous driving degrade when faced with environmental changes, limiting their real-world applicability. The method proposes using Real-Time Recurrent Reinforcement Learning (RTRRL), a biologically plausible online adaptation algorithm, to fine-tune a pretrained policy, and synergizes it with a Liquid-Resistance Liquid-Capacitance recurrent network model. The main experimental results demonstrate the effectiveness of this closed-loop approach in both a simulated CarRacing environment and a real-world line-following task using a RoboRacer car with an event camera.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，自动驾驶的预训练策略在面临环境变化时性能会下降，这限制了其实际应用。方法上，提出使用一种具有生物合理性的在线适应算法——实时循环强化学习（RTRRL）来微调预训练策略，并将其与一种受生物启发的液体电阻-液体电容循环网络模型相结合。主要实验结果表明，这种闭环方法在模拟的CarRacing环境以及使用配备事件相机的RoboRacer汽车进行的真实世界循线任务中均有效。</div>
</details>
</div>
<div class="card">
<div class="title">Entropy-Gated Selective Policy Optimization:Token-Level Gradient Allocation for Hybrid Training of Large Language Models</div>
<div class="meta-line">Authors: Yuelin Hu, Zhengxue Cheng, Wei Liu, Li Song</div>
<div class="meta-line">First: 2026-02-03T09:38:21+00:00 · Latest: 2026-02-03T09:38:21+00:00</div>
<div class="meta-line">Comments: accepted by cscwd2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03309v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03309v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hybrid training methods for large language models combine supervised fine tuning (SFT) on expert demonstrations with reinforcement learning (RL) on model rollouts, typically at the sample level. We propose Entropy Gated Selective Policy Optimization (EGSPO), a three stage framework that extends sample level mixing with token level gradient modulation.
  Stage 1, SFT expert learning, establishes a reliable warm up policy using expert demonstrations with a pure SFT loss. Stage 2, RL rollout generation, samples trajectories from the current policy and computes per token predictive entropy. Stage 3, the EGSPO mechanism, applies entropy gated gradient allocation: a predictive entropy module routes high entropy tokens to full PPO updates to encourage exploration, and low entropy tokens to attenuated PPO updates to reduce variance and preserve knowledge. Critically, both branches incorporate the advantage function A_t, ensuring that incorrect trajectories receive consistent negative learning signals and preventing reinforcement of confident errors.
  EGSPO achieves consistent improvements on mathematical reasoning benchmarks, with gains of 3.8 percent on AIME and 2.9 percent on MATH over the CHORD phi baseline, while incurring only 3.4 percent additional computational overhead.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>熵门控选择性策略优化：面向大语言模型混合训练的令牌级梯度分配方法</div>
<div class="mono" style="margin-top:8px">大语言模型的混合训练方法通常结合样本级的专家演示监督微调（SFT）与模型推演的强化学习（RL）。本文提出熵门控选择性策略优化（EGSPO），这是一个三阶段框架，通过令牌级梯度调制扩展了样本级混合方法。第一阶段为SFT专家学习，采用纯SFT损失通过专家演示建立可靠的预热策略。第二阶段为RL推演生成，从当前策略采样轨迹并计算各令牌的预测熵。第三阶段为EGSPO机制，实施熵门控梯度分配：预测熵模块将高熵令牌导向完整PPO更新以鼓励探索，低熵令牌则导向衰减PPO更新以降低方差并保留知识。关键的是，两个分支均包含优势函数A_t，确保错误轨迹获得一致的负向学习信号，防止强化自信错误。EGSPO在数学推理基准上取得持续改进，较CHORD phi基线在AIME和MATH分别提升3.8%和2.9%，仅产生3.4%的额外计算开销。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to improve hybrid training of large language models, which typically combines supervised fine-tuning (SFT) and reinforcement learning (RL) at the sample level, by introducing more fine-grained control. The method, Entropy-Gated Selective Policy Optimization (EGSPO), is a three-stage framework that modulates gradients at the token level: after initial SFT on expert demonstrations, it generates RL rollouts, computes per-token predictive entropy, and then uses this entropy to gate gradient allocation—high-entropy tokens receive full PPO updates to encourage exploration, while low-entropy tokens get attenuated updates to reduce variance and preserve knowledge, with both branches incorporating advantage signals to avoid reinforcing errors. Main experimental results show that EGSPO consistently enhances mathematical reasoning performance, achieving gains of 3.8% on AIME and 2.9% on MATH over the CHORD phi baseline, with only a 3.4% increase in computational overhead.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过引入更细粒度的控制来改进大语言模型的混合训练方法，传统方法通常在样本级别结合监督微调（SFT）和强化学习（RL）。所提出的方法名为熵门控选择性策略优化（EGSPO），这是一个三阶段框架，在词元级别调节梯度：首先在专家演示上进行SFT预热，然后生成RL轨迹并计算每个词元的预测熵，接着利用该熵值进行梯度分配门控——高熵词元接受完整的PPO更新以鼓励探索，而低熵词元则获得衰减的更新以减少方差并保留知识，两个分支均结合优势函数以避免强化错误。主要实验结果表明，EGSPO在数学推理基准上取得了稳定提升，相比CHORD phi基线在AIME和MATH上分别提高了3.8%和2.9%，同时仅增加了3.4%的计算开销。</div>
</details>
</div>
<div class="card">
<div class="title">medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions</div>
<div class="meta-line">Authors: Qianyi Xu, Gousia Habib, Feng Wu, Yanrui Du, Zhihui Chen, Swapnil Mishra, Dilruk Perera, Mengling Feng</div>
<div class="meta-line">First: 2026-02-03T09:30:32+00:00 · Latest: 2026-02-03T09:30:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03305v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03305v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) offers a powerful framework for optimizing dynamic treatment regimes (DTRs). However, clinical RL is fundamentally bottlenecked by reward engineering: the challenge of defining signals that safely and effectively guide policy learning in complex, sparse offline environments. Existing approaches often rely on manual heuristics that fail to generalize across diverse pathologies. To address this, we propose an automated pipeline leveraging Large Language Models (LLMs) for offline reward design and verification. We formulate the reward function using potential functions consisted of three core components: survival, confidence, and competence. We further introduce quantitative metrics to rigorously evaluate and select the optimal reward structure prior to deployment. By integrating LLM-driven domain knowledge, our framework automates the design of reward functions for specific diseases while significantly enhancing the performance of the resulting policies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>medR：基于三驱动势函数的临床离线强化学习奖励工程</div>
<div class="mono" style="margin-top:8px">强化学习为优化动态治疗方案提供了强大框架，但临床强化学习始终受限于奖励工程的核心瓶颈：如何在复杂稀疏的离线环境中定义能安全有效指导策略学习的奖励信号。现有方法多依赖人工启发式规则，难以泛化至不同病理场景。为此，我们提出一种基于大语言模型的自动化流程，用于离线奖励设计与验证。我们采用包含生存度、置信度与胜任度三个核心组件的势函数构建奖励函数，并引入量化指标在部署前严格评估筛选最优奖励结构。通过融合大语言模型驱动的领域知识，本框架实现了针对特定疾病的奖励函数自动化设计，同时显著提升了最终策略的性能表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of reward engineering in clinical offline reinforcement learning, where manually designed heuristics often fail to generalize across diseases. The authors propose an automated pipeline that uses Large Language Models to design and verify reward functions, formulating them as tri-drive potential functions based on survival, confidence, and competence components. Experimental results demonstrate that this LLM-integrated framework successfully automates reward design for specific pathologies and significantly improves the performance of learned treatment policies compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本文针对临床离线强化学习中奖励函数设计的挑战，即手动启发式方法难以在不同疾病间泛化的问题，提出了一种自动化流程。该方法利用大语言模型来自动设计和验证奖励函数，将其构建为基于生存、置信度和能力三个核心组件的三驱动势函数。实验结果表明，这一集成大语言模型的框架能够针对特定疾病自动化奖励设计，并显著提升了所学治疗策略的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Periodic Regularized Q-Learning</div>
<div class="meta-line">Authors: Hyukjun Yang, Han-Dong Lim, Donghwan Lee</div>
<div class="meta-line">First: 2026-02-03T09:28:06+00:00 · Latest: 2026-02-03T09:28:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03301v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03301v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In reinforcement learning (RL), Q-learning is a fundamental algorithm whose convergence is guaranteed in the tabular setting. However, this convergence guarantee does not hold under linear function approximation. To overcome this limitation, a significant line of research has introduced regularization techniques to ensure stable convergence under function approximation. In this work, we propose a new algorithm, periodic regularized Q-learning (PRQ). We first introduce regularization at the level of the projection operator and explicitly construct a regularized projected value iteration (RP-VI), subsequently extending it to a sample-based RL algorithm. By appropriately regularizing the projection operator, the resulting projected value iteration becomes a contraction. By extending this regularized projection into the stochastic setting, we establish the PRQ algorithm and provide a rigorous theoretical analysis that proves finite-time convergence guarantees for PRQ under linear function approximation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>周期性正则化Q学习</div>
<div class="mono" style="margin-top:8px">在强化学习中，Q学习是一种基础算法，其在表格设定下的收敛性已得到保证。然而，在线性函数逼近下该收敛保证不再成立。为克服这一局限，一个重要研究方向引入了正则化技术以确保函数逼近下的稳定收敛。本文提出一种新算法——周期性正则化Q学习（PRQ）。我们首先在投影算子层面引入正则化，显式构建正则化投影值迭代（RP-VI），随后将其扩展为基于样本的强化学习算法。通过对投影算子进行适当正则化，所得投影值迭代成为压缩映射。通过将正则化投影扩展至随机设定，我们建立了PRQ算法，并提供严格的理论分析，证明PRQ在线性函数逼近下具有有限时间收敛保证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the instability and lack of convergence guarantees for standard Q-learning when using linear function approximation, a common scenario in complex reinforcement learning tasks. The proposed method, Periodic Regularized Q-Learning (PRQ), introduces a novel regularization technique applied directly to the projection operator, first constructing a deterministic regularized projected value iteration (RP-VI) that is provably a contraction, and then extending it to a stochastic, sample-based algorithm. The main experimental results, derived from theoretical analysis, demonstrate that PRQ achieves finite-time convergence guarantees under linear function approximation, thereby providing a stable and theoretically sound alternative to previous approaches.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决标准Q学习在使用线性函数逼近时的不稳定性和缺乏收敛保证的问题，这在复杂强化学习任务中很常见。所提出的方法，即周期性正则化Q学习（PRQ），引入了一种直接应用于投影算子的新颖正则化技术，首先构建了一个确定性的正则化投影值迭代（RP-VI），该迭代被证明是一个压缩映射，然后将其扩展为随机的、基于样本的算法。主要的实验结果，基于理论分析，表明PRQ在线性函数逼近下实现了有限时间收敛保证，从而为先前方法提供了一个稳定且理论可靠的替代方案。</div>
</details>
</div>
<div class="card">
<div class="title">ECHO-2: A Large-Scale Distributed Rollout Framework for Cost-Efficient Reinforcement Learning</div>
<div class="meta-line">Authors: Jie Xiao, Meng Chen, Qingnan Ren, Song Jingwei, Jiaqi Huang, Yangshen Deng, Chris Tong, Wanyi Chen, Suli Wang, Ziqian Bi, Shuo Lu, Yiqun Duan, Xu Wang, Rymon Yu, Ween Yang, Lynn Ai, Eric Yang, Bill Shi</div>
<div class="meta-line">First: 2026-02-02T14:57:53+00:00 · Latest: 2026-02-03T08:46:33+00:00</div>
<div class="meta-line">Comments: 23 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02192v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02192v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is a critical stage in post-training large language models (LLMs), involving repeated interaction between rollout generation, reward evaluation, and centralized learning. Distributing rollout execution offers opportunities to leverage more cost-efficient inference resources, but introduces challenges in wide-area coordination and policy dissemination. We present ECHO-2, a distributed RL framework for post-training with remote inference workers and non-negligible dissemination latency. ECHO-2 combines centralized learning with distributed rollouts and treats bounded policy staleness as a user-controlled parameter, enabling rollout generation, dissemination, and training to overlap. We introduce an overlap-based capacity model that relates training time, dissemination latency, and rollout throughput, yielding a practical provisioning rule for sustaining learner utilization. To mitigate dissemination bottlenecks and lower cost, ECHO-2 employs peer-assisted pipelined broadcast and cost-aware activation of heterogeneous workers. Experiments on GRPO post-training of 4B and 8B models under real wide-area bandwidth regimes show that ECHO-2 significantly improves cost efficiency while preserving RL reward comparable to strong baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ECHO-2：面向成本高效强化学习的大规模分布式推演框架</div>
<div class="mono" style="margin-top:8px">强化学习（RL）是大语言模型（LLM）后训练的关键阶段，涉及推演生成、奖励评估与集中式学习的反复交互。分布式推演执行为利用更具成本效益的推理资源提供了可能，但引入了广域协调与策略传播的挑战。本文提出ECHO-2——一个面向后训练的分布式RL框架，支持远程推理工作节点且能处理不可忽略的传播延迟。ECHO-2将集中式学习与分布式推演相结合，将有界策略陈旧度作为用户可控参数，实现推演生成、传播与训练的重叠执行。我们提出基于重叠能力的容量模型，关联训练时间、传播延迟与推演吞吐量，形成维持学习器利用率的实用资源配置规则。为缓解传播瓶颈并降低成本，ECHO-2采用对等辅助流水线广播及异构工作节点的成本感知激活机制。在真实广域带宽环境下对4B与8B模型进行GRPO后训练的实验中，ECHO-2在保持与强基线相当RL奖励的同时，显著提升了成本效益。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for ECHO-2 is to reduce the cost of reinforcement learning (RL) for large language models by distributing rollout generation to remote, cost-efficient inference resources, while addressing the coordination and policy dissemination challenges this introduces. The method combines centralized learning with distributed rollouts, explicitly treating policy staleness as a tunable parameter to overlap rollout, dissemination, and training phases; it also employs an overlap-based capacity model for provisioning and techniques like peer-assisted pipelined broadcast to mitigate dissemination bottlenecks. Experimental results from GRPO post-training of 4B and 8B models under real wide-area bandwidth conditions demonstrate that ECHO-2 significantly improves cost efficiency while maintaining RL reward performance comparable to strong baselines.</div>
<div class="mono" style="margin-top:8px">ECHO-2的动机是通过将强化学习中的策略执行（rollout）分布到远程、成本效益高的推理资源上，以降低大语言模型后训练的成本，同时解决由此带来的广域协调和策略传播延迟的挑战。该方法将集中式学习与分布式策略执行相结合，将策略过时性作为可调参数，使策略生成、传播和训练过程能够重叠进行；并引入了基于重叠的容量模型进行资源调配，以及采用对等辅助的流水线广播等技术来缓解传播瓶颈。在真实广域带宽环境下对4B和8B模型进行GRPO后训练的实验结果表明，ECHO-2在保持与强基线相当的强化学习奖励性能的同时，显著提高了成本效率。</div>
</details>
</div>
<div class="card">
<div class="title">DCoPilot: Generative AI-Empowered Policy Adaptation for Dynamic Data Center Operations</div>
<div class="meta-line">Authors: Minghao Li, Ruihang Wang, Rui Tan, Yonggang Wen</div>
<div class="meta-line">First: 2026-02-02T14:18:52+00:00 · Latest: 2026-02-03T08:43:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02137v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02137v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern data centers (DCs) hosting artificial intelligence (AI)-dedicated devices operate at high power densities with rapidly varying workloads, making minute-level adaptation essential for safe and energy-efficient operation. However, manually designing piecewise deep reinforcement learning (DRL) agents cannot keep pace with frequent dynamics shifts and service-level agreement (SLA) changes of an evolving DC. This specification-to-policy lag causes a lack of timely, effective control policies, which may lead to service outages. To bridge the gap, we present DCoPilot, a hybrid framework for generative control policies in dynamic DC operation. DCoPilot synergizes two distinct generative paradigms, i.e., a large language model (LLM) that performs symbolic generation of structured reward forms, and a hypernetwork that conducts parametric generation of policy weights. DCoPilot operates through three coordinated phases: (i) simulation scale-up, which stress-tests reward candidates across diverse simulation-ready (SimReady) scenes; (ii) meta policy distillation, where a hypernetwork is trained to output policy weights conditioned on SLA and scene embeddings; and (iii) online adaptation, enabling zero-shot policy generation in response to updated specifications. Evaluated across five control task families spanning diverse DC components, DCoPilot achieves near-zero constraint violations and outperforms all baselines across specification variations. Ablation studies validate the effectiveness of LLM-based unified reward generation in enabling stable hypernetwork convergence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DCoPilot：面向动态数据中心运营的生成式AI赋能策略自适应</div>
<div class="mono" style="margin-top:8px">现代数据中心（DC）承载着人工智能（AI）专用设备，在高功率密度和快速变化的工作负载下运行，这使得分钟级的自适应成为确保安全与能效运行的关键。然而，手动设计分段深度强化学习（DRL）智能体难以跟上数据中心频繁的动态变化和服务级别协议（SLA）更新。这种从规范到策略的滞后导致缺乏及时有效的控制策略，可能引发服务中断。为弥补这一差距，我们提出了DCoPilot——一个面向动态数据中心运营的生成式控制策略混合框架。DCoPilot融合了两种不同的生成范式：一是通过大型语言模型（LLM）进行结构化奖励形式的符号生成，二是通过超网络进行策略权重的参数化生成。该框架通过三个协同阶段运行：（i）仿真扩展阶段，在多样化的仿真就绪（SimReady）场景中对候选奖励进行压力测试；（ii）元策略蒸馏阶段，训练超网络根据SLA和场景嵌入生成策略权重；（iii）在线自适应阶段，实现针对更新规范的零样本策略生成。在涵盖多种数据中心组件的五类控制任务评估中，DCoPilot实现了近乎零的约束违反，并在所有规范变体下超越基线方法。消融研究验证了基于LLM的统一奖励生成对确保超网络稳定收敛的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of adapting control policies in modern data centers, where high power densities and rapidly varying workloads necessitate minute-level adjustments for safety and energy efficiency, as manual design of deep reinforcement learning agents cannot keep pace with frequent dynamic shifts and service-level agreement changes. The proposed method, DCoPilot, is a hybrid framework that synergizes a large language model for symbolic generation of structured reward forms and a hypernetwork for parametric generation of policy weights, operating through three phases: simulation scale-up for stress-testing reward candidates, meta policy distillation to train the hypernetwork, and online adaptation for zero-shot policy generation. Experimental results across five control task families show that DCoPilot achieves near-zero constraint violations and outperforms all baselines under specification variations, with ablation studies validating the effectiveness of LLM-based reward generation in enabling stable hypernetwork convergence.</div>
<div class="mono" style="margin-top:8px">本文针对现代数据中心控制策略适应性的挑战，在高功率密度和快速变化的工作负载下，需要分钟级调整以确保安全和能效，而手动设计深度强化学习代理无法跟上频繁的动态变化和服务级别协议更新。所提出的方法DCoPilot是一个混合框架，结合了大型语言模型用于结构化奖励形式的符号生成和超网络用于策略权重的参数生成，通过三个阶段运行：模拟扩展以压力测试奖励候选方案，元策略蒸馏训练超网络，以及在线适应实现零样本策略生成。在五个控制任务系列上的实验结果表明，DCoPilot实现了接近零的约束违反，并在规范变化下优于所有基线方法，消融研究验证了基于大型语言模型的统一奖励生成在促进超网络稳定收敛方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Accordion-Thinking: Self-Regulated Step Summaries for Efficient and Readable LLM Reasoning</div>
<div class="meta-line">Authors: Zhicheng Yang, Zhijiang Guo, Yinya Huang, Yongxin Wang, Wenlei Shi, Yiwei Wang, Xiaodan Liang, Jing Tang</div>
<div class="meta-line">First: 2026-02-03T08:34:20+00:00 · Latest: 2026-02-03T08:34:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03249v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03249v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scaling test-time compute via long Chain-ofThought unlocks remarkable gains in reasoning capabilities, yet it faces practical limits due to the linear growth of KV cache and quadratic attention complexity. In this paper, we introduce Accordion-Thinking, an end-to-end framework where LLMs learn to self-regulate the granularity of the reasoning steps through dynamic summarization. This mechanism enables a Fold inference mode, where the model periodically summarizes its thought process and discards former thoughts to reduce dependency on historical tokens. We apply reinforcement learning to incentivize this capability further, uncovering a critical insight: the accuracy gap between the highly efficient Fold mode and the exhaustive Unfold mode progressively narrows and eventually vanishes over the course of training. This phenomenon demonstrates that the model learns to encode essential reasoning information into compact summaries, achieving effective compression of the reasoning context. Our Accordion-Thinker demonstrates that with learned self-compression, LLMs can tackle complex reasoning tasks with minimal dependency token overhead without compromising solution quality, and it achieves a 3x throughput while maintaining accuracy on a 48GB GPU memory configuration, while the structured step summaries provide a human-readable account of the reasoning process.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>手风琴思维：通过自调节步骤摘要实现高效可读的大语言模型推理</div>
<div class="mono" style="margin-top:8px">通过长链思维扩展测试时计算能显著提升推理能力，但受限于键值缓存的线性增长与注意力机制的二次复杂度。本文提出手风琴思维——一种端到端框架，使大语言模型通过动态摘要学习自调节推理步骤的粒度。该机制支持折叠推理模式：模型定期总结思维过程并丢弃先前思考，以减少对历史标记的依赖。我们应用强化学习进一步强化此能力，并发现关键现象：高效折叠模式与详尽展开模式间的准确率差距在训练过程中逐渐缩小直至消失。这表明模型学会将核心推理信息编码为紧凑摘要，实现推理上下文的有效压缩。实验表明，通过习得的自压缩能力，大语言模型能以最低的依赖标记开销处理复杂推理任务且不损失求解质量，在48GB GPU内存配置下实现3倍吞吐率的同时保持准确度，而结构化步骤摘要提供了人类可读的推理过程记录。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Accordion-Thinking, a framework motivated by the need to overcome the practical limitations of long Chain-of-Thought reasoning, such as linear KV cache growth and quadratic attention complexity. The method enables large language models to self-regulate reasoning step granularity through dynamic summarization, employing a Fold inference mode where the model periodically summarizes and discards previous thoughts to reduce historical token dependency, with reinforcement learning used to enhance this capability. Experimental results show that the accuracy gap between the efficient Fold mode and exhaustive Unfold mode narrows and vanishes during training, indicating effective compression of reasoning context, and the approach achieves a 3x throughput increase while maintaining accuracy on a 48GB GPU memory configuration, with structured step summaries providing human-readable reasoning accounts.</div>
<div class="mono" style="margin-top:8px">本文提出了Accordion-Thinking框架，其动机是解决长思维链推理中线性KV缓存增长和二次注意力复杂度等实际限制。该方法通过动态摘要使大语言模型能够自我调节推理步骤的粒度，采用折叠推理模式，模型定期总结并丢弃先前思考以减少对历史令牌的依赖，并利用强化学习进一步强化此能力。实验结果表明，高效折叠模式与详尽展开模式之间的准确率差距在训练过程中逐渐缩小并最终消失，证明了推理上下文的有效压缩，该方法在48GB GPU内存配置下实现了3倍吞吐量提升且保持准确率，同时结构化步骤摘要提供了人类可读的推理过程记录。</div>
</details>
</div>
<div class="card">
<div class="title">Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration</div>
<div class="meta-line">Authors: Zhicheng Yang, Zhijiang Guo, Yinya Huang, Yongxin Wang, Dongchun Xie, Hanhui Li, Yiwei Wang, Xiaodan Liang, Jing Tang</div>
<div class="meta-line">First: 2025-08-19T11:51:40+00:00 · Latest: 2026-02-03T08:25:52+00:00</div>
<div class="meta-line">Comments: 18 pages, 14 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.13755v5">Abs</a> · <a href="https://arxiv.org/pdf/2508.13755v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models, yet its full potential is hindered by two under-explored dimensions: Depth-the hardest problem a model can sample; Breadth-the number of instances consumed in a single iteration. We dissect the popular GRPO algorithm and reveal a systematic bias: the cumulative-advantage disproportionately weights samples with medium accuracy, while down-weighting the low-accuracy instances that are crucial for pushing reasoning boundaries. To rectify the depth neglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which re-weights hard problems through targeted multi-stage rollouts, thereby increasing the number of positive rollouts for hard problems. Empirically, naively enlarging rollout size only accelerates convergence and even hurts Pass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra inference cost at convergence. Just as we adaptively expanded the depth of exploration, we now ask whether aggressively scaling the breadth of training data can further amplify reasoning gains. To this end, we intensely scale batch size and replace PPO&#x27;s mini-batch iterations with full-batch updates over multiple epochs. Increasing breadth significantly enhances Pass@1 performance. Large-breadth training sustains high token-level entropy, indicating continued exploration and reduced gradient noise. We further present DARS-B, which augments DARS with large breadth, and demonstrate simultaneous gains in Pass@K and Pass@1. The results confirm that breadth and adaptive exploration across depth operate as orthogonal dimensions in RLVR, which are key to unleashing the reasoning power of RLVR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RLVR中的深度-广度协同：通过自适应探索释放大语言模型推理能力</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习已成为释放大语言模型推理能力的重要范式，但其潜力受限于两个未充分探索的维度：深度（模型能采样的最难问题）与广度（单次迭代消耗的实例数量）。我们剖析主流GRPO算法，揭示其存在系统性偏差：累积优势值过度加权中等准确率样本，却低估对突破推理边界至关重要的低准确率实例。为纠正深度忽视问题，我们提出难度自适应轨迹采样方法，通过定向多阶段轨迹对难题进行重加权，从而增加难题的正向轨迹数量。实验表明，单纯扩大轨迹规模仅加速收敛，甚至损害Pass@K指标。相比之下，DARS方法在不增加收敛时推理成本的前提下，持续提升Pass@K。在实现深度探索自适应扩展后，我们进一步探究激进扩展训练数据广度能否放大推理收益。为此，我们大幅扩展批次规模，将PPO的小批次迭代替换为多轮次的全批次更新。广度扩展显著提升Pass@1性能，大广度训练维持较高的词元级熵值，表明探索持续且梯度噪声降低。我们进一步提出DARS-B方法，将大广度机制融入DARS，实现在Pass@K与Pass@1指标上的同步提升。结果证实广度与深度自适应探索在RLVR中构成正交维度，是释放其推理能力的关键。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses limitations in Reinforcement Learning with Verifiable Reward (RLVR) for enhancing large language model reasoning, identifying a bias in algorithms like GRPO that neglect difficult problems crucial for depth. The authors propose Difficulty Adaptive Rollout Sampling (DARS) to re-weight hard problems via targeted rollouts, improving Pass@K without extra inference cost, and also scale training data breadth with large batch sizes and full-batch updates, which boosts Pass@1 by sustaining exploration. Experimental results show that combining these methods in DARS-B yields simultaneous gains in both Pass@K and Pass@1, confirming that adaptive depth exploration and scaled breadth are orthogonal and key to unlocking RLVR&#x27;s reasoning potential.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习与可验证奖励（RLVR）在提升大语言模型推理能力中的局限性，指出如GRPO等算法存在忽视困难问题（深度）的系统性偏差。作者提出了难度自适应采样（DARS）方法，通过定向多阶段采样重新加权难题，在不增加推理成本的情况下提高了Pass@K性能；同时，通过扩大批次大小和采用全批次更新来扩展训练数据广度（广度），从而提升Pass@1并维持探索性。实验结果表明，将这两种方法结合的DARS-B在Pass@K和Pass@1上均取得增益，证实了自适应深度探索与扩展广度是正交维度，对释放RLVR的推理能力至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">Variational Approach for Job Shop Scheduling</div>
<div class="meta-line">Authors: Seung Heon Oh, Jiwon Baek, Ki Young Cho, Hee Chang Yoon, Jong Hun Woo</div>
<div class="meta-line">First: 2026-01-30T23:55:18+00:00 · Latest: 2026-02-03T08:15:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00408v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.00408v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper proposes a novel Variational Graph-to-Scheduler (VG2S) framework for solving the Job Shop Scheduling Problem (JSSP), a critical task in manufacturing that directly impacts operational efficiency and resource utilization. Conventional Deep Reinforcement Learning (DRL) approaches often face challenges such as non-stationarity during training and limited generalization to unseen problem instances because they optimize representation learning and policy execution simultaneously. To address these issues, we introduce variational inference to the JSSP domain for the first time and derive a probabilistic objective based on the Evidence of Lower Bound (ELBO) with maximum entropy reinforcement learning. By mathematically decoupling representation learning from policy optimization, the VG2S framework enables the agent to learn robust structural representations of scheduling instances through a variational graph encoder. This approach significantly enhances training stability and robustness against hyperparameter variations. Extensive experiments demonstrate that the proposed method exhibits superior zero-shot generalization compared with state-of-the-art DRL baselines and traditional dispatching rules, particularly on large-scale and challenging benchmark instances such as DMU and SWV.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>车间调度问题的变分方法研究</div>
<div class="mono" style="margin-top:8px">本文针对直接影响制造运营效率与资源利用的车间调度问题，首次提出一种新颖的变分图到调度器框架。传统深度强化学习方法因同时优化表征学习与策略执行，常面临训练非平稳性及对未见问题泛化能力有限等挑战。为解决这些问题，我们首次将变分推断引入该领域，并结合最大熵强化学习推导出基于证据下界的概率目标。通过数学上解耦表征学习与策略优化，该框架使智能体能通过变分图编码器学习调度实例的鲁棒结构表征，显著提升了训练稳定性及对超参数变化的鲁棒性。大量实验表明，相较于先进深度强化学习基准与传统调度规则，所提方法在DMU、SWV等大规模复杂基准实例上展现出更优异的零样本泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of conventional Deep Reinforcement Learning (DRL) methods in Job Shop Scheduling, which suffer from training non-stationarity and poor generalization, this paper introduces a novel Variational Graph-to-Scheduler (VG2S) framework. The method applies variational inference to the scheduling domain for the first time, deriving a probabilistic objective based on the Evidence Lower Bound (ELBO) with maximum entropy reinforcement learning to mathematically decouple representation learning from policy optimization, thereby enabling robust structural representation learning via a variational graph encoder. Experimental results show that VG2S achieves superior zero-shot generalization compared to state-of-the-art DRL baselines and traditional dispatching rules, especially on large-scale benchmark instances like DMU and SWV, while also enhancing training stability and robustness to hyperparameter variations.</div>
<div class="mono" style="margin-top:8px">针对传统深度强化学习方法在作业车间调度问题中存在的训练非平稳性和泛化能力不足等挑战，本文提出了一种新颖的变分图到调度器框架。该方法首次将变分推断引入调度领域，基于证据下界和最大熵强化学习推导出概率目标，从而在数学上将表示学习与策略优化解耦，通过变分图编码器学习鲁棒的结构化表示。实验结果表明，与先进的深度强化学习基准和传统调度规则相比，该方法在零样本泛化能力上表现优异，特别是在DMU和SWV等大规模复杂基准实例上，同时显著提高了训练稳定性及对超参数变化的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">BayeSQP: Bayesian Optimization through Sequential Quadratic Programming</div>
<div class="meta-line">Authors: Paul Brunzema, Sebastian Trimpe</div>
<div class="meta-line">First: 2026-02-03T08:08:03+00:00 · Latest: 2026-02-03T08:08:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03232v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03232v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce BayeSQP, a novel algorithm for general black-box optimization that merges the structure of sequential quadratic programming with concepts from Bayesian optimization. BayeSQP employs second-order Gaussian process surrogates for both the objective and constraints to jointly model the function values, gradients, and Hessian from only zero-order information. At each iteration, a local subproblem is constructed using the GP posterior estimates and solved to obtain a search direction. Crucially, the formulation of the subproblem explicitly incorporates uncertainty in both the function and derivative estimates, resulting in a tractable second-order cone program for high probability improvements under model uncertainty. A subsequent one-dimensional line search via constrained Thompson sampling selects the next evaluation point. Empirical results show thatBayeSQP outperforms state-of-the-art methods in specific high-dimensional settings. Our algorithm offers a principled and flexible framework that bridges classical optimization techniques with modern approaches to black-box optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BayeSQP：基于序列二次规划的贝叶斯优化算法</div>
<div class="mono" style="margin-top:8px">本文提出BayeSQP，一种用于通用黑盒优化的新型算法，它将序列二次规划的结构与贝叶斯优化概念相结合。BayeSQP采用二阶高斯过程代理模型，仅利用零阶信息对目标函数和约束条件进行联合建模，涵盖函数值、梯度和海森矩阵。在每次迭代中，利用高斯过程后验估计构建局部子问题并求解以获得搜索方向。关键在于，该子问题的构建明确考虑了函数及导数估计中的不确定性，从而在模型不确定性下形成一个可求解的二阶锥规划问题，以实现高概率改进。随后通过约束汤普森采样进行一维线搜索以选择下一个评估点。实验结果表明，BayeSQP在特定高维场景中优于现有先进方法。该算法提供了一个原则性且灵活的框架，将经典优化技术与现代黑盒优化方法相融合。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces BayeSQP, a novel algorithm motivated by the need to enhance black-box optimization by merging classical sequential quadratic programming with Bayesian optimization. The method employs second-order Gaussian process surrogates to model objective and constraint functions, including their gradients and Hessians, using only zero-order information, and formulates a tractable second-order cone program subproblem that accounts for uncertainty in estimates to guide search directions via a constrained Thompson sampling line search. Experimental results demonstrate that BayeSQP outperforms state-of-the-art methods in specific high-dimensional settings, offering a principled framework that bridges classical and modern optimization approaches.</div>
<div class="mono" style="margin-top:8px">本文提出了BayeSQP算法，其动机是通过结合经典序列二次规划和贝叶斯优化来改进黑盒优化。该方法采用二阶高斯过程代理模型，仅利用零阶信息对目标函数和约束条件进行建模，包括梯度和Hessian矩阵，并构建了一个可处理的二阶锥规划子问题，该子问题考虑了估计中的不确定性，通过约束Thompson采样线搜索来指导搜索方向。实验结果表明，BayeSQP在特定高维设置中优于现有先进方法，提供了一个连接经典和现代优化技术的原则性灵活框架。</div>
</details>
</div>
<div class="card">
<div class="title">Q-Regularized Generative Auto-Bidding: From Suboptimal Trajectories to Optimal Policies</div>
<div class="meta-line">Authors: Mingming Zhang, Na Li, Zhuang Feiqing, Hongyang Zheng, Jiangbing Zhou, Wang Wuyin, Sheng-jie Sun, XiaoWei Chen, Junxiong Zhu, Lixin Zou, Chenliang Li</div>
<div class="meta-line">First: 2026-01-06T06:42:25+00:00 · Latest: 2026-02-03T08:03:42+00:00</div>
<div class="meta-line">Comments: Due to the company&#x27;s compliance requirements, we would like to wait until the paper is officially published before making it publicly available on arXiv</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02754v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.02754v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rapid development of e-commerce, auto-bidding has become a key asset in optimizing advertising performance under diverse advertiser environments. The current approaches focus on reinforcement learning (RL) and generative models. These efforts imitate offline historical behaviors by utilizing a complex structure with expensive hyperparameter tuning. The suboptimal trajectories further exacerbate the difficulty of policy learning.
  To address these challenges, we proposes QGA, a novel Q-value regularized Generative Auto-bidding method. In QGA, we propose to plug a Q-value regularization with double Q-learning strategy into the Decision Transformer backbone. This design enables joint optimization of policy imitation and action-value maximization, allowing the learned bidding policy to both leverage experience from the dataset and alleviate the adverse impact of the suboptimal trajectories. Furthermore, to safely explore the policy space beyond the data distribution, we propose a Q-value guided dual-exploration mechanism, in which the DT model is conditioned on multiple return-to-go targets and locally perturbed actions. This entire exploration process is dynamically guided by the aforementioned Q-value module, which provides principled evaluation for each candidate action. Experiments on public benchmarks and simulation environments demonstrate that QGA consistently achieves superior or highly competitive results compared to existing alternatives. Notably, in large-scale real-world A/B testing, QGA achieves a 3.27% increase in Ad GMV and a 2.49% improvement in Ad ROI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Q正则化生成式自动竞价：从次优轨迹到最优策略</div>
<div class="mono" style="margin-top:8px">随着电子商务的快速发展，自动竞价已成为优化多样化广告主环境下广告效果的关键技术。当前方法主要聚焦于强化学习与生成模型，这些方法通过复杂结构和昂贵的超参数调优来模仿离线历史行为，而次优轨迹进一步加剧了策略学习的难度。为应对这些挑战，我们提出QGA——一种新颖的Q值正则化生成式自动竞价方法。在QGA中，我们将基于双Q学习策略的Q值正则化模块嵌入决策Transformer主干网络。该设计实现了策略模仿与动作价值最大化的联合优化，使习得的竞价策略既能利用数据集经验，又能缓解次优轨迹的负面影响。此外，为安全探索数据分布之外的策略空间，我们提出Q值引导的双重探索机制：通过将DT模型与多组回报目标及局部扰动动作相结合，整个探索过程由前述Q值模块动态引导，为每个候选动作提供原则性评估。在公开基准与仿真环境中的实验表明，QGA相较现有方法始终取得更优或极具竞争力的结果。值得注意的是，在大型真实场景A/B测试中，QGA实现了广告交易总额3.27%的提升与广告投资回报率2.49%的改善。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenges of suboptimal historical trajectories and complex model tuning in existing auto-bidding methods, this paper proposes QGA, a Q-value regularized generative auto-bidding approach. The method integrates a Q-value regularization with double Q-learning into a Decision Transformer backbone, jointly optimizing policy imitation and action-value maximization to leverage dataset experience while mitigating suboptimal data effects, and employs a Q-guided dual-exploration mechanism for safe policy space exploration. Experimental results on benchmarks and simulations show QGA achieves superior or competitive performance, with large-scale real-world A/B testing demonstrating a 3.27% increase in Ad GMV and a 2.49% improvement in Ad ROI.</div>
<div class="mono" style="margin-top:8px">针对现有自动出价方法中历史轨迹次优和模型调优复杂的问题，本文提出了QGA，一种Q值正则化的生成式自动出价方法。该方法将Q值正则化与双重Q学习集成到决策变换器框架中，联合优化策略模仿和动作价值最大化，以利用数据集经验并减轻次优数据的影响，同时采用Q值引导的双重探索机制进行安全的策略空间探索。在基准测试和模拟环境中的实验结果表明，QGA取得了优越或极具竞争力的性能，大规模真实世界A/B测试显示广告商品交易总额提升了3.27%，广告投资回报率提高了2.49%。</div>
</details>
</div>
<div class="card">
<div class="title">GraphAllocBench: A Flexible Benchmark for Preference-Conditioned Multi-Objective Policy Learning</div>
<div class="meta-line">Authors: Zhiheng Jiang, Yunzhe Wang, Ryan Marr, Ellen Novoseller, Benjamin T. Files, Volkan Ustun</div>
<div class="meta-line">First: 2026-01-28T16:36:37+00:00 · Latest: 2026-02-03T07:26:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20753v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.20753v3">PDF</a> · <a href="https://github.com/jzh001/GraphAllocBench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Preference-Conditioned Policy Learning (PCPL) in Multi-Objective Reinforcement Learning (MORL) aims to approximate diverse Pareto-optimal solutions by conditioning policies on user-specified preferences over objectives. This enables a single model to flexibly adapt to arbitrary trade-offs at run-time by producing a policy on or near the Pareto front. However, existing benchmarks for PCPL are largely restricted to toy tasks and fixed environments, limiting their realism and scalability. To address this gap, we introduce GraphAllocBench, a flexible benchmark built on a novel graph-based resource allocation sandbox environment inspired by city management, which we call CityPlannerEnv. GraphAllocBench provides a rich suite of problems with diverse objective functions, varying preference conditions, and high-dimensional scalability. We also propose two new evaluation metrics -- Proportion of Non-Dominated Solutions (PNDS) and Ordering Score (OS) -- that directly capture preference consistency while complementing the widely used hypervolume metric. Through experiments with Multi-Layer Perceptrons (MLPs) and graph-aware models, we show that GraphAllocBench exposes the limitations of existing MORL approaches and paves the way for using graph-based methods such as Graph Neural Networks (GNNs) in complex, high-dimensional combinatorial allocation tasks. Beyond its predefined problem set, GraphAllocBench enables users to flexibly vary objectives, preferences, and allocation rules, establishing it as a versatile and extensible benchmark for advancing PCPL. Code: https://github.com/jzh001/GraphAllocBench</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GraphAllocBench：面向偏好条件多目标策略学习的灵活基准</div>
<div class="mono" style="margin-top:8px">多目标强化学习中的偏好条件策略学习旨在通过将策略与用户对目标的指定偏好条件相结合，逼近多样化的帕累托最优解。这使得单一模型能够在运行时通过生成位于或接近帕累托前沿的策略，灵活适应任意权衡。然而，现有PCPL基准主要局限于玩具任务和固定环境，限制了其实用性与可扩展性。为填补这一空白，我们提出了GraphAllocBench——一个基于新型图结构资源分配沙盒环境构建的灵活基准，该环境受城市管理启发，命名为CityPlannerEnv。GraphAllocBench提供包含多样化目标函数、可变偏好条件及高维可扩展性的丰富问题集。我们还提出了两个新评估指标——非支配解比例与排序分数，在补充广泛使用的超体积指标的同时，直接捕捉偏好一致性。通过多层感知机与图感知模型的实验，我们证明GraphAllocBench能揭示现有MORL方法的局限性，并为在复杂高维组合分配任务中应用图神经网络等图结构方法开辟道路。除预定义问题集外，GraphAllocBench允许用户灵活调整目标、偏好与分配规则，使其成为推进PCPL研究的通用可扩展基准。代码：https://github.com/jzh001/GraphAllocBench</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces GraphAllocBench, a new benchmark for Preference-Conditioned Policy Learning (PCPL) in Multi-Objective Reinforcement Learning (MORL), motivated by the limitations of existing benchmarks that are often simplistic and lack realism. The method centers on a flexible graph-based resource allocation environment called CityPlannerEnv, which simulates city management tasks with diverse objectives and scalable complexity, and proposes two new evaluation metrics—Proportion of Non-Dominated Solutions (PNDS) and Ordering Score (OS)—to better assess preference consistency alongside traditional hypervolume. Experimental results using MLPs and graph-aware models demonstrate that GraphAllocBench effectively reveals shortcomings in current MORL approaches and highlights the potential of graph-based methods like GNNs for high-dimensional combinatorial allocation problems, while offering extensibility for custom objectives and preferences.</div>
<div class="mono" style="margin-top:8px">本文提出了GraphAllocBench，这是一个用于多目标强化学习中偏好条件策略学习的新基准，其动机在于现有基准大多局限于简单任务且缺乏现实性。该方法基于一个名为CityPlannerEnv的灵活图资源分配环境，模拟城市管理任务，具有多样化的目标和可扩展的复杂性，并引入了两个新评估指标——非支配解比例和排序分数——以在传统超体积指标之外更好地衡量偏好一致性。实验使用多层感知机和图感知模型进行，结果表明GraphAllocBench能有效暴露当前多目标强化学习方法的不足，突显了图神经网络等图方法在高维组合分配问题中的潜力，同时支持用户灵活定制目标和偏好，具有可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">ForesightKV: Optimizing KV Cache Eviction for Reasoning Models by Learning Long-Term Contribution</div>
<div class="meta-line">Authors: Zican Dong, Peiyu Liu, Junyi Li, Zhipeng Chen, Han Peng, Shuo Wang, Wayne Xin Zhao</div>
<div class="meta-line">First: 2026-02-03T07:16:51+00:00 · Latest: 2026-02-03T07:16:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03203v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03203v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, large language models (LLMs) have shown remarkable reasoning abilities by producing long reasoning traces. However, as the sequence length grows, the key-value (KV) cache expands linearly, incurring significant memory and computation costs. Existing KV cache eviction methods mitigate this issue by discarding less important KV pairs, but often fail to capture complex KV dependencies, resulting in performance degradation. To better balance efficiency and performance, we introduce ForesightKV, a training-based KV cache eviction framework that learns to predict which KV pairs to evict during long-text generations. We first design the Golden Eviction algorithm, which identifies the optimal eviction KV pairs at each step using future attention scores. These traces and the scores at each step are then distilled via supervised training with a Pairwise Ranking Loss. Furthermore, we formulate cache eviction as a Markov Decision Process and apply the GRPO algorithm to mitigate the significant language modeling loss increase on low-entropy tokens. Experiments on AIME2024 and AIME2025 benchmarks of three reasoning models demonstrate that ForesightKV consistently outperforms prior methods under only half the cache budget, while benefiting synergistically from both supervised and reinforcement learning approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ForesightKV：通过学习长期贡献优化推理模型的KV缓存淘汰机制</div>
<div class="mono" style="margin-top:8px">近期，大语言模型通过生成长推理轨迹展现出卓越的推理能力。然而，随着序列长度增加，键值缓存呈线性扩张，导致显著的内存与计算开销。现有KV缓存淘汰方法通过丢弃次要KV对缓解此问题，但常因未能捕捉复杂KV依赖关系而导致性能下降。为更好平衡效率与性能，本文提出ForesightKV——一种基于训练的KV缓存淘汰框架，通过学习预测长文本生成过程中需淘汰的KV对。我们首先设计黄金淘汰算法，利用未来注意力分数逐步骤识别最优淘汰KV对；随后通过配对排序损失的监督训练，对这些轨迹及各步骤分数进行蒸馏。进一步将缓存淘汰建模为马尔可夫决策过程，并应用GRPO算法以缓解低熵词元上语言建模损失的大幅上升。在AIME2024与AIME2025基准测试中对三种推理模型的实验表明，ForesightKV仅需半数缓存预算即可持续超越现有方法，同时协同受益于监督学习与强化学习策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for ForesightKV stems from the significant memory and computational costs incurred by the linearly expanding key-value (KV) cache in large language models during long reasoning tasks, where existing eviction methods often fail to capture complex KV dependencies, leading to performance degradation. The method introduces a training-based KV cache eviction framework that first uses a Golden Eviction algorithm to identify optimal eviction pairs based on future attention scores, then distills these traces via supervised training with Pairwise Ranking Loss, and further formulates eviction as a Markov Decision Process optimized with the GRPO algorithm to mitigate language modeling loss on low-entropy tokens. Main experimental results on the AIME2024 and AIME2025 benchmarks with three reasoning models show that ForesightKV consistently outperforms prior methods using only half the cache budget, benefiting from both supervised and reinforcement learning approaches.</div>
<div class="mono" style="margin-top:8px">ForesightKV的提出动机在于大型语言模型进行长序列推理时，键值（KV）缓存线性增长导致显著的内存和计算成本，而现有的缓存淘汰方法常因未能捕捉复杂的KV依赖关系而导致性能下降。该方法引入了一种基于训练的KV缓存淘汰框架，首先通过Golden Eviction算法利用未来注意力分数识别最优淘汰对，然后通过带有Pairwise Ranking Loss的监督训练蒸馏这些轨迹，并进一步将缓存淘汰建模为马尔可夫决策过程，应用GRPO算法以减轻低熵词元上的语言建模损失。在AIME2024和AIME2025基准测试中对三种推理模型的主要实验结果表明，ForesightKV在仅使用一半缓存预算的情况下持续优于先前方法，并受益于监督学习和强化学习的协同作用。</div>
</details>
</div>
<div class="card">
<div class="title">From Scalar Rewards to Potential Trends: Shaping Potential Landscapes for Model-Based Reinforcement Learning</div>
<div class="meta-line">Authors: Yao-Hui Li, Zeyu Wang, Xin Li, Wei Pang, Yingfang Yuan, Zhengkun Chen, Boya Zhang, Riashat Islam, Alex Lamb, Yonggang Zhang</div>
<div class="meta-line">First: 2026-02-03T07:13:26+00:00 · Latest: 2026-02-03T07:13:26+00:00</div>
<div class="meta-line">Comments: 26 pages, 20 figures.Preprint. Work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03201v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03201v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Model-based reinforcement learning (MBRL) achieves high sample efficiency by simulating future trajectories with learned dynamics and reward models. However, its effectiveness is severely compromised in sparse reward settings. The core limitation lies in the standard paradigm of regressing ground-truth scalar rewards: in sparse environments, this yields a flat, gradient-free landscape that fails to provide directional guidance for planning. To address this challenge, we propose Shaping Landscapes with Optimistic Potential Estimates (SLOPE), a novel framework that shifts reward modeling from predicting scalars to constructing informative potential landscapes. SLOPE employs optimistic distributional regression to estimate high-confidence upper bounds, which amplifies rare success signals and ensures sufficient exploration gradients. Evaluations on 30+ tasks across 5 benchmarks demonstrate that SLOPE consistently outperforms leading baselines in fully sparse, semi-sparse, and dense rewards.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从标量奖励到势能趋势：构建基于模型强化学习的势能景观</div>
<div class="mono" style="margin-top:8px">基于模型的强化学习（MBRL）通过习得的动力学模型和奖励模型模拟未来轨迹，实现了高样本效率。然而，在稀疏奖励场景中其效能严重受限。核心局限在于回归真实标量奖励的标准范式：在稀疏环境中，这会产生平坦、无梯度的景观，无法为规划提供方向性指导。为解决这一挑战，我们提出基于乐观势能估计的景观塑造框架（SLOPE），该创新框架将奖励建模从预测标量转变为构建信息丰富的势能景观。SLOPE采用乐观分布回归来估计高置信度上界，从而放大稀疏的成功信号并确保充分的探索梯度。在5个基准测试的30余项任务评估表明，SLOPE在完全稀疏、半稀疏及稠密奖励场景中均持续优于主流基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of sparse rewards in model-based reinforcement learning (MBRL), where traditional scalar reward regression yields flat, uninformative landscapes that hinder planning. The proposed method, SLOPE, shifts from predicting scalar rewards to constructing potential landscapes via optimistic distributional regression, which estimates high-confidence upper bounds to amplify rare success signals and ensure explorable gradients. Experimental results across over 30 tasks in 5 benchmarks show that SLOPE consistently outperforms leading baselines in sparse, semi-sparse, and dense reward settings.</div>
<div class="mono" style="margin-top:8px">本文针对基于模型的强化学习（MBRL）中稀疏奖励的挑战，传统标量奖励回归会产生平坦、无信息的景观，从而阻碍规划。提出的方法SLOPE将奖励建模从预测标量转向构建潜在景观，通过乐观分布回归估计高置信度上界，以放大罕见成功信号并确保可探索的梯度。在5个基准测试的30多个任务上的实验结果表明，SLOPE在完全稀疏、半稀疏和密集奖励设置中均持续优于领先基线。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning with Promising Tokens for Large Language Models</div>
<div class="meta-line">Authors: Jing-Cheng Pang, Liang Lu, Xian Tang, Kun Jiang, Sijie Wu, Kai Zhang, Xubin Li</div>
<div class="meta-line">First: 2026-02-03T07:08:06+00:00 · Latest: 2026-02-03T07:08:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03195v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03195v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has emerged as a key paradigm for aligning and optimizing large language models (LLMs). Standard approaches treat the LLM as the policy and apply RL directly over the full vocabulary space. However, this formulation includes the massive tail of contextually irrelevant tokens in the action space, which could distract the policy from focusing on decision-making among the truly reasonable tokens. In this work, we verify that valid reasoning paths could inherently concentrate within a low-rank subspace. Based on this insight, we introduce Reinforcement Learning with Promising Tokens (RLPT), a framework that mitigates the action space issue by decoupling strategic decision-making from token generation. Specifically, RLPT leverages the semantic priors of the base model to identify a dynamic set of \emph{promising tokens} and constrains policy optimization exclusively to this refined subset via masking. Theoretical analysis and empirical results demonstrate that RLPT effectively reduces gradient variance, stabilizes the training process, and improves sample efficiency. Experiment results on math, coding, and telecom reasoning show that RLPT outperforms standard RL baselines and integrates effectively across various model sizes (4B and 8B) and RL algorithms (GRPO and DAPO).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于潜力词元强化学习的大语言模型优化方法</div>
<div class="mono" style="margin-top:8px">强化学习已成为对齐和优化大语言模型的关键范式。传统方法将大语言模型视为策略网络，并在完整词表空间上直接应用强化学习。然而，这种框架将大量上下文无关的词元纳入动作空间，可能分散策略对合理词元间决策的注意力。本研究验证了有效推理路径本质上可集中于低秩子空间。基于此发现，我们提出潜力词元强化学习框架，通过将策略决策与词元生成解耦来缓解动作空间问题。具体而言，该框架利用基础模型的语义先验识别动态的潜力词元集合，并通过掩码机制将策略优化严格约束至该精炼子集。理论分析与实验结果表明，该框架能有效降低梯度方差、稳定训练过程并提升样本效率。在数学、编程和电信推理任务上的实验显示，该框架优于传统强化学习基线，并能有效适配不同模型规模与强化学习算法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the observation that standard reinforcement learning (RL) approaches for large language models (LLMs) treat the entire vocabulary as the action space, which includes many irrelevant tokens that can distract the policy. To address this, the method, Reinforcement Learning with Promising Tokens (RLPT), decouples strategic decision-making from token generation by using the base model&#x27;s semantic priors to identify a dynamic subset of promising tokens and constraining policy optimization to this subset via masking. Experimental results on math, coding, and telecom reasoning tasks demonstrate that RLPT reduces gradient variance, stabilizes training, improves sample efficiency, and outperforms standard RL baselines across various model sizes and RL algorithms.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于观察到，针对大语言模型的标准强化学习方法将整个词汇表作为动作空间，其中包含许多无关标记，可能干扰策略决策。为解决此问题，所提出的方法——基于前景标记的强化学习（RLPT）——将策略决策与标记生成解耦，利用基础模型的语义先验识别动态的前景标记子集，并通过掩码将策略优化限制在该子集内。在数学、代码和电信推理任务上的实验结果表明，RLPT能有效降低梯度方差、稳定训练过程、提升样本效率，并在不同模型规模和强化学习算法上优于标准基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Prompt Augmentation Scales up GRPO Training on Mathematical Reasoning</div>
<div class="meta-line">Authors: Wenquan Lu, Hai Huang, Randall Balestriero</div>
<div class="meta-line">First: 2026-02-03T06:59:42+00:00 · Latest: 2026-02-03T06:59:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03190v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03190v1">PDF</a> · <a href="https://github.com/wenquanlu/prompt-augmentation-GRPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning algorithms such as group-relative policy optimization (GRPO) have demonstrated strong potential for improving the mathematical reasoning capabilities of large language models. However, prior work has consistently observed an entropy collapse phenomenon during reinforcement post-training, characterized by a monotonic decrease in policy entropy that ultimately leads to training instability and collapse. As a result, most existing approaches restrict training to short horizons (typically 5-20 epochs), limiting sustained exploration and hindering further policy improvement. In addition, nearly all prior work relies on a single, fixed reasoning prompt or template during training. In this work, we introduce prompt augmentation, a training strategy that instructs the model to generate reasoning traces under diverse templates and formats, thereby increasing rollout diversity. We show that, without a KL regularization term, prompt augmentation enables stable scaling of training duration under a fixed dataset and allows the model to tolerate low-entropy regimes without premature collapse. Empirically, a Qwen2.5-Math-1.5B model trained with prompt augmentation on the MATH Level 3-5 dataset achieves state-of-the-art performance, reaching 44.5 per-benchmark accuracy and 51.3 per-question accuracy on standard mathematical reasoning benchmarks, including AIME24, AMC, MATH500, Minerva, and OlympiadBench. The code and model checkpoints are available at https://github.com/wenquanlu/prompt-augmentation-GRPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>提示增强技术提升GRPO在数学推理任务中的训练规模</div>
<div class="mono" style="margin-top:8px">群体相对策略优化（GRPO）等强化学习算法已展现出提升大语言模型数学推理能力的巨大潜力。然而，先前研究普遍观察到强化后训练过程中出现的熵崩溃现象，其特征是策略熵单调递减，最终导致训练失稳与崩溃。因此，现有方法大多将训练限制在较短周期（通常为5-20轮），制约了持续探索并阻碍策略的进一步优化。此外，几乎所有先前研究在训练期间都依赖单一固定的推理提示模板。本研究提出提示增强训练策略，通过引导模型在不同模板与格式下生成推理轨迹，有效提升训练轨迹的多样性。实验表明，在未使用KL正则项的情况下，提示增强能够在固定数据集上实现训练时长的稳定扩展，并使模型能够适应低熵状态而避免过早崩溃。实证结果显示，采用提示增强策略在MATH Level 3-5数据集上训练的Qwen2.5-Math-1.5B模型取得了最先进的性能，在AIME24、AMC、MATH500、Minerva及OlympiadBench等标准数学推理基准测试中，分别达到44.5%的基准平均准确率和51.3%的题目平均准确率。代码与模型检查点已开源：https://github.com/wenquanlu/prompt-augmentation-GRPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the entropy collapse and limited training horizons observed in prior reinforcement learning approaches for improving mathematical reasoning in large language models, this paper introduces prompt augmentation as a method to increase rollout diversity by generating reasoning traces under varied templates and formats. The method enables stable scaling of training duration without KL regularization, allowing models to tolerate low-entropy regimes without collapse. Experimental results show that a Qwen2.5-Math-1.5B model trained with this strategy on the MATH Level 3-5 dataset achieves state-of-the-art performance, with 44.5 per-benchmark and 51.3 per-question accuracy on standard mathematical reasoning benchmarks.</div>
<div class="mono" style="margin-top:8px">针对强化学习在提升大语言模型数学推理能力时出现的熵崩溃和训练周期受限问题，本文提出了提示增强方法，通过让模型在多样化模板和格式下生成推理轨迹来增加训练数据的多样性。该方法无需KL正则项即可稳定扩展训练时长，使模型能够适应低熵状态而避免过早崩溃。实验结果表明，基于MATH Level 3-5数据集、采用提示增强训练的Qwen2.5-Math-1.5B模型取得了最先进的性能，在AIME24、AMC等标准数学推理基准上分别达到44.5的基准平均准确率和51.3的题目平均准确率。</div>
</details>
</div>
<div class="card">
<div class="title">StepScorer: Accelerating Reinforcement Learning with Step-wise Scoring and Psychological Regret Modeling</div>
<div class="meta-line">Authors: Zhe Xu</div>
<div class="meta-line">First: 2026-02-03T06:39:20+00:00 · Latest: 2026-02-03T06:39:20+00:00</div>
<div class="meta-line">Comments: 10 pages, 5 figures, 1 table</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03171v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03171v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning algorithms often suffer from slow convergence due to sparse reward signals, particularly in complex environments where feedback is delayed or infrequent. This paper introduces the Psychological Regret Model (PRM), a novel approach that accelerates learning by incorporating regret-based feedback signals after each decision step. Rather than waiting for terminal rewards, PRM computes a regret signal based on the difference between the expected value of the optimal action and the value of the action taken in each state. This transforms sparse rewards into dense feedback signals through a step-wise scoring framework, enabling faster convergence. We demonstrate that PRM achieves stable performance approximately 36\% faster than traditional Proximal Policy Optimization (PPO) in benchmark environments such as Lunar Lander. Our results indicate that PRM is particularly effective in continuous control tasks and environments with delayed feedback, making it suitable for real-world applications such as robotics, finance, and adaptive education where rapid policy adaptation is critical. The approach formalizes human-inspired counterfactual thinking as a computable regret signal, bridging behavioral economics and reinforcement learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>StepScorer：通过逐步评分与心理遗憾模型加速强化学习</div>
<div class="mono" style="margin-top:8px">强化学习算法常因稀疏奖励信号而收敛缓慢，尤其在反馈延迟或稀少的复杂环境中。本文提出心理遗憾模型（PRM），通过在每个决策步骤后引入基于遗憾的反馈信号来加速学习。PRM不再等待最终奖励，而是根据最优行动的期望值与每个状态下实际采取行动的价值差异计算遗憾信号。该模型通过逐步评分框架将稀疏奖励转化为密集反馈信号，从而实现更快收敛。实验表明，在Lunar Lander等基准环境中，PRM比传统近端策略优化（PPO）的稳定性能提升速度约快36%。研究证明PRM在连续控制任务和延迟反馈环境中尤为有效，适用于机器人、金融和自适应教育等需要快速策略调整的现实应用。该方法将受人类启发的反事实思维形式化为可计算的遗憾信号，搭建了行为经济学与强化学习的桥梁。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the slow convergence of reinforcement learning due to sparse rewards, this paper proposes the Psychological Regret Model (PRM) to accelerate learning by generating dense, regret-based feedback after each step. The method computes a regret signal as the difference between the expected value of the optimal action and the taken action, transforming sparse rewards into step-wise scores. Experimental results show that PRM achieves stable performance about 36% faster than Proximal Policy Optimization in environments like Lunar Lander, proving especially effective in continuous control and delayed-feedback tasks for applications in robotics and finance.</div>
<div class="mono" style="margin-top:8px">针对强化学习因稀疏奖励导致收敛缓慢的问题，本文提出心理遗憾模型，通过在每个决策步骤后生成基于遗憾的密集反馈来加速学习。该方法计算最优行动与已采取行动的期望值之差作为遗憾信号，将稀疏奖励转化为逐步评分。实验结果表明，在月球着陆器等基准环境中，心理遗憾模型比近端策略优化稳定性能提升约36%，在连续控制和延迟反馈任务中尤为有效，适用于机器人、金融等需要快速策略适应的实际应用。</div>
</details>
</div>
<div class="card">
<div class="title">Intelligent Front-End Personalization: AI-Driven UI Adaptation</div>
<div class="meta-line">Authors: Mona Rajhans</div>
<div class="meta-line">First: 2026-02-03T06:10:10+00:00 · Latest: 2026-02-03T06:10:10+00:00</div>
<div class="meta-line">Comments: To be published in proceedings of IEEE ACDSA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03154v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03154v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Front-end personalization has traditionally relied on static designs or rule-based adaptations, which fail to fully capture user behavior patterns. This paper presents an AI driven approach for dynamic front-end personalization, where UI layouts, content, and features adapt in real-time based on predicted user behavior. We propose three strategies: dynamic layout adaptation using user path prediction, content prioritization through reinforcement learning, and a comparative analysis of AI-driven vs. rule-based personalization. Technical implementation details, algorithms, system architecture, and evaluation methods are provided to illustrate feasibility and performance gains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>智能前端个性化：AI驱动的界面自适应</div>
<div class="mono" style="margin-top:8px">前端个性化传统上依赖静态设计或基于规则的适配，难以全面捕捉用户行为模式。本文提出一种AI驱动的动态前端个性化方法，通过预测用户行为实时调整界面布局、内容与功能。我们提出三项策略：基于用户路径预测的动态布局适配、通过强化学习实现内容优先级排序，以及AI驱动与规则式个性化的对比分析。文中提供了技术实现细节、算法、系统架构及评估方法，以论证方案的可行性与性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of static and rule-based front-end personalization in capturing user behavior, this paper introduces an AI-driven approach for dynamic UI adaptation. The method employs three core strategies: dynamic layout adaptation via user path prediction, content prioritization using reinforcement learning, and a comparative analysis against rule-based systems. Experimental results demonstrate the feasibility and performance gains of the AI-driven approach, as detailed through technical implementations, algorithms, and system evaluations.</div>
<div class="mono" style="margin-top:8px">本文针对静态和基于规则的前端个性化方法难以充分捕捉用户行为模式的局限性，提出了一种人工智能驱动的动态前端个性化方法。该方法采用三种核心策略：通过用户路径预测实现动态布局适配，利用强化学习进行内容优先级排序，并与基于规则的系统进行对比分析。实验结果表明，该人工智能驱动的方法具有可行性并能带来性能提升，相关技术实现、算法和系统评估细节在文中均有阐述。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Hinting Language Models Enhance Reinforcement Learning</div>
<div class="meta-line">Authors: Baohao Liao, Hanze Dong, Xinxing Xu, Christof Monz, Jiang Bian</div>
<div class="meta-line">First: 2026-02-03T05:56:20+00:00 · Latest: 2026-02-03T05:56:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03143v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03143v1">PDF</a> · <a href="https://github.com/BaohaoLiao/SAGE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Group Relative Policy Optimization (GRPO) has recently emerged as a practical recipe for aligning large language models with verifiable objectives. However, under sparse terminal rewards, GRPO often stalls because rollouts within a group frequently receive identical rewards, causing relative advantages to collapse and updates to vanish. We propose self-hint aligned GRPO with privileged supervision (SAGE), an on-policy reinforcement learning framework that injects privileged hints during training to reshape the rollout distribution under the same terminal verifier reward. For each prompt $x$, the model samples a compact hint $h$ (e.g., a plan or decomposition) and then generates a solution $τ$ conditioned on $(x,h)$. Crucially, the task reward $R(x,τ)$ is unchanged; hints only increase within-group outcome diversity under finite sampling, preventing GRPO advantages from collapsing under sparse rewards. At test time, we set $h=\varnothing$ and deploy the no-hint policy without any privileged information. Moreover, sampling diverse self-hints serves as an adaptive curriculum that tracks the learner&#x27;s bottlenecks more effectively than fixed hints from an initial policy or a stronger external model. Experiments over 6 benchmarks with 3 LLMs show that SAGE consistently outperforms GRPO, on average +2.0 on Llama-3.2-3B-Instruct, +1.2 on Qwen2.5-7B-Instruct and +1.3 on Qwen3-4B-Instruct. The code is available at https://github.com/BaohaoLiao/SAGE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自提示语言模型增强强化学习</div>
<div class="mono" style="margin-top:8px">组相对策略优化（GRPO）近期已成为将大语言模型与可验证目标对齐的实用方法。然而，在稀疏终端奖励下，GRPO常因组内轨迹常获相同奖励而停滞，导致相对优势坍缩、更新失效。我们提出带特权监督的自提示对齐GRPO（SAGE），这是一种在策略强化学习框架，通过在训练中注入特权提示来重塑相同终端验证器奖励下的轨迹分布。对于每个提示$x$，模型采样一个紧凑提示$h$（如计划或分解），随后基于$(x,h)$生成解$τ$。关键的是，任务奖励$R(x,τ)$保持不变；提示仅通过有限采样增加组内结果多样性，防止GRPO优势在稀疏奖励下坍缩。测试时设$h=\varnothing$并部署无提示策略，无需任何特权信息。此外，采样多样自提示可作为自适应课程，比初始策略或更强外部模型的固定提示更有效追踪学习瓶颈。在3种大语言模型、6个基准测试上的实验表明，SAGE始终优于GRPO：Llama-3.2-3B-Instruct平均提升+2.0，Qwen2.5-7B-Instruct提升+1.2，Qwen3-4B-Instruct提升+1.3。代码发布于https://github.com/BaohaoLiao/SAGE。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the issue in Group Relative Policy Optimization (GRPO) where sparse terminal rewards cause training to stall due to identical rewards within groups, leading to collapsed advantages and ineffective updates. The proposed method, Self-hint Aligned GRPO with Privileged Supervision (SAGE), introduces an on-policy reinforcement learning framework that injects compact, self-generated hints (e.g., plans or decompositions) during training to diversify rollout outcomes under the same terminal reward, thereby preventing advantage collapse while leaving the reward function unchanged; at test time, hints are removed to deploy a no-hint policy without privileged information. Main experimental results across six benchmarks with three large language models demonstrate that SAGE consistently outperforms standard GRPO, achieving average improvements of +2.0 on Llama-3.2-3B-Instruct, +1.2 on Qwen2.5-7B-Instruct, and +1.3 on Qwen3-4B-Instruct.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决组相对策略优化（GRPO）在稀疏终端奖励下训练停滞的问题，即组内奖励相同导致相对优势崩溃和更新失效。提出的方法称为带特权监督的自提示对齐GRPO（SAGE），这是一个在策略强化学习框架，通过在训练中注入紧凑的自生成提示（如计划或分解）来在相同终端奖励下多样化结果分布，从而防止优势崩溃，同时保持奖励函数不变；在测试时移除提示以部署无提示策略，无需特权信息。在六个基准测试和三种大语言模型上的主要实验结果表明，SAGE始终优于标准GRPO，在Llama-3.2-3B-Instruct上平均提升+2.0，在Qwen2.5-7B-Instruct上平均提升+1.2，在Qwen3-4B-Instruct上平均提升+1.3。</div>
</details>
</div>
<div class="card">
<div class="title">Contrastive Concept-Tree Search for LLM-Assisted Algorithm Discovery</div>
<div class="meta-line">Authors: Timothee Leleu, Sudeera Gunathilaka, Federico Ghimenti, Surya Ganguli</div>
<div class="meta-line">First: 2026-02-03T05:41:35+00:00 · Latest: 2026-02-03T05:41:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03132v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03132v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language Model (LLM)-assisted algorithm discovery is an iterative, black-box optimization process over programs to approximatively solve a target task, where an LLM proposes candidate programs and an external evaluator provides task feedback. Despite intense recent research on the topic and promising results, how can the LLM internal representation of the space of possible programs be maximally exploited to improve performance is an open question. Here, we introduce Contrastive Concept-Tree Search (CCTS), which extracts a hierarchical concept representation from the generated programs and learns a contrastive concept model that guides parent selection. By reweighting parents using a likelihood-ratio score between high- and low-performing solutions, CCTS biases search toward useful concept combinations and away from misleading ones, providing guidance through an explicit concept hierarchy rather than the algorithm lineage constructed by the LLM. We show that CCTS improves search efficiency over fitness-based baselines and produces interpretable, task-specific concept trees across a benchmark of open Erdős-type combinatorics problems. Our analysis indicates that the gains are driven largely by learning which concepts to avoid. We further validate these findings in a controlled synthetic algorithm-discovery environment, which reproduces qualitatively the search dynamics observed with the LLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对比概念树搜索：用于大语言模型辅助的算法发现</div>
<div class="mono" style="margin-top:8px">大语言模型（LLM）辅助的算法发现是一种基于程序迭代的黑盒优化过程，旨在近似解决目标任务，其中LLM提出候选程序，外部评估器提供任务反馈。尽管近期该领域研究活跃且成果显著，但如何最大限度利用LLM对可能程序空间的内在表征以提升性能仍是一个开放性问题。本文提出对比概念树搜索（CCTS），该方法从生成的程序中提取层次化概念表征，并学习指导父节点选择的对比概念模型。通过对高/低性能解决方案使用似然比评分重加权父节点，CCTS将搜索偏向有益概念组合并远离误导性组合，通过显式概念层次而非LLM构建的算法谱系提供指导。实验表明，在开放式埃尔德什型组合问题基准测试中，CCTS相比基于适应度的基线方法提升了搜索效率，并生成可解释的、任务特定的概念树。分析表明性能提升主要源于学习规避无效概念。我们进一步在受控合成算法发现环境中验证了这些发现，该环境定性复现了LLM观测到的搜索动态。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to better exploit the internal program representations of large language models (LLMs) during algorithm discovery, this paper introduces Contrastive Concept-Tree Search (CCTS), a method that extracts a hierarchical concept tree from generated programs and learns a contrastive model to guide parent selection by reweighting based on a likelihood-ratio score between high- and low-performing solutions. The main experimental results demonstrate that CCTS improves search efficiency over fitness-based baselines on a benchmark of Erdős-type combinatorics problems, producing interpretable concept trees, with analysis indicating gains are largely driven by learning which concepts to avoid; these findings are further validated in a controlled synthetic environment.</div>
<div class="mono" style="margin-top:8px">本研究旨在改进大型语言模型（LLM）辅助算法发现过程中对模型内部程序表示的利用，提出了对比概念树搜索（CCTS）方法，该方法从生成的程序中提取层次化概念树，并学习一个对比模型，通过基于高低性能解决方案的似然比得分重新加权来指导父节点选择。主要实验结果表明，在一系列埃尔德什型组合问题基准测试中，CCTS相比基于适应度的基线方法提高了搜索效率，生成了可解释的概念树，分析表明其增益主要来自学习应避免的概念；这些发现在受控的合成算法发现环境中得到了进一步验证。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Cropping and Rotation: Automated Evolution of Powerful Task-Specific Augmentations with Generative Models</div>
<div class="meta-line">Authors: Judah Goldfeder, Shreyes Kaliyur, Vaibhav Sourirajan, Patrick Minwan Puma, Philippe Martin Wyder, Yuhang Hu, Jiong Lin, Hod Lipson</div>
<div class="meta-line">First: 2026-02-03T05:29:26+00:00 · Latest: 2026-02-03T05:29:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03123v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03123v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Data augmentation has long been a cornerstone for reducing overfitting in vision models, with methods like AutoAugment automating the design of task-specific augmentations. Recent advances in generative models, such as conditional diffusion and few-shot NeRFs, offer a new paradigm for data augmentation by synthesizing data with significantly greater diversity and realism. However, unlike traditional augmentations like cropping or rotation, these methods introduce substantial changes that enhance robustness but also risk degrading performance if the augmentations are poorly matched to the task. In this work, we present EvoAug, an automated augmentation learning pipeline, which leverages these generative models alongside an efficient evolutionary algorithm to learn optimal task-specific augmentations. Our pipeline introduces a novel approach to image augmentation that learns stochastic augmentation trees that hierarchically compose augmentations, enabling more structured and adaptive transformations. We demonstrate strong performance across fine-grained classification and few-shot learning tasks. Notably, our pipeline discovers augmentations that align with domain knowledge, even in low-data settings. These results highlight the potential of learned generative augmentations, unlocking new possibilities for robust model training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越裁剪与旋转：基于生成模型的任务特定增强策略自动化演进</div>
<div class="mono" style="margin-top:8px">数据增强一直是缓解视觉模型过拟合的关键技术，如AutoAugment等方法实现了任务特定增强策略的自动化设计。生成模型的最新进展（如条件扩散模型和少样本神经辐射场）通过合成多样性更强、真实性更高的数据，为数据增强提供了新范式。然而，与裁剪或旋转等传统增强方法不同，这些方法引入的显著变化虽能提升模型鲁棒性，但若增强策略与任务匹配不当，也可能导致性能下降。本研究提出EvoAug——一种自动化增强学习框架，该框架结合生成模型与高效进化算法，以学习最优的任务特定增强策略。我们提出了一种新颖的图像增强方法，通过学习可分层组合增强操作的随机增强树，实现更具结构化和自适应性的数据变换。实验表明，该方法在细粒度分类和少样本学习任务中均表现优异。值得注意的是，即使在低数据场景下，该框架仍能发现符合领域知识的增强策略。这些成果凸显了学习型生成增强的潜力，为鲁棒模型训练开辟了新路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to move beyond simple geometric augmentations and harness the diversity of modern generative models for more effective task-specific data augmentation, this paper introduces EvoAug, an automated pipeline that combines generative models like conditional diffusion with an evolutionary algorithm to learn optimal augmentation strategies. The method learns stochastic augmentation trees that hierarchically compose transformations, enabling structured and adaptive image modifications tailored to specific tasks. Experimental results on fine-grained classification and few-shot learning tasks demonstrate strong performance, with the pipeline discovering augmentations that align with domain knowledge even in low-data settings, highlighting the potential of learned generative augmentations for robust model training.</div>
<div class="mono" style="margin-top:8px">本文的动机是超越简单的几何增强方法，利用现代生成模型的多样性来实现更有效的任务特定数据增强，为此提出了EvoAug自动化流程，该流程结合条件扩散等生成模型与进化算法来学习最优增强策略。该方法通过学习随机增强树来分层组合变换，实现针对特定任务的结构化自适应图像修改。在细粒度分类和少样本学习任务上的实验结果表明了其强大性能，即使在低数据设置下也能发现符合领域知识的增强方式，凸显了学习生成式增强在提升模型鲁棒性训练方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Quantized Evolution Strategies: High-precision Fine-tuning of Quantized LLMs at Low-precision Cost</div>
<div class="meta-line">Authors: Yinggan Xu, Risto Miikkulainen, Xin Qiu</div>
<div class="meta-line">First: 2026-02-03T05:24:31+00:00 · Latest: 2026-02-03T05:24:31+00:00</div>
<div class="meta-line">Comments: Preprint version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03120v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03120v1">PDF</a> · <a href="https://github.com/dibbla/Quantized-Evolution-Strategies">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Post-Training Quantization (PTQ) is essential for deploying Large Language Models (LLMs) on memory-constrained devices, yet it renders models static and difficult to fine-tune. Standard fine-tuning paradigms, including Reinforcement Learning (RL), fundamentally rely on backpropagation and high-precision weights to compute gradients. Thus they cannot be used on quantized models, where the parameter space is discrete and non-differentiable. While Evolution Strategies (ES) offer a backpropagation-free alternative, optimization of the quantized parameters can still fail due to vanishing or inaccurate gradient. This paper introduces Quantized Evolution Strategies (QES), an optimization paradigm that performs full-parameter fine-tuning directly in the quantized space. QES is based on two innovations: (1) it integrates accumulated error feedback to preserve high-precision gradient signals, and (2) it utilizes a stateless seed replay to reduce memory usage to low-precision inference levels. QES significantly outperforms the state-of-the-art zeroth-order fine-tuning method on arithmetic reasoning tasks, making direct fine-tuning for quantized models possible. It therefore opens up the possibility for scaling up LLMs entirely in the quantized space. The source code is available at https://github.com/dibbla/Quantized-Evolution-Strategies .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>量化进化策略：以低精度成本实现量化大语言模型的高精度微调</div>
<div class="mono" style="margin-top:8px">后训练量化（PTQ）对于在内存受限设备上部署大语言模型（LLM）至关重要，但它会使模型变得静态且难以微调。包括强化学习（RL）在内的标准微调范式根本上依赖于反向传播和高精度权重来计算梯度，因此无法应用于参数空间离散且不可微的量化模型。尽管进化策略（ES）提供了一种无需反向传播的替代方案，但量化参数的优化仍可能因梯度消失或不准确而失败。本文提出量化进化策略（QES），这是一种直接在量化空间中进行全参数微调的优化范式。QES基于两项创新：（1）集成累积误差反馈以保留高精度梯度信号；（2）利用无状态种子重放将内存使用降至低精度推理水平。QES在算术推理任务上显著优于当前最先进的零阶微调方法，使量化模型的直接微调成为可能，从而为完全在量化空间中扩展LLM开辟了道路。源代码发布于 https://github.com/dibbla/Quantized-Evolution-Strategies。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of fine-tuning quantized large language models (LLMs), which are static and non-differentiable after post-training quantization, making standard gradient-based methods inapplicable. The authors propose Quantized Evolution Strategies (QES), a method that enables full-parameter fine-tuning directly in the quantized space by integrating accumulated error feedback to maintain gradient signal accuracy and using stateless seed replay to reduce memory overhead to low-precision inference levels. Experimental results on arithmetic reasoning tasks demonstrate that QES significantly outperforms existing zeroth-order fine-tuning methods, enabling effective optimization of quantized models and paving the way for scaling LLMs entirely in quantized form.</div>
<div class="mono" style="margin-top:8px">本文旨在解决量化后大语言模型（LLMs）难以微调的问题，因为后训练量化使模型参数空间离散且不可微，导致标准基于反向传播的微调方法失效。作者提出了量化进化策略（QES），该方法通过整合累积误差反馈以保持高精度梯度信号，并利用无状态种子重放将内存使用降至低精度推理水平，从而直接在量化空间中进行全参数微调。在算术推理任务上的实验结果表明，QES显著优于现有的零阶微调方法，使得量化模型的有效微调成为可能，为完全在量化空间中扩展LLMs开辟了道路。</div>
</details>
</div>
<div class="card">
<div class="title">Kimi K2: Open Agentic Intelligence</div>
<div class="meta-line">Authors: Kimi Team, Yifan Bai, Yiping Bao, Y. Charles, Cheng Chen, Guanduo Chen, Haiting Chen, Huarong Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, Hao Ding, Mengnan Dong, Angang Du, Chenzhuang Du, Dikang Du, Yulun Du, Yu Fan, Yichen Feng, Kelin Fu, Bofei Gao, Chenxiao Gao, Hongcheng Gao, Peizhong Gao, Tong Gao, Yuyao Ge, Shangyi Geng, Qizheng Gu, Xinran Gu, Longyu Guan, Haiqing Guo, Jianhang Guo, Xiaoru Hao, Tianhong He, Weiran He, Wenyang He, Yunjia He, Chao Hong, Hao Hu, Yangyang Hu, Zhenxing Hu, Weixiao Huang, Zhiqi Huang, Zihao Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yongsheng Kang, Guokun Lai, Cheng Li, Fang Li, Haoyang Li, Ming Li, Wentao Li, Yang Li, Yanhao Li, Yiwei Li, Zhaowei Li, Zheming Li, Hongzhan Lin, Xiaohan Lin, Zongyu Lin, Chengyin Liu, Chenyu Liu, Hongzhang Liu, Jingyuan Liu, Junqi Liu, Liang Liu, Shaowei Liu, T. Y. Liu, Tianwei Liu, Weizhou Liu, Yangyang Liu, Yibo Liu, Yiping Liu, Yue Liu, Zhengying Liu, Enzhe Lu, Haoyu Lu, Lijun Lu, Yashuo Luo, Shengling Ma, Xinyu Ma, Yingwei Ma, Shaoguang Mao, Jie Mei, Xin Men, Yibo Miao, Siyuan Pan, Yebo Peng, Ruoyu Qin, Zeyu Qin, Bowen Qu, Zeyu Shang, Lidong Shi, Shengyuan Shi, Feifan Song, Jianlin Su, Zhengyuan Su, Lin Sui, Xinjie Sun, Flood Sung, Yunpeng Tai, Heyi Tang, Jiawen Tao, Qifeng Teng, Chaoran Tian, Chensi Wang, Dinglu Wang, Feng Wang, Hailong Wang, Haiming Wang, Jianzhou Wang, Jiaxing Wang, Jinhong Wang, Shengjie Wang, Shuyi Wang, Si Wang, Xinyuan Wang, Yao Wang, Yejie Wang, Yiqin Wang, Yuxin Wang, Yuzhi Wang, Zhaoji Wang, Zhengtao Wang, Zhengtao Wang, Zhexu Wang, Chu Wei, Qianqian Wei, Haoning Wu, Wenhao Wu, Xingzhe Wu, Yuxin Wu, Chenjun Xiao, Jin Xie, Xiaotong Xie, Weimin Xiong, Boyu Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinran Xu, Yangchuan Xu, Ziyao Xu, Jing Xu, Jing Xu, Junjie Yan, Yuzi Yan, Hao Yang, Xiaofei Yang, Yi Yang, Ying Yang, Zhen Yang, Zhilin Yang, Zonghan Yang, Haotian Yao, Xingcheng Yao, Wenjie Ye, Zhuorui Ye, Bohong Yin, Longhui Yu, Enming Yuan, Hongbang Yuan, Mengjie Yuan, Siyu Yuan, Haobing Zhan, Dehao Zhang, Hao Zhang, Wanlu Zhang, Xiaobin Zhang, Yadong Zhang, Yangkun Zhang, Yichi Zhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Haotian Zhao, Yikai Zhao, Zijia Zhao, Huabin Zheng, Shaojie Zheng, Longguang Zhong, Jianren Zhou, Xinyu Zhou, Zaida Zhou, Jinguo Zhu, Zhen Zhu, Weiyu Zhuang, Xinxing Zu</div>
<div class="meta-line">First: 2025-07-28T05:35:43+00:00 · Latest: 2026-02-03T04:57:00+00:00</div>
<div class="meta-line">Comments: tech report of Kimi K2, with minor updates</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.20534v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.20534v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32 billion activated parameters and 1 trillion total parameters. We propose the MuonClip optimizer, which improves upon Muon with a novel QK-clip technique to address training instability while enjoying the advanced token efficiency of Muon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero loss spike. During post-training, K2 undergoes a multi-stage post-training process, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement learning (RL) stage, where the model improves its capabilities through interactions with real and synthetic environments.
  Kimi K2 achieves state-of-the-art performance among open-source non-thinking models, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on SWE-Bench Multilingual -- surpassing most open and closed-sourced baselines in non-thinking settings. It also exhibits strong capabilities in coding, mathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6, 49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without extended thinking. These results position Kimi K2 as one of the most capable open-source large language models to date, particularly in software engineering and agentic tasks. We release our base and post-trained model checkpoints to facilitate future research and applications of agentic intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Kimi K2：开放智能体智能</div>
<div class="mono" style="margin-top:8px">我们推出Kimi K2，这是一个专家混合（MoE）大语言模型，拥有320亿激活参数和1万亿总参数。我们提出了MuonClip优化器，它在Muon基础上通过新颖的QK-clip技术改进，以解决训练不稳定性问题，同时保留了Muon先进的令牌效率。基于MuonClip，K2在15.5万亿令牌上进行了预训练，且零损失峰值。在后训练阶段，K2经历了多阶段后训练过程，其亮点包括大规模智能体数据合成流程和联合强化学习（RL）阶段，模型通过真实与合成环境的交互提升能力。
Kimi K2在开源非思考模型中实现了最先进的性能，在智能体能力方面表现突出。具体而言，K2在Tau2-Bench上获得66.1分，在ACEBench（英文）上获得76.5分，在SWE-Bench Verified上获得65.8分，在SWE-Bench Multilingual上获得47.3分——在非思考设置中超越了大多数开源和闭源基线。它还在编码、数学和推理任务中展现出强大能力，在LiveCodeBench v6上获得53.7分，在AIME 2025上获得49.5分，在GPQA-Diamond上获得75.1分，在OJBench上获得27.1分，均无需扩展思考。这些成果使Kimi K2成为迄今为止能力最强的开源大语言模型之一，尤其在软件工程和智能体任务方面。我们发布了基础和后训练模型检查点，以促进智能体智能的未来研究和应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Kimi K2, a large language model designed to advance agentic intelligence, motivated by the need for powerful open-source models capable of complex tasks like software engineering. The method employs a Mixture-of-Experts architecture with 32 billion activated parameters, utilizing the novel MuonClip optimizer to ensure stable training without loss spikes, and includes a multi-stage post-training process with agentic data synthesis and joint reinforcement learning. Experimental results show state-of-the-art performance among open-source non-thinking models, with top scores on benchmarks such as Tau2-Bench (66.1), ACEBench (76.5), and SWE-Bench Verified (65.8), along with strong capabilities in coding, mathematics, and reasoning tasks, positioning K2 as a leading model for agentic applications.</div>
<div class="mono" style="margin-top:8px">本文介绍了Kimi K2大语言模型，旨在推动智能体智能的发展，其动机是开发能够处理软件工程等复杂任务的强大开源模型。方法上采用混合专家架构，激活参数达320亿，并利用新颖的MuonClip优化器确保训练稳定无损失峰值，同时通过多阶段后训练流程，包括智能体数据合成和联合强化学习。实验结果表明，该模型在开源非思考模型中达到领先水平，在Tau2-Bench（66.1分）、ACEBench（76.5分）和SWE-Bench Verified（65.8分）等基准测试中取得最高分，并在编码、数学和推理任务上表现出色，确立了K2在智能体应用中的领先地位。</div>
</details>
</div>
<div class="card">
<div class="title">Align to Structure: Aligning Large Language Models with Structural Information</div>
<div class="meta-line">Authors: Zae Myung Kim, Anand Ramachandran, Farideh Tavazoee, Joo-Kyung Kim, Oleg Rokhlenko, Dongyeop Kang</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-04-04T17:40:04+00:00 · Latest: 2026-02-03T04:47:45+00:00</div>
<div class="meta-line">Comments: Accepted to AAAI 2026 AIA</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.03622v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.03622v2">PDF</a> · <a href="https://github.com/minnesotanlp/struct_align">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generating long, coherent text remains a challenge for large language models (LLMs), as they lack hierarchical planning and structured organization in discourse generation. We introduce Structural Alignment, a novel method that aligns LLMs with human-like discourse structures to enhance long-form text generation. By integrating linguistically grounded discourse frameworks into reinforcement learning, our approach guides models to produce coherent and well-organized outputs. We employ a dense reward scheme within a Proximal Policy Optimization framework, assigning fine-grained, token-level rewards based on the discourse distinctiveness relative to human writing. Two complementary reward models are evaluated: the first improves readability by scoring surface-level textual features to provide explicit structuring, while the second reinforces deeper coherence and rhetorical sophistication by analyzing global discourse patterns through hierarchical discourse motifs, outperforming both standard and RLHF-enhanced models in tasks such as essay generation and long-document summarization. All training data and code will be publicly shared at https://github.com/minnesotanlp/struct_align.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对齐结构：基于结构信息对齐大语言模型</div>
<div class="mono" style="margin-top:8px">生成长篇连贯文本对大语言模型（LLMs）仍是挑战，因其在语篇生成中缺乏层次化规划与结构化组织。我们提出结构对齐方法，通过将LLMs与类人语篇结构对齐以增强长文本生成能力。该方法将语言学基础的语篇框架融入强化学习，引导模型生成连贯且组织良好的输出。我们在近端策略优化框架中采用密集奖励机制，根据相对于人类写作的语篇区分度分配细粒度的词元级奖励。评估了两种互补的奖励模型：第一种通过表层文本特征评分提升可读性，提供显式结构指导；第二种通过层次化语篇模式分析全局语篇结构，强化深层连贯性与修辞复杂度。该方法在议论文生成与长文档摘要等任务中均优于标准模型及RLHF增强模型。所有训练数据与代码将公开于https://github.com/minnesotanlp/struct_align。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of generating long, coherent text with large language models (LLMs), which often lack hierarchical planning and structured organization. The proposed method, Structural Alignment, aligns LLMs with human-like discourse structures by integrating linguistically grounded frameworks into reinforcement learning, specifically using a dense reward scheme within Proximal Policy Optimization to assign token-level rewards based on discourse distinctiveness. Experimental results show that the approach, evaluated with two complementary reward models for surface-level features and global discourse patterns, outperforms both standard and RLHF-enhanced models in tasks like essay generation and long-document summarization.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型在生成长篇连贯文本时缺乏层次规划和结构化组织的问题，提出了一种名为“结构对齐”的新方法。该方法通过将基于语言学的语篇框架融入强化学习，引导模型生成连贯且组织良好的输出，具体在近端策略优化框架中使用密集奖励方案，根据与人类写作的语篇差异性分配细粒度的词元级奖励。实验结果表明，该方法评估了两种互补的奖励模型，分别用于改进表层文本特征和强化深层连贯性与修辞复杂性，在文章生成和长文档摘要等任务中优于标准和经过RLHF增强的模型。</div>
</details>
</div>
<div class="card">
<div class="title">Training and Simulation of Quadrupedal Robot in Adaptive Stair Climbing for Indoor Firefighting: An End-to-End Reinforcement Learning Approach</div>
<div class="meta-line">Authors: Baixiao Huang, Baiyu Huang, Yu Hou</div>
<div class="meta-line">First: 2026-02-03T04:23:50+00:00 · Latest: 2026-02-03T04:23:50+00:00</div>
<div class="meta-line">Comments: 8 pages, 9 figures, 43rd International Symposium on Automation and Robotics in Construction</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03087v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03087v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quadruped robots are used for primary searches during the early stages of indoor fires. A typical primary search involves quickly and thoroughly looking for victims under hazardous conditions and monitoring flammable materials. However, situational awareness in complex indoor environments and rapid stair climbing across different staircases remain the main challenges for robot-assisted primary searches. In this project, we designed a two-stage end-to-end deep reinforcement learning (RL) approach to optimize both navigation and locomotion. In the first stage, the quadrupeds, Unitree Go2, were trained to climb stairs in Isaac Lab&#x27;s pyramid-stair terrain. In the second stage, the quadrupeds were trained to climb various realistic indoor staircases in the Isaac Lab engine, with the learned policy transferred from the previous stage. These indoor staircases are straight, L-shaped, and spiral, to support climbing tasks in complex environments. This project explores how to balance navigation and locomotion and how end-to-end RL methods can enable quadrupeds to adapt to different stair shapes. Our main contributions are: (1) A two-stage end-to-end RL framework that transfers stair-climbing skills from abstract pyramid terrain to realistic indoor stair topologies. (2) A centerline-based navigation formulation that enables unified learning of navigation and locomotion without hierarchical planning. (3) Demonstration of policy generalization across diverse staircases using only local height-map perception. (4) An empirical analysis of success, efficiency, and failure modes under increasing stair difficulty.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>室内消防四足机器人自适应爬梯训练与仿真：一种端到端强化学习方法</div>
<div class="mono" style="margin-top:8px">四足机器人被用于室内火灾初期的初步搜救任务。典型的初步搜救需要在危险环境下快速彻底地搜寻受困者并监测易燃材料。然而，复杂室内环境中的态势感知能力以及在不同楼梯间的快速爬升能力，仍是机器人辅助初步搜救面临的主要挑战。本项目设计了一种两阶段端到端深度强化学习方法，以同步优化导航与运动控制。第一阶段，在Isaac Lab的金字塔阶梯地形中训练Unitree Go2四足机器人掌握爬梯技能。第二阶段，在Isaac Lab引擎中将前一阶段习得的策略迁移至多种真实室内楼梯环境进行训练。这些室内楼梯包括直梯、L形梯和螺旋梯，以支持复杂环境下的爬升任务。本研究探索了如何平衡导航与运动控制，以及端到端强化学习方法如何使四足机器人适应不同楼梯形态。主要贡献包括：（1）提出两阶段端到端强化学习框架，实现从抽象金字塔地形到真实室内楼梯拓扑的爬梯技能迁移；（2）基于中心线的导航建模方法，无需分层规划即可实现导航与运动的统一学习；（3）仅通过局部高度图感知即实现跨多样楼梯的策略泛化验证；（4）在递增的楼梯难度下对成功率、效率及失效模式进行实证分析。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of enabling quadruped robots to perform rapid and adaptive stair climbing in complex indoor firefighting scenarios, where situational awareness and locomotion across varied staircases are critical. The authors propose a two-stage end-to-end deep reinforcement learning approach: first training a Unitree Go2 robot on abstract pyramid-stair terrain in simulation, then transferring the learned policy to realistic indoor staircases (straight, L-shaped, and spiral) within the Isaac Lab engine, using a centerline-based navigation formulation to jointly optimize navigation and locomotion. Experimental results demonstrate successful policy generalization across diverse stair topologies using only local height-map perception, with empirical analysis evaluating success rates, efficiency, and failure modes as stair difficulty increases.</div>
<div class="mono" style="margin-top:8px">本文针对室内消防场景中四足机器人快速自适应爬越复杂楼梯的挑战，其中环境感知与跨不同楼梯的运动能力是关键问题。作者提出了一种两阶段端到端深度强化学习方法：首先在仿真中的抽象金字塔楼梯地形上训练Unitree Go2机器人，然后将在Isaac Lab引擎中将学习到的策略迁移到真实的室内楼梯（直梯、L形梯和螺旋梯）上，并采用基于中心线的导航框架来统一优化导航与运动控制。实验结果表明，仅使用局部高度图感知，所提策略能在多样楼梯拓扑上成功泛化，并通过实证分析了随楼梯难度增加的成功率、效率与故障模式。</div>
</details>
</div>
<div class="card">
<div class="title">Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning</div>
<div class="meta-line">Authors: Jiayao Mai, Bangyan Liao, Zhenjun Zhao, Yingping Zeng, Haoang Li, Javier Civera, Tailin Wu, Yi Zhou, Peidong Liu</div>
<div class="meta-line">First: 2026-02-03T04:19:48+00:00 · Latest: 2026-02-03T04:19:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03086v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03086v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Homotopy paradigm, a general principle for solving challenging problems, appears across diverse domains such as robust optimization, global optimization, polynomial root-finding, and sampling. Practical solvers for these problems typically follow a predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and task-specific. To address this, we unify these problems under a single framework, which enables the design of a general neural solver. Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as a sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for a class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into a single neural framework.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>神经预测校正器：利用强化学习解决同伦问题</div>
<div class="mono" style="margin-top:8px">同伦范式作为解决复杂问题的通用原理，广泛存在于鲁棒优化、全局优化、多项式求根和采样等多个领域。针对这些问题的实际求解器通常采用预测校正结构，但依赖人工设计的步长和迭代终止启发式规则，这些规则往往次优且任务特定。为此，我们将这些问题统一到单一框架下，从而设计出通用神经求解器。基于这一统一视角，我们提出神经预测校正器，用自动学习策略替代人工启发式规则。NPC将策略选择建模为序列决策问题，并利用强化学习自动发现高效策略。为进一步提升泛化能力，我们引入摊销训练机制，支持对一类问题进行一次性离线训练，并在新实例上实现高效在线推理。在四个代表性同伦问题上的实验表明，该方法能有效泛化至未见实例，其效率持续超越经典及专用基线方法，同时在多任务中展现出更优的稳定性，这凸显了将同伦方法统一至神经框架的重要价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the suboptimal and task-specific nature of hand-crafted heuristics in predictor-corrector solvers for homotopy problems, which arise in areas like robust optimization and polynomial root-finding. To overcome this, the authors propose Neural Predictor-Corrector (NPC), a unified framework that formulates heuristic selection as a sequential decision-making problem and uses reinforcement learning to automatically learn efficient policies, enhanced by an amortized training mechanism for generalization across problem instances. Experimental results on four homotopy problems show that NPC generalizes well to unseen instances, outperforming classical baselines in both efficiency and stability, demonstrating the benefits of a unified neural approach.</div>
<div class="mono" style="margin-top:8px">本文针对同伦问题（如鲁棒优化、多项式求根）中预测-校正求解器依赖手工启发式规则导致的次优和任务特定性问题，提出了一种统一框架。方法上，作者设计了神经预测校正器（NPC），将启发式选择构建为序列决策问题，利用强化学习自动学习高效策略，并通过摊销训练机制提升泛化能力，实现一次离线训练即可适用于同类问题。在四个代表性同伦问题上的实验表明，NPC能有效泛化至未见实例，在效率和稳定性上均优于传统及专用基线，验证了统一神经框架的价值。</div>
</details>
</div>
<div class="card">
<div class="title">TMS: Trajectory-Mixed Supervision for Reward-Free, On-Policy SFT</div>
<div class="meta-line">Authors: Rana Muhammad Shahroz Khan, Zijie Liu, Zhen Tan, Charles Fleming, Tianlong Chen</div>
<div class="meta-line">First: 2026-02-03T04:01:26+00:00 · Latest: 2026-02-03T04:01:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03073v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03073v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT) are the two dominant paradigms for enhancing Large Language Model (LLM) performance on downstream tasks. While RL generally preserves broader model capabilities (retention) better than SFT, it comes with significant costs: complex reward engineering, instability, and expensive on-policy sampling. In contrast, SFT is efficient but brittle, often suffering from catastrophic forgetting due to $\textbf{Supervision Mismatch}$: the divergence between the model&#x27;s evolving policy and static training labels. We address this trade-off with $\textbf{Trajectory-Mixed Supervision (TMS)}$, a reward-free framework that approximates the on-policy benefits of RL by creating a dynamic curriculum from the model&#x27;s own historical checkpoints. TMS minimizes $\textit{Policy-Label Divergence (PLD)}$, preventing the mode collapse that drives forgetting in standard SFT. Experiments across reasoning (MATH, GSM8K) and instruction-following benchmarks demonstrate that TMS effectively shifts the accuracy--retention Pareto frontier. While RL remains the gold standard for retention, TMS significantly outperforms standard and iterative SFT, bridging the gap to RL without requiring reward models or verifiers. Mechanistic analysis confirms that PLD drift accurately predicts forgetting and that TMS successfully mitigates this drift.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TMS：面向无奖励、同策略SFT的轨迹混合监督方法</div>
<div class="mono" style="margin-top:8px">强化学习（RL）与监督微调（SFT）是提升大语言模型（LLM）下游任务性能的两大主流范式。RL通常比SFT更能保持模型的广泛能力（留存性），但其代价高昂：需要复杂的奖励工程、训练不稳定且需进行昂贵的同策略采样。相比之下，SFT效率高但脆弱，常因**监督失配**（即模型动态演进的策略与静态训练标签之间的偏差）导致灾难性遗忘。本文提出**轨迹混合监督（TMS）** 框架以平衡这一矛盾：该无奖励框架通过从模型自身历史检查点构建动态课程，近似实现RL的同策略优势。TMS最小化**策略-标签偏差（PLD）**，从而避免标准SFT中导致遗忘的模式崩溃。在推理（MATH、GSM8K）和指令跟随基准测试上的实验表明，TMS能有效优化准确率-留存性的帕累托边界。虽然RL仍是留存性的黄金标准，但TMS显著优于标准及迭代式SFT，在无需奖励模型或验证器的条件下缩小了与RL的差距。机理分析证实，PLD漂移可准确预测遗忘现象，而TMS能有效抑制这种漂移。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the trade-off between Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT) for enhancing Large Language Models, where RL offers better capability retention but is costly and complex, while SFT is efficient but prone to catastrophic forgetting due to Supervision Mismatch. The proposed method, Trajectory-Mixed Supervision (TMS), is a reward-free framework that approximates on-policy RL benefits by creating a dynamic training curriculum from the model&#x27;s own historical checkpoints, aiming to minimize Policy-Label Divergence (PLD) to prevent mode collapse. Experimental results on reasoning and instruction-following benchmarks show that TMS shifts the accuracy-retention Pareto frontier, significantly outperforming standard and iterative SFT and bridging the gap to RL without requiring reward models, with mechanistic analysis confirming that PLD drift predicts forgetting and that TMS mitigates this drift.</div>
<div class="mono" style="margin-top:8px">本文针对增强大型语言模型性能时强化学习（RL）与监督微调（SFT）之间的权衡问题展开研究，其中RL能更好地保持模型能力但成本高昂且复杂，而SFT虽高效却因监督不匹配容易导致灾难性遗忘。所提出的方法——轨迹混合监督（TMS），是一种无奖励框架，通过从模型自身历史检查点创建动态训练课程来近似RL的在线策略优势，旨在最小化策略-标签分歧（PLD）以防止模式崩溃。在推理和指令遵循基准测试中的实验结果表明，TMS有效移动了准确率-保持率的帕累托前沿，显著优于标准和迭代SFT，并在无需奖励模型的情况下缩小了与RL的差距，机理分析证实PLD漂移能预测遗忘且TMS成功缓解了这种漂移。</div>
</details>
</div>
<div class="card">
<div class="title">GPG: A Simple and Strong Reinforcement Learning Baseline for Model Reasoning</div>
<div class="meta-line">Authors: Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, Yong Wang</div>
<div class="meta-line">First: 2025-04-03T12:53:41+00:00 · Latest: 2026-02-03T03:51:37+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.02546v4">Abs</a> · <a href="https://arxiv.org/pdf/2504.02546v4">PDF</a> · <a href="https://github.com/AMAP-ML/GPG">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) can directly enhance the reasoning capabilities of large language models without extensive reliance on Supervised Fine-Tuning (SFT). In this work, we revisit the traditional Policy Gradient (PG) mechanism and propose a minimalist RL approach termed Group Policy Gradient (GPG). Unlike conventional methods, GPG directly optimize the original RL objective, thus obviating the need for surrogate loss functions. By eliminating the critic and reference models, avoiding KL divergence constraints, and addressing the advantage and gradient estimation bias, our approach significantly simplifies the training process compared to Group Relative Policy Optimization (GRPO). Our approach achieves superior performance without relying on auxiliary techniques or adjustments. As illustrated in Figure 1, extensive experiments demonstrate that our method not only reduces computational costs but also consistently outperforms GRPO across various unimodal and multimodal tasks. Our code is available at https://github.com/AMAP-ML/GPG.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GPG：一种简洁而强大的模型推理强化学习基线方法</div>
<div class="mono" style="margin-top:8px">强化学习（RL）能够直接增强大语言模型的推理能力，而无需过度依赖监督微调（SFT）。本研究重新审视传统策略梯度（PG）机制，提出一种极简的强化学习方法——分组策略梯度（GPG）。与传统方法不同，GPG直接优化原始强化学习目标，从而无需使用替代损失函数。通过消除评论家模型和参考模型、避免KL散度约束，并解决优势函数与梯度估计偏差问题，我们的方法相比分组相对策略优化（GRPO）显著简化了训练流程。该方法在不依赖辅助技术或调整的情况下实现了更优性能。如图1所示，大量实验表明我们的方法不仅降低了计算成本，还在多种单模态与多模态任务中持续超越GRPO。代码已开源：https://github.com/AMAP-ML/GPG。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to enhance large language model reasoning without heavy reliance on supervised fine-tuning, this paper revisits the traditional Policy Gradient mechanism and introduces Group Policy Gradient (GPG), a minimalist reinforcement learning approach. The method directly optimizes the original RL objective, eliminating the need for surrogate losses, critic and reference models, and KL divergence constraints, thereby simplifying training compared to methods like GRPO. Experimental results across various unimodal and multimodal tasks demonstrate that GPG not only reduces computational costs but also consistently outperforms GRPO, achieving superior performance without auxiliary techniques.</div>
<div class="mono" style="margin-top:8px">本文旨在不依赖大量监督微调的情况下增强大语言模型的推理能力，为此重新审视了传统的策略梯度机制，并提出了一种极简的强化学习方法——组策略梯度（GPG）。该方法直接优化原始强化学习目标，无需替代损失函数、评论家模型、参考模型或KL散度约束，从而简化了训练过程。实验结果表明，在多种单模态和多模态任务中，GPG不仅降低了计算成本，而且性能持续优于GRPO，无需辅助技术即可实现卓越表现。</div>
</details>
</div>
<div class="card">
<div class="title">CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs</div>
<div class="meta-line">Authors: Zhiyuan Yao, Yi-Kai Zhang, Yuxin Chen, Yueqing Sun, Zishan Xu, Yu Yang, Tianhao Hu, Qi Gu, Hui Su, Xunliang Cai</div>
<div class="meta-line">First: 2026-02-03T03:14:36+00:00 · Latest: 2026-02-03T03:14:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03048v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03048v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key approach for enhancing LLM reasoning.However, standard frameworks like Group Relative Policy Optimization (GRPO) typically employ a uniform rollout budget, leading to resource inefficiency. Moreover, existing adaptive methods often rely on instance-level metrics, such as task pass rates, failing to capture the model&#x27;s dynamic learning state. To address these limitations, we propose CoBA-RL, a reinforcement learning algorithm designed to adaptively allocate rollout budgets based on the model&#x27;s evolving capability. Specifically, CoBA-RL utilizes a Capability-Oriented Value function to map tasks to their potential training gains and employs a heap-based greedy strategy to efficiently self-calibrate the distribution of computational resources to samples with high training value. Extensive experiments demonstrate that our approach effectively orchestrates the trade-off between exploration and exploitation, delivering consistent generalization improvements across multiple challenging benchmarks. These findings underscore that quantifying sample training value and optimizing budget allocation are pivotal for advancing LLM post-training efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoBA-RL：面向大语言模型强化学习的能力导向预算分配方法</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习已成为提升大语言模型推理能力的关键方法。然而，诸如组相对策略优化等标准框架通常采用均匀的模拟预算分配，导致资源效率低下。现有自适应方法多依赖任务通过率等实例级指标，难以捕捉模型动态学习状态。为突破这些局限，我们提出CoBA-RL强化学习算法，该算法能根据模型演化能力自适应分配模拟预算。具体而言，CoBA-RL采用能力导向价值函数将任务映射至潜在训练收益，并运用基于堆的贪心策略，高效地将计算资源自校准分配至高训练价值样本。大量实验表明，该方法有效协调探索与利用的平衡，在多个挑战性基准测试中实现稳定的泛化性能提升。这些发现证明，量化样本训练价值与优化预算分配是提升大语言模型后训练效率的关键。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for CoBA-RL stems from the inefficiency of uniform rollout budgets in RL frameworks like GRPO for LLM reasoning and the failure of existing adaptive methods to capture dynamic learning states. The method introduces a Capability-Oriented Value function to estimate potential training gains per task and uses a heap-based greedy strategy to allocate computational resources adaptively to high-value samples. Experimental results show that CoBA-RL effectively balances exploration and exploitation, leading to consistent generalization improvements across multiple challenging benchmarks, highlighting the importance of quantifying sample training value for efficient LLM post-training.</div>
<div class="mono" style="margin-top:8px">CoBA-RL的提出动机在于，针对大语言模型推理的强化学习框架（如GRPO）中均匀分配计算预算的效率低下，以及现有自适应方法无法捕捉模型动态学习状态的问题。该方法通过能力导向的价值函数来映射任务潜在训练收益，并采用基于堆的贪心策略，将计算资源自适应分配给高训练价值的样本。实验结果表明，CoBA-RL有效协调了探索与利用的平衡，在多个具有挑战性的基准测试中实现了稳定的泛化性能提升，这凸显了量化样本训练价值对于提高大语言模型后训练效率的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">Bongards at the Boundary of Perception and Reasoning: Programs or Language?</div>
<div class="meta-line">Authors: Cassidy Langenfeld, Claas Beger, Gloria Geng, Wasu Top Piriyakulkij, Keya Hu, Yewen Pu, Kevin Ellis</div>
<div class="meta-line">First: 2026-02-03T03:04:27+00:00 · Latest: 2026-02-03T03:04:27+00:00</div>
<div class="meta-line">Comments: 6 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03038v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03038v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>邦加德问题：感知与推理边界的程序化还是语言化？</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）在日常视觉任务（如自然图像描述或基于图像的常识问答）中已取得显著进展。然而，人类具备在全新情境中灵活运用视觉推理的惊人能力，这一能力可通过经典的邦加德视觉推理问题集进行严格测试。本文提出一种神经符号方法来解决此类问题：针对邦加德问题的假设解规则，我们利用大语言模型生成规则参数化的程序表示，并通过贝叶斯优化进行参数拟合。该方法在已知真实规则下的邦加德问题图像分类任务及从零解题任务中均得到验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to test visual reasoning in novel scenarios beyond everyday tasks, this paper addresses the classic Bongard problems to evaluate whether Vision-Language Models (VLMs) can emulate human-like reasoning. The method employs a neurosymbolic approach that uses Large Language Models (LLMs) to generate parameterized programmatic rules for hypothesized solutions and fits parameters via Bayesian optimization. Experimental results show the approach effectively classifies Bongard images using ground truth rules and demonstrates capability in solving problems from scratch, highlighting progress at the intersection of perception and reasoning.</div>
<div class="mono" style="margin-top:8px">本文旨在探索视觉语言模型在超越日常任务的新颖场景中的视觉推理能力，以经典的Bongard问题作为测试基准。研究方法采用神经符号方法，利用大语言模型生成假设解决方案的参数化程序表示，并通过贝叶斯优化进行参数拟合。实验结果表明，该方法能有效利用真实规则对Bongard图像进行分类，并在从零开始解决问题时展现出潜力，推动了感知与推理边界的研究进展。</div>
</details>
</div>
<div class="card">
<div class="title">Structuring Value Representations via Geometric Coherence in Markov Decision Processes</div>
<div class="meta-line">Authors: Zuyuan Zhang, Zeyu Fang, Tian Lan</div>
<div class="meta-line">First: 2026-02-03T01:35:58+00:00 · Latest: 2026-02-03T01:35:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02978v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02978v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Geometric properties can be leveraged to stabilize and speed reinforcement learning. Existing examples include encoding symmetry structure, geometry-aware data augmentation, and enforcing structural restrictions. In this paper, we take a novel view of RL through the lens of order theory and recast value function estimates into learning a desired poset (partially ordered set). We propose \emph{GCR-RL} (Geometric Coherence Regularized Reinforcement Learning) that computes a sequence of super-poset refinements -- by refining posets in previous steps and learning additional order relationships from temporal difference signals -- thus ensuring geometric coherence across the sequence of posets underpinning the learned value functions. Two novel algorithms by Q-learning and by actor--critic are developed to efficiently realize these super-poset refinements. Their theoretical properties and convergence rates are analyzed. We empirically evaluate GCR-RL in a range of tasks and demonstrate significant improvements in sample efficiency and stable performance over strong baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>马尔可夫决策过程中基于几何一致性的价值表示结构化</div>
<div class="mono" style="margin-top:8px">几何特性可用于稳定并加速强化学习。现有案例包括编码对称结构、几何感知数据增强及施加结构约束。本文通过序理论视角提出强化学习新范式，将价值函数估计重构为学习目标偏序集。我们提出GCR-RL（几何一致性正则化强化学习），通过细化历史偏序集并从时序差分信号学习新增序关系，计算超偏序集细化序列，从而确保支撑学习价值函数的偏序集序列具有几何一致性。基于Q学习和行动者-评论家框架的两种新算法被开发以实现超偏序集细化，其理论特性与收敛速率得到分析。我们在多任务中实证评估GCR-RL，相较于强基线方法，其样本效率与稳定性均取得显著提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the potential of geometric properties to enhance reinforcement learning stability and efficiency, this paper introduces a novel perspective that recasts value function estimation as learning a partially ordered set (poset). The method, termed Geometric Coherence Regularized Reinforcement Learning (GCR-RL), refines posets sequentially by incorporating temporal difference signals to ensure geometric coherence across value functions, with implementations via Q-learning and actor-critic algorithms. Experimental results across various tasks demonstrate that GCR-RL significantly improves sample efficiency and delivers stable performance compared to strong baselines.</div>
<div class="mono" style="margin-top:8px">本文的动机是利用几何特性提升强化学习的稳定性和效率，提出了一种新视角，将价值函数估计重新定义为学习偏序集。所提出的方法称为几何相干正则化强化学习（GCR-RL），通过结合时序差分信号逐步细化偏序集，确保价值函数间的几何一致性，并基于Q学习和演员-评论家算法实现了两种高效算法。在多个任务上的实验结果表明，GCR-RL相比强基线方法显著提高了样本效率并实现了稳定的性能表现。</div>
</details>
</div>
<div class="card">
<div class="title">POPI: Personalizing LLMs via Optimized Preference Inference</div>
<div class="meta-line">Authors: Yizhuo Chen, Xin Liu, Ruijie Wang, Zheng Li, Pei Chen, Changlong Yu, Priyanka Nigam, Meng Jiang, Bing Yin</div>
<div class="meta-line">First: 2025-10-17T23:07:57+00:00 · Latest: 2026-02-03T01:29:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.17881v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.17881v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are typically aligned with population-level preferences, despite substantial variation across individual users. While many LLM personalization methods exist, the underlying structure of user-level personalization is often left implicit. We formalize user-level, prompt-independent personalization as a decomposition into two components: preference inference and conditioned generation. We advocate for a modular design that decouples these components; identify natural language as a generator-agnostic interface between them; and characterize generator-transferability as a key implication of modular personalization. Guided by this abstraction, we introduce POPI, a novel instantiation of modular personalization that parameterizes both preference inference and conditioned generation as shared LLMs. POPI jointly optimizes the two components under a unified preference optimization objective, using reinforcement learning as an optimization tool. Across multiple benchmarks, POPI consistently improves personalization performance while reducing context overhead. We further demonstrate that the learned natural-language preference summaries transfer effectively to frozen, off-the-shelf LLMs, including black-box APIs, providing empirical evidence of modularity and generator-transferability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>POPI：基于优化偏好推断的大语言模型个性化方法</div>
<div class="mono" style="margin-top:8px">大语言模型通常基于群体级偏好进行对齐，但个体用户间存在显著差异。现有许多个性化方法常将用户级个性化的内在结构隐式处理。本研究将用户级、提示无关的个性化形式化为两个组件的分解：偏好推断与条件生成。我们倡导模块化设计以解耦这两个组件；将自然语言定位为两者间生成器无关的接口；并将生成器可迁移性界定为模块化个性化的关键特性。基于此理论框架，我们提出POPI——一种模块化个性化的新型实例，其将偏好推断和条件生成共同参数化为共享大语言模型。POPI在统一偏好优化目标下通过强化学习联合优化这两个组件。在多个基准测试中，POPI在降低上下文开销的同时持续提升个性化性能。我们进一步证明，学习到的自然语言偏好摘要能有效迁移至冻结的现成大语言模型（包括黑盒API），为模块化设计与生成器可迁移性提供了实证依据。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the observation that large language models (LLMs) are typically aligned with broad population-level preferences, overlooking individual user variations, and it seeks to formalize personalization as a modular process of preference inference and conditioned generation. The method introduces POPI, which parameterizes both components as shared LLMs and jointly optimizes them using reinforcement learning under a unified preference optimization objective. Experimental results across multiple benchmarks show that POPI consistently enhances personalization performance while reducing context overhead, and it demonstrates transferability of learned natural-language preference summaries to frozen, off-the-shelf LLMs, including black-box APIs.</div>
<div class="mono" style="margin-top:8px">该论文的动机是观察到大型语言模型通常与广泛的人群级偏好对齐，忽略了用户个体差异，旨在将个性化形式化为偏好推断和条件生成的模块化过程。方法上提出了POPI，将两个组件参数化为共享的大型语言模型，并使用强化学习在统一的偏好优化目标下进行联合优化。在多个基准测试中的实验结果表明，POPI持续提升了个性化性能并减少了上下文开销，同时证明了学习到的自然语言偏好摘要能够有效迁移到冻结的现成大型语言模型（包括黑盒API），为模块化和生成器可迁移性提供了实证依据。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Fast Monomial Orders for Gröbner Basis Computations</div>
<div class="meta-line">Authors: R. Caleb Bunch, Alperen A. Ergür, Melika Golestani, Jessie Tong, Malia Walewski, Yunus E. Zeytuncu</div>
<div class="meta-line">First: 2026-02-03T01:17:18+00:00 · Latest: 2026-02-03T01:17:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02972v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02972v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The efficiency of Gröbner basis computation, the standard engine for solving systems of polynomial equations, depends on the choice of monomial ordering. Despite a near-continuum of possible monomial orders, most implementations rely on static heuristics such as GrevLex, guided primarily by expert intuition. We address this gap by casting the selection of monomial orderings as a reinforcement learning problem over the space of admissible orderings. Our approach leverages domain-informed reward signals that accurately reflect the computational cost of Gröbner basis computations and admits efficient Monte Carlo estimation. Experiments on benchmark problems from systems biology and computer vision show that the resulting learned policies consistently outperform standard heuristics, yielding substantial reductions in computational cost. Moreover, we find that these policies resist distillation into simple interpretable models, providing empirical evidence that deep reinforcement learning allows the agents to exploit non-linear geometric structure beyond the scope of traditional heuristics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习快速单项式序以优化Gröbner基计算</div>
<div class="mono" style="margin-top:8px">Gröbner基计算作为求解多项式方程组的标准引擎，其效率取决于单项式序的选择。尽管存在近乎连续的潜在单项式序，大多数实现仍依赖静态启发式方法（如GrevLex），主要基于专家直觉。我们通过将单项式序选择建模为可行序空间上的强化学习问题来填补这一空白。该方法利用领域知识驱动的奖励信号，精确反映Gröbner基计算的计算成本，并支持高效的蒙特卡洛估计。在系统生物学和计算机视觉基准问题上的实验表明，习得的策略持续优于标准启发式方法，显著降低了计算成本。此外，这些策略难以简化为可解释的简单模型，这为深度强化学习能使智能体利用传统启发式方法无法捕捉的非线性几何结构提供了实证依据。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the critical impact of monomial ordering on the efficiency of Gröbner basis computations, which are fundamental for solving polynomial systems, noting that current methods rely on static heuristics like GrevLex based on expert intuition rather than optimized selection. To bridge this gap, the authors formulate the selection of monomial orders as a reinforcement learning problem, employing domain-informed reward signals that estimate computational costs and enable efficient Monte Carlo estimation. Experimental results on benchmark problems from systems biology and computer vision demonstrate that the learned policies consistently outperform traditional heuristics, achieving significant reductions in computational cost, and further analysis reveals these policies are not easily distilled into simple interpretable models, suggesting they leverage complex non-linear geometric structures beyond conventional approaches.</div>
<div class="mono" style="margin-top:8px">本文针对单项式序选择对Gröbner基计算效率的关键影响展开研究，指出当前方法主要依赖基于专家直觉的静态启发式策略（如GrevLex），而非优化选择。为解决这一问题，作者将单项式序选择构建为强化学习问题，利用反映计算成本的领域知识奖励信号，并采用高效的蒙特卡洛估计方法。在系统生物学和计算机视觉的基准问题上的实验表明，学习得到的策略持续优于传统启发式方法，显著降低了计算成本，且进一步分析发现这些策略难以简化为可解释的简单模型，证明其能够利用传统方法无法捕捉的非线性几何结构。</div>
</details>
</div>
<div class="card">
<div class="title">Co2PO: Coordinated Constrained Policy Optimization for Multi-Agent RL</div>
<div class="meta-line">Authors: Shrenik Patel, Christine Truong</div>
<div class="meta-line">First: 2026-02-03T01:09:31+00:00 · Latest: 2026-02-03T01:09:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02970v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02970v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Constrained multi-agent reinforcement learning (MARL) faces a fundamental tension between exploration and safety-constrained optimization. Existing leading approaches, such as Lagrangian methods, typically rely on global penalties or centralized critics that react to violations after they occur, often suppressing exploration and leading to over-conservatism. We propose Co2PO, a novel MARL communication-augmented framework that enables coordination-driven safety through selective, risk-aware communication. Co2PO introduces a shared blackboard architecture for broadcasting positional intent and yield signals, governed by a learned hazard predictor that proactively forecasts potential violations over an extended temporal horizon. By integrating these forecasts into a constrained optimization objective, Co2PO allows agents to anticipate and navigate collective hazards without the performance trade-offs inherent in traditional reactive constraints. We evaluate Co2PO across a suite of complex multi-agent safety benchmarks, where it achieves higher returns compared to leading constrained baselines while converging to cost-compliant policies at deployment. Ablation studies further validate the necessity of risk-triggered communication, adaptive gating, and shared memory components.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Co2PO：面向多智能体强化学习的协同约束策略优化</div>
<div class="mono" style="margin-top:8px">约束多智能体强化学习面临探索与安全约束优化的根本矛盾。现有主流方法（如拉格朗日法）通常依赖全局惩罚或集中式评价器在违规发生后进行响应，往往抑制探索并导致过度保守。本文提出Co2PO——一种通过选择性风险感知通信实现协同驱动安全的新型多智能体强化学习通信增强框架。Co2PO引入共享黑板架构，用于广播位置意图与让行信号，并由学习型风险预测器进行调控，该预测器能主动预测长时域内的潜在违规。通过将这些预测集成至约束优化目标，Co2PO使智能体能够预见并规避集体风险，同时避免传统反应式约束固有的性能折衷。我们在复杂多智能体安全基准测试中评估Co2PO，其相比主流约束基线获得更高回报，并在部署时收敛至符合成本约束的策略。消融研究进一步验证了风险触发通信、自适应门控与共享记忆组件的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of balancing exploration and safety in constrained multi-agent reinforcement learning, where existing methods like Lagrangian approaches often rely on reactive global penalties that can suppress exploration and lead to over-conservatism. To overcome this, the authors propose Co2PO, a communication-augmented framework that enables coordination-driven safety through selective, risk-aware communication, featuring a shared blackboard architecture for broadcasting intent and yield signals guided by a learned hazard predictor that proactively forecasts potential violations over a long horizon. Experimental results on complex multi-agent safety benchmarks show that Co2PO achieves higher returns than leading constrained baselines while converging to cost-compliant policies, with ablations confirming the importance of its risk-triggered communication, adaptive gating, and shared memory components.</div>
<div class="mono" style="margin-top:8px">本文针对约束多智能体强化学习中探索与安全之间的平衡难题，指出现有方法如拉格朗日方法通常依赖反应式的全局惩罚，可能抑制探索并导致过度保守。为此，作者提出Co2PO，一种通过选择性风险感知通信实现协调驱动安全的通信增强框架，其采用共享黑板架构广播意图和让行信号，并由一个学习型风险预测器指导，主动预测长期潜在违规。在复杂多智能体安全基准测试中，Co2PO相比主流约束基线实现了更高回报，同时收敛到符合成本约束的策略，消融研究进一步验证了其风险触发通信、自适应门控和共享内存组件的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning</div>
<div class="meta-line">Authors: Xiaoya Li, Xiaofei Sun, Albert Wang, Jiwei Li, Chris Shum</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-07-18T17:43:56+00:00 · Latest: 2026-02-03T01:07:23+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.14111v10">Abs</a> · <a href="https://arxiv.org/pdf/2507.14111v10">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The exponential growth in demand for GPU computing resources has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current SOTA models achieve low success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an automated reinforcement learning framework for CUDA optimization that employs a novel contrastive RL algorithm.
  CUDA-L1 achieves significant performance improvements on the CUDA optimization task: trained on A100, it delivers an average speedup of x3.12 with a median speedup of x1.42 against default baselines over across all 250 CUDA kernels of KernelBench, with peak speedups reaching x120. In addition to the default baseline provided by KernelBench, CUDA-L1 demonstrates x2.77 over Torch Compile, x2.88 over Torch Compile with reduce overhead, x2.81 over CUDA Graph implementations, and x7.72 over cuDNN libraries. Furthermore, the model also demonstrates portability across different GPU architectures.
  Beyond these benchmark results, CUDA-L1 demonstrates several properties: it 1) discovers a variety of CUDA optimization techniques and learns to combine them strategically to achieve optimal performance; 2) uncovers fundamental principles of CUDA optimization, such as the multiplicative nature of optimizations; 3) identifies non-obvious performance bottlenecks and rejects seemingly beneficial optimizations that actually harm performance. The capabilities demonstrate that, RL can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources. Project: deepreinforce-ai.github.io/cudal1_blog</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CUDA-L1：通过对比强化学习改进CUDA优化</div>
<div class="mono" style="margin-top:8px">GPU计算资源需求的指数级增长催生了自动化CUDA优化策略的迫切需求。尽管大语言模型在代码生成方面展现出潜力，但当前最先进模型在提升CUDA速度方面成功率较低。本文提出CUDA-L1——一种采用新型对比强化学习算法的自动化CUDA优化强化学习框架。
在CUDA优化任务中，CUDA-L1取得显著性能提升：基于A100训练后，在KernelBench全部250个CUDA内核上，相较于默认基线平均加速比达3.12倍，中位加速比为1.42倍，峰值加速比高达120倍。除KernelBench默认基线外，CUDA-L1相较Torch Compile实现2.77倍加速，相较降低开销的Torch Compile达2.88倍，相较CUDA Graph实现达2.81倍，相较cuDNN库更达到7.72倍加速。该模型还展现出跨GPU架构的移植能力。
除基准测试结果外，CUDA-L1具备以下特性：1）发现多种CUDA优化技术并学会策略性组合以实现最优性能；2）揭示CUDA优化的基本原理，如优化效果的乘性特征；3）识别非显性性能瓶颈，并拒绝实际损害性能的表面优化方案。这些能力表明，强化学习仅通过加速比奖励信号即可将初始性能欠佳的大语言模型转化为高效CUDA优化器，无需人类专家经验或领域知识。该范式为CUDA运算的自动化优化开辟了新路径，有望显著提升GPU效率并缓解日益增长的GPU计算资源压力。项目地址：deepreinforce-ai.github.io/cudal1_blog</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the urgent need for automated CUDA optimization due to growing GPU demand and the low success rates of current LLMs, this paper introduces CUDA-L1, a reinforcement learning framework using a novel contrastive RL algorithm. The method trains an LLM-based optimizer via speedup rewards without human expertise. Experimental results on the KernelBench suite show average speedups of 3.12x over default baselines, with peaks up to 120x, and outperforms other tools like Torch Compile and cuDNN, while also demonstrating portability across GPU architectures and the ability to discover and combine optimization principles.</div>
<div class="mono" style="margin-top:8px">本文的动机是GPU需求激增亟需自动化CUDA优化，而当前大语言模型优化成功率低，因此提出了CUDA-L1，这是一个采用新型对比强化学习算法的自动化强化学习框架。该方法仅基于加速奖励信号训练大语言模型优化器，无需人工专业知识。在KernelBench的250个内核上的实验结果显示，相比默认基线平均加速3.12倍，峰值达120倍，且优于Torch Compile、CUDA Graph和cuDNN等工具，同时展示了跨GPU架构的可移植性，并能自动发现和组合多种优化技术。</div>
</details>
</div>
<div class="card">
<div class="title">Embodiment-Aware Generalist Specialist Distillation for Unified Humanoid Whole-Body Control</div>
<div class="meta-line">Authors: Quanquan Peng, Yunfeng Lin, Yufei Xue, Jiangmiao Pang, Weinan Zhang</div>
<div class="meta-line">First: 2026-02-03T00:58:29+00:00 · Latest: 2026-02-03T00:58:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02960v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02960v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://eagle-wbc.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humanoid Whole-Body Controllers trained with reinforcement learning (RL) have recently achieved remarkable performance, yet many target a single robot embodiment. Variations in dynamics, degrees of freedom (DoFs), and kinematic topology still hinder a single policy from commanding diverse humanoids. Moreover, obtaining a generalist policy that not only transfers across embodiments but also supports richer behaviors-beyond simple walking to squatting, leaning-remains especially challenging. In this work, we tackle these obstacles by introducing EAGLE, an iterative generalist-specialist distillation framework that produces a single unified policy that controls multiple heterogeneous humanoids without per-robot reward tuning. During each cycle, embodiment-specific specialists are forked from the current generalist, refined on their respective robots, and new skills are distilled back into the generalist by training on the pooled embodiment set. Repeating this loop until performance convergence produces a robust Whole-Body Controller validated on robots such as Unitree H1, G1, and Fourier N1. We conducted experiments on five different robots in simulation and four in real-world settings. Through quantitative evaluations, EAGLE achieves high tracking accuracy and robustness compared to other methods, marking a step toward scalable, fleet-level humanoid control. See more details at https://eagle-wbc.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向统一仿人全身控制的具身感知通用专家蒸馏框架</div>
<div class="mono" style="margin-top:8px">基于强化学习训练的仿人全身控制器近期取得显著进展，但多数仅针对单一机器人本体。动力学特性、自由度及运动拓扑结构的差异仍阻碍单一策略操控多样化仿人机器人。此外，获得既能跨本体迁移、又支持更丰富行为（从简单行走到蹲姿、侧倾等）的通用策略尤为困难。本研究提出EAGLE框架，通过迭代式通用-专家蒸馏方法，生成无需针对各机器人单独调整奖励的单一统一策略，以控制多种异构仿人机器人。每轮循环中，从当前通用策略派生出针对特定本体的专家策略，在各自机器人上优化后，将新技能通过多本体数据集训练蒸馏回通用策略。经性能收敛循环后，最终获得在Unitree H1、G1及Fourier N1等机器人上验证的鲁棒全身控制器。我们在仿真环境中对五款机器人、现实场景中对四款机器人进行实验。定量评估表明，EAGLE相比其他方法实现了更高的跟踪精度与鲁棒性，为可扩展的集群级仿人控制迈出关键一步。详见：https://eagle-wbc.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to overcome the limitations of reinforcement learning-based humanoid whole-body controllers that are typically tailored to a single robot embodiment, struggling with variations in dynamics, degrees of freedom, and kinematic topology across diverse humanoids. The method introduces EAGLE, an iterative generalist-specialist distillation framework that produces a unified policy by forking embodiment-specific specialists from a generalist, refining them on individual robots, and distilling new skills back into the generalist through training on a pooled set of embodiments. Experimental results, validated on robots like Unitree H1 and Fourier N1 in both simulation and real-world settings, demonstrate that EAGLE achieves high tracking accuracy and robustness compared to other methods, advancing scalable fleet-level humanoid control.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决基于强化学习的人形机器人全身控制器通常针对单一机器人形态设计的问题，这些控制器难以适应不同人形机器人在动力学、自由度和运动学拓扑上的差异。方法上提出了EAGLE，一种迭代的通用-专家蒸馏框架，通过从通用策略派生出针对特定形态的专家策略，在各自机器人上优化后，将新技能蒸馏回通用策略，并在多机器人集合上训练。实验结果表明，在仿真和真实环境中对Unitree H1等机器人进行验证后，EAGLE相比其他方法实现了更高的跟踪精度和鲁棒性，推动了可扩展的群体级人形机器人控制的发展。</div>
</details>
</div>
<div class="card">
<div class="title">Human-Centric Traffic Signal Control for Equity: A Multi-Agent Action Branching Deep Reinforcement Learning Approach</div>
<div class="meta-line">Authors: Xiaocai Zhang, Neema Nassir, Lok Sang Chan, Milad Haghani</div>
<div class="meta-line">First: 2026-02-03T00:56:03+00:00 · Latest: 2026-02-03T00:56:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02959v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02959v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Coordinating traffic signals along multimodal corridors is challenging because many multi-agent deep reinforcement learning (DRL) approaches remain vehicle-centric and struggle with high-dimensional discrete action spaces. We propose MA2B-DDQN, a human-centric multi-agent action-branching double Deep Q-Network (DQN) framework that explicitly optimizes traveler-level equity. Our key contribution is an action-branching discrete control formulation that decomposes corridor control into (i) local, per-intersection actions that allocate green time between the next two phases and (ii) a single global action that selects the total duration of those phases. This decomposition enables scalable coordination under discrete control while reducing the effective complexity of joint decision-making. We also design a human-centric reward that penalizes the number of delayed individuals in the corridor, accounting for pedestrians, vehicle occupants, and transit passengers. Extensive evaluations across seven realistic traffic scenarios in Melbourne, Australia, demonstrate that our approach significantly reduces the number of impacted travelers, outperforming existing DRL and baseline methods. Experiments confirm the robustness of our model, showing minimal variance across diverse settings. This framework not only advocates for a fairer traffic signal system but also provides a scalable solution adaptable to varied urban traffic conditions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>以人为本的公平交通信号控制：一种多智能体动作分支深度强化学习方法</div>
<div class="mono" style="margin-top:8px">协调多模式走廊沿线的交通信号具有挑战性，因为许多多智能体深度强化学习方法仍以车辆为中心，且难以处理高维离散动作空间。我们提出MA2B-DDQN——一种以人为本的多智能体动作分支双深度Q网络框架，该框架显式优化出行者层面的公平性。核心贡献在于动作分支离散控制架构：将走廊控制分解为（i）本地单交叉口动作（分配相邻两相位绿灯时间）与（ii）全局单动作（确定相位总时长）。该分解在离散控制下实现可扩展协调，同时降低联合决策的有效复杂度。我们还设计了以人为本的奖励函数，通过惩罚走廊内延误个体数量（涵盖行人、车辆乘员及公交乘客）来优化系统。在澳大利亚墨尔本七种真实交通场景中的广泛评估表明，本方法显著减少受影响出行者数量，优于现有深度强化学习方法及基准方法。实验验证了模型的鲁棒性，在不同场景中表现出极低的性能波动。该框架不仅倡导更公平的交通信号系统，更为多样化城市交通条件提供了可扩展的适配方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of coordinating traffic signals in multimodal corridors, motivated by the limitations of existing vehicle-centric multi-agent deep reinforcement learning (DRL) approaches that struggle with high-dimensional discrete action spaces and often neglect traveler-level equity. The authors propose MA2B-DDQN, a human-centric multi-agent action-branching double Deep Q-Network framework, which decomposes corridor control into local per-intersection actions for green time allocation and a single global action for phase duration selection, thereby reducing decision complexity and enabling scalable coordination. The method employs a human-centric reward function that penalizes the number of delayed individuals, including pedestrians, vehicle occupants, and transit passengers. Experimental evaluations across seven realistic traffic scenarios in Melbourne, Australia, demonstrate that this approach significantly reduces the number of impacted travelers, outperforming existing DRL and baseline methods while showing robust performance with minimal variance across diverse settings.</div>
<div class="mono" style="margin-top:8px">本文针对多模式交通走廊中信号灯协调的挑战，其动机在于现有以车辆为中心的多智能体深度强化学习方法存在局限性，难以处理高维离散动作空间且常忽视出行者层面的公平性。作者提出了MA2B-DDQN，一种以人为中心的多智能体动作分支双深度Q网络框架，该方法将走廊控制分解为用于绿灯时间分配的局部交叉口动作和用于相位时长选择的单一全局动作，从而降低了决策复杂性并实现了可扩展的协调。该方法采用了一个以人为中心的奖励函数，对包括行人、车辆乘员和公交乘客在内的延误个体数量进行惩罚。在澳大利亚墨尔本的七个现实交通场景中的实验评估表明，该方法显著减少了受影响的出行者数量，优于现有的深度强化学习和基线方法，同时在多样化设置中表现出鲁棒性且方差最小。</div>
</details>
</div>
<div class="card">
<div class="title">How Does the Lagrangian Guide Safe Reinforcement Learning through Diffusion Models?</div>
<div class="meta-line">Authors: Xiaoyuan Cheng, Wenxuan Yuan, Boyang Li, Yuanchao Xu, Yiming Yang, Hao Liang, Bei Peng, Robert Loftin, Zhuo Sun, Yukun Hu</div>
<div class="meta-line">First: 2026-02-02T23:53:53+00:00 · Latest: 2026-02-02T23:53:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02924v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02924v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion policy sampling enables reinforcement learning (RL) to represent multimodal action distributions beyond suboptimal unimodal Gaussian policies. However, existing diffusion-based RL methods primarily focus on offline settings for reward maximization, with limited consideration of safety in online settings. To address this gap, we propose Augmented Lagrangian-Guided Diffusion (ALGD), a novel algorithm for off-policy safe RL. By revisiting optimization theory and energy-based model, we show that the instability of primal-dual methods arises from the non-convex Lagrangian landscape. In diffusion-based safe RL, the Lagrangian can be interpreted as an energy function guiding the denoising dynamics. Counterintuitively, direct usage destabilizes both policy generation and training. ALGD resolves this issue by introducing an augmented Lagrangian that locally convexifies the energy landscape, yielding a stabilized policy generation and training process without altering the distribution of the optimal policy. Theoretical analysis and extensive experiments demonstrate that ALGD is both theoretically grounded and empirically effective, achieving strong and stable performance across diverse environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>拉格朗日函数如何通过扩散模型引导安全强化学习？</div>
<div class="mono" style="margin-top:8px">扩散策略采样使强化学习能够表示超越次优单峰高斯策略的多峰动作分布。然而，现有基于扩散的强化学习方法主要关注离线环境下的奖励最大化，对在线环境中的安全性考量有限。为填补这一空白，我们提出增强拉格朗日引导扩散算法——一种用于离策略安全强化学习的新算法。通过重访优化理论与能量模型，我们证明原始对偶方法的不稳定性源于非凸拉格朗日函数形态。在基于扩散的安全强化学习中，拉格朗日函数可解释为引导去噪动态的能量函数。反直觉的是，直接使用会同时破坏策略生成与训练的稳定性。ALGD通过引入增强拉格朗日函数局部凸化能量形态来解决此问题，在不改变最优策略分布的前提下实现稳定的策略生成与训练过程。理论分析与大量实验表明，ALGD兼具理论依据与实证有效性，在多样化环境中均能实现强劲而稳定的性能表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the instability of diffusion-based safe reinforcement learning (RL) in online settings, where existing methods often fail to ensure safety during training. The authors propose the Augmented Lagrangian-Guided Diffusion (ALGD) algorithm, which reformulates the Lagrangian as an energy function to guide denoising dynamics and introduces an augmented Lagrangian to convexify the landscape, stabilizing policy generation without changing optimal policy distributions. Experimental results show that ALGD achieves robust and stable performance across various environments, validating its theoretical foundations and practical effectiveness.</div>
<div class="mono" style="margin-top:8px">本文针对在线安全强化学习中基于扩散模型的方法存在训练不稳定的问题，提出了一种新颖的增强拉格朗日引导扩散（ALGD）算法。该方法通过将拉格朗日函数重构为能量函数来指导去噪过程，并引入增强拉格朗日局部凸化能量景观，从而稳定策略生成而不改变最优策略分布。大量实验表明，ALGD在不同环境中实现了强大且稳定的性能，验证了其理论合理性和实证有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Notes on the Reward Representation of Posterior Updates</div>
<div class="meta-line">Authors: Pedro A. Ortega</div>
<div class="meta-line">First: 2026-02-02T23:37:39+00:00 · Latest: 2026-02-02T23:37:39+00:00</div>
<div class="meta-line">Comments: Technical report, 9 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02912v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02912v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many ideas in modern control and reinforcement learning treat decision-making as inference: start from a baseline distribution and update it when a signal arrives. We ask when this can be made literal rather than metaphorical. We study the special case where a KL-regularized soft update is exactly a Bayesian posterior inside a single fixed probabilistic model, so the update variable is a genuine channel through which information is transmitted. In this regime, behavioral change is driven only by evidence carried by that channel: the update must be explainable as an evidence reweighing of the baseline. This yields a sharp identification result: posterior updates determine the relative, context-dependent incentive signal that shifts behavior, but they do not uniquely determine absolute rewards, which remain ambiguous up to context-specific baselines. Requiring one reusable continuation value across different update directions adds a further coherence constraint linking the reward descriptions associated with different conditioning orders.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>后验更新奖励表征的若干注记</div>
<div class="mono" style="margin-top:8px">现代控制与强化学习中的许多思想将决策视为推断过程：从基准分布出发，在信号到达时进行更新。本文探讨了何时可将这种隐喻性描述转化为字面意义上的精确对应。我们研究了KL正则化软更新在特定固定概率模型内精确等价于贝叶斯后验的特殊情形，此时更新变量成为信息传递的真实通道。在此机制下，行为变化完全由该通道承载的证据驱动：更新必须可解释为对基准分布的证据重加权。这产生了一个尖锐的识别结果：后验更新决定了驱动行为变化的相对性、上下文依赖的激励信号，但无法唯一确定绝对奖励值——后者仍受制于上下文特定基准的模糊性。要求在不同更新方向间保持可复用的延续价值，则进一步增加了连接不同条件顺序所对应奖励描述的相干性约束。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates when decision-making updates in control and reinforcement learning, often framed metaphorically as inference, can be interpreted literally as Bayesian posterior updates within a fixed probabilistic model. The method focuses on the specific regime where a KL-regularized soft update corresponds exactly to a genuine Bayesian update, meaning behavioral change is driven solely by evidence transmitted through an update variable. The main experimental results establish a sharp identification: posterior updates determine the relative, context-dependent incentive signals that shift behavior but do not uniquely pin down absolute rewards, which remain ambiguous up to context-specific baselines; imposing coherence across different update directions further constrains the reward descriptions.</div>
<div class="mono" style="margin-top:8px">本文研究了在控制和强化学习中，通常被隐喻为推理的决策更新何时能被字面解释为固定概率模型内的贝叶斯后验更新。方法聚焦于一种特定机制，其中KL正则化的软更新精确对应于真正的贝叶斯更新，意味着行为变化仅由通过更新变量传递的证据驱动。主要实验结果确立了一个清晰的识别结论：后验更新决定了驱动行为的相对、上下文相关的激励信号，但并未唯一确定绝对奖励，后者在上下文特定基线范围内仍存在模糊性；要求在不同更新方向上具有可重用的延续值，则进一步约束了与不同条件顺序相关的奖励描述。</div>
</details>
</div>
<div class="card">
<div class="title">From Self-Evolving Synthetic Data to Verifiable-Reward RL: Post-Training Multi-turn Interactive Tool-Using Agents</div>
<div class="meta-line">Authors: Jiaxuan Gao, Jiaao Chen, Chuyi He, Wei-Chen Wang, Shusheng Xu, Hanrui Wang, Di Jin, Yi Wu</div>
<div class="meta-line">Venue: ICML 2026</div>
<div class="meta-line">First: 2026-01-30T06:01:23+00:00 · Latest: 2026-02-02T23:32:08+00:00</div>
<div class="meta-line">Comments: Submitted to ICML 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22607v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.22607v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Interactive tool-using agents must solve real-world tasks via multi-turn interaction with both humans and external environments, requiring dialogue state tracking, multi-step tool execution, while following complex instructions. Post-training such agents is challenging because synthesis for high-quality multi-turn tool-use data is difficult to scale, and reinforcement learning (RL) could face noisy signals caused by user simulation, leading to degraded training efficiency. We propose a unified framework that combines a self-evolving data agent with verifier-based RL. Our system, EigenData, is a hierarchical multi-agent engine that synthesizes tool-grounded dialogues together with executable per-instance checkers, and improves generation reliability via closed-loop self-evolving process that updates prompts and workflow. Building on the synthetic data, we develop an RL recipe that first fine-tunes the user model and then applies GRPO-style training with trajectory-level group-relative advantages and dynamic filtering, yielding consistent improvements beyond SFT. Evaluated on tau^2-bench, our best model reaches 73.0% pass^1 on Airline and 98.3% pass^1 on Telecom, matching or exceeding frontier models. Overall, our results suggest a scalable pathway for bootstrapping complex tool-using behaviors without expensive human annotation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从自演进合成数据到可验证奖励强化学习：训练后多轮交互式工具使用智能体</div>
<div class="mono" style="margin-top:8px">交互式工具使用智能体需通过与人及外部环境的多轮交互解决现实任务，要求具备对话状态追踪、多步骤工具执行及复杂指令遵循能力。此类智能体的训练后优化面临挑战：高质量多轮工具使用数据的合成难以规模化，而强化学习可能因用户模拟产生的噪声信号导致训练效率下降。我们提出一个统一框架，将自演进数据智能体与基于验证器的强化学习相结合。我们的系统EigenData是一个分层多智能体引擎，可合成工具落地的对话及可执行的实例级检查器，并通过更新提示词与工作流的闭环自演进过程提升生成可靠性。基于合成数据，我们开发了一种强化学习方案：先微调用户模型，再采用GRPO风格训练（结合轨迹级组间相对优势与动态过滤），实现了超越监督微调的持续改进。在tau^2-bench评估中，最佳模型在Airline任务达到73.0% pass^1，在Telecom任务达到98.3% pass^1，匹配或超越了前沿模型。总体而言，我们的研究为无需昂贵人工标注即可引导复杂工具使用行为提供了可扩展路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the challenges in post-training multi-turn interactive tool-using agents, where scaling high-quality synthetic dialogue data is difficult and reinforcement learning often suffers from noisy signals from user simulations. The method introduces a unified framework called EigenData, which combines a self-evolving data agent that synthesizes tool-grounded dialogues with executable checkers and a verifier-based RL approach that fine-tunes user models and employs GRPO-style training with trajectory-level advantages and dynamic filtering. Experimental results on the tau^2-bench show that the best model achieves 73.0% pass^1 on Airline and 98.3% pass^1 on Telecom, matching or surpassing frontier models, demonstrating a scalable pathway for bootstrapping complex tool-using behaviors without costly human annotation.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决多轮交互式工具使用智能体在训练后所面临的挑战，即高质量合成对话数据难以扩展，且强化学习常因用户模拟产生的噪声信号而效率低下。方法上提出了一个名为EigenData的统一框架，结合了自演化数据代理（用于合成带可执行检查器的工具接地对话）和基于验证器的强化学习，后者先微调用户模型，再采用GRPO风格训练，利用轨迹级组相对优势和动态过滤。在tau^2-bench上的实验结果表明，最佳模型在Airline任务上达到73.0% pass^1，在Telecom任务上达到98.3% pass^1，匹配或超越了前沿模型，为无需昂贵人工标注的复杂工具使用行为提供了一条可扩展的路径。</div>
</details>
</div>
<div class="card">
<div class="title">Spatiotemporal Decision Transformer for Traffic Coordination</div>
<div class="meta-line">Authors: Haoran Su, Yandong Sun, Hanxiao Deng</div>
<div class="meta-line">First: 2026-02-02T23:19:13+00:00 · Latest: 2026-02-02T23:19:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02903v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02903v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traffic signal control is a critical challenge in urban transportation, requiring coordination among multiple intersections to optimize network-wide traffic flow. While reinforcement learning has shown promise for adaptive signal control, existing methods struggle with multi-agent coordination and sample efficiency. We introduce MADT (Multi-Agent Decision Transformer), a novel approach that reformulates multi-agent traffic signal control as a sequence modeling problem. MADT extends the Decision Transformer paradigm to multi-agent settings by incorporating: (1) a graph attention mechanism for modeling spatial dependencies between intersections, (2) a|temporal transformer encoder for capturing traffic dynamics, and (3) return-to-go conditioning for target performance specification. Our approach enables offline learning from historical traffic data, with architecture design that facilitates potential online fine-tuning. Experiments on synthetic grid networks and real-world traffic scenarios demonstrate that MADT achieves state-of-the-art performance, reducing average travel time by 5-6% compared to the strongest baseline while exhibiting superior coordination among adjacent intersections.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向交通协调的时空决策变换器</div>
<div class="mono" style="margin-top:8px">交通信号控制是城市交通中的关键挑战，需要多个交叉口之间的协调以优化全网交通流。尽管强化学习在自适应信号控制中展现出潜力，现有方法在多智能体协调和样本效率方面仍存在不足。本文提出MADT（多智能体决策变换器），通过将多智能体交通信号控制重构为序列建模问题，创新性地扩展了决策变换器范式至多智能体场景。MADT融合三大核心组件：（1）用于建模交叉口空间依赖关系的图注意力机制；（2）捕捉交通动态的时序变换器编码器；（3）指定目标性能的回报条件化机制。该方法支持从历史交通数据中进行离线学习，其架构设计便于潜在的在线微调。在合成网格路网和真实交通场景中的实验表明，MADT实现了最先进的性能表现：相较于最强基线方法，平均行程时间降低5-6%，同时在相邻交叉口间展现出更优的协调能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of multi-intersection traffic signal coordination for optimizing network-wide flow, motivated by the limitations of existing reinforcement learning methods in coordination and sample efficiency. The authors introduce MADT, a Multi-Agent Decision Transformer that reformulates traffic signal control as a sequence modeling problem, incorporating a graph attention mechanism for spatial dependencies, a temporal transformer encoder for dynamics, and return-to-go conditioning for performance specification, enabling offline learning from historical data. Experimental results on synthetic and real-world scenarios show that MADT achieves state-of-the-art performance, reducing average travel time by 5-6% compared to strong baselines while demonstrating superior intersection coordination.</div>
<div class="mono" style="margin-top:8px">本文针对多路口交通信号协调以优化全网流量的挑战，其动机是现有强化学习方法在协调和样本效率上的不足。作者提出了多智能体决策变换器（MADT），将交通信号控制重构为序列建模问题，结合了用于空间依赖的图注意力机制、捕捉动态的时间变换器编码器以及用于性能指定的回报条件，支持从历史数据中进行离线学习。在合成网格网络和真实交通场景上的实验表明，MADT实现了最先进的性能，相比最强基线平均行程时间减少了5-6%，同时展现出更优的相邻路口协调能力。</div>
</details>
</div>
<div class="card">
<div class="title">Manifold-Constrained Energy-Based Transition Models for Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Zeyu Fang, Zuyuan Zhang, Mahdi Imani, Tian Lan</div>
<div class="meta-line">First: 2026-02-02T23:15:43+00:00 · Latest: 2026-02-02T23:15:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02900v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02900v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Model-based offline reinforcement learning is brittle under distribution shift: policy improvement drives rollouts into state--action regions weakly supported by the dataset, where compounding model error yields severe value overestimation. We propose Manifold-Constrained Energy-based Transition Models (MC-ETM), which train conditional energy-based transition models using a manifold projection--diffusion negative sampler. MC-ETM learns a latent manifold of next states and generates near-manifold hard negatives by perturbing latent codes and running Langevin dynamics in latent space with the learned conditional energy, sharpening the energy landscape around the dataset support and improving sensitivity to subtle out-of-distribution deviations. For policy optimization, the learned energy provides a single reliability signal: rollouts are truncated when the minimum energy over sampled next states exceeds a threshold, and Bellman backups are stabilized via pessimistic penalties based on Q-value-level dispersion across energy-guided samples. We formalize MC-ETM through a hybrid pessimistic MDP formulation and derive a conservative performance bound separating in-support evaluation error from truncation risk. Empirically, MC-ETM improves multi-step dynamics fidelity and yields higher normalized returns on standard offline control benchmarks, particularly under irregular dynamics and sparse data coverage.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于流形约束能量转移模型的离线强化学习</div>
<div class="mono" style="margin-top:8px">基于模型的离线强化学习在分布偏移下表现脆弱：策略改进将轨迹推至数据集支持薄弱的状态-动作区域，此时复合模型误差会导致严重的价值高估。我们提出流形约束能量转移模型（MC-ETM），通过流形投影-扩散负采样器训练条件能量转移模型。MC-ETM学习潜在下一状态流形，通过扰动潜在编码并在潜在空间中基于学习到的条件能量运行朗之万动力学，生成近流形硬负样本，从而锐化数据集支撑区域附近的能量景观，提升对细微分布外偏移的敏感性。在策略优化中，学习到的能量提供单一可靠性信号：当采样下一状态的最小能量超过阈值时截断轨迹，并通过基于能量引导样本间Q值离散度的悲观惩罚稳定贝尔曼备份。我们通过混合悲观MDP框架形式化MC-ETM，并推导出分离支持集内评估误差与截断风险的保守性能边界。实验表明，MC-ETM提升了多步动态保真度，在标准离线控制基准上获得更高归一化回报，尤其在非规则动态和稀疏数据覆盖场景中表现突出。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the brittleness of model-based offline reinforcement learning under distribution shift, where policy improvement can lead to compounding model error and severe value overestimation. The authors propose Manifold-Constrained Energy-based Transition Models (MC-ETM), which train conditional energy-based transition models using a manifold projection–diffusion negative sampler to learn a latent manifold of next states and generate near-manifold hard negatives, sharpening the energy landscape around dataset support. For policy optimization, the learned energy provides a reliability signal to truncate rollouts and stabilize Bellman backups via pessimistic penalties. Empirically, MC-ETM improves multi-step dynamics fidelity and achieves higher normalized returns on standard offline control benchmarks, especially under irregular dynamics and sparse data coverage.</div>
<div class="mono" style="margin-top:8px">本文针对基于模型的离线强化学习在分布偏移下的脆弱性问题，即策略改进可能导致复合模型误差和严重的价值高估。作者提出了流形约束能量基转移模型（MC-ETM），该方法通过流形投影-扩散负采样器训练条件能量基转移模型，以学习下一状态的潜在流形并生成近流形硬负样本，从而锐化数据集支持附近的能量景观。在策略优化中，学习到的能量提供可靠性信号，用于截断轨迹并通过基于能量引导样本的Q值分散的悲观惩罚来稳定贝尔曼备份。实验表明，MC-ETM提高了多步动态保真度，并在标准离线控制基准上实现了更高的归一化回报，特别是在不规则动态和稀疏数据覆盖的情况下。</div>
</details>
</div>
<div class="card">
<div class="title">TraceNAS: Zero-shot LLM Pruning via Gradient Trace Correlation</div>
<div class="meta-line">Authors: Prajna G. Malettira, Manish Nagaraj, Arjun Roy, Shubham Negi, Kaushik Roy</div>
<div class="meta-line">First: 2026-02-02T22:49:39+00:00 · Latest: 2026-02-02T22:49:39+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02891v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02891v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Structured pruning is essential for efficient deployment of Large Language Models (LLMs). The varying sensitivity of LLM sub-blocks to pruning necessitates the identification of optimal non-uniformly pruned models. Existing methods evaluate the importance of layers, attention heads, or weight channels in isolation. Such localized focus ignores the complex global structural dependencies that exist across the model. Training-aware structured pruning addresses global dependencies, but its computational cost can be just as expensive as post-pruning training. To alleviate the computational burden of training-aware pruning and capture global structural dependencies, we propose TraceNAS, a training-free Neural Architecture Search (NAS) framework that jointly explores structured pruning of LLM depth and width. TraceNAS identifies pruned models that maintain a high degree of loss landscape alignment with the pretrained model using a scale-invariant zero-shot proxy, effectively selecting models that exhibit maximal performance potential during post-pruning training. TraceNAS is highly efficient, enabling high-fidelity discovery of pruned models on a single GPU in 8.5 hours, yielding a 10$\times$ reduction in GPU-hours compared to training-aware methods. Evaluations on the Llama and Qwen families demonstrate that TraceNAS is competitive with training-aware baselines across commonsense and reasoning benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TraceNAS：基于梯度迹相关的零样本大语言模型剪枝方法</div>
<div class="mono" style="margin-top:8px">结构化剪枝对大语言模型的高效部署至关重要。由于大语言模型各子块对剪枝的敏感度不同，需要识别最优的非均匀剪枝模型。现有方法仅单独评估层、注意力头或权重通道的重要性，这种局部聚焦忽略了模型内部复杂的全局结构依赖。训练感知的结构化剪枝能处理全局依赖，但其计算成本可能与剪枝后训练相当昂贵。为减轻训练感知剪枝的计算负担并捕获全局结构依赖，我们提出TraceNAS——一种免训练的神经架构搜索框架，可联合探索大语言模型深度与宽度的结构化剪枝。该方法通过尺度不变的零样本代理指标，识别与预训练模型保持高度损失空间对齐的剪枝模型，有效筛选出在剪枝后训练中具备最大性能潜力的架构。TraceNAS效率极高，仅需单GPU运行8.5小时即可实现高保真剪枝模型搜索，相比训练感知方法减少10倍GPU时耗。在Llama和Qwen系列模型上的评估表明，该框架在常识与推理基准测试中与训练感知基线方法具有竞争力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the computational inefficiency of training-aware structured pruning methods for Large Language Models (LLMs) while still capturing the global structural dependencies that isolated, local pruning evaluations miss. The proposed method, TraceNAS, is a training-free Neural Architecture Search framework that jointly prunes model depth and width using a scale-invariant zero-shot proxy based on gradient trace correlation to identify pruned architectures whose loss landscapes align well with the original pretrained model. Main experimental results show that TraceNAS efficiently discovers high-performance pruned models on a single GPU in 8.5 hours, achieving a 10x reduction in GPU-hours compared to training-aware baselines while remaining competitive with them on commonsense and reasoning benchmarks for Llama and Qwen model families.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大型语言模型结构化剪枝中训练感知方法计算成本高的问题，同时捕捉局部评估所忽略的全局结构依赖性。所提出的方法TraceNAS是一种免训练的神经架构搜索框架，它通过基于梯度轨迹相关性的尺度不变零样本代理，联合剪枝模型的深度和宽度，以识别其损失景观与原始预训练模型保持高度对齐的剪枝架构。主要实验结果表明，TraceNAS能在单GPU上8.5小时内高效发现高性能剪枝模型，相比训练感知方法减少10倍GPU时耗，并在Llama和Qwen模型系列的常识与推理基准测试中与这些基线保持竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">On the Convergence of Experience Replay in Policy Optimization: Characterizing Bias, Variance, and Finite-Time Convergence</div>
<div class="meta-line">Authors: Hua Zheng, Wei Xie, M. Ben Feng</div>
<div class="meta-line">First: 2021-10-17T19:28:45+00:00 · Latest: 2026-02-02T22:10:09+00:00</div>
<div class="meta-line">Comments: 37 pages; v5 retains only the portion of v4 covering the theoretical results on experience replay</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2110.08902v5">Abs</a> · <a href="https://arxiv.org/pdf/2110.08902v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Experience replay is a core ingredient of modern deep reinforcement learning, yet its benefits in policy optimization are poorly understood beyond empirical heuristics. This paper develops a novel theoretical framework for experience replay in modern policy gradient methods, where two sources of dependence fundamentally complicate analysis: Markovian correlations along trajectories and policy drift across optimization iterations. We introduce a new proof technique based on auxiliary Markov chains and lag-based decoupling that makes these dependencies tractable. Within this framework, we derive finite-time bias bounds for policy-gradient estimators under replay, identifying how bias scales with the cumulative policy update, the mixing time of the underlying dynamics, and the age of buffered data, thereby formalizing the practitioner&#x27;s rule of avoiding overly stale replay. We further provide a correlation-aware variance decomposition showing how sample dependence governs gradient variance from replay and when replay is beneficial. Building on these characterizations, we establish the finite-time convergence guarantees for experience-replay-based policy optimization, explicitly quantifying how buffer size, sample correlation, and mixing jointly determine the convergence rate and revealing an inherent bias-variance trade-off: larger buffers can reduce variance by averaging less correlated samples but can increase bias as data become stale. These results offer a principled guide for buffer sizing and replay schedules, bridging prior empirical findings with quantitative theory.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>论经验回放在策略优化中的收敛性：偏差、方差与有限时间收敛性刻画</div>
<div class="mono" style="margin-top:8px">经验回放是现代深度强化学习的核心组成部分，但其在策略优化中的优势除经验性启发外缺乏深入理解。本文为现代策略梯度方法中的经验回放构建了新颖的理论框架，其中两种依赖关系使分析变得复杂：轨迹中的马尔可夫相关性及优化迭代间的策略漂移。我们提出基于辅助马尔可夫链和滞后解耦的新证明技术，以处理这些依赖关系。在此框架下，推导了回放机制下策略梯度估计量的有限时间偏差界，明确了偏差如何随累积策略更新量、底层动态的混合时间及缓存数据时效性变化，从而形式化了实践中避免使用过度陈旧回放数据的准则。进一步提出相关性感知的方差分解方法，揭示样本依赖性如何主导回放梯度方差及回放的适用场景。基于这些刻画，建立了基于经验回放的策略优化的有限时间收敛保证，明确量化了缓冲区大小、样本相关性与混合性如何共同决定收敛速率，并揭示了固有的偏差-方差权衡：扩大缓冲区可通过平均低相关性样本来降低方差，但数据陈旧化可能增加偏差。这些结果为缓冲区规模设定与回放调度提供了理论指导，将先前的实证发现与量化理论相衔接。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to theoretically understand the benefits and mechanisms of experience replay in deep reinforcement learning, which has been largely guided by empirical heuristics. The authors develop a novel theoretical framework using auxiliary Markov chains and lag-based decoupling to analyze the dependencies from Markovian trajectories and policy drift, enabling tractable analysis of policy gradient methods with replay. Their main results include finite-time bias bounds that scale with policy updates, mixing time, and data age, a correlation-aware variance decomposition showing when replay reduces variance, and finite-time convergence guarantees that quantify how buffer size and correlation affect rates, revealing a bias-variance trade-off where larger buffers can lower variance but increase bias from stale data.</div>
<div class="mono" style="margin-top:8px">本文的动机源于需要从理论上理解深度强化学习中经验回放机制的益处与原理，该领域此前主要依赖经验启发。作者提出了一种新颖的理论框架，利用辅助马尔可夫链和基于滞后的解耦技术，分析了马尔可夫轨迹依赖性和策略漂移，使得带经验回放的策略梯度方法可进行理论分析。主要实验结果包括：推导了有限时间偏差界，其随策略更新、混合时间和数据陈旧度而变化；提出了相关性感知的方差分解，阐明了回放降低方差的条件；并建立了有限时间收敛保证，量化了缓冲区大小和相关性对收敛速率的影响，揭示了一个偏差-方差权衡——更大的缓冲区可通过平均低相关性样本来降低方差，但也会因数据陈旧而增加偏差。</div>
</details>
</div>
<div class="card">
<div class="title">IMAGINE: Intelligent Multi-Agent Godot-based Indoor Networked Exploration</div>
<div class="meta-line">Authors: Tiago Leite, Maria Conceição, António Grilo</div>
<div class="meta-line">First: 2026-02-02T22:08:41+00:00 · Latest: 2026-02-02T22:08:41+00:00</div>
<div class="meta-line">Comments: 12 pages, submitted to a journal</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02858v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02858v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The exploration of unknown, Global Navigation Satellite System (GNSS) denied environments by an autonomous communication-aware and collaborative group of Unmanned Aerial Vehicles (UAVs) presents significant challenges in coordination, perception, and decentralized decision-making. This paper implements Multi-Agent Reinforcement Learning (MARL) to address these challenges in a 2D indoor environment, using high-fidelity game-engine simulations (Godot) and continuous action spaces. Policy training aims to achieve emergent collaborative behaviours and decision-making under uncertainty using Network-Distributed Partially Observable Markov Decision Processes (ND-POMDPs). Each UAV is equipped with a Light Detection and Ranging (LiDAR) sensor and can share data (sensor measurements and a local occupancy map) with neighbouring agents. Inter-agent communication constraints include limited range, bandwidth and latency. Extensive ablation studies evaluated MARL training paradigms, reward function, communication system, neural network (NN) architecture, memory mechanisms, and POMDP formulations. This work jointly addresses several key limitations in prior research, namely reliance on discrete actions, single-agent or centralized formulations, assumptions of a priori knowledge and permanent connectivity, inability to handle dynamic obstacles, short planning horizons and architectural complexity in Recurrent NNs/Transformers. Results show that the scalable training paradigm, combined with a simplified architecture, enables rapid autonomous exploration of an indoor area. The implementation of Curriculum-Learning (five increasingly complex levels) also enabled faster, more robust training. This combination of high-fidelity simulation, MARL formulation, and computational efficiency establishes a strong foundation for deploying learned cooperative strategies in physical robotic systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IMAGINE：基于Godot的智能多智能体室内网络化探索</div>
<div class="mono" style="margin-top:8px">自主通信感知与协作的无人机群在未知且无全球导航卫星系统（GNSS）信号的环境中进行探索，在协调、感知和分散决策方面面临重大挑战。本文采用多智能体强化学习（MARL）在二维室内环境中应对这些挑战，利用高保真游戏引擎仿真（Godot）和连续动作空间。策略训练旨在通过网络分布式部分可观测马尔可夫决策过程（ND-POMDP）实现不确定条件下的涌现协作行为和决策。每架无人机配备激光雷达传感器，可与邻近智能体共享数据（传感器测量值和局部占据地图）。智能体间通信约束包括有限范围、带宽和延迟。广泛的消融研究评估了MARL训练范式、奖励函数、通信系统、神经网络架构、记忆机制和POMDP公式。这项工作共同解决了先前研究中的若干关键局限，包括依赖离散动作、单智能体或集中式公式、假设先验知识和永久连接、无法处理动态障碍、短规划视野以及循环神经网络/Transformer的架构复杂性。结果表明，可扩展的训练范式结合简化架构，能够实现室内区域的快速自主探索。课程学习（五个渐进复杂级别）的实施也实现了更快、更稳健的训练。高保真仿真、MARL公式和计算效率的结合，为在物理机器人系统中部署习得的协作策略奠定了坚实基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of enabling a team of UAVs to collaboratively explore unknown indoor environments without GNSS, where coordination, perception, and decentralized decision-making under communication constraints are difficult. The method employs Multi-Agent Reinforcement Learning (MARL) within a high-fidelity Godot game-engine simulation, using continuous action spaces and formulating the problem as a Network-Distributed POMDP; each UAV uses LiDAR and shares local data with neighbors under limited communication range and bandwidth. Experimental results from extensive ablation studies demonstrate that the proposed scalable training paradigm and simplified neural architecture, aided by curriculum learning, enable rapid and robust autonomous exploration of the indoor area, effectively overcoming prior limitations like discrete actions, centralized control, and assumptions of permanent connectivity.</div>
<div class="mono" style="margin-top:8px">本文旨在解决无人机群在无全球导航卫星系统的未知室内环境中协同探索的难题，其挑战包括协调、感知以及在通信受限下的分散决策。研究方法采用多智能体强化学习，基于Godot游戏引擎的高保真仿真，使用连续动作空间，并将问题建模为网络分布式部分可观测马尔可夫决策过程；每架无人机配备激光雷达，并在有限的通信范围和带宽内与邻近智能体共享本地数据。主要实验结果通过大量消融研究表明，所提出的可扩展训练范式和简化的神经网络架构，结合课程学习方法，能够实现快速、稳健的室内区域自主探索，有效克服了先前研究中离散动作、集中控制、假设永久连通性等关键局限。</div>
</details>
</div>
<div class="card">
<div class="title">Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Internet Text</div>
<div class="meta-line">Authors: Ximing Lu, David Acuna, Jaehun Jung, Jian Hu, Di Zhang, Shizhe Diao, Yunheng Zou, Shaokun Zhang, Brandon Cui, Mingjie Liu, Hyunwoo Kim, Prithviraj Ammanabrolu, Jan Kautz, Yi Dong, Yejin Choi</div>
<div class="meta-line">First: 2026-01-30T13:39:11+00:00 · Latest: 2026-02-02T21:54:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22975v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.22975v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has become a cornerstone for unlocking complex reasoning in Large Language Models (LLMs). Yet, scaling up RL is bottlenecked by limited existing verifiable data, where improvements increasingly saturate over prolonged training. To overcome this, we propose Golden Goose, a simple trick to synthesize unlimited RLVR tasks from unverifiable internet text by constructing a multiple-choice question-answering version of the fill-in-the-middle task. Given a source text, we prompt an LLM to identify and mask key reasoning steps, then generate a set of diverse, plausible distractors. This enables us to leverage reasoning-rich unverifiable corpora typically excluded from prior RLVR data construction (e.g., science textbooks) to synthesize GooseReason-0.7M, a large-scale RLVR dataset with over 0.7 million tasks spanning mathematics, programming, and general scientific domains. Empirically, GooseReason effectively revives models saturated on existing RLVR data, yielding robust, sustained gains under continuous RL and achieving new state-of-the-art results for 1.5B and 4B-Instruct models across 15 diverse benchmarks. Finally, we deploy Golden Goose in a real-world setting, synthesizing RLVR tasks from raw FineWeb scrapes for the cybersecurity domain, where no prior RLVR data exists. Training Qwen3-4B-Instruct on the resulting data GooseReason-Cyber sets a new state-of-the-art in cybersecurity, surpassing a 7B domain-specialized model with extensive domain-specific pre-training and post-training. This highlights the potential of automatically scaling up RLVR data by exploiting abundant, reasoning-rich, unverifiable internet text.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>金鹅：从未经验证的互联网文本中合成无限RLVR任务的简易技巧</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）已成为解锁大型语言模型复杂推理能力的关键技术。然而，现有可验证数据的稀缺限制了强化学习的扩展，导致模型在长期训练中改进逐渐饱和。为突破此瓶颈，我们提出“金鹅”方法——通过将“中间填空”任务转化为多项选择题形式，从未经验证的互联网文本中合成无限RLVR任务。该方法首先提示大语言模型识别并遮蔽原文中的关键推理步骤，随后生成多样化的合理干扰项，从而能够利用通常被传统RLVR数据构建排除的、富含推理内容的未验证语料（如科学教科书），合成包含超过70万个任务的GooseReason-0.7M数据集，涵盖数学、编程及通用科学领域。实验表明，GooseReason能有效激活在现有RLVR数据上饱和的模型，在持续强化学习中实现稳健的长期增益，并在15个多样化基准测试中为15亿和40亿参数指令模型创造了最新最优结果。最后，我们将金鹅方法应用于网络安全领域，从原始FineWeb数据中合成RLVR任务——该领域此前不存在RLVR数据。基于合成数据GooseReason-Cyber训练的Qwen3-40亿参数指令模型，在网络安全任务中超越了经过大量领域预训练与后训练的70亿参数专业模型，创造了该领域新纪录。这彰显了通过挖掘海量富含推理的未验证互联网文本，自动扩展RLVR数据资源的巨大潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to overcome the bottleneck in scaling Reinforcement Learning with Verifiable Rewards (RLVR), where progress saturates due to limited existing verifiable data. The method, named Golden Goose, synthesizes unlimited RLVR tasks from unverifiable internet text by converting fill-in-the-middle tasks into multiple-choice questions; it uses an LLM to mask key reasoning steps in source text and generate plausible distractors, enabling the creation of a large-scale dataset called GooseReason-0.7M from reasoning-rich corpora like science textbooks. The main experimental results show that training on this data revives saturated models, yielding sustained gains and achieving state-of-the-art results for 1.5B and 4B-Instruct models across 15 benchmarks, and a domain-specific application in cybersecurity also sets a new state-of-the-art by surpassing a larger specialized model.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决可验证奖励强化学习（RLVR）因现有可验证数据有限而导致训练饱和的扩展瓶颈。所提出的方法名为Golden Goose，通过将填空任务转化为多项选择题，从未经验证的互联网文本中合成无限的RLVR任务；该方法利用大语言模型对源文本中的关键推理步骤进行掩码并生成合理的干扰项，从而能够从科学教科书等富含推理的语料中构建大规模数据集GooseReason-0.7M。主要实验结果表明，基于此数据的训练使饱和模型重新获得提升，在15个多样基准测试中为1.5B和4B-Instruct模型带来了持续增益并达到新的最先进水平，且在网络安全领域的应用也超越了更大的专业模型，设立了新的性能标杆。</div>
</details>
</div>
<div class="card">
<div class="title">AutoSizer: Automatic Sizing of Analog and Mixed-Signal Circuits via Large Language Model (LLM) Agents</div>
<div class="meta-line">Authors: Xi Yu, Dmitrii Torbunov, Soumyajit Mandal, Yihui Ren</div>
<div class="meta-line">First: 2026-02-02T21:51:55+00:00 · Latest: 2026-02-02T21:51:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02849v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02849v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The design of Analog and Mixed-Signal (AMS) integrated circuits remains heavily reliant on expert knowledge, with transistor sizing a major bottleneck due to nonlinear behavior, high-dimensional design spaces, and strict performance constraints. Existing Electronic Design Automation (EDA) methods typically frame sizing as static black-box optimization, resulting in inefficient and less robust solutions. Although Large Language Models (LLMs) exhibit strong reasoning abilities, they are not suited for precise numerical optimization in AMS sizing. To address this gap, we propose AutoSizer, a reflective LLM-driven meta-optimization framework that unifies circuit understanding, adaptive search-space construction, and optimization orchestration in a closed loop. It employs a two-loop optimization framework, with an inner loop for circuit sizing and an outer loop that analyzes optimization dynamics and constraints to iteratively refine the search space from simulation feedback. We further introduce AMS-SizingBench, an open benchmark comprising 24 diverse AMS circuits in SKY130 CMOS technology, designed to evaluate adaptive optimization policies under realistic simulator-based constraints. AutoSizer experimentally achieves higher solution quality, faster convergence, and higher success rate across varying circuit difficulties, outperforming both traditional optimization methods and existing LLM-based agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AutoSizer：基于大语言模型（LLM）代理的模拟与混合信号电路自动尺寸优化</div>
<div class="mono" style="margin-top:8px">模拟与混合信号（AMS）集成电路的设计仍高度依赖专家知识，其中晶体管尺寸优化因非线性行为、高维设计空间和严格性能约束而成为主要瓶颈。现有电子设计自动化（EDA）方法通常将尺寸优化视为静态黑盒优化问题，导致解决方案效率低下且鲁棒性不足。尽管大语言模型（LLM）展现出强大的推理能力，但其不适用于AMS尺寸优化中的精确数值优化。为填补这一空白，我们提出AutoSizer——一种基于反思型LLM驱动的元优化框架，通过闭环统一整合电路理解、自适应搜索空间构建与优化协同。该框架采用双循环优化结构：内循环执行电路尺寸优化，外循环分析优化动态与约束条件，并依据仿真反馈迭代优化搜索空间。我们进一步推出AMS-SizingBench开放基准测试集，包含基于SKY130 CMOS工艺的24个多样化AMS电路，旨在评估真实仿真器约束下的自适应优化策略。实验表明，AutoSizer在不同电路复杂度下均能实现更优解质量、更快收敛速度和更高成功率，其性能超越传统优化方法与现有基于LLM的代理方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the expert-dependent and inefficient nature of analog and mixed-signal circuit sizing, which suffers from nonlinearities and high-dimensional design spaces, this paper introduces AutoSizer, a framework that leverages large language model agents for reflective meta-optimization. The method employs a two-loop structure where an inner loop performs circuit sizing and an outer loop iteratively refines the search space based on simulation feedback and constraint analysis, unifying circuit understanding with adaptive optimization orchestration. Experimental results on the newly introduced AMS-SizingBench, comprising 24 circuits in SKY130 technology, demonstrate that AutoSizer achieves superior solution quality, faster convergence, and higher success rates compared to traditional optimization and existing LLM-based approaches.</div>
<div class="mono" style="margin-top:8px">针对模拟和混合信号电路设计中依赖专家经验、存在非线性与高维设计空间导致尺寸调整效率低下的问题，本文提出了AutoSizer，一个利用大语言模型代理进行反射式元优化的框架。该方法采用双循环结构，内循环执行电路尺寸调整，外循环根据仿真反馈和约束分析迭代优化搜索空间，将电路理解与自适应优化编排相统一。在基于SKY130工艺新构建的包含24个电路的AMS-SizingBench上的实验结果表明，AutoSizer相比传统优化方法和现有基于大语言模型的代理，在解决方案质量、收敛速度和成功率方面均表现更优。</div>
</details>
</div>
<div class="card">
<div class="title">Causal Flow Q-Learning for Robust Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Mingxuan Li, Junzhe Zhang, Elias Bareinboim</div>
<div class="meta-line">First: 2026-02-02T21:50:52+00:00 · Latest: 2026-02-02T21:50:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02847v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02847v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Expressive policies based on flow-matching have been successfully applied in reinforcement learning (RL) more recently due to their ability to model complex action distributions from offline data. These algorithms build on standard policy gradients, which assume that there is no unmeasured confounding in the data. However, this condition does not necessarily hold for pixel-based demonstrations when a mismatch exists between the demonstrator&#x27;s and the learner&#x27;s sensory capabilities, leading to implicit confounding biases in offline data. We address the challenge by investigating the problem of confounded observations in offline RL from a causal perspective. We develop a novel causal offline RL objective that optimizes policies&#x27; worst-case performance that may arise due to confounding biases. Based on this new objective, we introduce a practical implementation that learns expressive flow-matching policies from confounded demonstrations, employing a deep discriminator to assess the discrepancy between the target policy and the nominal behavioral policy. Experiments across 25 pixel-based tasks demonstrate that our proposed confounding-robust augmentation procedure achieves a success rate 120\% that of confounding-unaware, state-of-the-art offline RL methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向鲁棒离线强化学习的因果流Q学习</div>
<div class="mono" style="margin-top:8px">基于流匹配的表达性策略因其能够从离线数据中建模复杂动作分布，近年来已成功应用于强化学习。这类算法建立在标准策略梯度之上，其假设数据中不存在未测量的混杂因素。然而，当演示者与学习者的感知能力存在差异时，这一条件在基于像素的演示任务中未必成立，从而导致离线数据中存在隐含的混杂偏差。我们从因果视角研究离线强化学习中的混杂观测问题，提出一种新颖的因果离线强化学习目标函数，该函数通过优化策略在混杂偏差影响下的最差性能来应对挑战。基于此目标，我们实现了一种从混杂演示中学习流匹配策略的实用方法，采用深度判别器评估目标策略与名义行为策略之间的差异。在25个基于像素的任务实验中，我们提出的混杂鲁棒增强方法取得了比未考虑混杂因素的先进离线强化学习方法高出120%的成功率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the challenge of unmeasured confounding in offline reinforcement learning, particularly when pixel-based demonstrations suffer from sensory mismatches between demonstrator and learner, leading to biased data. The method introduces a novel causal offline RL objective that optimizes for worst-case performance under confounding, implemented via flow-matching policies and a deep discriminator to measure policy discrepancies. Experimental results across 25 pixel-based tasks show that this confounding-robust approach achieves a success rate 120% higher than state-of-the-art methods unaware of confounding.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决离线强化学习中未测量的混杂问题，特别是在基于像素的演示中，演示者与学习者的感知能力不匹配会导致数据偏差。方法上，提出了一种新颖的因果离线强化学习目标，优化在混杂情况下的最差性能，通过流匹配策略和深度判别器实现以衡量策略差异。实验结果表明，在25个基于像素的任务中，这种混杂鲁棒性增强方法的成功率比未考虑混杂的先进离线强化学习方法高出120%。</div>
</details>
</div>
<div class="card">
<div class="title">From Tokens to Numbers: Continuous Number Modeling for SVG Generation</div>
<div class="meta-line">Authors: Michael Ogezi, Martin Bell, Freda Shi, Ethan Smith</div>
<div class="meta-line">First: 2026-02-02T21:20:38+00:00 · Latest: 2026-02-02T21:20:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02820v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02820v1">PDF</a> · <a href="http://github.com/mikeogezi/CNM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">For certain image generation tasks, vector graphics such as Scalable Vector Graphics (SVGs) offer clear benefits such as increased flexibility, size efficiency, and editing ease, but remain less explored than raster-based approaches. A core challenge is that the numerical, geometric parameters, which make up a large proportion of SVGs, are inefficiently encoded as long sequences of tokens. This slows training, reduces accuracy, and hurts generalization. To address these problems, we propose Continuous Number Modeling (CNM), an approach that directly models numbers as first-class, continuous values rather than discrete tokens. This formulation restores the mathematical elegance of the representation by aligning the model&#x27;s inputs with the data&#x27;s continuous nature, removing discretization artifacts introduced by token-based encoding. We then train a multimodal transformer on 2 million raster-to-SVG samples, followed by fine-tuning via reinforcement learning using perceptual feedback to further improve visual quality. Our approach improves training speed by over 30% while maintaining higher perceptual fidelity compared to alternative approaches. This work establishes CNM as a practical and efficient approach for high-quality vector generation, with potential for broader applications. We make our code available http://github.com/mikeogezi/CNM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从标记到数字：面向SVG生成的连续数值建模</div>
<div class="mono" style="margin-top:8px">对于特定图像生成任务，可缩放矢量图形（SVG）等矢量格式具有灵活性高、存储效率优、编辑便捷等明显优势，但其研究深度仍不及基于栅格的方法。核心挑战在于：构成SVG主体的数值化几何参数被低效编码为长标记序列，导致训练速度慢、精度降低且泛化能力受损。为解决这些问题，我们提出连续数值建模（CNM）方法，将数值直接作为连续的一类对象建模，而非离散标记。该方案通过使模型输入与数据的连续特性对齐，消除了基于标记编码引入的离散化伪影，从而恢复了表征的数学优雅性。我们在200万个栅格转SVG样本上训练多模态Transformer，随后通过强化学习结合感知反馈进行微调以进一步提升视觉质量。相比其他方法，本方法在保持更高感知保真度的同时，训练速度提升超30%。本研究确立了CNM作为高质量矢量生成的实用高效方法，具备更广泛的应用潜力。代码已开源：http://github.com/mikeogezi/CNM。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the inefficiency of tokenizing numerical parameters in SVG generation, which slows training and reduces accuracy, this paper introduces Continuous Number Modeling (CNM) to directly treat numbers as continuous values instead of discrete tokens, aligning the model with the data&#x27;s inherent nature. The method involves training a multimodal transformer on a large dataset of raster-to-SVG samples and fine-tuning it with reinforcement learning based on perceptual feedback. Experimental results show that CNM improves training speed by over 30% while achieving higher perceptual fidelity compared to token-based alternatives, establishing it as an efficient approach for high-quality vector graphics generation.</div>
<div class="mono" style="margin-top:8px">本文针对SVG生成中将数值参数编码为离散标记导致的训练效率低和精度下降问题，提出连续数值建模（CNM）方法，将数字直接作为连续值处理，以匹配数据的连续特性。该方法首先在大量栅格到SVG样本上训练多模态变换器，然后通过基于感知反馈的强化学习进行微调。实验结果表明，CNM在保持更高感知保真度的同时，将训练速度提升了30%以上，为高质量矢量图形生成提供了一种实用高效的方法。</div>
</details>
</div>
<div class="card">
<div class="title">SERA: Soft-Verified Efficient Repository Agents</div>
<div class="meta-line">Authors: Ethan Shen, Danny Tormoen, Saurabh Shah, Ali Farhadi, Tim Dettmers</div>
<div class="meta-line">First: 2026-01-28T17:27:08+00:00 · Latest: 2026-02-02T19:55:32+00:00</div>
<div class="meta-line">Comments: 21 main pages, 6 pages appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20789v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.20789v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2&#x27;s Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SERA：软验证高效代码库智能体</div>
<div class="mono" style="margin-top:8px">开源权重编码智能体本应具备超越闭源系统的根本优势：可针对私有代码库进行专业化训练，将仓库特定信息直接编码至权重中。然而训练成本与复杂性使这一优势长期停留于理论层面。我们证明其实用化现已可行。本文提出软验证高效代码库智能体（SERA），一种高效的编码智能体训练方法，能够快速低成本创建专用于私有代码库的智能体。仅通过监督微调（SFT），SERA在完全开源（开放数据、方法、代码）模型中取得最先进成果，同时达到前沿开源权重模型（如Devstral-Small-2）的性能水平。创建SERA模型成本比强化学习低26倍，比先前合成数据方法低57倍即可达到同等性能。我们的软验证生成（SVG）方法可从单个代码库生成数千条轨迹，结合成本效益实现私有代码库专业化。除仓库专业化外，我们将SVG应用于更大规模代码库语料，生成超20万条合成轨迹，并利用该数据集详细分析编码智能体训练的缩放规律、消融实验及混杂因素。总体而言，我们相信本工作将极大加速开源编码智能体研究，并展示可专业化适配私有代码库的开源模型优势。我们将SERA作为Ai2开源编码智能体系列的首个模型发布，同步开放全部代码、数据及Claude Code集成以支持研究社区。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind SERA is to make it practical for open-weight coding agents to specialize in private codebases, an advantage that has been theoretical due to high training costs. The method introduces Soft-Verified Efficient Repository Agents (SERA), which uses an efficient approach called Soft Verified Generation (SVG) to generate thousands of training trajectories from a single repository, enabling specialization through supervised finetuning (SFT). The main experimental results show that SERA achieves state-of-the-art performance among fully open-source models, matches the performance of frontier open-weight models like Devstral-Small-2, and is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance, while also generating over 200,000 synthetic trajectories for broader analysis.</div>
<div class="mono" style="margin-top:8px">SERA的研究动机是使开源编码智能体能够实际应用于私有代码库的专业化，这一优势此前因训练成本高昂而仅停留在理论层面。该方法提出了软验证高效仓库智能体（SERA），采用一种名为软验证生成（SVG）的高效技术，从单个代码库生成数千条训练轨迹，通过监督微调（SFT）实现专业化。主要实验结果表明，SERA在完全开源的模型中达到了最先进的性能，与Devstral-Small-2等前沿开源权重模型表现相当，且达到同等性能的成本比强化学习低26倍，比之前的合成数据方法低57倍，同时还生成了超过20万条合成轨迹用于更广泛的分析。</div>
</details>
</div>
<div class="card">
<div class="title">Automatic Design of Optimization Test Problems with Large Language Models</div>
<div class="meta-line">Authors: Wojciech Achtelik, Hubert Guzowski, Maciej Smołka, Jacek Mańdziuk</div>
<div class="meta-line">First: 2026-02-02T19:42:14+00:00 · Latest: 2026-02-02T19:42:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02724v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02724v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The development of black-box optimization algorithms depends on the availability of benchmark suites that are both diverse and representative of real-world problem landscapes. Widely used collections such as BBOB and CEC remain dominated by hand-crafted synthetic functions and provide limited coverage of the high-dimensional space of Exploratory Landscape Analysis (ELA) features, which in turn biases evaluation and hinders training of meta-black-box optimizers. We introduce Evolution of Test Functions (EoTF), a framework that automatically generates continuous optimization test functions whose landscapes match a specified target ELA feature vector. EoTF adapts LLM-driven evolutionary search, originally proposed for heuristic discovery, to evolve interpretable, self-contained numpy implementations of objective functions by minimizing the distance between sampled ELA features of generated candidates and a target profile. In experiments on 24 noiseless BBOB functions and a contamination-mitigating suite of 24 MA-BBOB hybrid functions, EoTF reliably produces non-trivial functions with closely matching ELA characteristics and preserves optimizer performance rankings under fixed evaluation budgets, supporting their validity as surrogate benchmarks. While a baseline neural-network-based generator achieves higher accuracy in 2D, EoTF substantially outperforms it in 3D and exhibits stable solution quality as dimensionality increases, highlighting favorable scalability. Overall, EoTF offers a practical route to scalable, portable, and interpretable benchmark generation targeted to desired landscape properties.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型的优化测试问题自动设计</div>
<div class="mono" style="margin-top:8px">黑盒优化算法的发展依赖于兼具多样性和真实问题场景代表性的基准测试集。当前广泛使用的BBOB、CEC等基准库仍以人工设计的合成函数为主，对探索性景观分析特征的高维空间覆盖有限，导致评估偏差并阻碍元黑盒优化器的训练。本文提出测试函数演化框架，通过自动生成景观特征匹配目标ELA特征向量的连续优化测试函数。该框架将最初用于启发式发现的LLM驱动进化搜索进行适配，通过最小化生成函数的采样ELA特征与目标特征间的距离，演化出可解释、自包含的numpy目标函数实现。在24个无噪声BBOB函数和24个MA-BBOB混合函数的实验中，该框架能稳定生成具有高度匹配ELA特征的非平凡函数，并在固定评估预算下保持优化器性能排序，验证了其作为替代基准的有效性。虽然基于神经网络的基线方法在二维场景中精度更高，但本框架在三维场景中显著优于基线，且随着维度增加保持稳定的解质量，展现出良好的可扩展性。总体而言，该框架为实现面向目标景观特性的可扩展、可移植、可解释的基准生成提供了实用路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limited diversity and representativeness of hand-crafted synthetic functions in existing benchmark suites like BBOB and CEC, which bias optimization algorithm evaluation, this paper introduces the Evolution of Test Functions (EoTF) framework to automatically generate continuous optimization test functions. The method adapts LLM-driven evolutionary search to evolve interpretable, self-contained numpy implementations of objective functions, minimizing the distance between sampled Exploratory Landscape Analysis (ELA) features of candidates and a target profile. Experimental results on 24 noiseless BBOB functions and 24 MA-BBOB hybrid functions show that EoTF reliably produces non-trivial functions with closely matching ELA characteristics, preserves optimizer performance rankings, and substantially outperforms a neural-network-based baseline in 3D with stable scalability as dimensionality increases, offering a practical route to targeted benchmark generation.</div>
<div class="mono" style="margin-top:8px">针对现有基准测试套件（如BBOB和CEC）中手工合成的测试函数多样性不足、代表性有限，导致优化算法评估存在偏差的问题，本文提出了测试函数进化（EoTF）框架，以自动生成连续优化测试函数。该方法采用基于大语言模型的进化搜索，演化出可解释、自包含的numpy目标函数实现，通过最小化生成候选函数的探索性景观分析（ELA）特征与目标特征向量之间的距离来匹配指定景观特性。在24个无噪声BBOB函数和24个MA-BBOB混合函数上的实验结果表明，EoTF能可靠地生成具有紧密匹配ELA特征的非平凡函数，保持优化器性能排序，且在3D中显著优于基于神经网络的基线方法，并随着维度增加展现出稳定的可扩展性，为针对特定景观属性的基准生成提供了实用途径。</div>
</details>
</div>
<div class="card">
<div class="title">Hierarchical Entity-centric Reinforcement Learning with Factored Subgoal Diffusion</div>
<div class="meta-line">Authors: Dan Haramati, Carl Qi, Tal Daniel, Amy Zhang, Aviv Tamar, George Konidaris</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-02T19:40:54+00:00 · Latest: 2026-02-02T19:40:54+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02722v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02722v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/view/hecrl">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a hierarchical entity-centric framework for offline Goal-Conditioned Reinforcement Learning (GCRL) that combines subgoal decomposition with factored structure to solve long-horizon tasks in domains with multiple entities. Achieving long-horizon goals in complex environments remains a core challenge in Reinforcement Learning (RL). Domains with multiple entities are particularly difficult due to their combinatorial complexity. GCRL facilitates generalization across goals and the use of subgoal structure, but struggles with high-dimensional observations and combinatorial state-spaces, especially under sparse reward. We employ a two-level hierarchy composed of a value-based GCRL agent and a factored subgoal-generating conditional diffusion model. The RL agent and subgoal generator are trained independently and composed post hoc through selective subgoal generation based on the value function, making the approach modular and compatible with existing GCRL algorithms. We introduce new variations to benchmark tasks that highlight the challenges of multi-entity domains, and show that our method consistently boosts performance of the underlying RL agent on image-based long-horizon tasks with sparse rewards, achieving over 150% higher success rates on the hardest task in our suite and generalizing to increasing horizons and numbers of entities. Rollout videos are provided at: https://sites.google.com/view/hecrl</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于因子化子目标扩散的层次化实体中心强化学习</div>
<div class="mono" style="margin-top:8px">本文提出一种面向离线目标条件强化学习的层次化实体中心框架，通过结合子目标分解与因子化结构，解决多实体领域中的长时程任务。在复杂环境中实现长时程目标仍是强化学习的核心挑战，多实体领域因其组合复杂性尤为困难。目标条件强化学习虽能促进目标泛化与子目标结构利用，但在高维观测与组合状态空间（尤其是稀疏奖励下）仍面临困难。我们采用基于价值的目标条件强化学习智能体与因子化子目标生成条件扩散模型构成的双层架构。强化学习智能体与子目标生成器独立训练，并通过基于价值函数的选择性子目标生成进行事后组合，使方法具备模块化特性且兼容现有目标条件强化学习算法。我们引入凸显多实体领域挑战的新基准任务变体，实验表明本方法在基于图像、奖励稀疏的长时程任务中持续提升底层强化学习智能体性能，在任务套件中最难任务上实现超过150%的成功率提升，并能泛化至更长的时程与更多实体数量。演示视频见：https://sites.google.com/view/hecrl</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of achieving long-horizon goals in complex reinforcement learning environments with multiple entities, where combinatorial complexity and sparse rewards hinder performance. The proposed method introduces a hierarchical, entity-centric framework that combines a value-based goal-conditioned RL agent with a factored subgoal-generating conditional diffusion model; these components are trained independently and then composed through selective subgoal generation guided by the value function. Experimental results on new benchmark tasks for multi-entity domains demonstrate that this approach significantly boosts the underlying agent&#x27;s performance, achieving over 150% higher success rates on the hardest image-based tasks and showing strong generalization to longer horizons and increased entity counts.</div>
<div class="mono" style="margin-top:8px">本文旨在解决具有多个实体的复杂强化学习环境中实现长期目标的挑战，其中组合复杂性和稀疏奖励阻碍了性能。所提出的方法引入了一个分层的、以实体为中心的框架，结合了基于价值的目标条件强化学习智能体和一个分解的子目标生成条件扩散模型；这些组件被独立训练，然后通过基于价值函数的选择性子目标生成进行组合。在多实体领域的新基准任务上的实验结果表明，该方法显著提升了基础智能体的性能，在最困难的基于图像的任务上实现了超过150%的成功率提升，并展现出对更长任务周期和更多实体数量的良好泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Maximum Likelihood Reinforcement Learning</div>
<div class="meta-line">Authors: Fahim Tajwar, Guanning Zeng, Yueer Zhou, Yuda Song, Daman Arora, Yiding Jiang, Jeff Schneider, Ruslan Salakhutdinov, Haiwen Feng, Andrea Zanette</div>
<div class="meta-line">First: 2026-02-02T19:23:42+00:00 · Latest: 2026-02-02T19:23:42+00:00</div>
<div class="meta-line">Comments: Project website and code: https://zanette-labs.github.io/MaxRL/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02710v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02710v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://zanette-labs.github.io/MaxRL/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning is the method of choice to train models in sampling-based setups with binary outcome feedback, such as navigation, code generation, and mathematical problem solving. In such settings, models implicitly induce a likelihood over correct rollouts. However, we observe that reinforcement learning does not maximize this likelihood, and instead optimizes only a lower-order approximation. Inspired by this observation, we introduce Maximum Likelihood Reinforcement Learning (MaxRL), a sampling-based framework to approximate maximum likelihood using reinforcement learning techniques. MaxRL addresses the challenges of non-differentiable sampling by defining a compute-indexed family of sample-based objectives that interpolate between standard reinforcement learning and exact maximum likelihood as additional sampling compute is allocated. The resulting objectives admit a simple, unbiased policy-gradient estimator and converge to maximum likelihood optimization in the infinite-compute limit. Empirically, we show that MaxRL Pareto-dominates existing methods in all models and tasks we tested, achieving up to 20x test-time scaling efficiency gains compared to its GRPO-trained counterpart. We also observe MaxRL to scale better with additional data and compute. Our results suggest MaxRL is a promising framework for scaling RL training in correctness based settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>最大似然强化学习</div>
<div class="mono" style="margin-top:8px">强化学习是基于采样的二元结果反馈场景（如导航、代码生成和数学问题求解）中训练模型的首选方法。在此类设置中，模型隐式地诱导出正确轨迹的似然分布。然而，我们观察到强化学习并未最大化该似然，而仅优化了低阶近似。受此启发，我们提出最大似然强化学习（MaxRL），这是一个基于采样的框架，利用强化学习技术逼近最大似然。MaxRL通过定义一系列计算索引的基于采样的目标函数，解决了不可微分采样的挑战——这些目标函数在标准强化学习与精确最大似然之间平滑过渡，并随额外计算资源的分配逐步逼近后者。所得目标函数支持简单、无偏的策略梯度估计器，并在无限计算极限下收敛至最大似然优化。实验表明，MaxRL在我们测试的所有模型和任务中均帕累托占优于现有方法，相比基于GRPO训练的对照模型，其测试时缩放效率提升最高达20倍。我们还观察到MaxRL能更好地随数据和计算资源的增加而扩展。结果表明，MaxRL是基于正确性场景中扩展强化学习训练的前瞻性框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the observation that standard reinforcement learning does not maximize the true likelihood over correct rollouts in sampling-based tasks with binary feedback, this paper introduces Maximum Likelihood Reinforcement Learning (MaxRL). The method defines a family of sample-based objectives that bridge reinforcement learning and exact maximum likelihood, using a simple unbiased policy-gradient estimator that converges to maximum likelihood optimization with increased compute. Experimental results demonstrate that MaxRL Pareto-dominates existing methods across tested models and tasks, achieving up to 20x gains in test-time scaling efficiency and showing improved scalability with data and compute.</div>
<div class="mono" style="margin-top:8px">本文的动机是观察到标准强化学习在基于采样的二元反馈任务中并未最大化正确轨迹的真实似然，因此提出了最大似然强化学习（MaxRL）。该方法定义了一系列基于采样的目标函数，连接了强化学习与精确最大似然，采用一种简单无偏的策略梯度估计器，在计算增加时收敛至最大似然优化。实验结果表明，MaxRL在所测试的所有模型和任务中帕累托占优于现有方法，实现了高达20倍的测试时缩放效率提升，并展现出更好的数据和计算可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">BinaryPPO: Efficient Policy Optimization for Binary Classification</div>
<div class="meta-line">Authors: Punya Syon Pandey, Zhijing Jin</div>
<div class="meta-line">First: 2026-02-02T19:22:45+00:00 · Latest: 2026-02-02T19:22:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02708v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02708v1">PDF</a> · <a href="https://github.com/psyonp/BinaryPPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Supervised fine-tuning (SFT) is the standard approach for binary classification tasks such as toxicity detection, factuality verification, and causal inference. However, SFT often performs poorly in real-world settings with label noise, class imbalance, or sparse supervision. We introduce BinaryPPO, an offline reinforcement learning large language model (LLM) framework that reformulates binary classification as a reward maximization problem. Our method leverages a variant of Proximal Policy Optimization (PPO) with a confidence-weighted reward function that penalizes uncertain or incorrect predictions, enabling the model to learn robust decision policies from static datasets without online interaction. Across eight domain-specific benchmarks and multiple models with differing architectures, BinaryPPO improves accuracy by 40-60 percentage points, reaching up to 99%, substantially outperforming supervised baselines. We provide an in-depth analysis of the role of reward shaping, advantage scaling, and policy stability in enabling this improvement. Overall, we demonstrate that confidence-based reward design provides a robust alternative to SFT for binary classification. Our code is available at https://github.com/psyonp/BinaryPPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BinaryPPO：面向二元分类的高效策略优化方法</div>
<div class="mono" style="margin-top:8px">监督微调（SFT）是处理毒性检测、事实性验证和因果推断等二元分类任务的标准方法，但在存在标签噪声、类别不平衡或稀疏监督的实际场景中往往表现不佳。本文提出BinaryPPO——一种将二元分类重构为奖励最大化问题的离线强化学习大语言模型框架。该方法采用改进的近端策略优化算法，通过置信度加权的奖励函数对不确定或错误的预测进行惩罚，使模型能够从静态数据集中学习稳健的决策策略而无需在线交互。在八个领域特定基准测试及多种架构模型的实验中，BinaryPPO将准确率提升40-60个百分点，最高可达99%，显著超越监督基线。我们深入分析了奖励塑形、优势缩放和策略稳定性对性能提升的作用机制。研究表明，基于置信度的奖励设计为二元分类提供了比SFT更稳健的替代方案。代码已开源：https://github.com/psyonp/BinaryPPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the limitations of supervised fine-tuning (SFT) for binary classification in noisy, imbalanced, or sparsely supervised real-world settings. It introduces BinaryPPO, an offline reinforcement learning framework that reformulates binary classification as a reward maximization problem, using a variant of Proximal Policy Optimization (PPO) with a confidence-weighted reward function to penalize uncertain or incorrect predictions and learn robust policies from static datasets. Experimental results across eight domain-specific benchmarks show that BinaryPPO improves accuracy by 40-60 percentage points, reaching up to 99%, substantially outperforming supervised baselines, with analysis highlighting the importance of reward shaping, advantage scaling, and policy stability.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决监督微调（SFT）在标签噪声、类别不平衡或稀疏监督的现实场景中进行二分类时的性能局限。方法上提出了BinaryPPO，一种离线强化学习框架，将二分类重新定义为奖励最大化问题，采用近端策略优化（PPO）变体，通过置信度加权的奖励函数惩罚不确定或错误的预测，从而从静态数据集中学习鲁棒的决策策略。在八个领域特定基准测试中，BinaryPPO将准确率提升了40-60个百分点，最高达到99%，显著优于监督基线，并通过深入分析揭示了奖励塑造、优势缩放和策略稳定性在此改进中的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System</div>
<div class="meta-line">Authors: Yinjie Wang, Tianbao Xie, Ke Shen, Mengdi Wang, Ling Yang</div>
<div class="meta-line">First: 2026-02-02T18:59:04+00:00 · Latest: 2026-02-02T18:59:04+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/Gen-Verse/Open-AgentRL</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02488v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02488v1">PDF</a> · <a href="https://github.com/Gen-Verse/Open-AgentRL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RLAnything：在完全动态强化学习系统中锻造环境、策略与奖励模型</div>
<div class="mono" style="margin-top:8px">我们提出RLAnything，一种通过闭环优化动态锻造环境、策略和奖励模型的强化学习框架，可增强学习信号并强化适用于任何大语言模型或智能体场景的整体强化学习系统。具体而言，策略通过整合来自逐步信号与结果信号的反馈进行训练，而奖励模型则通过一致性反馈联合优化，进而进一步提升策略训练效果。此外，我们基于理论驱动的自动环境适应机制，通过利用来自奖励模型与策略模型的批评反馈，提升两者的训练效果，实现从经验中学习。实证表明，每个新增组件均持续提升系统整体性能，RLAnything在多种代表性大语言模型与智能体任务中取得显著增益：在OSWorld上将Qwen3-VL-8B-Thinking提升9.1%，在AlfWorld和LiveBench上分别将Qwen2.5-7B-Instruct提升18.7%和11.9%。我们还发现，优化后的奖励模型信号优于依赖人工标注的结果。代码：https://github.com/Gen-Verse/Open-AgentRL</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces RLAnything, a reinforcement learning framework designed to enhance learning in LLM and agentic scenarios by dynamically co-optimizing environment, policy, and reward models through closed-loop feedback. The method integrates step-wise and outcome signals for policy training, jointly optimizes the reward model via consistency feedback, and employs theory-driven automatic environment adaptation using critic feedback to improve learning from experience. Experimental results show that each component contributes to system improvement, with RLAnything achieving significant performance gains, such as boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, while also demonstrating that optimized reward signals surpass those based on human labels.</div>
<div class="mono" style="margin-top:8px">本文提出了RLAnything，这是一个强化学习框架，旨在通过闭环反馈动态协同优化环境、策略和奖励模型，以增强大语言模型和智能体场景中的学习效果。该方法整合了逐步和结果信号进行策略训练，通过一致性反馈联合优化奖励模型，并利用理论驱动的自动环境适应机制，借助批评者反馈从经验中学习。实验结果表明，每个组件均提升了系统性能，RLAnything取得了显著的效果提升，例如在OSWorld上将Qwen3-VL-8B-Thinking的性能提高了9.1%，在AlfWorld和LiveBench上将Qwen2.5-7B-Instruct的性能分别提高了18.7%和11.9%，同时证明优化后的奖励信号优于依赖人工标签的结果。</div>
</details>
</div>
<div class="card">
<div class="title">Expanding the Capabilities of Reinforcement Learning via Text Feedback</div>
<div class="meta-line">Authors: Yuda Song, Lili Chen, Fahim Tajwar, Remi Munos, Deepak Pathak, J. Andrew Bagnell, Aarti Singh, Andrea Zanette</div>
<div class="meta-line">First: 2026-02-02T18:56:56+00:00 · Latest: 2026-02-02T18:56:56+00:00</div>
<div class="meta-line">Comments: 43 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02482v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02482v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label. At the other extreme, distillation offers dense supervision but requires demonstrations, which are costly and difficult to scale. We study text feedback as an intermediate signal: richer than scalar rewards, yet cheaper than complete demonstrations. Textual feedback is a natural mode of human interaction and is already abundant in many real-world settings, where users, annotators, and automated judges routinely critique LLM outputs. Towards leveraging text feedback at scale, we formalize a multi-turn RL setup, RL from Text Feedback (RLTF), where text feedback is available during training but not at inference. Therefore, models must learn to internalize the feedback in order to improve their test-time single-turn performance. To do this, we propose two methods: Self Distillation (RLTF-SD), which trains the single-turn policy to match its own feedback-conditioned second-turn generations; and Feedback Modeling (RLTF-FM), which predicts the feedback as an auxiliary objective. We provide theoretical analysis on both methods, and empirically evaluate on reasoning puzzles, competition math, and creative writing tasks. Our results show that both methods consistently outperform strong baselines across benchmarks, highlighting the potential of RL with an additional source of rich supervision at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过文本反馈拓展强化学习能力</div>
<div class="mono" style="margin-top:8px">强化学习在大型语言模型后训练中的成功源于一个信息量极低的来源：每次训练轮次仅提供单比特信息，如二元奖励或偏好标签。另一极端，蒸馏方法虽能提供密集监督，但需要成本高昂且难以扩展的演示样本。本研究将文本反馈作为一种中间信号进行探索：它比标量奖励更丰富，又比完整演示更经济。文本反馈是人类互动的自然模式，已在众多现实场景中广泛存在，用户、标注者和自动评估系统常以此评判模型输出。为规模化利用文本反馈，我们形式化了一个多轮强化学习框架——基于文本反馈的强化学习（RLTF），其中训练阶段可获得文本反馈，但推理阶段则无。因此，模型必须学会内化反馈以提升测试时的单轮表现。为此，我们提出两种方法：自蒸馏（RLTF-SD），训练单轮策略以匹配其自身基于反馈的第二轮生成；以及反馈建模（RLTF-FM），通过预测反馈作为辅助目标。我们对两种方法进行了理论分析，并在推理谜题、竞赛数学和创意写作任务上进行了实证评估。结果表明，两种方法在多个基准测试中均持续优于强基线，凸显了强化学习结合规模化丰富监督源的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of sparse binary rewards and costly demonstrations in reinforcement learning for large language models, this paper explores text feedback as a richer yet scalable intermediate supervision signal. The method formalizes RL from Text Feedback (RLTF), a multi-turn setup where models receive textual critiques during training but must internalize them for improved single-turn inference, proposing two approaches: Self Distillation (RLTF-SD) to align the policy with its feedback-conditioned generations, and Feedback Modeling (RLTF-FM) to predict feedback as an auxiliary task. Experimental results on reasoning puzzles, competition math, and creative writing tasks demonstrate that both methods consistently outperform strong baselines, underscoring the potential of leveraging textual feedback for enhanced RL performance.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决强化学习在大语言模型微调中面临的二元奖励信息稀疏和演示数据成本高昂的局限，探索将文本反馈作为一种更丰富且可扩展的中间监督信号。研究方法形式化了基于文本反馈的强化学习框架，即在训练中提供多轮文本反馈，使模型必须内化这些反馈以提升单轮推理性能，并提出了两种方法：自我蒸馏通过训练单轮策略匹配其反馈条件下的第二轮生成，以及反馈建模将预测反馈作为辅助目标。在推理谜题、竞赛数学和创意写作任务上的实验结果表明，这两种方法均持续优于强基线，凸显了利用文本反馈提升强化学习性能的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Conflict-Aware Client Selection for Multi-Server Federated Learning</div>
<div class="meta-line">Authors: Mingwei Hong, Zheng Lin, Zehang Lin, Lin Li, Miao Yang, Xia Du, Zihan Fang, Zhaolu Kang, Dianxin Luan, Shunzhi Zhu</div>
<div class="meta-line">First: 2026-02-02T18:47:16+00:00 · Latest: 2026-02-02T18:47:16+00:00</div>
<div class="meta-line">Comments: 6 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02458v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02458v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated learning (FL) has emerged as a promising distributed machine learning (ML) that enables collaborative model training across clients without exposing raw data, thereby preserving user privacy and reducing communication costs. Despite these benefits, traditional single-server FL suffers from high communication latency due to the aggregation of models from a large number of clients. While multi-server FL distributes workloads across edge servers, overlapping client coverage and uncoordinated selection often lead to resource contention, causing bandwidth conflicts and training failures. To address these limitations, we propose a decentralized reinforcement learning with conflict risk prediction, named RL CRP, to optimize client selection in multi-server FL systems. Specifically, each server estimates the likelihood of client selection conflicts using a categorical hidden Markov model based on its sparse historical client selection sequence. Then, a fairness-aware reward mechanism is incorporated to promote long-term client participation for minimizing training latency and resource contention. Extensive experiments demonstrate that the proposed RL-CRP framework effectively reduces inter-server conflicts and significantly improves training efficiency in terms of convergence speed and communication cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向多服务器联邦学习的冲突感知客户端选择</div>
<div class="mono" style="margin-top:8px">联邦学习作为一种分布式机器学习范式，支持客户端在不暴露原始数据的情况下协同训练模型，从而保护用户隐私并降低通信开销。然而传统单服务器联邦学习因需聚合海量客户端模型而存在高通信延迟问题。多服务器联邦学习虽能将负载分散至边缘服务器，但客户端覆盖重叠与选择失序常引发资源竞争，导致带宽冲突与训练失败。为此，我们提出一种融合冲突风险预测的去中心化强化学习方法（RL-CRP），以优化多服务器联邦学习系统中的客户端选择。具体而言，各服务器基于稀疏历史选择序列，通过分类隐马尔可夫模型预测客户端选择冲突概率，并引入公平感知奖励机制以促进客户端长期参与，从而最小化训练延迟与资源竞争。大量实验表明，所提RL-CRP框架能有效降低服务器间冲突，在收敛速度与通信成本方面显著提升训练效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the communication latency and resource contention issues in traditional single-server and uncoordinated multi-server federated learning (FL), where overlapping client coverage can lead to bandwidth conflicts and training failures. To address this, the authors propose RL-CRP, a method that uses decentralized reinforcement learning combined with a conflict risk predictor, which employs a categorical hidden Markov model to estimate client selection conflicts from sparse historical sequences and incorporates a fairness-aware reward to encourage long-term client participation. Experimental results show that the RL-CRP framework effectively reduces inter-server conflicts and significantly improves training efficiency, including faster convergence and lower communication costs.</div>
<div class="mono" style="margin-top:8px">本文的动机在于解决传统单服务器联邦学习中的高通信延迟以及多服务器联邦学习中因客户端覆盖重叠和选择不协调导致的资源竞争问题，如带宽冲突和训练失败。为此，作者提出了RL-CRP方法，该方法采用去中心化强化学习结合冲突风险预测，利用分类隐马尔可夫模型基于稀疏的历史客户端选择序列来估计冲突风险，并引入公平感知奖励机制以促进客户端的长期参与。实验结果表明，RL-CRP框架能有效减少服务器间冲突，并在收敛速度和通信成本方面显著提升训练效率。</div>
</details>
</div>
<div class="card">
<div class="title">World-Gymnast: Training Robots with Reinforcement Learning in a World Model</div>
<div class="meta-line">Authors: Ansh Kumar Sharma, Yixiang Sun, Ninghao Lu, Yunzhe Zhang, Jiarao Liu, Sherry Yang</div>
<div class="meta-line">First: 2026-02-02T18:44:45+00:00 · Latest: 2026-02-02T18:44:45+00:00</div>
<div class="meta-line">Comments: https://world-gymnast.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02454v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02454v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://world-gymnast.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models learned from real-world video-action data, we ask the question of whether training a policy in a world model can be more effective than supervised learning or software simulation in achieving better real-robot performance. We propose World-Gymnast, which performs RL finetuning of a vision-language-action (VLA) policy by rolling out the policy in an action-conditioned video world model and rewarding the rollouts with a vision-language model (VLM). On the Bridge robot setup, World-Gymnast outperforms SFT by as much as 18x and outperforms software simulator by as much as 2x. More importantly, World-Gymnast demonstrates intriguing capabilities of RL with a world model, including training on diverse language instructions and novel scenes from the world model, test-time training in a novel scene, and online iterative world model and policy improvement. Our results suggest learning a world model and training robot policies in the cloud could be the key to bridging the gap between robots that work in demonstrations and robots that can work in anyone&#x27;s household.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>World-Gymnast：在世界模型中通过强化学习训练机器人</div>
<div class="mono" style="margin-top:8px">机器人与物理世界交互学习从根本上受限于物理交互成本。两种替代方案——基于专家演示的监督微调（SFT）与基于软件模拟器的强化学习（RL）——分别受限于可用专家数据量和操作任务的仿真到现实差距。随着从真实世界视频-动作数据学习的世界模型近期兴起，我们探讨在世界模型中训练策略是否比监督学习或软件仿真更能有效提升真实机器人性能。我们提出World-Gymnast方法，通过在动作条件化视频世界模型中展开策略、并利用视觉语言模型（VLM）对推演结果进行奖励，实现对视觉-语言-动作（VLA）策略的强化学习微调。在Bridge机器人实验平台上，World-Gymnast性能超越SFT达18倍，超越软件模拟器达2倍。更重要的是，该方法揭示了世界模型强化学习的独特能力：包括基于世界模型对多样化语言指令和新场景进行训练、在新场景中进行测试时训练、以及在线迭代优化世界模型与策略。我们的研究表明，学习世界模型并在云端训练机器人策略，可能是弥合演示型机器人与家用普适型机器人之间鸿沟的关键。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the high cost and limitations of physical robot interaction, supervised fine-tuning from demonstrations, and the sim-to-real gap in software simulators, this paper investigates whether training a policy within a learned world model can yield superior real-robot performance. The method, World-Gymnast, employs reinforcement learning to fine-tune a vision-language-action policy by rolling it out in an action-conditioned video world model and evaluating the rollouts using a vision-language model as a reward function. Experimental results on the Bridge robot setup show that World-Gymnast outperforms supervised fine-tuning by up to 18 times and software simulation by up to 2 times, while also demonstrating capabilities like training on diverse language instructions, adapting to novel scenes, and enabling online iterative improvement of both the world model and policy.</div>
<div class="mono" style="margin-top:8px">本文的动机在于解决机器人物理交互成本高昂、专家演示数据有限以及软件模拟器存在仿真到现实差距的问题，旨在探究在世界模型中训练策略是否能获得更优的真实机器人性能。该方法名为World-Gymnast，它通过强化学习对视觉-语言-动作策略进行微调，具体做法是在一个动作条件化的视频世界模型中展开策略，并使用视觉语言模型作为奖励函数来评估这些展开。在Bridge机器人实验装置上的主要结果表明，World-Gymnast的性能最高可达监督微调的18倍和软件模拟的2倍，同时展现了基于多样化语言指令进行训练、适应新场景以及实现世界模型与策略在线迭代改进等引人关注的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Maximizing Reliability with Bayesian Optimization</div>
<div class="meta-line">Authors: Jack M. Buckingham, Ivo Couckuyt, Juergen Branke</div>
<div class="meta-line">First: 2026-02-02T18:31:58+00:00 · Latest: 2026-02-02T18:31:58+00:00</div>
<div class="meta-line">Comments: 25 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02432v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02432v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bayesian optimization (BO) is a popular, sample-efficient technique for expensive, black-box optimization. One such problem arising in manufacturing is that of maximizing the reliability, or equivalently minimizing the probability of a failure, of a design which is subject to random perturbations - a problem that can involve extremely rare failures ($P_\mathrm{fail} = 10^{-6}-10^{-8}$). In this work, we propose two BO methods based on Thompson sampling and knowledge gradient, the latter approximating the one-step Bayes-optimal policy for minimizing the logarithm of the failure probability. Both methods incorporate importance sampling to target extremely small failure probabilities. Empirical results show the proposed methods outperform existing methods in both extreme and non-extreme regimes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于贝叶斯优化的可靠性最大化方法</div>
<div class="mono" style="margin-top:8px">贝叶斯优化是一种针对高成本黑箱优化问题的样本高效方法。在制造业中，常需在随机扰动下最大化设计可靠性（即最小化失效概率），这类问题可能涉及极低失效概率（$P_\mathrm{fail} = 10^{-6}-10^{-8}$）。本研究提出两种基于汤普森采样与知识梯度的贝叶斯优化方法，后者通过近似一步贝叶斯最优策略来最小化失效概率对数。两种方法均采用重要性采样以应对极低失效概率场景。实验结果表明，所提方法在极端与非极端工况下均优于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of maximizing design reliability in manufacturing, where failure probabilities can be extremely rare (e.g., 10^-6 to 10^-8), this paper introduces two Bayesian optimization methods based on Thompson sampling and knowledge gradient to efficiently minimize the logarithm of failure probability. The methods incorporate importance sampling to effectively target these small probabilities. Experimental results demonstrate that the proposed approaches outperform existing methods across both extreme and non-extreme failure probability regimes.</div>
<div class="mono" style="margin-top:8px">本文针对制造业中设计可靠性最大化问题，其中失效概率极低（如10^-6至10^-8），提出了两种基于汤普森采样和知识梯度的贝叶斯优化方法，以高效最小化失效概率的对数值。这些方法结合了重要性采样技术，以有效处理极低概率事件。实验结果表明，所提方法在极端和非极端失效概率场景下均优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement Learning</div>
<div class="meta-line">Authors: Samuel Nellessen, Tal Kachman</div>
<div class="meta-line">First: 2026-02-02T17:56:55+00:00 · Latest: 2026-02-02T17:56:55+00:00</div>
<div class="meta-line">Comments: Under review. 8 main pages, 2 figures, 2 tables. Appendix included</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02395v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02395v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The evolution of large language models into autonomous agents introduces adversarial failures that exploit legitimate tool privileges, transforming safety evaluation in tool-augmented environments from a subjective NLP task into an objective control problem. We formalize this threat model as Tag-Along Attacks: a scenario where a tool-less adversary &quot;tags along&quot; on the trusted privileges of a safety-aligned Operator to induce prohibited tool use through conversation alone. To validate this threat, we present Slingshot, a &#x27;cold-start&#x27; reinforcement learning framework that autonomously discovers emergent attack vectors, revealing a critical insight: in our setting, learned attacks tend to converge to short, instruction-like syntactic patterns rather than multi-turn persuasion. On held-out extreme-difficulty tasks, Slingshot achieves a 67.0% success rate against a Qwen2.5-32B-Instruct-AWQ Operator (vs. 1.7% baseline), reducing the expected attempts to first success (on solved tasks) from 52.3 to 1.3. Crucially, Slingshot transfers zero-shot to several model families, including closed-source models like Gemini 2.5 Flash (56.0% attack success rate) and defensive-fine-tuned open-source models like Meta-SecAlign-8B (39.2% attack success rate). Our work establishes Tag-Along Attacks as a first-class, verifiable threat model and shows that effective agentic attacks can be elicited from off-the-shelf open-weight models through environment interaction alone.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大卫对阵歌利亚：基于强化学习的可验证智能体间越狱攻击</div>
<div class="mono" style="margin-top:8px">大型语言模型向自主智能体的演进引入了利用合法工具权限的对抗性失效，将工具增强环境中的安全评估从主观自然语言处理任务转变为客观控制问题。我们将此威胁模型形式化为&#x27;尾随攻击&#x27;：无工具攻击者通过对话&#x27;尾随&#x27;安全对齐操作者的受信权限，仅通过交谈诱导其进行被禁止的工具使用。为验证该威胁，我们提出&#x27;弹弓&#x27;框架——一种&#x27;冷启动&#x27;强化学习方法，能自主发现涌现的攻击向量，揭示关键洞见：在我们的设定中，习得攻击倾向于收敛为简短、类指令的句法模式，而非多轮说服。在保留的极端难度任务上，弹弓对Qwen2.5-32B-Instruct-AWQ操作者的攻击成功率达67.0%（基线为1.7%），将首次成功所需预期尝试次数（在已破解任务中）从52.3次降至1.3次。关键的是，弹弓能零样本迁移至多个模型家族，包括Gemini 2.5 Flash等闭源模型（56.0%攻击成功率）和Meta-SecAlign-8B等防御性微调开源模型（39.2%攻击成功率）。本研究确立了尾随攻击作为一类可验证的一级威胁模型，并证明仅通过环境交互即可从现成的开放权重模型中引发有效的智能体攻击。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the security risks posed by autonomous agents exploiting tool privileges, this paper formalizes Tag-Along Attacks, where a tool-less adversary manipulates a safety-aligned agent into prohibited actions through conversation. The method introduces Slingshot, a reinforcement learning framework that autonomously discovers attack vectors, revealing that effective attacks converge to short, instruction-like patterns rather than complex persuasion. Experimental results show Slingshot achieves a 67.0% success rate against a Qwen2.5-32B-Instruct-AWQ operator, drastically reducing attempts to first success, and demonstrates strong zero-shot transferability to both closed-source and defensively fine-tuned models.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决自主智能体利用工具权限带来的安全风险，提出了“跟随攻击”威胁模型，即无工具对手通过对话操纵安全对齐的操作员执行禁止操作。方法上引入了Slingshot强化学习框架，能自主发现攻击向量，揭示有效攻击往往收敛为简短指令式模式而非多轮说服。实验结果表明，Slingshot对Qwen2.5-32B-Instruct-AWQ操作员的攻击成功率达67.0%，显著降低了首次成功所需尝试次数，并在闭源和防御性微调模型上展现出强大的零样本迁移能力。</div>
</details>
</div>
<div class="card">
<div class="title">Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data</div>
<div class="meta-line">Authors: Yuval Ran-Milo, Yotam Alexander, Shahar Mendel, Nadav Cohen</div>
<div class="meta-line">First: 2026-01-21T16:36:19+00:00 · Latest: 2026-02-02T17:56:01+00:00</div>
<div class="meta-line">Comments: 87 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15158v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.15158v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformers trained via Reinforcement Learning (RL) with outcome-based supervision can spontaneously develop the ability to generate intermediate reasoning steps (Chain-of-Thought). Yet the mechanism by which sparse rewards drive policy gradient to discover such systematic reasoning remains poorly understood. We address this by analyzing the policy gradient dynamics of single-layer Transformers on a synthetic graph traversal task that cannot be solved without Chain-of-Thought but admits a simple iterative solution. We prove that despite training solely on final-answer correctness, policy gradient drives the Transformer to converge to a structured, interpretable algorithm that iteratively traverses the graph vertex-by-vertex. We characterize the distributional properties required for this emergence, identifying the critical role of &quot;simple examples&quot;: instances requiring fewer reasoning steps. When the training distribution places sufficient mass on these simpler examples, the Transformer learns a generalizable traversal strategy that extrapolates to longer chains; when this mass vanishes, policy gradient learning becomes infeasible. We corroborate our theoretical results through experiments on synthetic data and with real-world language models on mathematical reasoning tasks, validating that our theoretical findings carry over to practical settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于结果的强化学习可证明引导Transformer进行推理，但仅适用于特定数据</div>
<div class="mono" style="margin-top:8px">通过基于结果的监督进行强化学习训练的Transformer能够自发产生中间推理步骤（思维链）。然而，稀疏奖励如何驱动策略梯度发现这种系统性推理的机制仍不明确。我们通过分析单层Transformer在合成图遍历任务上的策略梯度动态来研究此问题，该任务必须依赖思维链才能解决，但允许简单的迭代解法。我们证明，尽管仅基于最终答案正确性进行训练，策略梯度仍能驱动Transformer收敛至一种结构化、可解释的逐顶点迭代遍历算法。我们刻画了这种能力涌现所需的分布特性，指出“简单样本”（需要较少推理步骤的实例）的关键作用：当训练分布中此类简单样本具有足够权重时，Transformer能学习可泛化的遍历策略并外推至更长推理链；若此类样本缺失，策略梯度学习将无法实现。我们通过合成数据实验及真实数学推理任务上的语言模型实验验证了理论结果，证明理论发现适用于实际场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates how Transformers trained with outcome-based reinforcement learning (RL) can develop systematic reasoning capabilities like Chain-of-Thought. The authors analyze policy gradient dynamics on a synthetic graph traversal task that necessitates intermediate reasoning steps, proving that despite receiving only sparse rewards based on final answers, a single-layer Transformer converges to an interpretable, iterative algorithm for vertex-by-vertex traversal. Their theoretical analysis identifies that the training data distribution must include sufficient &#x27;simple examples&#x27;—instances requiring fewer reasoning steps—for this emergence to occur and enable generalization to longer chains; otherwise, learning fails. Experimental validation on both synthetic data and real-world language models for mathematical reasoning confirms these theoretical insights transfer to practical settings.</div>
<div class="mono" style="margin-top:8px">本文研究了基于结果的强化学习训练的Transformer如何能发展出类似思维链的系统性推理能力。作者在一个需要中间推理步骤的合成图遍历任务上分析了策略梯度动态，证明尽管仅基于最终答案获得稀疏奖励，单层Transformer会收敛到一个可解释的、逐顶点迭代遍历的算法。理论分析指出，训练数据分布必须包含足够多的“简单示例”——即需要较少推理步骤的实例——这种能力才会涌现并能泛化到更长链；否则学习将失败。在合成数据以及真实世界语言模型上进行数学推理的实验验证，证实了这些理论发现可迁移到实际场景中。</div>
</details>
</div>
<div class="card">
<div class="title">Personalized Image Generation via Human-in-the-loop Bayesian Optimization</div>
<div class="meta-line">Authors: Rajalaxmi Rajagopalan, Debottam Dutta, Yu-Lin Wei, Romit Roy Choudhury</div>
<div class="meta-line">First: 2026-02-02T17:51:30+00:00 · Latest: 2026-02-02T17:51:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02388v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02388v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Imagine Alice has a specific image $x^\ast$ in her mind, say, the view of the street in which she grew up during her childhood. To generate that exact image, she guides a generative model with multiple rounds of prompting and arrives at an image $x^{p*}$. Although $x^{p*}$ is reasonably close to $x^\ast$, Alice finds it difficult to close that gap using language prompts. This paper aims to narrow this gap by observing that even after language has reached its limits, humans can still tell when a new image $x^+$ is closer to $x^\ast$ than $x^{p*}$. Leveraging this observation, we develop MultiBO (Multi-Choice Preferential Bayesian Optimization) that carefully generates $K$ new images as a function of $x^{p*}$, gets preferential feedback from the user, uses the feedback to guide the diffusion model, and ultimately generates a new set of $K$ images. We show that within $B$ rounds of user feedback, it is possible to arrive much closer to $x^\ast$, even though the generative model has no information about $x^\ast$. Qualitative scores from $30$ users, combined with quantitative metrics compared across $5$ baselines, show promising results, suggesting that multi-choice feedback from humans can be effectively harnessed for personalized image generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于人机交互贝叶斯优化的个性化图像生成</div>
<div class="mono" style="margin-top:8px">假设Alice心中有一幅特定图像$x^\ast$，例如她童年成长街道的景象。为生成该图像，她通过多轮提示引导生成模型，得到图像$x^{p*}$。虽然$x^{p*}$已接近$x^\ast$，但Alice发现仅凭语言提示难以完全弥合差距。本文指出：即使语言描述已达极限，人类仍能判断新图像$x^+$是否比$x^{p*}$更接近$x^\ast$。基于此，我们提出MultiBO（多选择偏好贝叶斯优化）方法：以$x^{p*}$为基准生成$K$幅新图像，获取用户偏好反馈，利用反馈指导扩散模型，最终生成新一轮$K$幅图像。实验表明，在$B$轮用户反馈内，即使生成模型未获取$x^\ast$的直接信息，仍能显著逼近目标图像。通过30名用户的定性评分，以及与5个基线模型的定量指标对比，结果证实人类的多选择反馈可有效助力个性化图像生成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of personalized image generation when language prompts alone are insufficient to capture a user&#x27;s specific mental image. The proposed method, Multi-Choice Preferential Bayesian Optimization (MultiBO), leverages human preferential feedback by generating multiple candidate images, asking users to select which is closer to their target, and using this feedback to iteratively guide a diffusion model. Experimental results from 30 users and quantitative comparisons against five baselines demonstrate that this human-in-the-loop approach significantly narrows the gap between generated images and the user&#x27;s intended image within a limited number of feedback rounds.</div>
<div class="mono" style="margin-top:8px">本文针对个性化图像生成中语言提示无法精确捕捉用户心中特定图像的问题展开研究。提出的方法名为多选择偏好贝叶斯优化（MultiBO），它利用人类偏好反馈：生成多个候选图像，让用户选择哪个更接近目标图像，并以此反馈迭代引导扩散模型。来自30名用户的定性评分以及与五个基线方法的定量比较结果表明，这种人在回路的交互方法能在有限反馈轮次内，显著缩小生成图像与用户目标图像之间的差距。</div>
</details>
</div>
<div class="card">
<div class="title">SWE-Universe: Scale Real-World Verifiable Environments to Millions</div>
<div class="meta-line">Authors: Mouxiang Chen, Lei Zhang, Yunlong Feng, Xuwu Wang, Wenting Zhao, Ruisheng Cao, Jiaxi Yang, Jiawei Chen, Mingze Li, Zeyao Ma, Hao Ge, Zongmeng Zhang, Zeyu Cui, Dayiheng Liu, Jingren Zhou, Jianling Sun, Junyang Lin, Binyuan Hui</div>
<div class="meta-line">First: 2026-02-02T17:20:30+00:00 · Latest: 2026-02-02T17:20:30+00:00</div>
<div class="meta-line">Comments: 13 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02361v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02361v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model. This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning. Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified. Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWE-Universe：将真实世界可验证环境扩展至百万规模</div>
<div class="mono" style="margin-top:8px">我们提出SWE-Universe，一个可扩展的高效框架，用于从GitHub拉取请求自动构建真实世界软件工程可验证环境。为克服自动构建中普遍存在的生产成功率低、验证器弱、成本高昂等挑战，本框架采用基于高效定制训练模型的构建智能体。该智能体通过迭代自验证与循环内黑客检测，确保可靠生成高保真可验证任务。运用此方法，我们将真实世界多语言软件工程环境扩展至百万规模（807,693个）。我们通过大规模智能体中训练与强化学习证明了该环境的深层价值。最终，我们将此技术应用于Qwen3-Max-Thinking模型，在SWE-Bench Verified基准测试中获得75.3%的得分。本工作为推进下一代编码智能体发展提供了关键资源与稳健方法论。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind SWE-Universe is to address the challenges of automatically constructing real-world software engineering verifiable environments, such as low yield and high cost, by scaling them from GitHub pull requests. The method employs a building agent powered by a custom-trained model that uses iterative self-verification and in-loop hacking detection to reliably generate high-fidelity tasks. Main experimental results show the framework successfully scales to 807,693 multilingual environments, enhances agent performance through mid-training and reinforcement learning, and achieves a 75.3% score on SWE-Bench Verified with Qwen3-Max-Thinking, providing a critical resource for advancing coding agents.</div>
<div class="mono" style="margin-top:8px">SWE-Universe 的动机是通过从 GitHub 拉取请求中扩展真实世界软件工程可验证环境，以解决自动构建中产量低、成本高等挑战。该方法采用由定制训练模型驱动的构建代理，通过迭代自验证和循环内黑客检测来可靠生成高保真任务。主要实验结果表明，该框架成功扩展到 807,693 个多语言环境，通过中期训练和强化学习提升了代理性能，并在 SWE-Bench Verified 上使用 Qwen3-Max-Thinking 实现了 75.3% 的得分，为推进下一代编码代理提供了关键资源和方法论。</div>
</details>
</div>
<div class="card">
<div class="title">Position: Explaining Behavioral Shifts in Large Language Models Requires a Comparative Approach</div>
<div class="meta-line">Authors: Martino Ciaperoni, Marzio Di Vece, Luca Pappalardo, Fosca Giannotti, Francesco Giannini</div>
<div class="meta-line">First: 2026-02-02T16:36:21+00:00 · Latest: 2026-02-02T16:36:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02304v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02304v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large-scale foundation models exhibit behavioral shifts: intervention-induced behavioral changes that appear after scaling, fine-tuning, reinforcement learning or in-context learning. While investigating these phenomena have recently received attention, explaining their appearance is still overlooked. Classic explainable AI (XAI) methods can surface failures at a single checkpoint of a model, but they are structurally ill-suited to justify what changed internally across different checkpoints and which explanatory claims are warranted about that change. We take the position that behavioral shifts should be explained comparatively: the core target should be the intervention-induced shift between a reference model and an intervened model, rather than any single model in isolation. To this aim we formulate a Comparative XAI ($Δ$-XAI) framework with a set of desiderata to be taken into account when designing proper explaining methods. To highlight how $Δ$-XAI methods work, we introduce a set of possible pipelines, relate them to the desiderata, and provide a concrete $Δ$-XAI experiment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>立场：解释大语言模型的行为转变需采用比较性方法</div>
<div class="mono" style="margin-top:8px">大规模基础模型展现出行为转变现象：这些由干预引发的行为变化在模型扩展、微调、强化学习或上下文学习后出现。尽管此类现象的研究近期受到关注，但其成因解释仍被忽视。经典可解释人工智能（XAI）方法虽能揭示模型单个检查点的缺陷，但其结构本质上无法合理解释不同检查点间的内部变化，亦难以判定关于该变化的解释主张是否成立。我们主张行为转变应通过比较进行解释：核心目标应是参照模型与受干预模型间由干预引发的转变，而非孤立分析单一模型。为此，我们构建了比较性XAI（Δ-XAI）框架，提出设计合理解释方法时需考量的一系列要素。为阐明Δ-XAI方法的运作机制，我们引入多种可能的实施流程，将其与要素体系关联，并提供具体的Δ-XAI实验案例。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need to explain behavioral shifts in large language models that arise from interventions like scaling or fine-tuning, arguing that traditional explainable AI methods are inadequate for comparing changes across model checkpoints. It proposes a Comparative XAI (Δ-XAI) framework with specific desiderata to focus explanations on the differences between a reference model and an intervened model, rather than analyzing any single model in isolation. The authors outline potential Δ-XAI pipelines aligned with these desiderata and demonstrate their approach through a concrete experimental example to illustrate how it can justify explanatory claims about internal changes.</div>
<div class="mono" style="margin-top:8px">本文的动机在于需要解释大语言模型因干预（如扩展或微调）而产生的行为变化，认为传统的可解释人工智能方法不足以比较不同模型检查点之间的差异。它提出了一个比较性可解释人工智能（Δ-XAI）框架，包含特定要求，将解释重点放在参考模型与干预模型之间的差异上，而非孤立分析单个模型。作者概述了符合这些要求的潜在Δ-XAI流程，并通过具体实验示例展示了该方法如何为内部变化的解释性主张提供依据。</div>
</details>
</div>
<div class="card">
<div class="title">Advancing General-Purpose Reasoning Models with Modular Gradient Surgery</div>
<div class="meta-line">Authors: Min Cai, Yu Liang, Longzheng Wang, Yan Wang, Yueyang Zhang, Long Xia, Zhiyuan Sun, Xi Ye, Daiting Shi</div>
<div class="meta-line">First: 2026-02-02T16:34:39+00:00 · Latest: 2026-02-02T16:34:39+00:00</div>
<div class="meta-line">Comments: Preprint; Code: https://github.com/StringNLPLAB/MGS; Website: https://modular-gradient-surgery.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02301v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02301v1">PDF</a> · <a href="https://github.com/StringNLPLAB/MGS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://modular-gradient-surgery.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has played a central role in recent advances in large reasoning models (LRMs), yielding strong gains in verifiable and open-ended reasoning. However, training a single general-purpose LRM across diverse domains remains challenging due to pronounced domain heterogeneity. Through a systematic study of two widely used strategies, Sequential RL and Mixed RL, we find that both incur substantial cross-domain interference at the behavioral and gradient levels, resulting in limited overall gains. To address these challenges, we introduce **M**odular **G**radient **S**urgery (**MGS**), which resolves gradient conflicts at the module level within the transformer. When applied to Llama and Qwen models, MGS achieves average improvements of 4.3 (16.6\%) and 4.5 (11.1\%) points, respectively, over standard multi-task RL across three representative domains (math, general chat, and instruction following). Further analysis demonstrates that MGS remains effective under prolonged training. Overall, our study clarifies the sources of interference in multi-domain RL and presents an effective solution for training general-purpose LRMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过模块化梯度手术推进通用推理模型发展</div>
<div class="mono" style="margin-top:8px">强化学习（RL）在大型推理模型（LRM）的最新进展中发挥了核心作用，在可验证和开放式推理方面取得了显著提升。然而，由于显著的领域异质性，跨多个领域训练单一通用LRM仍具挑战性。通过对两种广泛使用的策略——顺序RL和混合RL——进行系统研究，我们发现两者在行为和梯度层面均产生显著的跨领域干扰，导致整体增益有限。为解决这些问题，我们提出了**模块化梯度手术（MGS）**，该方法在Transformer内部模块层面解决梯度冲突。应用于Llama和Qwen模型时，MGS在三个代表性领域（数学、通用对话和指令遵循）上，相比标准多任务RL分别实现了平均4.3（16.6%）和4.5（11.1%）个百分点的提升。进一步分析表明，MGS在长时间训练下仍保持有效性。总体而言，本研究阐明了多领域RL中干扰的来源，并为训练通用LRM提供了有效解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of training a single general-purpose large reasoning model across diverse domains, where domain heterogeneity causes substantial cross-domain interference in standard reinforcement learning approaches like Sequential RL and Mixed RL, limiting overall performance gains. The authors introduce Modular Gradient Surgery (MGS), a method that resolves gradient conflicts at the module level within transformer architectures to mitigate this interference. Experimental results on Llama and Qwen models show that MGS achieves average improvements of 4.3 and 4.5 points, respectively, over standard multi-task RL across math, general chat, and instruction following domains, with further analysis confirming its effectiveness under prolonged training.</div>
<div class="mono" style="margin-top:8px">本文旨在解决在多样化领域训练单一通用大型推理模型所面临的挑战，其中领域异质性导致顺序强化学习和混合强化学习等标准方法产生显著的跨领域干扰，从而限制了整体性能提升。作者提出了模块化梯度手术（MGS）方法，通过在Transformer架构中模块层面解决梯度冲突来减轻这种干扰。在Llama和Qwen模型上的实验结果表明，MGS在数学、通用聊天和指令遵循三个代表性领域上，相比标准多任务强化学习分别平均提升了4.3和4.5个点，进一步分析证实了其在长时间训练下的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Choice-Model-Assisted Q-learning for Delayed-Feedback Revenue Management</div>
<div class="meta-line">Authors: Owen Shen, Patrick Jaillet</div>
<div class="meta-line">First: 2026-02-02T16:23:56+00:00 · Latest: 2026-02-02T16:23:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02283v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02283v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study reinforcement learning for revenue management with delayed feedback, where a substantial fraction of value is determined by customer cancellations and modifications observed days after booking. We propose \emph{choice-model-assisted RL}: a calibrated discrete choice model is used as a fixed partial world model to impute the delayed component of the learning target at decision time. In the fixed-model deployment regime, we prove that tabular Q-learning with model-imputed targets converges to an $O(\varepsilon/(1-γ))$ neighborhood of the optimal Q-function, where $\varepsilon$ summarizes partial-model error, with an additional $O(t^{-1/2})$ sampling term. Experiments in a simulator calibrated from 61{,}619 hotel bookings (1{,}088 independent runs) show: (i) no statistically detectable difference from a maturity-buffer DQN baseline in stationary settings; (ii) positive effects under in-family parameter shifts, with significant gains in 5 of 10 shift scenarios after Holm--Bonferroni correction (up to 12.4\%); and (iii) consistent degradation under structural misspecification, where the choice model assumptions are violated (1.4--2.6\% lower revenue). These results characterize when partial behavioral models improve robustness under shift and when they introduce harmful bias.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于选择模型辅助Q学习的延迟反馈收益管理</div>
<div class="mono" style="margin-top:8px">本研究针对具有延迟反馈的收益管理问题开展强化学习研究，其中大部分价值由预订后数日观测到的客户取消与修改行为决定。我们提出\emph{选择模型辅助强化学习}方法：采用校准的离散选择模型作为固定部分世界模型，在决策时估算学习目标的延迟分量。在固定模型部署机制下，我们证明采用模型估算目标的表格Q学习将收敛至最优Q函数的$O(\varepsilon/(1-γ))邻域（其中$\varepsilon$表征部分模型误差），并附加$O(t^{-1/2})$采样项。基于61,619条酒店预订数据校准的模拟器实验（1,088次独立运行）显示：（1）在稳态环境下与成熟缓冲DQN基线无统计显著差异；（2）在参数族内偏移下呈现积极效应，经Holm-Bonferroni校正后10个偏移场景中有5个获得显著收益提升（最高达12.4%）；（3）在结构误设（违反选择模型假设）时出现持续性能下降（收益降低1.4-2.6%）。这些结果明确了部分行为模型在何种偏移条件下能提升鲁棒性，以及在何种情况下会引入有害偏差。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of delayed feedback in revenue management, where customer cancellations and modifications observed long after booking decisions complicate reinforcement learning. The authors propose a choice-model-assisted RL approach that uses a calibrated discrete choice model as a fixed partial world model to impute delayed components of the learning target at decision time. Theoretically, they prove that tabular Q-learning with model-imputed targets converges to a neighborhood of the optimal Q-function, with error bounded by model inaccuracy and sampling noise. Experiments in a hotel booking simulator, based on 61,619 bookings, show the method performs comparably to a baseline in stationary settings, yields revenue gains up to 12.4% under certain parameter shifts, but suffers revenue losses of 1.4–2.6% when the choice model is misspecified, highlighting its robustness benefits and bias risks.</div>
<div class="mono" style="margin-top:8px">本文针对收益管理中延迟反馈的挑战展开研究，其中客户取消和修改行为在预订决策后多日才被观察到，这给强化学习带来了困难。作者提出了一种选择模型辅助的强化学习方法，使用校准的离散选择模型作为固定的部分世界模型，在决策时估算学习目标的延迟部分。理论分析证明，基于模型估算目标的表格Q学习能收敛到最优Q函数的邻域，误差受模型不准确性和采样噪声限制。在基于61,619个酒店预订数据的模拟实验中，该方法在平稳环境下与基线方法无显著差异，在部分参数偏移场景下收益提升最高达12.4%，但当选择模型假设被违反时，收益下降1.4–2.6%，揭示了该方法在偏移下的鲁棒性优势及模型误设带来的偏差风险。</div>
</details>
</div>
<div class="card">
<div class="title">HER: Human-like Reasoning and Reinforcement Learning for LLM Role-playing</div>
<div class="meta-line">Authors: Chengyu Du, Xintao Wang, Aili Chen, Weiyuan Li, Rui Xu, Junteng Liu, Zishan Huang, Rong Tian, Zijun Sun, Yuhao Li, Liheng Feng, Deming Ding, Pengyu Zhao, Yanghua Xiao</div>
<div class="meta-line">First: 2026-01-29T09:35:27+00:00 · Latest: 2026-02-02T16:22:28+00:00</div>
<div class="meta-line">Comments: 41pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21459v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.21459v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM role-playing, i.e., using LLMs to simulate specific personas, has emerged as a key capability in various applications, such as companionship, content creation, and digital games. While current models effectively capture character tones and knowledge, simulating the inner thoughts behind their behaviors remains a challenge. Towards cognitive simulation in LLM role-play, previous efforts mainly suffer from two deficiencies: data with high-quality reasoning traces, and reliable reward signals aligned with human preferences. In this paper, we propose HER, a unified framework for cognitive-level persona simulation. HER introduces dual-layer thinking, which distinguishes characters&#x27; first-person thinking from LLMs&#x27; third-person thinking. To bridge these gaps, we curate reasoning-augmented role-playing data via reverse engineering and construct human-aligned principles and reward models. Leveraging these resources, we train HER models based on Qwen3-32B via supervised and reinforcement learning. Extensive experiments validate the effectiveness of our approach. Notably, our models significantly outperform the Qwen3-32B baseline, achieving a 30.26 improvement on the CoSER benchmark and a 14.97 gain on the Minimax Role-Play Bench. Our datasets, principles, and models will be released to facilitate future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HER：面向大语言模型角色扮演的人类化推理与强化学习框架</div>
<div class="mono" style="margin-top:8px">大语言模型角色扮演，即利用大语言模型模拟特定人物角色，已成为陪伴对话、内容创作、数字游戏等应用中的关键能力。现有模型虽能有效捕捉角色语调和知识，但模拟其行为背后的内在思维仍具挑战。为实现大语言模型角色扮演的认知模拟，先前研究主要存在两大不足：缺乏高质量推理轨迹的数据，以及缺少符合人类偏好的可靠奖励信号。本文提出HER——一个认知层面人物模拟的统一框架。该框架引入双层思维机制，区分角色的第一人称思维与大语言模型的第三人称思维。为弥合这些差距，我们通过逆向工程构建了推理增强的角色扮演数据，并建立了符合人类偏好的原则体系与奖励模型。基于这些资源，我们以Qwen3-32B为基础模型，通过监督学习与强化学习训练HER模型。大量实验验证了该方法的有效性：我们的模型显著超越Qwen3-32B基线，在CoSER基准上提升30.26分，在Minimax角色扮演基准上提升14.97分。我们将公开数据集、原则体系与模型以促进后续研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of simulating the inner cognitive processes behind persona behaviors in LLM role-playing, which is crucial for applications like companionship and gaming. The proposed HER framework introduces dual-layer thinking to separate character-specific first-person reasoning from the model&#x27;s third-person perspective, and it curates reasoning-augmented data via reverse engineering while developing human-aligned reward models. Experimental results show that HER models, trained on Qwen3-32B with supervised and reinforcement learning, significantly outperform the baseline, achieving improvements of 30.26 on the CoSER benchmark and 14.97 on the Minimax Role-Play Bench.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型角色扮演中模拟人物行为背后内在认知过程的挑战展开研究，这对于陪伴、游戏等应用至关重要。提出的HER框架引入了双层思维机制，以区分角色第一人称推理与模型的第三人称视角，并通过逆向工程构建了推理增强的数据集，同时开发了与人类偏好对齐的奖励模型。实验结果表明，基于Qwen3-32B通过监督学习和强化学习训练的HER模型显著优于基线，在CoSER基准上提升了30.26分，在Minimax角色扮演基准上提升了14.97分。</div>
</details>
</div>
<div class="card">
<div class="title">Kimi K2.5: Visual Agentic Intelligence</div>
<div class="meta-line">Authors: Kimi Team, Tongtong Bai, Yifan Bai, Yiping Bao, S. H. Cai, Yuan Cao, Y. Charles, H. S. Che, Cheng Chen, Guanduo Chen, Huarong Chen, Jia Chen, Jiahao Chen, Jianlong Chen, Jun Chen, Kefan Chen, Liang Chen, Ruijue Chen, Xinhao Chen, Yanru Chen, Yanxu Chen, Yicun Chen, Yimin Chen, Yingjiang Chen, Yuankun Chen, Yujie Chen, Yutian Chen, Zhirong Chen, Ziwei Chen, Dazhi Cheng, Minghan Chu, Jialei Cui, Jiaqi Deng, Muxi Diao, Hao Ding, Mengfan Dong, Mengnan Dong, Yuxin Dong, Yuhao Dong, Angang Du, Chenzhuang Du, Dikang Du, Lingxiao Du, Yulun Du, Yu Fan, Shengjun Fang, Qiulin Feng, Yichen Feng, Garimugai Fu, Kelin Fu, Hongcheng Gao, Tong Gao, Yuyao Ge, Shangyi Geng, Chengyang Gong, Xiaochen Gong, Zhuoma Gongque, Qizheng Gu, Xinran Gu, Yicheng Gu, Longyu Guan, Yuanying Guo, Xiaoru Hao, Weiran He, Wenyang He, Yunjia He, Chao Hong, Hao Hu, Jiaxi Hu, Yangyang Hu, Zhenxing Hu, Ke Huang, Ruiyuan Huang, Weixiao Huang, Zhiqi Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yu Jing, Guokun Lai, Aidi Li, C. Li, Cheng Li, Fang Li, Guanghe Li, Guanyu Li, Haitao Li, Haoyang Li, Jia Li, Jingwei Li, Junxiong Li, Lincan Li, Mo Li, Weihong Li, Wentao Li, Xinhang Li, Xinhao Li, Yang Li, Yanhao Li, Yiwei Li, Yuxiao Li, Zhaowei Li, Zheming Li, Weilong Liao, Jiawei Lin, Xiaohan Lin, Zhishan Lin, Zichao Lin, Cheng Liu, Chenyu Liu, Hongzhang Liu, Liang Liu, Shaowei Liu, Shudong Liu, Shuran Liu, Tianwei Liu, Tianyu Liu, Weizhou Liu, Xiangyan Liu, Yangyang Liu, Yanming Liu, Yibo Liu, Yuanxin Liu, Yue Liu, Zhengying Liu, Zhongnuo Liu, Enzhe Lu, Haoyu Lu, Zhiyuan Lu, Junyu Luo, Tongxu Luo, Yashuo Luo, Long Ma, Yingwei Ma, Shaoguang Mao, Yuan Mei, Xin Men, Fanqing Meng, Zhiyong Meng, Yibo Miao, Minqing Ni, Kun Ouyang, Siyuan Pan, Bo Pang, Yuchao Qian, Ruoyu Qin, Zeyu Qin, Jiezhong Qiu, Bowen Qu, Zeyu Shang, Youbo Shao, Tianxiao Shen, Zhennan Shen, Juanfeng Shi, Lidong Shi, Shengyuan Shi, Feifan Song, Pengwei Song, Tianhui Song, Xiaoxi Song, Hongjin Su, Jianlin Su, Zhaochen Su, Lin Sui, Jinsong Sun, Junyao Sun, Tongyu Sun, Flood Sung, Yunpeng Tai, Chuning Tang, Heyi Tang, Xiaojuan Tang, Zhengyang Tang, Jiawen Tao, Shiyuan Teng, Chaoran Tian, Pengfei Tian, Ao Wang, Bowen Wang, Chensi Wang, Chuang Wang, Congcong Wang, Dingkun Wang, Dinglu Wang, Dongliang Wang, Feng Wang, Hailong Wang, Haiming Wang, Hengzhi Wang, Huaqing Wang, Hui Wang, Jiahao Wang, Jinhong Wang, Jiuzheng Wang, Kaixin Wang, Linian Wang, Qibin Wang, Shengjie Wang, Shuyi Wang, Si Wang, Wei Wang, Xiaochen Wang, Xinyuan Wang, Yao Wang, Yejie Wang, Yipu Wang, Yiqin Wang, Yucheng Wang, Yuzhi Wang, Zhaoji Wang, Zhaowei Wang, Zhengtao Wang, Zhexu Wang, Zihan Wang, Zizhe Wang, Chu Wei, Ming Wei, Chuan Wen, Zichen Wen, Chengjie Wu, Haoning Wu, Junyan Wu, Rucong Wu, Wenhao Wu, Yuefeng Wu, Yuhao Wu, Yuxin Wu, Zijian Wu, Chenjun Xiao, Jin Xie, Xiaotong Xie, Yuchong Xie, Yifei Xin, Bowei Xing, Boyu Xu, Jianfan Xu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinbo Xu, Xinran Xu, Yangchuan Xu, Yichang Xu, Yuemeng Xu, Zelai Xu, Ziyao Xu, Junjie Yan, Yuzi Yan, Guangyao Yang, Hao Yang, Junwei Yang, Kai Yang, Ningyuan Yang, Ruihan Yang, Xiaofei Yang, Xinlong Yang, Ying Yang, Yi Yang, Yi Yang, Zhen Yang, Zhilin Yang, Zonghan Yang, Haotian Yao, Dan Ye, Wenjie Ye, Zhuorui Ye, Bohong Yin, Chengzhen Yu, Longhui Yu, Tao Yu, Tianxiang Yu, Enming Yuan, Mengjie Yuan, Xiaokun Yuan, Yang Yue, Weihao Zeng, Dunyuan Zha, Haobing Zhan, Dehao Zhang, Hao Zhang, Jin Zhang, Puqi Zhang, Qiao Zhang, Rui Zhang, Xiaobin Zhang, Y. Zhang, Yadong Zhang, Yangkun Zhang, Yichi Zhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yushun Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Chenguang Zhao, Feifan Zhao, Jinxiang Zhao, Shuai Zhao, Xiangyu Zhao, Yikai Zhao, Zijia Zhao, Huabin Zheng, Ruihan Zheng, Shaojie Zheng, Tengyang Zheng, Junfeng Zhong, Longguang Zhong, Weiming Zhong, M. Zhou, Runjie Zhou, Xinyu Zhou, Zaida Zhou, Jinguo Zhu, Liya Zhu, Xinhao Zhu, Yuxuan Zhu, Zhen Zhu, Jingze Zhuang, Weiyu Zhuang, Ying Zou, Xinxing Zu</div>
<div class="meta-line">First: 2026-02-02T16:17:38+00:00 · Latest: 2026-02-02T16:17:38+00:00</div>
<div class="meta-line">Comments: Kimi K2.5 tech report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02276v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02276v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to $4.5\times$ over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Kimi K2.5：视觉智能体模型</div>
<div class="mono" style="margin-top:8px">我们推出Kimi K2.5，这是一个旨在推进通用智能体能力的开源多模态智能体模型。K2.5强调文本与视觉的联合优化，使两种模态相互增强。这包括一系列技术，如联合文本-视觉预训练、零视觉监督微调以及联合文本-视觉强化学习。在此多模态基础上，K2.5引入了智能体集群，这是一个自主的并行智能体编排框架，能够动态地将复杂任务分解为异构子问题并并行执行。广泛评估表明，Kimi K2.5在编程、视觉、推理及智能体任务等多个领域均取得了最先进的成果。智能体集群相较于单智能体基线，延迟最多可降低4.5倍。我们发布了训练后的Kimi K2.5模型检查点，以促进智能体智能的未来研究和实际应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Kimi K2.5, an open-source multimodal agentic model motivated by the goal of advancing general agentic intelligence through enhanced synergy between text and vision modalities. Its method features joint text-vision pre-training, fine-tuning, and reinforcement learning, and it introduces Agent Swarm, a framework for dynamically decomposing complex tasks into parallel, heterogeneous sub-tasks for concurrent execution. Experimental results demonstrate state-of-the-art performance across coding, vision, reasoning, and agentic benchmarks, with Agent Swarm reducing latency by up to 4.5 times compared to single-agent baselines.</div>
<div class="mono" style="margin-top:8px">本文介绍了Kimi K2.5，一个开源的具身智能多模态模型，其动机是通过增强文本与视觉模态的协同来推进通用具身智能的发展。方法上，它采用了联合文本-视觉预训练、微调和强化学习等一系列技术，并引入了Agent Swarm框架，该框架能将复杂任务动态分解为异构子问题并并行执行。主要实验结果表明，该模型在编程、视觉、推理和具身任务等多个领域取得了最先进的性能，且Agent Swarm相比单智能体基线将延迟降低了最高4.5倍。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Markov Decision Processes under Fully Bandit Feedback</div>
<div class="meta-line">Authors: Zhengjia Zhuo, Anupam Gupta, Viswanath Nagarajan</div>
<div class="meta-line">First: 2026-02-02T16:03:24+00:00 · Latest: 2026-02-02T16:03:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02260v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02260v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A standard assumption in Reinforcement Learning is that the agent observes every visited state-action pair in the associated Markov Decision Process (MDP), along with the per-step rewards. Strong theoretical results are known in this setting, achieving nearly-tight $Θ(\sqrt{T})$-regret bounds. However, such detailed feedback can be unrealistic, and recent research has investigated more restricted settings such as trajectory feedback, where the agent observes all the visited state-action pairs, but only a single \emph{aggregate} reward. In this paper, we consider a far more restrictive ``fully bandit&#x27;&#x27; feedback model for episodic MDPs, where the agent does not even observe the visited state-action pairs -- it only learns the aggregate reward. We provide the first efficient bandit learning algorithm for episodic MDPs with $\widetilde{O}(\sqrt{T})$ regret. Our regret has an exponential dependence on the horizon length $\H$, which we show is necessary. We also obtain improved nearly-tight regret bounds for ``ordered&#x27;&#x27; MDPs; these can be used to model classical stochastic optimization problems such as $k$-item prophet inequality and sequential posted pricing. Finally, we evaluate the empirical performance of our algorithm for the setting of $k$-item prophet inequalities; despite the highly restricted feedback, our algorithm&#x27;s performance is comparable to that of a state-of-art learning algorithm (UCB-VI) with detailed state-action feedback.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>完全赌博机反馈下的马尔可夫决策过程学习</div>
<div class="mono" style="margin-top:8px">强化学习中的一个标准假设是智能体能够观察到马尔可夫决策过程中访问的每个状态-动作对以及每步奖励，该设定下已有接近紧界的$Θ(\sqrt{T})$遗憾度理论结果。然而，这种详细反馈可能不切实际，近期研究开始探索更受限的设定（如轨迹反馈），即智能体仅能观察到所有访问过的状态-动作对和单一聚合奖励。本文针对片段式MDP提出一种更为受限的“完全赌博机”反馈模型：智能体甚至无法观测访问过的状态-动作对，仅能获得聚合奖励。我们首次提出针对片段式MDP的高效赌博机学习算法，其遗憾度为$\widetilde{O}(\sqrt{T})$，且遗憾度对时间跨度$\H$存在指数级依赖（我们证明该依赖是必要的）。同时，我们为“有序”MDP改进了接近紧界的遗憾度，该模型可用于描述经典随机优化问题（如$k$项先知不等式与序列定价）。最后，我们在$k$项先知不等式场景中评估算法性能：尽管反馈信息高度受限，其表现仍与具备完整状态-动作反馈的先进学习算法（UCB-VI）相当。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the unrealistic assumption of detailed state-action feedback in standard Reinforcement Learning, this paper investigates a highly restrictive fully bandit feedback model for episodic Markov Decision Processes (MDPs), where only an aggregate reward per episode is observed. The method introduces the first efficient bandit learning algorithm designed for this setting, achieving a regret bound of \(\widetilde{O}(\sqrt{T})\) with an exponential dependence on the horizon length, which is proven necessary. Experimental results on problems like the k-item prophet inequality show that the algorithm&#x27;s performance is comparable to state-of-the-art methods like UCB-VI that rely on full state-action feedback, despite the severe information limitation.</div>
<div class="mono" style="margin-top:8px">针对标准强化学习中详细状态-动作反馈假设不切实际的问题，本文研究了情节式马尔可夫决策过程（MDP）中一种高度受限的完全赌博机反馈模型，其中智能体仅能观察到每个情节的总奖励。方法上，提出了首个适用于该设置的高效赌博机学习算法，实现了具有指数级依赖于时间视野的 \(\widetilde{O}(\sqrt{T})\) 遗憾上界，并证明该依赖性是必要的。在k项先知不等式等问题的实验中，尽管反馈信息极为有限，该算法的性能仍可与依赖完整状态-动作反馈的先进算法（如UCB-VI）相媲美。</div>
</details>
</div>
<div class="card">
<div class="title">Segment to Focus: Guiding Latent Action Models in the Presence of Distractors</div>
<div class="meta-line">Authors: Hamza Adnan, Matthew T. Jackson, Alexey Zakharov</div>
<div class="meta-line">First: 2026-02-02T16:03:19+00:00 · Latest: 2026-02-02T16:03:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02259v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02259v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Latent Action Models (LAMs) learn to extract action-relevant representations solely from raw observations, enabling reinforcement learning from unlabelled videos and significantly scaling available training data. However, LAMs face a critical challenge in disentangling action-relevant features from action-correlated noise (e.g., background motion). Failing to filter these distractors causes LAMs to capture spurious correlations and build sub-optimal latent action spaces. In this paper, we introduce MaskLAM -- a lightweight modification to LAM training to mitigate this issue by incorporating visual agent segmentation. MaskLAM utilises segmentation masks from pretrained foundation models to weight the LAM reconstruction loss, thereby prioritising salient information over background elements while requiring no architectural modifications. We demonstrate the effectiveness of our method on continuous-control MuJoCo tasks, modified with action-correlated background noise. Our approach yields up to a 4x increase in accrued rewards compared to standard baselines and a 3x improvement in the latent action quality, as evidenced by linear probe evaluation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分割以聚焦：在干扰物存在下引导潜在动作模型</div>
<div class="mono" style="margin-top:8px">潜在动作模型（LAMs）通过学习从原始观察中提取与动作相关的表征，实现了从未标记视频中进行强化学习，并显著扩展了可用训练数据。然而，LAMs面临一个关键挑战：如何将动作相关特征与动作相关噪声（如背景运动）分离。若未能过滤这些干扰物，LAMs会捕捉虚假相关性并构建次优的潜在动作空间。本文提出MaskLAM——一种对LAM训练的轻量级改进方法，通过引入视觉智能体分割来缓解此问题。MaskLAM利用预训练基础模型生成的分割掩码对LAM重建损失进行加权，从而在无需修改架构的情况下，优先处理显著信息而非背景元素。我们在添加了动作相关背景噪声的连续控制MuJoCo任务上验证了方法的有效性。实验表明，与标准基线相比，我们的方法使累积奖励提升高达4倍，并通过线性探针评估证实潜在动作质量提高了3倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge in Latent Action Models (LAMs) where action-correlated background distractors can lead to spurious correlations and suboptimal latent spaces. To mitigate this, the authors propose MaskLAM, a lightweight training modification that uses pretrained segmentation masks to weight the reconstruction loss, focusing on salient agent information without architectural changes. Experimental results on MuJoCo tasks with added background noise show MaskLAM achieves up to a 4x reward increase and a 3x improvement in latent action quality over baselines.</div>
<div class="mono" style="margin-top:8px">本文针对潜在动作模型在存在动作相关背景干扰时易捕获虚假关联、导致次优潜在空间的问题，提出了一种轻量级训练改进方法MaskLAM。该方法利用预训练分割掩码加权重构损失，无需修改模型结构即可聚焦于智能体关键信息。在添加背景噪声的MuJoCo连续控制任务上的实验表明，MaskLAM相比基线方法将累积奖励提升了最高4倍，潜在动作质量提高了3倍。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260204_0411.html">20260204_0411</a>
<a href="archive/20260202_0320.html">20260202_0320</a>
<a href="archive/20260201_0343.html">20260201_0343</a>
<a href="archive/20260131_0357.html">20260131_0357</a>
<a href="archive/20260130_0355.html">20260130_0355</a>
<a href="archive/20260129_0353.html">20260129_0353</a>
<a href="archive/20260128_0342.html">20260128_0342</a>
<a href="archive/20260127_0320.html">20260127_0320</a>
<a href="archive/20260126_0313.html">20260126_0313</a>
<a href="archive/20260125_0321.html">20260125_0321</a>
<a href="archive/20260124_0344.html">20260124_0344</a>
<a href="archive/20260123_0345.html">20260123_0345</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0408.html">20260121_0408</a>
<a href="archive/20260120_0316.html">20260120_0316</a>
<a href="archive/20260119_0312.html">20260119_0312</a>
<a href="archive/20260118_0323.html">20260118_0323</a>
<a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
