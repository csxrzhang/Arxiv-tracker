<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-27 04:00</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260227_0400</div>
    <div class="row"><div class="card">
<div class="title">Stagewise Reinforcement Learning and the Geometry of the Regret Landscape</div>
<div class="meta-line">Authors: Chris Elliott, Einar Urdshals, David Quarel, Matthew Farrugia-Roberts, Daniel Murfet</div>
<div class="meta-line">First: 2026-01-12T13:25:21+00:00 · Latest: 2026-02-25T18:40:10+00:00</div>
<div class="meta-line">Comments: 48 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07524v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.07524v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Singular learning theory characterizes Bayesian learning as an evolving tradeoff between accuracy and complexity, with transitions between qualitatively different solutions as sample size increases. We extend this theory to reinforcement learning, proving that the concentration of a generalized posterior over policies is governed by the local learning coefficient (LLC), an invariant of the geometry of the regret function. This theory predicts that deep reinforcement learning with SGD should proceed from simple policies with high regret to complex policies with low regret. We verify this prediction empirically in a gridworld environment exhibiting stagewise policy development: phase transitions over training manifest as &quot;opposing staircases&quot; where regret decreases sharply while the LLC increases.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>阶段性强化学习与遗憾函数几何结构</div>
<div class="mono" style="margin-top:8px">奇异学习理论将贝叶斯学习描述为准确性与复杂性之间动态权衡的过程，随着样本量增加会出现不同质态解之间的跃迁。我们将该理论拓展至强化学习领域，证明策略广义后验分布的集中性由遗憾函数几何结构的不变量——局部学习系数（LLC）所决定。该理论预测采用随机梯度下降的深度强化学习会经历从高遗憾简单策略到低遗憾复杂策略的演进过程。我们在网格世界环境中通过实证验证了这一预测：训练过程中出现的阶段性策略发展表现为&quot;对向阶梯&quot;现象，即遗憾值急剧下降的同时局部学习系数同步上升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by extending singular learning theory from Bayesian learning to reinforcement learning (RL), this paper introduces a geometric perspective on the regret landscape. The method involves proving that the concentration of a generalized posterior over policies is governed by a local learning coefficient (LLC), an invariant derived from the geometry of the regret function. Experimental results in a gridworld environment empirically validate the theory, showing stagewise policy development where training exhibits phase transitions visualized as &quot;opposing staircases&quot;—sharp decreases in regret accompanied by increases in the LLC, confirming the predicted progression from simple, high-regret policies to complex, low-regret ones.</div>
<div class="mono" style="margin-top:8px">本文的动机是将奇异学习理论从贝叶斯学习扩展到强化学习领域，从几何角度研究遗憾函数的景观。方法上，论文证明了策略的广义后验集中性由局部学习系数（LLC）所主导，该系数是遗憾函数几何结构的不变量。在网格世界环境中的实验结果表明，训练过程呈现出阶段性的策略发展：训练中的相变表现为“反向阶梯”现象，即遗憾急剧下降的同时LLC上升，这验证了理论预测——学习会从简单高遗憾策略逐步过渡到复杂低遗憾策略。</div>
</details>
</div>
<div class="card">
<div class="title">Provable Last-Iterate Convergence for Multi-Objective Safe LLM Alignment via Optimistic Primal-Dual</div>
<div class="meta-line">Authors: Yining Li, Peizhong Ju, Ness Shroff</div>
<div class="meta-line">First: 2026-02-25T17:54:52+00:00 · Latest: 2026-02-25T17:54:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22146v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22146v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning from Human Feedback (RLHF) plays a significant role in aligning Large Language Models (LLMs) with human preferences. While RLHF with expected reward constraints can be formulated as a primal-dual optimization problem, standard primal-dual methods only guarantee convergence with a distributional policy where the saddle-point problem is in convex-concave form. Moreover, standard primal-dual methods may exhibit instability or divergence in the last iterate under policy parameterization in practical applications. In this work, we propose a universal primal-dual framework for safe RLHF that unifies a broad class of existing alignment algorithms, including safe-RLHF, one-shot, and multi-shot based methods. Building on this framework, we introduce an optimistic primal-dual (OPD) algorithm that incorporates predictive updates for both primal and dual variables to stabilize saddle-point dynamics. We establish last-iterate convergence guarantees for the proposed method, covering both exact policy optimization in the distributional space and convergence to a neighborhood of the optimal solution whose gap is related to approximation error and bias under parameterized policies. Our analysis reveals that optimism plays a crucial role in mitigating oscillations inherent to constrained alignment objectives, thereby closing a key theoretical gap between constrained RL and practical RLHF.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于乐观原始对偶的多目标安全大语言模型对齐可证明末次迭代收敛性</div>
<div class="mono" style="margin-top:8px">基于人类反馈的强化学习（RLHF）在使大语言模型（LLM）与人类偏好对齐方面发挥重要作用。虽然带期望奖励约束的RLHF可表述为原始对偶优化问题，但标准原始对偶方法仅能保证在凸凹形式的鞍点问题下，基于分布策略的收敛性。此外，在实际应用中，标准原始对偶方法在策略参数化下可能出现末次迭代不稳定或发散。本研究提出一个通用的安全RLHF原始对偶框架，统一了包括安全RLHF、单次及多次迭代方法在内的广泛现有对齐算法。基于此框架，我们引入一种乐观原始对偶（OPD）算法，通过对原始变量和对偶变量进行预测性更新来稳定鞍点动态。我们为所提方法建立了末次迭代收敛保证，涵盖分布空间中的精确策略优化，以及在参数化策略下收敛至最优解邻域（其间隙与近似误差和偏差相关）。分析表明，乐观机制对缓解约束对齐目标固有的振荡至关重要，从而弥合了约束强化学习与实际RLHF之间的关键理论鸿沟。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the need to stabilize and ensure convergence in the last iteration of multi-objective safe alignment for Large Language Models (LLMs), as standard primal-dual methods often exhibit instability or divergence under policy parameterization. The method introduces a universal primal-dual framework that unifies existing alignment algorithms and proposes an optimistic primal-dual (OPD) algorithm with predictive updates for both primal and dual variables to stabilize saddle-point dynamics. The main experimental results demonstrate that OPD achieves provable last-iterate convergence, covering exact policy optimization in distributional space and convergence to a neighborhood of the optimal solution, with the gap related to approximation error and bias, thereby mitigating oscillations in constrained alignment objectives.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于需要稳定并确保大型语言模型（LLM）多目标安全对齐在最后迭代中的收敛性，因为标准原始对偶方法在策略参数化下常出现不稳定或发散问题。方法上，提出了一个统一现有对齐算法的通用原始对偶框架，并引入乐观原始对偶（OPD）算法，通过对原始变量和对偶变量进行预测性更新来稳定鞍点动态。主要实验结果表明，OPD实现了可证明的最后迭代收敛，覆盖了分布空间中的精确策略优化以及收敛到最优解邻域，其间隙与近似误差和偏差相关，从而缓解了约束对齐目标中的振荡问题。</div>
</details>
</div>
<div class="card">
<div class="title">Active operator learning with predictive uncertainty quantification for partial differential equations</div>
<div class="meta-line">Authors: Nick Winovich, Mitchell Daneker, Lu Lu, Guang Lin</div>
<div class="meta-line">First: 2025-03-05T04:48:14+00:00 · Latest: 2026-02-25T17:27:35+00:00</div>
<div class="meta-line">Comments: Submitted to the Journal of Computational Physics</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.03178v4">Abs</a> · <a href="https://arxiv.org/pdf/2503.03178v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the increased prevalence of neural operators being used to provide rapid solutions to partial differential equations (PDEs), understanding the accuracy of model predictions and the associated error levels is necessary for deploying reliable surrogate models in scientific applications. Existing uncertainty quantification (UQ) frameworks employ ensembles or Bayesian methods, which can incur substantial computational costs during both training and inference. We propose a lightweight predictive UQ method tailored for Deep operator networks (DeepONets) that also generalizes to other operator networks. Numerical experiments on linear and nonlinear PDEs demonstrate that the framework&#x27;s uncertainty estimates are unbiased and provide accurate out-of-distribution uncertainty predictions with a sufficiently large training dataset. Our framework provides fast inference and uncertainty estimates that can efficiently drive outer-loop analyses that would be prohibitively expensive with conventional solvers. We demonstrate how predictive uncertainties can be used in the context of Bayesian optimization and active learning problems to yield improvements in accuracy and data-efficiency for outer-loop optimization procedures. In the active learning setup, we extend the framework to Fourier Neural Operators (FNO) and describe a generalized method for other operator networks. To enable real-time deployment, we introduce an inference strategy based on precomputed trunk outputs and a sparse placement matrix, reducing evaluation time by more than a factor of five. Our method provides a practical route to uncertainty-aware operator learning in time-sensitive settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>偏微分方程主动算子学习与预测不确定性量化</div>
<div class="mono" style="margin-top:8px">随着神经算子被广泛用于快速求解偏微分方程，理解模型预测的准确性及相关误差水平对于在科学应用中部署可靠的代理模型至关重要。现有不确定性量化框架采用集成或贝叶斯方法，在训练和推断阶段均可能产生高昂计算成本。本文提出一种专为深度算子网络设计的轻量级预测不确定性量化方法，该方法可推广至其他算子网络。在线性与非线性偏微分方程上的数值实验表明，该框架的不确定性估计具有无偏性，并在训练数据集足够大时能提供准确的分布外不确定性预测。本框架提供快速推断与不确定性估计，可高效驱动传统求解器难以承担的外循环分析。我们展示了预测不确定性如何应用于贝叶斯优化和主动学习问题，以提升外循环优化过程的精度与数据效率。在主动学习场景中，我们将框架扩展至傅里叶神经算子，并描述了适用于其他算子网络的通用方法。为实现实时部署，我们引入基于预计算主干输出和稀疏放置矩阵的推断策略，将评估时间缩短五倍以上。该方法为时间敏感场景下的不确定性感知算子学习提供了实用路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for reliable uncertainty quantification (UQ) in neural operator models for solving partial differential equations (PDEs), as existing UQ methods are often computationally expensive. The authors propose a lightweight predictive UQ framework tailored for DeepONets, which also generalizes to other operator networks like Fourier Neural Operators (FNO), and introduce an inference strategy using precomputed trunk outputs to speed up evaluation by over five times. Experimental results on linear and nonlinear PDEs show that the framework provides unbiased uncertainty estimates and accurate out-of-distribution predictions with sufficient training data, enabling efficient applications in Bayesian optimization and active learning to improve accuracy and data-efficiency in outer-loop analyses.</div>
<div class="mono" style="margin-top:8px">本文针对求解偏微分方程的神经算子模型需要可靠的不确定性量化问题，现有方法通常计算成本高昂。作者提出了一种轻量级预测性不确定性量化框架，专为DeepONets设计，并可推广至其他算子网络如傅里叶神经算子，同时引入基于预计算主干输出的推理策略，将评估时间减少五倍以上。在线性和非线性偏微分方程上的实验结果表明，该框架在足够训练数据下能提供无偏的不确定性估计和准确的分布外预测，从而在贝叶斯优化和主动学习等应用中有效提升外部循环分析的精度和数据效率。</div>
</details>
</div>
<div class="card">
<div class="title">SWE-Protégé: Learning to Selectively Collaborate With an Expert Unlocks Small Language Models as Software Engineering Agents</div>
<div class="meta-line">Authors: Patrick Tser Jern Kon, Archana Pradeep, Ang Chen, Alexander P. Ellis, Warren Hunt, Zijian Wang, John Yang, Samuel Thompson</div>
<div class="meta-line">First: 2026-02-25T17:11:49+00:00 · Latest: 2026-02-25T17:11:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22124v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22124v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Small language models (SLMs) offer compelling advantages in cost, latency, and adaptability, but have so far lagged behind larger models on long-horizon software engineering tasks such as SWE-bench, where they suffer from pervasive action looping and low resolution rates. We introduce SWE-Protégé, a post-training framework that reframes software repair as an expert-protégé collaboration problem. In SWE-Protégé, an SLM remains the sole decision-maker while learning to selectively seek guidance from a strong expert model, recognize stalled states, and follow through on expert feedback. Our approach combines supervised fine-tuning on expert-augmented trajectories with agentic reinforcement learning that explicitly discourages degenerative looping and unproductive expert collaboration. We lightly post-train Qwen2.5-Coder-7B-Instruct to achieve 42.4% Pass@1 on SWE-bench Verified, a +25.4% improvement over the prior SLM state of the art, while using expert assistance sparsely (~4 calls per task and 11% of total tokens).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWE-Protégé：学习选择性协同专家解锁小语言模型作为软件工程智能体</div>
<div class="mono" style="margin-top:8px">小语言模型（SLM）在成本、延迟和适应性方面具有显著优势，但在SWE-bench等长周期软件工程任务中始终落后于大模型，普遍存在动作循环和低解决率问题。我们提出SWE-Protégé后训练框架，将软件修复重构为专家-学徒协同问题。该框架使SLM保持独立决策能力，同时学习选择性寻求专家模型指导、识别停滞状态并执行专家反馈。该方法结合专家增强轨迹的监督微调与智能体强化学习，显式抑制退化循环和无效专家协作。通过对Qwen2.5-Coder-7B-Instruct进行轻量后训练，在SWE-bench Verified上达到42.4%的Pass@1，较先前SLM最佳水平提升25.4%，且专家调用稀疏（平均每任务约4次调用，占总token数11%）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to enhance the performance of small language models (SLMs) on complex software engineering tasks like SWE-bench, where they typically struggle with issues such as action looping and low resolution rates. The method introduces SWE-Protégé, a post-training framework that treats software repair as a collaborative process between an SLM (the protégé) and a stronger expert model, enabling the SLM to learn when to seek expert guidance, detect stalled states, and act on feedback. Experimental results show that applying this framework to Qwen2.5-Coder-7B-Instruct achieves a 42.4% Pass@1 on SWE-bench Verified, marking a 25.4% improvement over previous SLM benchmarks, while maintaining efficiency by using expert assistance sparingly—about 4 calls per task and 11% of total tokens.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升小型语言模型在复杂软件工程任务（如SWE-bench）上的性能，这些模型通常存在动作循环和解决率低的问题。方法上提出了SWE-Protégé，这是一个后训练框架，将软件修复重构为专家与学徒的协作问题，让小型模型作为唯一决策者学习选择性寻求专家指导、识别停滞状态并执行反馈。主要实验结果表明，对Qwen2.5-Coder-7B-Instruct进行轻量后训练后，在SWE-bench Verified上达到了42.4%的Pass@1，较之前小型模型的最佳性能提升了25.4%，同时通过稀疏使用专家协助（约每任务4次调用，占总令牌数的11%）保持了高效性。</div>
</details>
</div>
<div class="card">
<div class="title">A Distributional Treatment of Real2Sim2Real for Object-Centric Agent Adaptation in Vision-Driven Deformable Linear Object Manipulation</div>
<div class="meta-line">Authors: Georgios Kamaras, Subramanian Ramamoorthy</div>
<div class="meta-line">Venue: In IEEE Robotics and Automation Letters, Volume 10, Issue 8, August 2025, Pages 8075-8082</div>
<div class="meta-line">First: 2025-02-25T20:01:06+00:00 · Latest: 2026-02-25T17:09:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.18615v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.18615v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present an integrated (or end-to-end) framework for the Real2Sim2Real problem of manipulating deformable linear objects (DLOs) based on visual perception. Working with a parameterised set of DLOs, we use likelihood-free inference (LFI) to compute the posterior distributions for the physical parameters using which we can approximately simulate the behaviour of each specific DLO. We use these posteriors for domain randomisation while training, in simulation, object-specific visuomotor policies (i.e. assuming only visual and proprioceptive sensory) for a DLO reaching task, using model-free reinforcement learning. We demonstrate the utility of this approach by deploying sim-trained DLO manipulation policies in the real world in a zero-shot manner, i.e. without any further fine-tuning. In this context, we evaluate the capacity of a prominent LFI method to perform fine classification over the parametric set of DLOs, using only visual and proprioceptive data obtained in a dynamic manipulation trajectory. We then study the implications of the resulting domain distributions in sim-based policy learning and real-world performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向视觉驱动可变形线性物体操控中物体中心智能体适应的Real2Sim2Real分布化处理方法</div>
<div class="mono" style="margin-top:8px">本文针对基于视觉感知的可变形线性物体操控的Real2Sim2Real问题，提出一种集成式（端到端）框架。通过对参数化DLO集合采用无似然推断计算物理参数的后验分布，实现对特定DLO行为的近似仿真。在仿真训练中，我们利用这些后验分布进行领域随机化，采用无模型强化学习方法训练面向DLO抓取任务的物体特异性视觉运动策略（仅依赖视觉与本体感知）。通过零样本方式（无需额外微调）将仿真训练的DLO操控策略部署至现实场景，验证了该方法的实用性。在此背景下，我们评估了主流LFI方法仅利用动态操控轨迹中获取的视觉与本体感知数据，在参数化DLO集合上进行精细分类的能力，进而研究了所得领域分布对仿真策略学习与现实场景性能的影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of adapting robotic agents to manipulate diverse deformable linear objects (DLOs) in the real world, this paper introduces an integrated Real2Sim2Real framework. The method employs likelihood-free inference on visual and proprioceptive data from a dynamic manipulation trajectory to compute posterior distributions over the physical parameters of a parameterized DLO set, enabling approximate simulation; these posteriors are then used for domain randomization during the simulation training of object-specific visuomotor policies via model-free reinforcement learning for a reaching task. The main experimental results demonstrate that policies trained solely in simulation can be deployed zero-shot in the real world without fine-tuning, and the study evaluates the LFI method&#x27;s capacity for fine classification over DLO parameters and analyzes the impact of the derived domain distributions on policy learning and real-world performance.</div>
<div class="mono" style="margin-top:8px">本文针对机器人在现实世界中操纵多样化可变形线性物体的适应性问题，提出了一个集成的Real2Sim2Real框架。该方法基于动态操纵轨迹中获取的视觉和本体感知数据，采用无似然推理计算参数化DLO集合的物理参数后验分布，从而实现近似仿真；并利用这些后验分布在仿真中进行领域随机化，通过无模型强化学习训练用于抓取任务的物体特异性视觉运动策略。主要实验结果表明，仅在仿真中训练的策略能够以零样本方式在现实世界中部署而无需微调，同时研究评估了无似然推理方法对DLO参数进行精细分类的能力，并分析了所得领域分布对策略学习及实际性能的影响。</div>
</details>
</div>
<div class="card">
<div class="title">RebuttalAgent: Strategic Persuasion in Academic Rebuttal via Theory of Mind</div>
<div class="meta-line">Authors: Zhitao He, Zongwei Lyu, Yi R Fung</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-22T07:36:48+00:00 · Latest: 2026-02-25T16:22:14+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15715v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.15715v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although artificial intelligence (AI) has become deeply integrated into various stages of the research workflow and achieved remarkable advancements, academic rebuttal remains a significant and underexplored challenge. This is because rebuttal is a complex process of strategic communication under severe information asymmetry rather than a simple technical debate. Consequently, current approaches struggle as they largely imitate surface-level linguistics, missing the essential element of perspective-taking required for effective persuasion. In this paper, we introduce RebuttalAgent, the first framework to ground academic rebuttal in Theory of Mind (ToM), operationalized through a ToM-Strategy-Response (TSR) framework that models reviewer mental state, formulates persuasion strategy, and generates evidence-based response. To train our agent, we construct RebuttalBench, a large-scale dataset synthesized via a novel critique-and-refine approach. Our training process consists of two stages, beginning with a supervised fine-tuning phase to equip the agent with ToM-based analysis and strategic planning capabilities, followed by a reinforcement learning phase leveraging the self-reward mechanism for scalable self-improvement. For reliable and efficient automated evaluation, we further develop Rebuttal-RM, a specialized evaluator trained on over 100K samples of multi-source rebuttal data, which achieves scoring consistency with human preferences surpassing powerful judge GPT-4.1. Extensive experiments show RebuttalAgent significantly outperforms the base model by an average of 18.3% on automated metrics, while also outperforming advanced proprietary models across both automated and human evaluations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>反驳智能体：基于心智理论的学术反驳策略性说服框架</div>
<div class="mono" style="margin-top:8px">尽管人工智能已深度融入研究流程各环节并取得显著进展，学术反驳仍是重要且尚未充分探索的挑战。这是因为反驳是在严重信息不对称下进行的策略性沟通过程，而非简单的技术辩论。现有方法因主要模仿表层语言特征，缺乏有效说服所需的核心视角采择能力而效果有限。本文提出首个基于心智理论的学术反驳框架RebuttalAgent，通过心智理论-策略-响应三层架构实现：建模审稿人心理状态、制定说服策略、生成证据驱动的回应。为训练智能体，我们采用新型批判-精炼方法构建大规模数据集RebuttalBench。训练过程分为两阶段：首先通过监督微调使智能体掌握心智理论分析与策略规划能力，随后利用自奖励机制进行强化学习以实现可扩展的自我改进。为构建可靠高效的自动评估体系，我们进一步开发基于超10万条多源反驳数据训练的专业评估器Rebuttal-RM，其评分结果与人类偏好的一致性已超越GPT-4.1。大量实验表明，RebuttalAgent在自动指标上平均超越基线模型18.3%，同时在自动与人工评估中均优于先进的专有模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the underexplored challenge of academic rebuttal as a strategic communication process under information asymmetry, this paper introduces RebuttalAgent, a framework grounded in Theory of Mind (ToM) through a ToM-Strategy-Response pipeline to model reviewer mental states and formulate persuasive, evidence-based responses. The method involves training on a novel large-scale dataset, RebuttalBench, synthesized via critique-and-refine, using a two-stage process of supervised fine-tuning followed by reinforcement learning with self-reward, and employs a specialized automated evaluator, Rebuttal-RM, for reliable assessment. Experimental results demonstrate that RebuttalAgent outperforms the base model by an average of 18.3% on automated metrics and surpasses advanced proprietary models in both automated and human evaluations, with the evaluator achieving scoring consistency superior to GPT-4.1.</div>
<div class="mono" style="margin-top:8px">本文针对学术反驳这一在信息不对称下进行战略沟通的未充分探索的挑战，提出了RebuttalAgent框架，该框架基于心智理论，通过心智理论-策略-响应流程来建模审稿人心理状态并制定有说服力的、基于证据的回应。方法包括使用通过批判性精炼合成的新型大规模数据集RebuttalBench进行训练，采用监督微调后接基于自奖励的强化学习的两阶段过程，并利用专门的自动化评估器Rebuttal-RM进行可靠评估。实验结果表明，RebuttalAgent在自动化指标上平均优于基础模型18.3%，并在自动化和人工评估中均超越了先进的专有模型，其评估器的评分一致性超过了GPT-4.1。</div>
</details>
</div>
<div class="card">
<div class="title">Quantum feedback control with a transformer neural network architecture</div>
<div class="meta-line">Authors: Pranav Vaidhyanathan, Florian Marquardt, Mark T. Mitchison, Natalia Ares</div>
<div class="meta-line">Venue: Phys. Rev. Research 8, L012043, Published 24 February, 2026</div>
<div class="meta-line">First: 2024-11-28T16:42:30+00:00 · Latest: 2026-02-25T14:54:20+00:00</div>
<div class="meta-line">Comments: 9 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.19253v2">Abs</a> · <a href="https://arxiv.org/pdf/2411.19253v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Attention-based neural networks such as transformers have revolutionized various fields such as natural language processing, genomics, and vision. Here, we demonstrate the use of transformers for quantum feedback control through both a supervised and reinforcement learning approach. In particular, due to the transformer&#x27;s ability to capture long-range temporal correlations and training efficiency, we show that it can surpass some of the limitations of previous control approaches, e.g.~those based on recurrent neural networks trained using a similar approach or policy based reinforcement learning. We numerically show, for the example of state stabilization of a two-level system, that our bespoke transformer architecture can achieve near unit fidelity to a target state in a short time even in the presence of inefficient measurement and Hamiltonian perturbations that were not included in the training set as well as the control of non-Markovian systems. We also demonstrate that our transformer can perform energy minimization of non-integrable many-body quantum systems when trained for reinforcement learning tasks. Our approach can be used for quantum error correction, fast control of quantum states in the presence of colored noise, as well as real-time tuning, and characterization of quantum devices.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于Transformer神经网络架构的量子反馈控制</div>
<div class="mono" style="margin-top:8px">以Transformer为代表的注意力神经网络已彻底变革自然语言处理、基因组学和视觉等多个领域。本文通过监督学习与强化学习两种方法，展示了Transformer在量子反馈控制中的应用。得益于Transformer捕获长程时间关联的能力及训练效率优势，我们证明其能突破以往控制方法的局限（例如采用相似训练方式的循环神经网络或基于策略的强化学习方法）。以两能级系统状态稳定为例，数值模拟表明：即使面对训练集未包含的低效测量、哈密顿量扰动以及非马尔可夫系统控制，我们定制的Transformer架构仍可在短时间内实现接近单位保真度的目标态逼近。同时，当针对强化学习任务训练时，我们的Transformer还能执行不可积多体量子系统的能量最小化。该方法可应用于量子纠错、色噪声环境下的量子态快速控制，以及量子器件的实时调谐与表征。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the success of transformers in capturing long-range dependencies across various domains, this paper explores their application to quantum feedback control to overcome limitations of prior methods like recurrent neural networks. The method employs a transformer neural network architecture trained via both supervised and reinforcement learning approaches. Key experimental results from numerical simulations demonstrate that the transformer achieves near-unit fidelity for stabilizing a two-level system&#x27;s state rapidly, even under inefficient measurements and untrained Hamiltonian perturbations, and effectively minimizes energy in non-integrable many-body quantum systems, showcasing robustness and versatility for quantum error correction and device control.</div>
<div class="mono" style="margin-top:8px">受Transformer在捕捉长程依赖关系方面成功的启发，本文探索了其在量子反馈控制中的应用，以克服先前方法（如循环神经网络）的局限性。该方法采用基于Transformer的神经网络架构，通过监督学习和强化学习两种方式进行训练。数值模拟的关键实验结果表明，该Transformer能够快速实现两能级系统状态的稳定，达到接近单位的保真度，即使在低效测量和未训练的哈密顿扰动下也能保持性能，并能有效最小化不可积多体量子系统的能量，展现了其在量子纠错和设备控制方面的鲁棒性和通用性。</div>
</details>
</div>
<div class="card">
<div class="title">FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures</div>
<div class="meta-line">Authors: Jifeng Song, Arun Das, Pan Wang, Hui Ji, Kun Zhao, Yufei Huang</div>
<div class="meta-line">First: 2026-01-12T21:57:52+00:00 · Latest: 2026-02-25T13:52:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08026v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.08026v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scientific compound figures combine multiple labeled panels into a single image, but captions in real pipelines are often missing or only provide figure-level summaries, making panel-level understanding difficult. In this paper, we propose FigEx2, visual-conditioned framework that localizes panels and generates panel-wise captions directly from the compound figure. To mitigate the impact of diverse phrasing in open-ended captioning, we introduce a noise-aware gated fusion module that adaptively filters token-level features to stabilize the detection query space. Furthermore, we employ a staged optimization strategy combining supervised learning with reinforcement learning (RL), utilizing CLIP-based alignment and BERTScore-based semantic rewards to enforce strict multimodal consistency. To support high-quality supervision, we curate BioSci-Fig-Cap, a refined benchmark for panel-level grounding, alongside cross-disciplinary test suites in physics and chemistry. Experimental results demonstrate that FigEx2 achieves a superior 0.726 mAP@0.5:0.95 for detection and significantly outperforms Qwen3-VL-8B by 0.51 in METEOR and 0.24 in BERTScore. Notably, FigEx2 exhibits remarkable zero-shot transferability to out-of-distribution scientific domains without any fine-tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FigEx2：面向科学复合图表的视觉条件化面板检测与描述生成</div>
<div class="mono" style="margin-top:8px">科学复合图表将多个带标签的面板整合为单张图像，但实际流程中的描述文本常缺失或仅提供图表级摘要，导致面板级理解困难。本文提出FigEx2——一种视觉条件化框架，可直接从复合图表中定位面板并生成面板级描述。为缓解开放式描述中多样化表述的影响，我们引入噪声感知门控融合模块，自适应过滤词元级特征以稳定检测查询空间。此外，采用结合监督学习与强化学习的阶段式优化策略，利用基于CLIP的对齐奖励和基于BERTScore的语义奖励，确保严格的多模态一致性。为提供高质量监督数据，我们构建了面板级定位基准数据集BioSci-Fig-Cap，以及跨物理与化学学科的测试集。实验表明：FigEx2在检测任务上达到0.726 mAP@0.5:0.95的优异性能，在METEOR和BERTScore指标上分别显著超越Qwen3-VL-8B模型0.51和0.24。值得注意的是，FigEx2在未经微调的情况下，对分布外科学领域展现出卓越的零样本迁移能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that scientific compound figures often lack detailed panel-level captions, hindering fine-grained understanding. The proposed method, FigEx2, is a visual-conditioned framework that localizes panels and generates panel-specific captions directly from the figure; it introduces a noise-aware gated fusion module to stabilize detection by filtering token-level noise and employs a staged optimization strategy combining supervised and reinforcement learning with CLIP and BERTScore rewards for multimodal consistency. Main experimental results show FigEx2 achieves 0.726 mAP@0.5:0.95 for detection and outperforms Qwen3-VL-8B by 0.51 in METEOR and 0.24 in BERTScore, while demonstrating strong zero-shot transfer to out-of-distribution scientific domains without fine-tuning.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于科学复合图中常缺少面板级详细标注，阻碍了细粒度理解。所提出的方法FigEx2是一个视觉条件框架，可直接从复合图中定位面板并生成面板级描述；它引入了噪声感知门控融合模块，通过过滤词元级噪声来稳定检测，并采用结合监督学习与强化学习的阶段优化策略，利用CLIP和BERTScore奖励确保多模态一致性。主要实验结果表明，FigEx2在检测上达到0.726 mAP@0.5:0.95，在METEOR和BERTScore上分别显著超越Qwen3-VL-8B模型0.51和0.24分，且无需微调即展现出对分布外科学领域的出色零样本迁移能力。</div>
</details>
</div>
<div class="card">
<div class="title">PepCompass: Navigating peptide embedding spaces using Riemannian Geometry</div>
<div class="meta-line">Authors: Marcin Możejko, Adam Bielecki, Jurand Prądzyński, Marcin Traskowski, Antoni Janowski, Hyun-Su Lee, Marcelo Der Torossian Torres, Michał Kmicikiewicz, Paulina Szymczak, Karol Jurasz, Michał Kucharczyk, Cesar de la Fuente-Nunez, Ewa Szczurek</div>
<div class="meta-line">First: 2025-10-02T13:07:37+00:00 · Latest: 2026-02-25T13:26:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.01988v5">Abs</a> · <a href="https://arxiv.org/pdf/2510.01988v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Antimicrobial peptide discovery is challenged by the astronomical size of peptide space and the relative scarcity of active peptides. Generative models provide continuous latent &quot;maps&quot; of peptide space, but conventionally ignore decoder-induced geometry and rely on flat Euclidean metrics, rendering exploration and optimization distorted and inefficient. Prior manifold-based remedies assume fixed intrinsic dimensionality, which critically fails in practice for peptide data. Here, we introduce PepCompass, a geometry-aware framework for peptide exploration and optimization. At its core, we define a Union of $κ$-Stable Riemannian Manifolds $\mathbb{M}^κ$, a family of decoder-induced manifolds that captures local geometry while ensuring computational stability. We propose two local exploration methods: Second-Order Riemannian Brownian Efficient Sampling, which provides a convergent second-order approximation to Riemannian Brownian motion, and Mutation Enumeration in Tangent Space, which reinterprets tangent directions as discrete amino-acid substitutions. Combining these yields Local Enumeration Bayesian Optimization (LE-BO), an efficient algorithm for local activity optimization. Finally, we introduce Potential-minimizing Geodesic Search (PoGS), which interpolates between prototype embeddings along property-enriched geodesics, biasing discovery toward seeds, i.e. peptides with favorable activity. In-vitro validation confirms the effectiveness of PepCompass: PoGS yields four novel seeds, and subsequent optimization with LE-BO discovers 25 highly active peptides with broad-spectrum activity, including against resistant bacterial strains. These results demonstrate that geometry-informed exploration provides a powerful new paradigm for antimicrobial peptide design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PepCompass：利用黎曼几何导航肽嵌入空间</div>
<div class="mono" style="margin-top:8px">抗菌肽发现面临肽空间规模庞大与活性肽相对稀少的挑战。生成模型虽能提供肽空间的连续潜在“图谱”，但传统方法忽略解码器诱导的几何结构并依赖平坦欧几里得度量，导致探索与优化过程扭曲且低效。现有基于流形的方法假设固有维度固定，这在肽数据实践中存在严重缺陷。本文提出PepCompass——一种几何感知的肽探索与优化框架。其核心是定义$κ$-稳定黎曼流形并集$\mathbb{M}^κ$，该解码器诱导流形族能捕捉局部几何特征并确保计算稳定性。我们提出两种局部探索方法：二阶黎曼布朗高效采样（提供黎曼布朗运动的收敛二阶近似）与切空间突变枚举（将切方向重新解释为离散氨基酸替换）。结合二者形成局部枚举贝叶斯优化算法，实现高效的局部活性优化。最后，我们提出势最小化测地线搜索，通过沿属性增强的测地线在原型嵌入间插值，使发现过程偏向具有优势活性的种子肽。体外实验验证了PepCompass的有效性：测地线搜索获得四种新型种子肽，经局部枚举贝叶斯优化后进一步发现25种具有广谱活性（包括抗耐药菌株）的高活性肽。这些结果表明，几何感知探索为抗菌肽设计提供了强大的新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenges of exploring vast peptide spaces with scarce active candidates and the limitations of Euclidean metrics in generative models, this paper introduces PepCompass, a geometry-aware framework that models peptide embeddings using a Union of κ-Stable Riemannian Manifolds to capture local decoder-induced geometry. The method includes two local exploration techniques: Second-Order Riemannian Brownian Efficient Sampling for stable sampling and Mutation Enumeration in Tangent Space for discrete amino-acid substitutions, combined into Local Enumeration Bayesian Optimization (LE-BO) for efficient activity optimization, alongside Potential-minimizing Geodesic Search (PoGS) for property-enriched interpolation between prototypes. Experimental results from in-vitro validation show that PoGS generated four novel seed peptides, and LE-BO subsequently discovered 25 highly active peptides with broad-spectrum efficacy, including against resistant bacterial strains, demonstrating the power of geometry-informed exploration for antimicrobial peptide design.</div>
<div class="mono" style="margin-top:8px">本文针对抗菌肽发现中肽空间巨大而活性肽稀缺的挑战，以及生成模型中欧几里得度量忽略几何结构导致的探索低效问题，提出了PepCompass框架，其核心是使用κ-稳定黎曼流形联合体来建模肽嵌入的局部几何。方法包括两种局部探索技术：二阶黎曼布朗高效采样用于稳定采样，切空间突变枚举将切方向解释为离散氨基酸替换，两者结合形成局部枚举贝叶斯优化（LE-BO）以实现高效活性优化，同时引入势最小化测地线搜索（PoGS）在原型间沿属性增强的测地线进行插值。体外实验验证表明，PoGS生成了四种新型种子肽，LE-BO随后发现了25种具有广谱活性的高活性肽，包括对抗耐药菌株，证明了基于几何的探索为抗菌肽设计提供了强大新范式。</div>
</details>
</div>
<div class="card">
<div class="title">Distill and Align Decomposition for Enhanced Claim Verification</div>
<div class="meta-line">Authors: Jabez Magomere, Elena Kochkina, Samuel Mensah, Simerjot Kaur, Fernando Acero, Arturo Oncevay, Charese H. Smiley, Xiaomo Liu, Manuela Veloso</div>
<div class="meta-line">First: 2026-02-25T12:32:04+00:00 · Latest: 2026-02-25T12:32:04+00:00</div>
<div class="meta-line">Comments: EACL Findings 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21857v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21857v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Complex claim verification requires decomposing sentences into verifiable subclaims, yet existing methods struggle to align decomposition quality with verification performance. We propose a reinforcement learning (RL) approach that jointly optimizes decomposition quality and verifier alignment using Group Relative Policy Optimization (GRPO). Our method integrates: (i) structured sequential reasoning; (ii) supervised finetuning on teacher-distilled exemplars; and (iii) a multi-objective reward balancing format compliance, verifier alignment, and decomposition quality. Across six evaluation settings, our trained 8B decomposer improves downstream verification performance to (71.75%) macro-F1, outperforming prompt-based approaches ((+1.99), (+6.24)) and existing RL methods ((+5.84)). Human evaluation confirms the high quality of the generated subclaims. Our framework enables smaller language models to achieve state-of-the-art claim verification by jointly optimising for verification accuracy and decomposition quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于蒸馏与对齐分解的增强式声明验证</div>
<div class="mono" style="margin-top:8px">复杂声明验证需将句子分解为可验证的子声明，但现有方法难以协调分解质量与验证性能。我们提出一种强化学习方法，通过群体相对策略优化联合优化分解质量与验证器对齐。该方法整合了：（i）结构化序列推理；（ii）基于教师蒸馏范例的监督微调；（iii）平衡格式合规性、验证器对齐与分解质量的多目标奖励机制。在六种评估场景中，我们训练的80亿参数分解器将下游验证性能提升至宏观F1值71.75%，优于基于提示的方法（+1.99/+6.24）和现有强化学习方法（+5.84）。人工评估证实了生成子声明的高质量。该框架通过联合优化验证准确性与分解质量，使小型语言模型能够实现最先进的声明验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of aligning decomposition quality with verification performance in complex claim verification. The authors propose a reinforcement learning approach using Group Relative Policy Optimization (GRPO) that jointly optimizes decomposition and verifier alignment through structured sequential reasoning, supervised finetuning on teacher-distilled exemplars, and a multi-objective reward balancing format compliance, verifier alignment, and decomposition quality. Experimental results across six evaluation settings show their trained 8B decomposer achieves 71.75% macro-F1, outperforming prompt-based approaches by 1.99-6.24 points and existing RL methods by 5.84 points, with human evaluation confirming high-quality subclaim generation.</div>
<div class="mono" style="margin-top:8px">本文针对复杂声明验证中分解质量与验证性能难以对齐的问题，提出了一种基于强化学习的方法。该方法采用群体相对策略优化（GRPO），通过结构化顺序推理、基于教师蒸馏示例的监督微调，以及平衡格式合规性、验证器对齐和分解质量的多目标奖励机制，联合优化分解过程与验证器对齐。在六个评估场景中的实验结果表明，训练后的80亿参数分解器实现了71.75%的宏观F1分数，相比基于提示的方法提升1.99-6.24个百分点，优于现有强化学习方法5.84个百分点，人工评估也证实了生成子声明的高质量。</div>
</details>
</div>
<div class="card">
<div class="title">Object-Centric World Models from Few-Shot Annotations for Sample-Efficient Reinforcement Learning</div>
<div class="meta-line">Authors: Weipu Zhang, Adam Jelley, Trevor McInroe, Amos Storkey, Gang Wang</div>
<div class="meta-line">First: 2025-01-27T19:07:06+00:00 · Latest: 2026-02-25T12:27:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.16443v2">Abs</a> · <a href="https://arxiv.org/pdf/2501.16443v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While deep reinforcement learning (RL) from pixels has achieved remarkable success, its sample inefficiency remains a critical limitation for real-world applications. Model-based RL (MBRL) addresses this by learning a world model to generate simulated experience, but standard approaches that rely on pixel-level reconstruction losses often fail to capture small, task-critical objects in complex, dynamic scenes. We posit that an object-centric (OC) representation can direct model capacity toward semantically meaningful entities, improving dynamics prediction and sample efficiency. In this work, we introduce OC-STORM, an object-centric MBRL framework that enhances a learned world model with object representations extracted by a pretrained segmentation network. By conditioning on a minimal number of annotated frames, OC-STORM learns to track decision-relevant object dynamics and inter-object interactions without extensive labeling or access to privileged information. Empirical results demonstrate that OC-STORM significantly outperforms the STORM baseline on the Atari 100k benchmark and achieves state-of-the-art sample efficiency on challenging boss fights in the visually complex game Hollow Knight. Our findings underscore the potential of integrating OC priors into MBRL for complex visual domains. Project page: https://oc-storm.weipuzhang.com</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于少量标注的以对象为中心的世界模型用于样本高效强化学习</div>
<div class="mono" style="margin-top:8px">尽管基于像素的深度强化学习（RL）已取得显著成功，但其样本效率低下仍是实际应用中的关键限制。基于模型的强化学习（MBRL）通过学习世界模型生成模拟经验来解决这一问题，但依赖像素级重建损失的标准方法往往难以捕捉复杂动态场景中微小但任务关键的对象。我们认为，以对象为中心（OC）的表征能将模型能力导向语义实体，从而改善动态预测和样本效率。本文提出OC-STORM，一种以对象为中心的MBRL框架，通过预训练分割网络提取的对象表征增强学习到的世界模型。仅需基于少量标注帧进行条件化，OC-STORM即可学习跟踪决策相关的对象动态和对象间交互，无需大量标注或特权信息。实验结果表明，OC-STORM在Atari 100k基准测试中显著优于STORM基线，并在视觉复杂游戏《空洞骑士》的挑战性首领战中实现了最先进的样本效率。我们的发现凸显了将OC先验整合到MBRL中以处理复杂视觉领域的潜力。项目页面：https://oc-storm.weipuzhang.com</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the sample inefficiency of pixel-based deep reinforcement learning (RL) in complex visual environments by proposing an object-centric model-based RL (MBRL) framework. The motivation is that standard MBRL methods using pixel reconstruction often miss small, critical objects, so the authors introduce OC-STORM, which enhances a learned world model with object representations from a pretrained segmentation network, conditioned on few annotated frames to track object dynamics and interactions without extensive labeling. Experimental results show that OC-STORM outperforms the STORM baseline on the Atari 100k benchmark and achieves state-of-the-art sample efficiency on challenging boss fights in the game Hollow Knight, demonstrating the value of object-centric priors for improving RL sample efficiency.</div>
<div class="mono" style="margin-top:8px">本文针对基于像素的深度强化学习在复杂视觉环境中样本效率低下的问题，提出了一种以物体为中心的基于模型的强化学习框架。其动机在于标准方法依赖像素重建，常忽略小型关键物体，因此作者引入了OC-STORM，该方法利用预训练分割网络提取的物体表征来增强学习的世界模型，仅需少量标注帧即可跟踪物体动态和交互，无需大量标注。实验结果表明，OC-STORM在Atari 100k基准测试中优于STORM基线，并在视觉复杂的游戏《空洞骑士》的困难首领战中实现了最先进的样本效率，验证了物体中心先验对于提升强化学习样本效率的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">JSAM: Privacy Straggler-Resilient Joint Client Selection and Incentive Mechanism Design in Differentially Private Federated Learning</div>
<div class="meta-line">Authors: Ruichen Xu, Ying-Jun Angela Zhang, Jianwei Huang</div>
<div class="meta-line">First: 2026-02-25T12:22:48+00:00 · Latest: 2026-02-25T12:22:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21844v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21844v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Differentially private federated learning faces a fundamental tension: privacy protection mechanisms that safeguard client data simultaneously create quantifiable privacy costs that discourage participation, undermining the collaborative training process. Existing incentive mechanisms rely on unbiased client selection, forcing servers to compensate even the most privacy-sensitive clients (&quot;privacy stragglers&quot;), leading to systemic inefficiency and suboptimal resource allocation. We introduce JSAM (Joint client Selection and privacy compensAtion Mechanism), a Bayesian-optimal framework that simultaneously optimizes client selection probabilities and privacy compensation to maximize training effectiveness under budget constraints. Our approach transforms a complex 2N-dimensional optimization problem into an efficient three-dimensional formulation through novel theoretical characterization of optimal selection strategies. We prove that servers should preferentially select privacy-tolerant clients while excluding high-sensitivity participants, and uncover the counter-intuitive insight that clients with minimal privacy sensitivity may incur the highest cumulative costs due to frequent participation. Extensive evaluations on MNIST and CIFAR-10 demonstrate that JSAM achieves up to 15% improvement in test accuracy compared to existing unbiased selection mechanisms while maintaining cost efficiency across varying data heterogeneity levels.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>JSAM：差分隐私联邦学习中的抗隐私滞后联合客户端选择与激励机制设计</div>
<div class="mono" style="margin-top:8px">差分隐私联邦学习面临一个根本性矛盾：保护客户端数据的隐私机制会同时产生可量化的隐私成本，抑制参与意愿，从而破坏协作训练过程。现有激励机制依赖无偏客户端选择，迫使服务器即使对最敏感的“隐私滞后”客户端进行补偿，导致系统效率低下和资源分配次优。我们提出JSAM（联合客户端选择与隐私补偿机制），这是一个贝叶斯最优框架，可在预算约束下同步优化客户端选择概率和隐私补偿，以最大化训练效果。通过创新性地理论刻画最优选择策略，我们将复杂的2N维优化问题转化为高效的三维形式。我们证明服务器应优先选择隐私容忍度高的客户端，同时排除高敏感度参与者，并揭示了一个反直觉的发现：隐私敏感度最低的客户端可能因频繁参与而产生最高的累计成本。在MNIST和CIFAR-10数据集上的大量实验表明，相较于现有无偏选择机制，JSAM在保持不同数据异质性水平下成本效率的同时，测试准确率最高提升15%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the tension in differentially private federated learning where privacy protection incurs costs that deter client participation, and existing incentive mechanisms inefficiently compensate all clients, including privacy-sensitive &#x27;stragglers&#x27;. The authors propose JSAM, a Bayesian-optimal framework that jointly optimizes client selection probabilities and privacy compensation to maximize training effectiveness under budget constraints, transforming a complex high-dimensional problem into an efficient three-dimensional formulation through novel theoretical insights. Experimental results on MNIST and CIFAR-10 show JSAM achieves up to 15% higher test accuracy than unbiased selection methods while maintaining cost efficiency across varying data heterogeneity levels.</div>
<div class="mono" style="margin-top:8px">本文针对差分隐私联邦学习中隐私保护导致参与成本、现有激励机制低效补偿所有客户端（包括隐私敏感的“掉队者”）的问题，提出JSAM这一贝叶斯最优框架，在预算约束下联合优化客户端选择概率和隐私补偿以最大化训练效果，通过新颖的理论洞察将复杂高维问题转化为高效的三维形式。在MNIST和CIFAR-10数据集上的实验表明，JSAM相比无偏选择机制实现了高达15%的测试精度提升，同时在数据异构性变化下保持了成本效率。</div>
</details>
</div>
<div class="card">
<div class="title">Resisting Contextual Interference in RAG via Parametric-Knowledge Reinforcement</div>
<div class="meta-line">Authors: Chenyu Lin, Yilin Wen, Du Su, Hexiang Tan, Fei Sun, Muhan Chen, Chenfu Bao, Zhonghou Lyu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-06-05T15:34:15+00:00 · Latest: 2026-02-25T12:22:09+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.05154v4">Abs</a> · <a href="https://arxiv.org/pdf/2506.05154v4">PDF</a> · <a href="https://github.com/lcy80366872/knowledgeable-R1">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieval-augmented generation (RAG) improves performance on knowledge-intensive tasks but can be derailed by wrong, irrelevant, or conflicting retrieved text, causing models to rely on inaccurate evidence and cascade errors. We propose Knowledgeable-R1, a reinforcement-learning framework that explicitly trains large language models to use parametric knowledge (PK) to resist contextual interference while still exploiting external context when it is reliably helpful. Knowledgeable-R1 introduces a joint sampling scheme that generates paired responses with and without retrieval, and learns both local advantages (within each decoding regime) and global advantages under the same input to quantify when to ignore misleading context versus adopt it. We employ an asymmetric advantage transformation that amplifies exploratory behaviors toward parametric knowledge. Experiments show that Knowledgeable-R1 significantly improves robustness and reasoning accuracy in knowledge conflict scenarios and general RAG scenarios, outperforming SOTA baselines by +22.89% in counterfactual scenarios, and without degradation when the retrieved context is fully accurate.Our code are available at https://github.com/lcy80366872/knowledgeable-R1.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过参数知识强化抵御RAG中的上下文干扰</div>
<div class="mono" style="margin-top:8px">检索增强生成（RAG）提升了知识密集型任务的性能，但可能因错误、无关或冲突的检索文本而偏离正轨，导致模型依赖不准确的证据并引发级联错误。我们提出Knowledgeable-R1，一种强化学习框架，通过显式训练大语言模型利用参数知识（PK）来抵御上下文干扰，同时在外部上下文可靠有益时仍加以利用。该框架引入联合采样方案，生成带检索与不带检索的配对响应，并学习同一输入下的局部优势（各解码机制内）与全局优势，以量化何时忽略误导性上下文或采纳之。我们采用非对称优势变换，强化对参数知识的探索行为。实验表明，Knowledgeable-R1在知识冲突场景和一般RAG场景中显著提升了鲁棒性与推理准确率，在反事实场景中超越SOTA基线+22.89%，且在检索上下文完全准确时性能无衰减。代码发布于https://github.com/lcy80366872/knowledgeable-R1。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the problem of contextual interference in retrieval-augmented generation (RAG), where incorrect or irrelevant retrieved text can mislead models, causing error cascades. To mitigate this, the authors propose Knowledgeable-R1, a reinforcement learning framework that trains large language models to rely on their internal parametric knowledge to resist misleading context while still leveraging helpful external information. The method uses a joint sampling scheme to generate paired responses with and without retrieval, learning both local and global advantages to decide when to ignore or adopt retrieved context, enhanced by an asymmetric advantage transformation to encourage parametric knowledge use. Experimental results demonstrate that Knowledgeable-R1 significantly boosts robustness and reasoning accuracy in knowledge conflict and general RAG scenarios, outperforming state-of-the-art baselines by +22.89% in counterfactual settings without degrading performance when context is accurate.</div>
<div class="mono" style="margin-top:8px">本文针对检索增强生成（RAG）中的上下文干扰问题展开研究，即错误或不相关的检索文本会误导模型并引发错误传播。为解决此问题，作者提出了Knowledgeable-R1，一个强化学习框架，通过训练大语言模型依赖其内部参数化知识来抵抗误导性上下文，同时仍能利用有益的外部信息。该方法采用联合采样方案生成带检索和不带检索的配对响应，学习局部和全局优势以决定何时忽略或采纳检索内容，并通过非对称优势变换强化对参数化知识的探索。实验结果表明，Knowledgeable-R1在知识冲突和一般RAG场景中显著提高了鲁棒性和推理准确性，在反事实场景下优于现有最佳基线22.89%，且在上下文准确时性能无下降。</div>
</details>
</div>
<div class="card">
<div class="title">Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration</div>
<div class="meta-line">Authors: Zhicheng Yang, Zhijiang Guo, Yinya Huang, Yongxin Wang, Dongchun Xie, Hanhui Li, Yiwei Wang, Xiaodan Liang, Jing Tang</div>
<div class="meta-line">First: 2025-08-19T11:51:40+00:00 · Latest: 2026-02-25T12:13:07+00:00</div>
<div class="meta-line">Comments: 20 pages, 17 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.13755v6">Abs</a> · <a href="https://arxiv.org/pdf/2508.13755v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models, yet its full potential is hindered by two under-explored dimensions: Depth-the hardest problem a model can sample; Breadth-the number of instances consumed in a single iteration. We dissect the popular GRPO algorithm and reveal a systematic bias: the cumulative-advantage disproportionately weights samples with medium accuracy, while down-weighting the low-accuracy instances that are crucial for pushing reasoning boundaries. To rectify the depth neglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which re-weights hard problems through targeted multi-stage rollouts, thereby increasing the number of positive rollouts for hard problems. Empirically, naively enlarging rollout size only accelerates convergence and even hurts Pass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra inference cost at convergence. Just as we adaptively expanded the depth of exploration, we now ask whether aggressively scaling the breadth of training data can further amplify reasoning gains. To this end, we intensely scale batch size and replace PPO&#x27;s mini-batch iterations with full-batch updates over multiple epochs. Increasing breadth significantly enhances Pass@1 performance. Large-breadth training sustains high token-level entropy, indicating continued exploration and reduced gradient noise. We further present DARS-B, which augments DARS with large breadth, and demonstrate simultaneous gains in Pass@K and Pass@1. The results confirm that breadth and adaptive exploration across depth operate as orthogonal dimensions in RLVR, which are key to unleashing the reasoning power of RLVR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RLVR中的深度-广度协同：通过自适应探索释放大语言模型推理能力</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习已成为释放大语言模型推理能力的重要范式，但其潜力受限于两个未充分探索的维度：深度（模型能采样的最难问题）与广度（单次迭代消耗的实例数量）。我们剖析主流GRPO算法，揭示其存在系统性偏差：累积优势值过度加权中等准确率样本，而低估对突破推理边界至关重要的低准确率样本。为纠正深度忽视问题，我们提出难度自适应轨迹采样方法，通过定向多阶段轨迹对难题进行重加权，从而增加难题的正向轨迹数量。实验表明，单纯扩大轨迹规模仅加速收敛，甚至损害Pass@K指标。相比之下，DARS方法在不增加收敛时推理成本的前提下，持续提升Pass@K。在实现深度自适应探索后，我们进一步探究大幅扩展训练数据广度能否增强推理收益。为此，我们大幅扩展批次规模，将PPO的小批次迭代替换为多轮次的全批次更新。广度扩展显著提升Pass@1性能，大广度训练维持较高的词元级熵值，表明持续探索与梯度噪声降低。我们进一步提出DARS-B方法，将DARS与大广度训练结合，实现在Pass@K与Pass@1指标上的同步提升。结果证实广度与深度自适应探索在RLVR中构成正交维度，是释放其推理能力的关键。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses limitations in Reinforcement Learning with Verifiable Reward (RLVR) for enhancing large language model reasoning, identifying a bias in algorithms like GRPO that neglect difficult problems crucial for depth. The authors propose Difficulty Adaptive Rollout Sampling (DARS) to re-weight hard problems via targeted rollouts, improving Pass@K without extra inference cost, and further scale training data breadth with large-batch updates, which boosts Pass@1 by sustaining exploration. Experimental results show that combining DARS with breadth scaling (DARS-B) yields simultaneous gains in both Pass@K and Pass@1, confirming depth and breadth as orthogonal dimensions key to unlocking RLVR&#x27;s reasoning potential.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习与可验证奖励（RLVR）在提升大语言模型推理能力中的局限性，揭示了如GRPO等算法存在的偏见，即忽视了对推理深度至关重要的难题。作者提出了难度自适应采样（DARS）方法，通过定向采样重新加权难题，在不增加推理成本的情况下提高了Pass@K指标，并进一步通过大规模批次更新扩展训练数据广度，从而提升Pass@1性能并保持探索性。实验结果表明，将DARS与广度扩展结合（DARS-B）可在Pass@K和Pass@1上同时取得增益，证实了深度与广度作为正交维度是释放RLVR推理能力的关键。</div>
</details>
</div>
<div class="card">
<div class="title">Generalisation of RLHF under Reward Shift and Clipped KL Regularisation</div>
<div class="meta-line">Authors: Kenton Tang, Yuzhu Chen, Fengxiang He</div>
<div class="meta-line">First: 2026-02-25T10:36:17+00:00 · Latest: 2026-02-25T10:36:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21765v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21765v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Alignment and adaptation in large language models heavily rely on reinforcement learning from human feedback (RLHF); yet, theoretical understanding of its generalisability remains premature, especially when the learned reward could shift, and the KL control is estimated and clipped. To address this issue, we develop generalisation theory for RLHF that explicitly accounts for (1) \emph{reward shift}: reward models are trained on preference data from earlier or mixed behaviour policies while RLHF optimises the current policy on its own rollouts; and (2) \emph{clipped KL regularisation}: the KL regulariser is estimated from sampled log-probability ratios and then clipped for stabilisation, resulting in an error to RLHF. We present generalisation bounds for RLHF, suggesting that the generalisation error stems from a sampling error from prompts and rollouts, a reward shift error, and a KL clipping error. We also discuss special cases of (1) initialising RLHF parameters with a uniform prior over a finite space, and (2) training RLHF by stochastic gradient descent, as an Ornstein-Uhlenbeck process. The theory yields practical implications in (1) optimal KL clipping threshold, and (2) budget allocation in prompts, rollouts, and preference data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>奖励偏移与截断KL正则化下RLHF的泛化理论</div>
<div class="mono" style="margin-top:8px">大语言模型的校准与适配高度依赖基于人类反馈的强化学习（RLHF），但其泛化性的理论理解仍不成熟，尤其在所学奖励可能偏移、KL控制需估计并截断的情况下。为此，我们构建了RLHF的泛化理论，明确纳入：（1）奖励偏移：奖励模型基于早期或混合行为策略的偏好数据训练，而RLHF在当前策略自身推演中优化；（2）截断KL正则化：KL正则项通过采样对数概率比估计并截断以稳定训练，从而引入误差。我们提出了RLHF的泛化界，表明泛化误差源于提示与推演的采样误差、奖励偏移误差及KL截断误差。同时探讨了两种特例：（1）在有限空间上以均匀先验初始化RLHF参数；（2）通过随机梯度下降（视为Ornstein-Uhlenbeck过程）训练RLHF。该理论对（1）最优KL截断阈值及（2）提示、推演与偏好数据的资源分配具有实践指导意义。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limited theoretical understanding of reinforcement learning from human feedback (RLHF) by developing a generalization theory that explicitly accounts for two practical challenges: reward shift, where reward models are trained on data from different behavior policies than the one being optimized, and clipped KL regularization, which introduces estimation errors due to sampling and clipping. The method involves deriving generalization bounds for RLHF, which decompose the error into components from prompt and rollout sampling, reward shift, and KL clipping. The main experimental results, derived theoretically, suggest practical implications for determining an optimal KL clipping threshold and for allocating budgets effectively across prompts, rollouts, and preference data collection.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习从人类反馈中泛化理论不足的问题，提出了一个考虑两个实际挑战的理论框架：奖励偏移（即奖励模型基于不同行为策略的数据训练，而优化当前策略时产生偏差）和裁剪KL正则化（由于采样和裁剪引入估计误差）。方法是通过推导RLHF的泛化边界，将误差分解为提示与轨迹采样、奖励偏移和KL裁剪误差。理论分析得出的主要结果提供了实践启示，包括确定最优KL裁剪阈值以及在提示、轨迹和偏好数据收集之间进行有效的预算分配。</div>
</details>
</div>
<div class="card">
<div class="title">RABot: Reinforcement-Guided Graph Augmentation for Imbalanced and Noisy Social Bot Detection</div>
<div class="meta-line">Authors: Longlong Zhang, Xi Wang, Haotong Du, Yangyi Xu, Zhuo Liu, Yang Liu</div>
<div class="meta-line">First: 2026-02-25T10:02:57+00:00 · Latest: 2026-02-25T10:02:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21749v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21749v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Social bot detection is pivotal for safeguarding the integrity of online information ecosystems. Although recent graph neural network (GNN) solutions achieve strong results, they remain hindered by two practical challenges: (i) severe class imbalance arising from the high cost of generating bots, and (ii) topological noise introduced by bots that skillfully mimic human behavior and forge deceptive links. We propose the Reinforcement-guided graph Augmentation social Bot detector (RABot), a multi-granularity graph-augmentation framework that addresses both issues in a unified manner. RABot employs a neighborhood-aware oversampling strategy that linearly interpolates minority-class embeddings within local subgraphs, thereby stabilizing the decision boundary under low-resource regimes. Concurrently, a reinforcement-learning-driven edge-filtering module combines similarity-based edge features with adaptive threshold optimization to excise spurious interactions during message passing, yielding a cleaner topology. Extensive experiments on three real-world benchmarks and four GNN backbones demonstrate that RABot consistently surpasses state-of-the-art baselines. In addition, since its augmentation and filtering modules are orthogonal to the underlying architecture, RABot can be seamlessly integrated into existing GNN pipelines to boost performance with minimal overhead.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RABot：面向不平衡与噪声社交机器人检测的强化学习引导图增强方法</div>
<div class="mono" style="margin-top:8px">社交机器人检测对维护在线信息生态系统的完整性至关重要。尽管当前基于图神经网络（GNN）的解决方案已取得显著成果，但仍受限于两大实际挑战：（一）生成机器人的高成本导致的严重类别不平衡；（二）机器人通过模拟人类行为与伪造欺骗性链接引入的拓扑噪声。本文提出强化学习引导的图增强社交机器人检测器（RABot），这是一个多粒度图增强框架，以统一方式解决上述问题。RABot采用邻域感知的过采样策略，通过在局部子图中对少数类嵌入进行线性插值，从而在低资源场景下稳定决策边界。同时，基于强化学习的边过滤模块将基于相似性的边特征与自适应阈值优化相结合，在消息传递过程中剔除虚假交互，获得更纯净的拓扑结构。在三个真实场景基准数据集和四种GNN骨干网络上的大量实验表明，RABot持续超越现有最优基线方法。此外，由于该框架的增强与过滤模块与底层架构正交，RABot可无缝集成至现有GNN流程中，以最小开销显著提升检测性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by two practical challenges in social bot detection: severe class imbalance due to the high cost of generating bots, and topological noise from bots that mimic human behavior and create deceptive links. To address these, the authors propose RABot, a multi-granularity graph-augmentation framework that combines a neighborhood-aware oversampling strategy for minority-class embeddings with a reinforcement-learning-driven edge-filtering module to remove spurious interactions. Experimental results on three real-world benchmarks and four GNN backbones show that RABot consistently outperforms state-of-the-art baselines, and its modular design allows seamless integration into existing GNN pipelines with minimal overhead.</div>
<div class="mono" style="margin-top:8px">该论文的动机源于社交机器人检测中的两个实际挑战：由于生成机器人成本高昂导致的严重类别不平衡，以及机器人模仿人类行为并创建欺骗性链接引入的拓扑噪声。为解决这些问题，作者提出了RABot，一个多粒度图增强框架，结合了针对少数类嵌入的邻域感知过采样策略和基于强化学习的边过滤模块以消除虚假交互。在三个真实世界基准和四个GNN骨干网络上的实验结果表明，RABot始终优于最先进的基线方法，且其模块化设计能以最小开销无缝集成到现有GNN流程中提升性能。</div>
</details>
</div>
<div class="card">
<div class="title">The Art of Efficient Reasoning: Data, Reward, and Optimization</div>
<div class="meta-line">Authors: Taiqiang Wu, Zenan Xu, Bo Zhou, Ngai Wong</div>
<div class="meta-line">First: 2026-02-24T14:28:16+00:00 · Latest: 2026-02-25T09:40:11+00:00</div>
<div class="meta-line">Comments: Tech Report, Insights on Efficient Reasoning via Reward Shaping</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20945v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.20945v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高效推理的艺术：数据、奖励与优化</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）持续受益于规模化思维链（CoT）推理，但也面临沉重的计算开销。为解决此问题，高效推理旨在激励简短而准确的思维轨迹，通常通过强化学习（RL）的奖励塑形实现。本文系统研究了LLMs高效推理的机制。为全面评估，我们倡导采用更细粒度的指标，包括基于正确性的长度分布以及在2k至32k广泛令牌预算范围内的性能表现。首先，我们揭示训练过程遵循两阶段范式：长度适应与推理精炼。随后，我们在统一协议下开展大量实验（约20万GPU小时），解构训练提示与推演、奖励塑形及优化策略。关键发现之一是：在相对简单的提示上训练，可确保正向奖励信号的密度，从而避免长度塌缩。同时，习得的长度偏置具备跨领域泛化能力。我们将所有发现提炼为有价值的洞见与实践指南，并在Qwen3系列（0.6B至30B参数规模）中进一步验证，证明了方法的鲁棒性与泛化性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates how to make Large Language Models (LLMs) reason more efficiently by producing shorter yet accurate reasoning chains, motivated by the computational overhead of standard scaled Chain-of-Thought methods. The method systematically studies efficient reasoning mechanics through extensive experiments involving reward shaping with Reinforcement Learning (RL), analyzing training prompts, rollouts, and optimization strategies. Key experimental results reveal a two-stage training paradigm of length adaptation followed by reasoning refinement, and demonstrate that training on easier prompts prevents length collapse and that the learned length bias generalizes across domains, with findings validated across model sizes from 0.6B to 30B parameters.</div>
<div class="mono" style="margin-top:8px">本文研究如何使大语言模型（LLM）进行更高效的推理，即生成更短但准确的思维链，其动机在于标准规模化思维链方法带来的沉重计算开销。该方法通过强化学习进行奖励塑造，系统性地研究了高效推理的机制，并进行了大量实验，解构了训练提示、过程、奖励函数和优化策略。主要实验结果表明，训练遵循长度适应和推理精炼的两阶段范式，并且在相对简单的提示上训练可以避免长度崩溃，同时学习到的长度偏好能够跨领域泛化，这些发现在0.6B到30B参数规模的模型系列中得到了验证。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Cross-Domain Offline Reinforcement Learning with Dynamics- and Value-Aligned Data Filtering</div>
<div class="meta-line">Authors: Zhongjian Qiao, Rui Yang, Jiafei Lyu, Chenjia Bai, Xiu Li, Siyang Gao, Shuang Qiu</div>
<div class="meta-line">First: 2025-12-02T05:45:40+00:00 · Latest: 2026-02-25T09:39:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.02435v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.02435v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cross-domain offline reinforcement learning (RL) aims to train a well-performing agent in the target environment, leveraging both a limited target domain dataset and a source domain dataset with (possibly) sufficient data coverage. Due to the underlying dynamics misalignment between source and target domains, naively merging the two datasets may incur inferior performance. Recent advances address this issue by selectively leveraging source domain samples whose dynamics align well with the target domain. However, our work demonstrates that dynamics alignment alone is insufficient, by examining the limitations of prior frameworks and deriving a new target domain sub-optimality bound for the policy learned on the source domain. More importantly, our theory underscores an additional need for \textit{value alignment}, i.e., selecting high-quality, high-value samples from the source domain, a critical dimension overlooked by existing works. Motivated by such theoretical insight, we propose \textbf{\underline{D}}ynamics- and \textbf{\underline{V}}alue-aligned \textbf{\underline{D}}ata \textbf{\underline{F}}iltering (DVDF) method, a novel unified cross-domain RL framework that selectively incorporates source domain samples exhibiting strong alignment in \textit{both dynamics and values}. We empirically study a range of dynamics shift scenarios, including kinematic and morphology shifts, and evaluate DVDF on various tasks and datasets, even in the challenging setting where the target domain dataset contains an extremely limited amount of data. Extensive experiments demonstrate that DVDF consistently outperforms strong baselines with significant improvements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于动态与价值对齐数据筛选的高效跨域离线强化学习</div>
<div class="mono" style="margin-top:8px">跨域离线强化学习旨在利用有限的目标域数据集和（可能）数据覆盖充分的源域数据集，在目标环境中训练出性能优异的智能体。由于源域与目标域之间存在潜在动态特性差异，直接合并两个数据集可能导致性能下降。近期研究通过选择性利用动态特性与目标域对齐的源域样本来解决此问题。然而，本文通过分析现有框架的局限性，并推导出在源域学习策略的新目标域次优性边界，证明仅依赖动态对齐是不充分的。更重要的是，我们的理论强调了对“价值对齐”的额外需求，即从源域筛选高质量、高价值的样本——这是现有工作中被忽视的关键维度。基于这一理论洞见，我们提出了“动态与价值对齐数据筛选”（DVDF）方法，这是一种新颖的统一跨域强化学习框架，能选择性整合在“动态与价值”两方面均表现出强对齐性的源域样本。我们通过实验研究了包括运动学与形态学变化在内的多种动态偏移场景，并在不同任务和数据集上评估DVDF，即使在目标域数据集数据量极少的挑战性设定下，该方法仍表现优异。大量实验表明，DVDF始终显著优于现有基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of cross-domain offline reinforcement learning, where leveraging source domain data alongside limited target domain data is hindered by dynamics misalignment, which prior methods only partially mitigate by filtering for dynamics-aligned samples. The authors theoretically demonstrate that dynamics alignment alone is insufficient and identify a critical need for value alignment—selecting high-value source samples—to ensure policy quality. They propose the Dynamics- and Value-aligned Data Filtering (DVDF) method, a unified framework that filters source data based on both criteria. Experimental results across various dynamics shift scenarios, including kinematic and morphology changes, even with extremely limited target data, show that DVDF consistently and significantly outperforms strong baselines.</div>
<div class="mono" style="margin-top:8px">本文针对跨领域离线强化学习中的挑战，即利用源领域数据辅助有限的目标领域数据时，因动态特性不匹配而影响性能，先前方法仅通过筛选动态对齐的样本进行部分缓解。作者从理论上证明仅动态对齐不足，并指出必须同时进行价值对齐——即筛选高价值的源样本——以确保策略质量。他们提出了动态与价值对齐数据过滤（DVDF）方法，这是一个基于双重标准筛选源数据的统一框架。在包括运动学和形态变化在内的多种动态偏移场景下的实验结果表明，即使在目标领域数据极少的情况下，DVDF也始终显著优于现有基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating the relationship between regularity and learnability in recursive numeral systems using Reinforcement Learning</div>
<div class="meta-line">Authors: Andrea Silvi, Ponrawee Prasertsom, Jennifer Culbertson, Devdatt Dubhashi, Moa Johansson, Kenny Smith</div>
<div class="meta-line">First: 2026-02-25T09:27:02+00:00 · Latest: 2026-02-25T09:27:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21720v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21720v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human recursive numeral systems (i.e., counting systems such as English base-10 numerals), like many other grammatical systems, are highly regular. Following prior work that relates cross-linguistic tendencies to biases in learning, we ask whether regular systems are common because regularity facilitates learning. Adopting methods from the Reinforcement Learning literature, we confirm that highly regular human(-like) systems are easier to learn than unattested but possible irregular systems. This asymmetry emerges under the natural assumption that recursive numeral systems are designed for generalisation from limited data to represent all integers exactly. We also find that the influence of regularity on learnability is absent for unnatural, highly irregular systems, whose learnability is influenced instead by signal length, suggesting that different pressures may influence learnability differently in different parts of the space of possible numeral systems. Our results contribute to the body of work linking learnability to cross-linguistic prevalence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>运用强化学习评估递归数字系统中规律性与可学习性的关系</div>
<div class="mono" style="margin-top:8px">人类递归数字系统（如英语十进制计数）与许多语法系统一样具有高度规律性。基于先前将跨语言倾向与学习偏倚相关联的研究，我们探讨规律性系统是否因其促进学习而普遍存在。通过采用强化学习方法，我们证实高度规律的人类（类人）系统比未经验证但可能的不规则系统更易学习。这种不对称性源于一个自然假设：递归数字系统旨在通过有限数据泛化以精确表示所有整数。我们还发现，对于非自然的高度不规则系统，规律性对可学习性的影响消失，其可学习性转而受信号长度影响，这表明不同压力可能对数字系统可能空间的不同区域产生差异化影响。本研究为连接可学习性与跨语言普遍性的研究体系提供了新证据。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the cross-linguistic prevalence of regular recursive numeral systems, this study investigates whether such regularity enhances learnability. The method employs Reinforcement Learning to simulate learning under a natural assumption that numeral systems must generalize from limited data to represent all integers exactly. Experimental results confirm that highly regular, human-like systems are indeed easier to learn than unattested irregular systems, but this effect disappears for unnatural, highly irregular systems where learnability is instead influenced by signal length, indicating that different pressures operate in different parts of the system space.</div>
<div class="mono" style="margin-top:8px">本研究受递归数字系统在跨语言中普遍呈现规律性的现象驱动，旨在探究这种规律性是否促进了系统的可学习性。方法上采用强化学习来模拟学习过程，基于数字系统需从有限数据泛化以精确表示所有整数的自然假设。实验结果表明，高度规律、类人的系统确实比未出现的不规则系统更易学习，但这种规律性的影响对于非自然的高度不规则系统则消失，其可学习性转而受信号长度影响，这表明在可能的数字系统空间的不同部分，影响可学习性的压力因素有所不同。</div>
</details>
</div>
<div class="card">
<div class="title">Two-Stage Active Distribution Network Voltage Control via LLM-RL Collaboration: A Hybrid Knowledge-Data-Driven Approach</div>
<div class="meta-line">Authors: Xu Yang, Chenhui Lin, Xiang Ma, Dong Liu, Ran Zheng, Haotian Liu, Wenchuan Wu</div>
<div class="meta-line">First: 2026-02-25T09:22:27+00:00 · Latest: 2026-02-25T09:22:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21715v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21715v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The growing integration of distributed photovoltaics (PVs) into active distribution networks (ADNs) has exacerbated operational challenges, making it imperative to coordinate diverse equipment to mitigate voltage violations and enhance power quality. Although existing data-driven approaches have demonstrated effectiveness in the voltage control problem, they often require extensive trial-and-error exploration and struggle to incorporate heterogeneous information, such as day-ahead forecasts and semantic-based grid codes. Considering the operational scenarios and requirements in real-world ADNs, in this paper, we propose a hybrid knowledge-data-driven approach that leverages dynamic collaboration between a large language model (LLM) agent and a reinforcement learning (RL) agent to achieve two-stage voltage control. In the day-ahead stage, the LLM agent receives coarse region-level forecasts and generates scheduling strategies for on-load tap changer (OLTC) and shunt capacitors (SCs) to regulate the overall voltage profile. Then in the intra-day stage, based on accurate node-level measurements, the RL agent refines terminal voltages by deriving reactive power generation strategies for PV inverters. On top of the LLM-RL collaboration framework, we further propose a self-evolution mechanism for the LLM agent and a pretrain-finetune pipeline for the RL agent, effectively enhancing and coordinating the policies for both agents. The proposed approach not only aligns more closely with practical operational characteristics but also effectively utilizes the inherent knowledge and reasoning capabilities of the LLM agent, significantly improving training efficiency and voltage control performance. Comprehensive comparisons and ablation studies demonstrate the effectiveness of the proposed method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型与强化学习协同的两阶段主动配电网电压控制：一种混合知识数据驱动方法</div>
<div class="mono" style="margin-top:8px">分布式光伏在主动配电网中的大规模接入加剧了运行挑战，亟需协调多样化设备以抑制电压越限并提升电能质量。现有数据驱动方法虽在电压控制中展现成效，但常需大量试错探索且难以融合日前预测、基于语义的电网规范等异构信息。针对实际主动配电网的运行场景与需求，本文提出一种混合知识数据驱动方法，通过大语言模型智能体与强化学习智能体的动态协同实现两阶段电压控制。在日前阶段，大语言模型智能体接收粗粒度区域级预测数据，生成有载调压变压器与并联电容器的调度策略以调节整体电压态势；在日内阶段，强化学习智能体基于精确节点级量测数据，通过制定光伏逆变器无功出力策略精细化调节端电压。在协同框架基础上，进一步设计了大语言模型智能体的自进化机制与强化学习智能体的预训练-微调流程，有效提升并协调了双智能体的策略性能。该方法不仅更贴合实际运行特性，还充分发挥了大语言模型智能体的内在知识与推理能力，显著提升了训练效率与电压控制性能。综合对比与消融实验验证了所提方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the operational challenges from increasing distributed photovoltaics in active distribution networks, which require coordinating diverse equipment to prevent voltage violations, this paper proposes a hybrid knowledge-data-driven method for two-stage voltage control. The method leverages collaboration between a large language model (LLM) agent and a reinforcement learning (RL) agent: the LLM uses day-ahead coarse forecasts to schedule equipment like tap changers and capacitors, while the RL uses intra-day precise measurements to refine voltages via PV inverter control, enhanced by a self-evolution mechanism for the LLM and a pretrain-finetune pipeline for the RL. Experimental results from comparisons and ablation studies show the approach improves training efficiency and voltage control performance by effectively utilizing LLM knowledge and aligning with practical grid operations.</div>
<div class="mono" style="margin-top:8px">本文的动机是应对分布式光伏大量接入主动配电网所带来的运行挑战，需协调多样设备以抑制电压越限并提升电能质量。为此，提出了一种混合知识-数据驱动的两阶段电压控制方法，该方法通过大型语言模型（LLM）智能体与强化学习（RL）智能体的动态协作实现：LLM利用日前粗粒度预测生成有载调压变压器和并联电容器等设备的调度策略，而RL则基于日内精确量测通过光伏逆变器无功控制细化电压，并辅以LLM的自进化机制和RL的预训练-微调流程来增强策略协调。主要实验结果表明，所提方法能有效利用LLM的固有知识与推理能力，显著提升了训练效率和电压控制性能，并通过综合对比与消融研究验证了其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Hierarchical Lead Critic based Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: David Eckel, Henri Meeß</div>
<div class="meta-line">First: 2026-02-25T08:33:39+00:00 · Latest: 2026-02-25T08:33:39+00:00</div>
<div class="meta-line">Comments: 16 pages, 10 Figures, Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21680v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21680v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cooperative Multi-Agent Reinforcement Learning (MARL) solves complex tasks that require coordination from multiple agents, but is often limited to either local (independent learning) or global (centralized learning) perspectives. In this paper, we introduce a novel sequential training scheme and MARL architecture, which learns from multiple perspectives on different hierarchy levels. We propose the Hierarchical Lead Critic (HLC) - inspired by natural emerging distributions in team structures, where following high-level objectives combines with low-level execution. HLC demonstrates that introducing multiple hierarchies, leveraging local and global perspectives, can lead to improved performance with high sample efficiency and robust policies. Experimental results conducted on cooperative, non-communicative, and partially observable MARL benchmarks demonstrate that HLC outperforms single hierarchy baselines and scales robustly with increasing amounts of agents and difficulty.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于分层主导批评器的多智能体强化学习</div>
<div class="mono" style="margin-top:8px">协作式多智能体强化学习（MARL）旨在解决需要多个智能体协调的复杂任务，但通常局限于局部（独立学习）或全局（集中式学习）视角。本文提出一种新颖的顺序训练方案与MARL架构，能够从不同层级的多重视角进行学习。我们受团队结构中自然涌现的分布模式启发，提出分层主导批评器（HLC）——其将高层目标导向与低层执行相结合。HLC表明，通过引入多层级结构并融合局部与全局视角，能够以高样本效率和强鲁棒性策略提升性能。在协作式、非通信及部分可观测的MARL基准测试中，HLC均优于单层基线方法，并能随智能体数量与任务难度增加而稳健扩展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses limitations in cooperative Multi-Agent Reinforcement Learning (MARL), which often relies on either purely local or global perspectives, by proposing a novel hierarchical approach to integrate multiple viewpoints. The method introduces a Hierarchical Lead Critic (HLC) architecture, inspired by natural team structures, that sequentially trains agents using both high-level objectives and low-level execution to combine local and global insights. Experimental results on cooperative, non-communicative, and partially observable benchmarks show that HLC outperforms single-hierarchy baselines, achieving higher sample efficiency and more robust policies that scale effectively with increasing agent numbers and task difficulty.</div>
<div class="mono" style="margin-top:8px">本文针对协作多智能体强化学习（MARL）中常局限于局部或全局视角的问题，提出了一种新颖的分层方法来整合多视角学习。该方法受自然团队结构启发，设计了分层主导评论家（HLC）架构，通过顺序训练结合高层目标与底层执行，以融合局部和全局信息。在协作、非通信和部分可观测的MARL基准测试中，实验结果表明HLC优于单层基线方法，具有更高的样本效率和更鲁棒的策略，并能随着智能体数量和任务难度的增加而稳健扩展。</div>
</details>
</div>
<div class="card">
<div class="title">SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards</div>
<div class="meta-line">Authors: Dengjia Zhang, Xiaoou Liu, Lu Cheng, Yaqing Wang, Kenton Murray, Hua Wei</div>
<div class="meta-line">First: 2026-02-24T18:04:54+00:00 · Latest: 2026-02-25T07:50:58+00:00</div>
<div class="meta-line">Comments: Accepted by PAKDD&#x27;26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21158v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.21158v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly deployed as multi-step decision-making agents, where effective reward design is essential for guiding learning. Although recent work explores various forms of reward shaping and step-level credit assignment, a key signal remains largely overlooked: the intrinsic uncertainty of LLMs. Uncertainty reflects model confidence, reveals where exploration is needed, and offers valuable learning cues even in failed trajectories. We introduce SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards, a reinforcement learning framework that incorporates uncertainty directly into the reward design. SELAUR integrates entropy-, least-confidence-, and margin-based metrics into a combined token-level uncertainty estimate, providing dense confidence-aligned supervision, and employs a failure-aware reward reshaping mechanism that injects these uncertainty signals into step- and trajectory-level rewards to improve exploration efficiency and learning stability. Experiments on two benchmarks, ALFWorld and WebShop, show that our method consistently improves success rates over strong baselines. Ablation studies further demonstrate how uncertainty signals enhance exploration and robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SELAUR：基于不确定性感知奖励的自演进大语言模型智能体</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）正日益被部署为多步决策智能体，其中有效的奖励设计对引导学习至关重要。尽管近期研究探索了多种形式的奖励塑形与步级信用分配，但一个关键信号仍被普遍忽视：LLMs的内在不确定性。不确定性反映模型置信度，揭示需要探索的区域，即使在失败轨迹中也能提供有价值的学习线索。我们提出SELAUR：基于不确定性感知奖励的自演进LLM智能体，这是一个将不确定性直接融入奖励设计的强化学习框架。SELAUR整合基于熵、最小置信度和间隔的度量，形成组合的词元级不确定性估计，提供密集的置信度对齐监督，并采用失败感知的奖励重塑机制，将这些不确定性信号注入步级和轨迹级奖励，以提升探索效率与学习稳定性。在ALFWorld和WebShop两个基准测试上的实验表明，本方法在强基线模型上持续提升成功率。消融研究进一步验证了不确定性信号如何增强探索能力与鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this work is that while large language models (LLMs) are increasingly used as multi-step decision-making agents, the intrinsic uncertainty of LLMs—a key signal reflecting model confidence and exploration needs—remains largely overlooked in reward design. The proposed method, SELAUR, is a reinforcement learning framework that directly incorporates uncertainty into rewards by integrating entropy-, least-confidence-, and margin-based metrics into a token-level uncertainty estimate and using a failure-aware reward reshaping mechanism to inject these signals into step- and trajectory-level rewards. Main experimental results on the ALFWorld and WebShop benchmarks show that the method consistently improves success rates over strong baselines, with ablation studies confirming that uncertainty signals enhance exploration and robustness.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，尽管大语言模型越来越多地被部署为多步决策智能体，但其内在的不确定性——这一反映模型置信度和探索需求的关键信号——在奖励设计中基本被忽视。所提出的方法SELAUR是一个强化学习框架，它通过整合基于熵、最小置信度和间隔的度量到令牌级不确定性估计中，并采用一个失败感知的奖励重塑机制，将这些不确定性信号注入到步骤级和轨迹级奖励中，从而直接将不确定性纳入奖励设计。在ALFWorld和WebShop两个基准上的主要实验结果表明，该方法相较于强基线模型持续提高了成功率，消融研究进一步证实了不确定性信号能增强探索能力和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">CCCaption: Dual-Reward Reinforcement Learning for Complete and Correct Image Captioning</div>
<div class="meta-line">Authors: Zhijiang Tang, Linhua Wang, Jiaxin Qi, Weihao Jiang, Peng Hou, Anxiang Zeng, Jianqiang Huang</div>
<div class="meta-line">Venue: CVPR 2026</div>
<div class="meta-line">First: 2026-02-25T07:34:26+00:00 · Latest: 2026-02-25T07:34:26+00:00</div>
<div class="meta-line">Comments: Accept by CVPR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21655v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21655v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image captioning remains a fundamental task for vision language understanding, yet ground-truth supervision still relies predominantly on human-annotated references. Because human annotations reflect subjective preferences and expertise, ground-truth captions are often incomplete or even incorrect, which in turn limits caption models. We argue that caption quality should be assessed by two objective aspects: completeness (does the caption cover all salient visual facts?) and correctness (are the descriptions true with respect to the image?). To this end, we introduce CCCaption: a dual-reward reinforcement learning framework with a dedicated fine-tuning corpus that explicitly optimizes these properties to generate \textbf{C}omplete and \textbf{C}orrect \textbf{Captions}. For completeness, we use diverse LVLMs to disentangle the image into a set of visual queries, and reward captions that answer more of these queries, with a dynamic query sampling strategy to improve training efficiency. For correctness, we penalize captions that contain hallucinations by validating the authenticity of sub-caption queries, which are derived from the caption decomposition. Our symmetric dual-reward optimization jointly maximizes completeness and correctness, guiding models toward captions that better satisfy these objective criteria. Extensive experiments across standard captioning benchmarks show consistent improvements, offering a principled path to training caption models beyond human-annotation imitation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CCCaption：基于双奖励强化学习的完整正确图像描述生成</div>
<div class="mono" style="margin-top:8px">图像描述生成作为视觉语言理解的基础任务，其监督信号仍主要依赖人工标注的参考描述。由于人工标注反映主观偏好与专业差异，真实标注常存在描述不完整甚至错误的问题，进而制约描述模型的性能。我们认为描述质量应从两个客观维度评估：完整性（是否覆盖所有显著视觉事实？）与正确性（描述内容是否与图像真实相符？）。为此，我们提出CCCaption：一种配备专用微调语料库的双奖励强化学习框架，通过显式优化这两项特性来生成\textbf{完整}且\textbf{正确}的\textbf{描述}。在完整性方面，我们采用多样化LVLM将图像解构为视觉查询集合，通过动态查询采样策略提升训练效率，并对回答更多查询的描述给予奖励；在正确性方面，通过对描述分解生成的子描述查询进行真实性验证，对存在幻觉的描述实施惩罚。我们的对称双奖励优化机制联合最大化完整性与正确性，引导模型生成更符合客观标准的描述。在标准描述基准上的大量实验表明该方法能带来持续性能提升，为突破人工标注模仿的训练范式提供了理论路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of subjective human annotations that often lead to incomplete or incorrect image captions, this paper introduces CCCaption, a dual-reward reinforcement learning framework designed to optimize caption completeness and correctness. The method employs diverse large vision-language models to generate visual queries for assessing completeness and validates sub-caption queries to penalize hallucinations for correctness, using a symmetric dual-reward optimization. Experimental results on standard benchmarks demonstrate consistent improvements, offering a principled approach to training caption models beyond mere imitation of human annotations.</div>
<div class="mono" style="margin-top:8px">针对人类标注的主观性常导致图像描述不完整或不正确的问题，本文提出了CCCaption，一种双奖励强化学习框架，旨在优化描述的完整性和正确性。该方法利用多样化的大型视觉语言模型生成视觉查询以评估完整性，并通过验证子描述查询来惩罚幻觉以确保正确性，采用对称双奖励优化进行联合训练。在标准基准测试上的广泛实验显示了一致的性能提升，为训练图像描述模型提供了一条超越单纯模仿人类标注的可行路径。</div>
</details>
</div>
<div class="card">
<div class="title">SERL: Self-Examining Reinforcement Learning on Open-Domain</div>
<div class="meta-line">Authors: Weixuan Ou, Yanzhao Zheng, Shuoshuo Sun, Wei Zhang, Baohua Dong, Hangcheng Zhu, Ruohui Huang, Gang Yu, Pengwei Yan, Yifan Qiao</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2025-11-11T07:18:55+00:00 · Latest: 2026-02-25T07:24:40+00:00</div>
<div class="meta-line">Comments: Accepted by the 40th AAAI Conference on Artificial Intelligence (AAAI 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.07922v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.07922v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has been shown to improve the capabilities of large language models (LLMs). However, applying RL to open-domain tasks faces two key challenges: (1) the inherent subjectivity of these tasks prevents the verifiable rewards as required by Reinforcement Learning with Verifiable Rewards (RLVR); (2) Reinforcement Learning from Human Feedback (RLHF) relies on external reward mechanisms. To overcome these limitations, we propose Self-Examining Reinforcement Learning (SERL), a novel self-improving framework where the LLM serves as both Actor and Judge. SERL introduces two synergistic reward mechanisms without any external signals. On the one hand, to improve the Actor&#x27;s capability, we derive rewards from Copeland-style pairwise comparison judgments across a group of generated responses. On the other hand, a self-consistency reward that encourages coherent judgments is proposed to improve the Judge&#x27;s reliability. This process refines the Judge&#x27;s capability, which in turn provides a more robust reward for Actor. Experiments show that our method outperforms existing self-improvement training methods. SERL improves the LC win rate of Qwen3-8B on AlpacaEval 2 from 52.37% to 59.90%. To the best of our knowledge, our method achieves state-of-the-art performance among self-improving approaches. Furthermore, it achieves a performance comparable to significantly larger models like Qwen3-32B, demonstrating superior effectiveness and robustness on open-domain tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SERL：开放域自检式强化学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL）已被证明能提升大语言模型（LLM）的能力。然而，将RL应用于开放域任务面临两大挑战：（1）任务固有的主观性阻碍了可验证奖励强化学习（RLVR）所需的可验证奖励；（2）基于人类反馈的强化学习（RLHF）依赖外部奖励机制。为克服这些局限，我们提出自检式强化学习（SERL），一种新颖的自改进框架，其中LLM同时扮演行动者与评判者角色。SERL引入两种无需外部信号的协同奖励机制：一方面，为提升行动者能力，我们通过一组生成响应的科普兰式成对比较判断推导奖励；另一方面，提出鼓励一致判断的自洽奖励以提升评判者可靠性。该过程优化评判者能力，进而为行动者提供更稳健的奖励。实验表明，本方法优于现有自改进训练方法：SERL将Qwen3-8B在AlpacaEval 2的LC胜率从52.37%提升至59.90%。据我们所知，该方法在自改进方法中达到最先进性能，且其表现可与Qwen3-32B等显著更大的模型相媲美，在开放域任务中展现出卓越的有效性与鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Self-Examining Reinforcement Learning (SERL), a framework designed to address the limitations of existing reinforcement learning methods for open-domain tasks, where subjective outputs hinder verifiable rewards and reliance on external feedback is costly. The method employs a large language model as both an Actor generating responses and a Judge evaluating them, utilizing two internal reward mechanisms: a Copeland-style pairwise comparison reward to improve the Actor&#x27;s output quality and a self-consistency reward to enhance the Judge&#x27;s reliability, creating a synergistic self-improvement loop. Experimental results demonstrate that SERL outperforms prior self-improvement methods, boosting the Qwen3-8B model&#x27;s win rate on AlpacaEval 2 from 52.37% to 59.90%, achieving state-of-the-art performance comparable to much larger models like Qwen3-32B in open-domain tasks.</div>
<div class="mono" style="margin-top:8px">本文提出了自检强化学习（SERL）框架，旨在解决现有强化学习方法在开放域任务中的局限性，即任务的主观性导致难以获得可验证奖励，且依赖外部反馈机制成本高昂。该方法让大语言模型同时扮演生成回答的“行动者”和评估回答的“评判者”，采用两种内部奖励机制：基于Copeland风格成对比较的奖励以提升行动者的输出质量，以及自一致性奖励以提高评判者的可靠性，从而形成协同自我改进循环。实验结果表明，SERL优于先前的自我改进方法，将Qwen3-8B模型在AlpacaEval 2上的胜率从52.37%提升至59.90%，在开放域任务中达到了与Qwen3-32B等更大模型相当的最先进性能。</div>
</details>
</div>
<div class="card">
<div class="title">AgentLTV: An Agent-Based Unified Search-and-Evolution Framework for Automated Lifetime Value Prediction</div>
<div class="meta-line">Authors: Chaowei Wu, Huazhu Chen, Congde Yuan, Qirui Yang, Guoqing Song, Yue Gao, Li Luo, Frank Youhua Chen, Mengzhuo Guo</div>
<div class="meta-line">Venue: KDD 2026</div>
<div class="meta-line">First: 2026-02-25T06:58:18+00:00 · Latest: 2026-02-25T06:58:18+00:00</div>
<div class="meta-line">Comments: 12 pages, 4 figures, submitted to KDD 2026: 32nd ACM SIGKDD Conference on Knowledge Discovery and Data Mining, ADS Track</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21634v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21634v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Lifetime Value (LTV) prediction is critical in advertising, recommender systems, and e-commerce. In practice, LTV data patterns vary across decision scenarios. As a result, practitioners often build complex, scenario-specific pipelines and iterate over feature processing, objective design, and tuning. This process is expensive and hard to transfer. We propose AgentLTV, an agent-based unified search-and-evolution framework for automated LTV modeling. AgentLTV treats each candidate solution as an {executable pipeline program}. LLM-driven agents generate code, run and repair pipelines, and analyze execution feedback. Two decision agents coordinate a two-stage search. The Monte Carlo Tree Search (MCTS) stage explores a broad space of modeling choices under a fixed budget, guided by the Polynomial Upper Confidence bounds for Trees criterion and a Pareto-aware multi-metric value function. The Evolutionary Algorithm (EA) stage refines the best MCTS program via island-based evolution with crossover, mutation, and migration. Experiments on a large-scale proprietary dataset and a public benchmark show that AgentLTV consistently discovers strong models across ranking and error metrics. Online bucket-level analysis further indicates improved ranking consistency and value calibration, especially for high-value and negative-LTV segments. We summarize practitioner-oriented takeaways: use MCTS for rapid adaptation to new data patterns, use EA for stable refinement, and validate deployment readiness with bucket-level ranking and calibration diagnostics. The proposed AgentLTV has been successfully deployed online.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AgentLTV：基于智能体的统一搜索与进化框架用于自动化生命周期价值预测</div>
<div class="mono" style="margin-top:8px">生命周期价值（LTV）预测在广告、推荐系统和电子商务中至关重要。实践中，LTV数据模式随决策场景变化，从业者常需构建复杂、场景专用的流程，并在特征处理、目标设计和调优上反复迭代，这一过程成本高昂且难以迁移。本文提出AgentLTV——一个基于智能体的统一搜索与进化框架，用于自动化LTV建模。该框架将候选解决方案视为{可执行的流程程序}，由LLM驱动的智能体生成代码、运行修复流程并分析执行反馈。两个决策智能体协调两阶段搜索：蒙特卡洛树搜索阶段在固定预算下，基于多项式树置信上界准则和帕累托感知多指标价值函数探索广阔的建模选择空间；进化算法阶段通过基于岛屿的交叉、变异和迁移进化，优化最佳MCTS程序。在大规模专有数据集和公共基准测试上的实验表明，AgentLTV在排序和误差指标上均能持续发现强效模型。在线分桶分析进一步显示其在排序一致性和价值校准方面的提升，尤其对高价值和负LTV用户群。实践启示包括：使用MCTS快速适应新数据模式，利用EA进行稳定优化，并通过分桶排序与校准诊断验证部署就绪度。AgentLTV已成功在线部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for AgentLTV stems from the high cost and limited transferability of manually constructing scenario-specific pipelines for Lifetime Value (LTV) prediction. The method introduces an automated framework where LLM-driven agents treat candidate solutions as executable pipeline programs, coordinating a two-stage search: a Monte Carlo Tree Search (MCTS) stage for broad exploration under a budget and an Evolutionary Algorithm (EA) stage for refinement via island-based evolution. Main experimental results on a proprietary dataset and public benchmark show that AgentLTV consistently discovers strong models across ranking and error metrics, with online deployment demonstrating improved ranking consistency and value calibration, particularly for high-value and negative-LTV segments.</div>
<div class="mono" style="margin-top:8px">AgentLTV的提出动机在于，为不同决策场景手动构建生命周期价值预测流程成本高昂且难以迁移。该方法采用一个基于智能体的自动化框架，将候选解决方案视为可执行的流程程序，通过LLM驱动的智能体协调两阶段搜索：蒙特卡洛树搜索阶段在预算内进行广泛探索，进化算法阶段则通过基于岛屿的进化进行细化。在大型专有数据集和公共基准上的主要实验结果表明，AgentLTV在排序和误差指标上均能持续发现强效模型，在线部署进一步显示出排序一致性和价值校准的提升，尤其对于高价值和负生命周期价值用户群体。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Correcting VLA: Online Action Refinement via Sparse World Imagination</div>
<div class="meta-line">Authors: Chenyv Liu, Wentao Tan, Lei Zhu, Fengling Li, Jingjing Li, Guoli Yang, Heng Tao Shen</div>
<div class="meta-line">First: 2026-02-25T06:58:06+00:00 · Latest: 2026-02-25T06:58:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21633v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21633v1">PDF</a> · <a href="https://github.com/Kisaragi0/SC-VLA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Standard vision-language-action (VLA) models rely on fitting statistical data priors, limiting their robust understanding of underlying physical dynamics. Reinforcement learning enhances physical grounding through exploration yet typically relies on external reward signals that remain isolated from the agent&#x27;s internal states. World action models have emerged as a promising paradigm that integrates imagination and control to enable predictive planning. However, they rely on implicit context modeling, lacking explicit mechanisms for self-improvement. To solve these problems, we propose Self-Correcting VLA (SC-VLA), which achieve self-improvement by intrinsically guiding action refinement through sparse imagination. We first design sparse world imagination by integrating auxiliary predictive heads to forecast current task progress and future trajectory trends, thereby constraining the policy to encode short-term physical evolution. Then we introduce the online action refinement module to reshape progress-dependent dense rewards, adjusting trajectory orientation based on the predicted sparse future states. Evaluations on challenging robot manipulation tasks from simulation benchmarks and real-world settings demonstrate that SC-VLA achieve state-of-the-art performance, yielding the highest task throughput with 16% fewer steps and a 9% higher success rate than the best-performing baselines, alongside a 14% gain in real-world experiments. Code is available at https://github.com/Kisaragi0/SC-VLA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自校正视觉语言动作模型：基于稀疏世界想象的在线动作优化</div>
<div class="mono" style="margin-top:8px">标准视觉-语言-动作模型依赖统计先验数据拟合，限制了其对底层物理动态的鲁棒理解。强化学习通过探索增强物理基础，但通常依赖与智能体内部状态隔离的外部奖励信号。世界动作模型作为新兴范式，融合想象与控制以实现预测性规划，但其依赖隐式上下文建模，缺乏显式的自我改进机制。为解决这些问题，我们提出自校正视觉语言动作模型，通过稀疏想象内在引导动作优化实现自我改进。我们首先设计稀疏世界想象模块，集成辅助预测头来预估当前任务进度与未来轨迹趋势，从而约束策略编码短期物理演化。随后引入在线动作优化模块，重塑进度相关的密集奖励，依据预测的稀疏未来状态调整轨迹方向。在仿真基准和真实场景的机器人操作任务评估表明，该方法取得最先进性能：相比最优基线方法，步骤数减少16%，成功率提升9%，任务吞吐量最高；真实实验中获得14%的性能增益。代码发布于https://github.com/Kisaragi0/SC-VLA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses limitations in standard vision-language-action models, which lack robust physical grounding, and reinforcement learning approaches, which depend on external rewards. It proposes Self-Correcting VLA (SC-VLA), a method that achieves self-improvement through sparse world imagination, integrating auxiliary predictive heads to forecast task progress and future trajectory trends, thereby encoding short-term physical evolution into the policy. An online action refinement module then reshapes progress-dependent dense rewards based on predicted sparse future states. Experimental results on robot manipulation tasks in simulation and real-world settings show state-of-the-art performance, with SC-VLA achieving 16% fewer steps, a 9% higher success rate than baselines, and a 14% gain in real-world experiments.</div>
<div class="mono" style="margin-top:8px">该论文针对标准视觉-语言-动作模型缺乏稳健物理基础以及强化学习方法依赖外部奖励的局限性，提出了自校正VLA（SC-VLA）方法。该方法通过稀疏世界想象实现自我改进，整合辅助预测头来预测任务进度和未来轨迹趋势，从而将短期物理演化编码到策略中；在线动作细化模块则根据预测的稀疏未来状态重塑依赖于进度的密集奖励。在模拟和真实世界的机器人操作任务实验中，SC-VLA取得了最先进的性能，相比最佳基线方法步骤减少16%、成功率提高9%，并在真实世界实验中获得了14%的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Aligning Audio Captions with Human Preferences</div>
<div class="meta-line">Authors: Kartik Hegde, Rehana Mahfuz, Yinyi Guo, Erik Visser</div>
<div class="meta-line">First: 2025-09-18T06:33:44+00:00 · Latest: 2026-02-25T06:36:29+00:00</div>
<div class="meta-line">Comments: Submitted for review to Interspeech 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.14659v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.14659v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current audio captioning relies on supervised learning with paired audio-caption data, which is costly to curate and may not reflect human preferences in real-world scenarios. To address this, we propose a preference-aligned audio captioning framework based on Reinforcement Learning from Human Feedback (RLHF). To capture nuanced preferences, we train a Contrastive Language-Audio Pretraining (CLAP) based reward model using human-labeled pairwise preference data. This reward model is integrated into an RL framework to fine-tune any baseline captioning system without ground-truth annotations. Extensive human evaluations across multiple datasets show that our method produces captions preferred over baseline models, particularly when baselines fail to provide correct and natural captions. Furthermore, our framework achieves performance comparable to supervised approaches with ground-truth data, demonstrating effective alignment with human preferences and scalability in real-world use.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于人类偏好的音频描述对齐方法</div>
<div class="mono" style="margin-top:8px">当前音频描述技术依赖成对音频-文本数据的监督学习，这类数据标注成本高昂且难以反映真实场景中的人类偏好。为此，我们提出一种基于人类反馈强化学习的偏好对齐音频描述框架。为捕捉细微偏好，我们使用人工标注的成对偏好数据训练基于对比语言-音频预训练的奖励模型。该奖励模型被集成至强化学习框架中，可在无需真实标注的情况下微调任意基线描述系统。跨多数据集的广泛人工评估表明，本方法生成的描述优于基线模型，尤其在基线模型无法提供正确自然描述时效果显著。此外，本框架在真实标注数据上的表现与监督学习方法相当，证明了其与人类偏好的有效对齐及实际应用的扩展性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the high cost and potential misalignment of supervised audio captioning datasets with human preferences, this paper proposes a Reinforcement Learning from Human Feedback (RLHF) framework to align captions with human judgments. The method involves training a Contrastive Language-Audio Pretraining (CLAP)-based reward model on human pairwise preferences, which then guides the reinforcement learning fine-tuning of any baseline captioning system without requiring ground-truth annotations. Experimental results from human evaluations across multiple datasets show that the framework generates captions significantly preferred over baseline models, especially when baselines fail, and achieves performance comparable to supervised approaches, demonstrating effective preference alignment and real-world scalability.</div>
<div class="mono" style="margin-top:8px">针对监督式音频描述数据标注成本高且可能与人类偏好不一致的问题，本文提出了一种基于人类反馈强化学习（RLHF）的框架，以使描述与人类偏好对齐。方法上，利用人类成对偏好数据训练一个基于对比语言-音频预训练（CLAP）的奖励模型，进而通过强化学习微调任意基线描述系统，无需真实标注。在多个数据集上的人工评估实验结果表明，该框架生成的描述明显优于基线模型，尤其在基线失败时表现突出，且达到了与监督方法相当的性能，证明了其有效的人类偏好对齐能力和实际应用的可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">DCoPilot: Generative AI-Empowered Policy Adaptation for Dynamic Data Center Operations</div>
<div class="meta-line">Authors: Minghao Li, Ruihang Wang, Rui Tan, Yonggang Wen</div>
<div class="meta-line">First: 2026-02-02T14:18:52+00:00 · Latest: 2026-02-25T06:25:37+00:00</div>
<div class="meta-line">Comments: Accepted as a full paper at HSCC/ICCPS 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02137v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.02137v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern data centers (DCs) hosting artificial intelligence (AI)-dedicated devices operate at high power densities with rapidly varying workloads, making minute-level adaptation essential for safe and energy-efficient operation. However, manually designing piecewise deep reinforcement learning (DRL) agents cannot keep pace with frequent dynamics shifts and service-level agreement (SLA) changes of an evolving DC. This specification-to-policy lag causes a lack of timely, effective control policies, which may lead to service outages. To bridge the gap, we present DCoPilot, a hybrid framework for generative control policies in dynamic DC operation. DCoPilot synergizes two distinct generative paradigms, i.e., a large language model (LLM) that performs symbolic generation of structured reward forms, and a hypernetwork that conducts parametric generation of policy weights. DCoPilot operates through three coordinated phases: (i) simulation scale-up, which stress-tests reward candidates across diverse simulation-ready (SimReady) scenes; (ii) meta policy distillation, where a hypernetwork is trained to output policy weights conditioned on SLA and scene embeddings; and (iii) online adaptation, enabling zero-shot policy generation in response to updated specifications. Evaluated across five control task families spanning diverse DC components, DCoPilot achieves near-zero constraint violations and outperforms all baselines across specification variations. Ablation studies validate the effectiveness of LLM-based unified reward generation in enabling stable hypernetwork convergence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DCoPilot：面向动态数据中心运营的生成式AI赋能策略自适应框架</div>
<div class="mono" style="margin-top:8px">搭载人工智能专用设备的现代数据中心运行于高功率密度且负载快速变化的环境中，分钟级自适应对安全节能运营至关重要。然而，人工设计的分段深度强化学习智能体难以适应数据中心频繁的动态切换与服务等级协议变更。这种从规范到策略的滞后导致缺乏及时有效的控制策略，可能引发服务中断。为弥合此差距，我们提出DCoPilot——一个面向动态数据中心运营的生成式控制策略混合框架。该框架协同融合两种生成范式：执行结构化奖励形式符号生成的大型语言模型，以及执行策略权重参数化生成的超网络。DCoPilot通过三个协调阶段运行：（1）仿真扩展阶段，在多样化仿真就绪场景中对候选奖励进行压力测试；（2）元策略蒸馏阶段，训练超网络根据SLA与场景嵌入生成策略权重；（3）在线自适应阶段，实现针对更新规范的零样本策略生成。在涵盖五大数据中心组件的控制任务族评估中，DCoPilot实现了近乎零约束违反，并在所有规范变体上超越基线方法。消融实验验证了基于LLM的统一奖励生成对稳定超网络收敛的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of manually designing deep reinforcement learning agents for dynamic data center operations, which cannot keep pace with frequent workload and service-level agreement changes, risking service outages. To bridge this gap, it introduces DCoPilot, a hybrid framework that synergizes a large language model for symbolic reward generation and a hypernetwork for parametric policy weight generation, operating through simulation scale-up, meta policy distillation, and online adaptation phases. Experimental results across five control task families show that DCoPilot achieves near-zero constraint violations and outperforms baselines under varying specifications, with ablation studies validating the effectiveness of LLM-based reward generation for stable hypernetwork convergence.</div>
<div class="mono" style="margin-top:8px">本文针对动态数据中心操作中手动设计深度强化学习代理无法跟上频繁工作负载和服务级别协议变化的问题，这可能导致服务中断。为弥补这一差距，提出了DCoPilot混合框架，它结合大型语言模型进行符号化奖励生成和超网络进行参数化策略权重生成，通过模拟扩展、元策略蒸馏和在线适应三个阶段运作。在五个控制任务系列上的实验结果表明，DCoPilot实现了接近零的约束违反，并在不同规范下优于所有基线方法，消融研究验证了基于LLM的统一奖励生成对稳定超网络收敛的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Evolutionary System Prompt Learning for Reinforcement Learning in LLMs</div>
<div class="meta-line">Authors: Lunjun Zhang, Ryan Chen, Bradly C. Stadie</div>
<div class="meta-line">First: 2026-02-16T12:34:27+00:00 · Latest: 2026-02-25T03:50:21+00:00</div>
<div class="meta-line">Comments: 39 pages, 22 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14697v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.14697v3">PDF</a> · <a href="https://github.com/LunjunZhang/E-SPL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Building agentic systems that can autonomously self-improve from experience is a longstanding goal of AI. Large language models (LLMs) today primarily self-improve via two mechanisms: self-reflection for context updates, and reinforcement learning (RL) for weight updates. In this work, we propose Evolutionary System Prompt Learning (E-SPL), a method for jointly improving model contexts and model weights. In each RL iteration, E-SPL samples trajectories under multiple system prompts in parallel, then jointly applies RL updates to LLM weights and evolutionary updates to system prompts. System prompts evolve via mutation and crossover, two genetic operators driven by LLM self-reflection; selection is based on relative performance ratings updated across RL iterations. E-SPL encourages a natural division between declarative knowledge encoded in prompts and procedural knowledge encoded in weights, resulting in improved performance across reasoning and agentic tasks. For instance, in an easy-to-hard (AIME $\rightarrow$ BeyondAIME) generalization setting, E-SPL improves RL success rate from 38.8% $\rightarrow$ 45.1% while also outperforming reflective prompt evolution (40.0%). Overall, our results demonstrate that RL and system prompt evolution are deeply synergistic, and combining the two yields consistent gains in sample efficiency and generalization. Code: https://github.com/LunjunZhang/E-SPL</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向大语言模型强化学习的进化系统提示学习方法</div>
<div class="mono" style="margin-top:8px">构建能够从经验中自主自我改进的智能体系统是人工智能的长期目标。当前大语言模型主要通过两种机制实现自我改进：基于自我反思的上下文更新，以及基于强化学习的权重更新。本研究提出进化系统提示学习方法，这是一种联合改进模型上下文与模型权重的技术。在每次强化学习迭代中，该方法并行采样多个系统提示下的轨迹，随后联合实施对大语言模型权重的强化学习更新以及对系统提示的进化更新。系统提示通过变异和交叉这两种由大语言模型自我反思驱动的遗传算子进行演化；选择机制基于跨强化学习迭代更新的相对性能评级。该方法促进了提示编码的陈述性知识与权重编码的程序性知识之间的自然分工，从而在推理和智能体任务中实现性能提升。例如，在易到难（AIME → BeyondAIME）的泛化场景中，该方法将强化学习成功率从38.8%提升至45.1%，同时优于反思性提示进化方法（40.0%）。总体而言，我们的结果表明强化学习与系统提示进化具有深度协同效应，二者结合能在样本效率和泛化能力方面获得持续增益。代码：https://github.com/LunjunZhang/E-SPL</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the goal of developing autonomous AI systems that can self-improve, this paper introduces Evolutionary System Prompt Learning (E-SPL), a method that jointly optimizes large language model (LLM) weights via reinforcement learning (RL) and system prompts via an evolutionary algorithm. The method operates by sampling trajectories under multiple system prompts in parallel during RL iterations, applying RL updates to model weights while evolving prompts through mutation and crossover operators driven by LLM self-reflection, with selection based on performance ratings. Experimental results show that E-SPL improves performance on reasoning and agentic tasks, notably increasing RL success rates from 38.8% to 45.1% in an easy-to-hard generalization setting and outperforming reflective prompt evolution alone, demonstrating synergistic gains in sample efficiency and generalization.</div>
<div class="mono" style="margin-top:8px">本研究旨在构建能够自主从经验中自我改进的智能体系统，提出了进化系统提示学习（E-SPL）方法，以联合优化大语言模型（LLM）的权重和系统提示。该方法在每次强化学习迭代中并行采样多个系统提示下的轨迹，一方面通过强化学习更新模型权重，另一方面利用LLM自反思驱动的突变和交叉遗传算子进化提示，并根据性能评分进行选择。实验结果表明，E-SPL在推理和智能体任务上提升了性能，特别是在从易到难的泛化设置中，将强化学习成功率从38.8%提高至45.1%，并优于单纯的反思提示进化，证明了二者在样本效率和泛化能力上的协同增益。</div>
</details>
</div>
<div class="card">
<div class="title">ARLArena: A Unified Framework for Stable Agentic Reinforcement Learning</div>
<div class="meta-line">Authors: Xiaoxuan Wang, Han Zhang, Haixin Wang, Yidan Shi, Ruoyan Li, Kaiqiao Han, Chenyi Tong, Haoran Deng, Renliang Sun, Alexander Taylor, Yanqiao Zhu, Jason Cong, Yizhou Sun, Wei Wang</div>
<div class="meta-line">First: 2026-02-25T03:43:34+00:00 · Latest: 2026-02-25T03:43:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21534v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21534v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic reinforcement learning (ARL) has rapidly gained attention as a promising paradigm for training agents to solve complex, multi-step interactive tasks. Despite encouraging early results, ARL remains highly unstable, often leading to training collapse. This instability limits scalability to larger environments and longer interaction horizons, and constrains systematic exploration of algorithmic design choices. In this paper, we first propose ARLArena, a stable training recipe and systematic analysis framework that examines training stability in a controlled and reproducible setting. ARLArena first constructs a clean and standardized testbed. Then, we decompose policy gradient into four core design dimensions and assess the performance and stability of each dimension. Through this fine-grained analysis, we distill a unified perspective on ARL and propose SAMPO, a stable agentic policy optimization method designed to mitigate the dominant sources of instability in ARL. Empirically, SAMPO achieves consistently stable training and strong performance across diverse agentic tasks. Overall, this study provides a unifying policy gradient perspective for ARL and offers practical guidance for building stable and reproducible LLM-based agent training pipelines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ARLArena：稳定智能体强化学习的统一框架</div>
<div class="mono" style="margin-top:8px">智能体强化学习（ARL）作为一种训练智能体解决复杂多步交互任务的前沿范式，已迅速受到关注。尽管早期成果令人鼓舞，但ARL仍存在高度不稳定性，常导致训练崩溃。这种不稳定性限制了其向更大环境和更长交互周期的扩展，并制约了对算法设计选择的系统性探索。本文首先提出ARLArena——一个稳定的训练方案与系统性分析框架，可在受控可复现环境中检验训练稳定性。ARLArena首先构建了清晰标准化的测试平台，随后将策略梯度分解为四个核心设计维度，并评估各维度的性能与稳定性。通过这种细粒度分析，我们提炼出ARL的统一视角，进而提出SAMPO方法——一种旨在缓解ARL主要不稳定源的稳定智能体策略优化方法。实验表明，SAMPO在多样化智能体任务中均能实现持续稳定的训练与优异性能。总体而言，本研究为ARL提供了统一的策略梯度视角，并为构建稳定可复现的基于大语言模型的智能体训练流程提供了实践指导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the instability and training collapse issues that hinder the scalability and systematic exploration of agentic reinforcement learning (ARL) for complex tasks. The method introduces ARLArena, a framework that first establishes a standardized testbed and then decomposes policy gradient into four core design dimensions to analyze stability, leading to the proposal of SAMPO, a stable optimization method designed to mitigate key instability sources. The main experimental results demonstrate that SAMPO achieves consistently stable training and strong performance across diverse agentic tasks, providing a unified perspective and practical guidance for stable LLM-based agent training.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，智能体强化学习（ARL）在复杂任务中存在训练不稳定和崩溃的问题，这限制了其可扩展性和算法设计的系统探索。方法上，研究提出了ARLArena框架，首先构建了一个标准化的测试平台，然后将策略梯度分解为四个核心设计维度以分析稳定性，并在此基础上提出了SAMPO这一旨在缓解主要不稳定源的稳定优化方法。主要实验结果表明，SAMPO在多种智能体任务中均能实现持续稳定的训练和强大的性能，为基于大语言模型的智能体训练提供了统一视角和实用指导。</div>
</details>
</div>
<div class="card">
<div class="title">Training Generalizable Collaborative Agents via Strategic Risk Aversion</div>
<div class="meta-line">Authors: Chengrui Qu, Yizhou Zhang, Nicholas Lanzetti, Eric Mazumdar</div>
<div class="meta-line">First: 2026-02-25T03:06:59+00:00 · Latest: 2026-02-25T03:06:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21515v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21515v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many emerging agentic paradigms require agents to collaborate with one another (or people) to achieve shared goals. Unfortunately, existing approaches to learning policies for such collaborative problems produce brittle solutions that fail when paired with new partners. We attribute these failures to a combination of free-riding during training and a lack of strategic robustness. To address these problems, we study the concept of strategic risk aversion and interpret it as a principled inductive bias for generalizable cooperation with unseen partners. While strategically risk-averse players are robust to deviations in their partner&#x27;s behavior by design, we show that, in collaborative games, they also (1) can have better equilibrium outcomes than those at classical game-theoretic concepts like Nash, and (2) exhibit less or no free-riding. Inspired by these insights, we develop a multi-agent reinforcement learning (MARL) algorithm that integrates strategic risk aversion into standard policy optimization methods. Our empirical results across collaborative benchmarks (including an LLM collaboration task) validate our theory and demonstrate that our approach consistently achieves reliable collaboration with heterogeneous and previously unseen partners across collaborative tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过战略风险规避训练可泛化的协作智能体</div>
<div class="mono" style="margin-top:8px">许多新兴的智能体范式要求智能体之间（或与人）协作以实现共同目标。然而，现有针对此类协作问题的策略学习方法往往产生脆弱方案，在与新伙伴合作时容易失效。我们将这些失败归因于训练中的搭便车行为以及战略鲁棒性的缺失。为解决这些问题，我们研究了战略风险规避的概念，并将其解释为一种与未见伙伴实现可泛化合作的原则性归纳偏置。虽然战略风险规避型参与者设计上能抵御伙伴行为偏差，但我们证明在协作博弈中，它们还具备以下优势：（1）相比纳什等经典博弈论概念，可能获得更优的均衡结果；（2）表现出更少或零搭便车行为。基于这些发现，我们开发了一种多智能体强化学习算法，将战略风险规避整合到标准策略优化方法中。在包括大语言模型协作任务在内的多个协作基准测试中，实证结果验证了我们的理论，表明该方法能持续实现与异构及未见伙伴的可靠协作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the brittleness of existing collaborative agents when paired with new partners, this paper attributes the issue to free-riding and a lack of strategic robustness. The method introduces strategic risk aversion as an inductive bias, integrating it into multi-agent reinforcement learning (MARL) to train agents that are robust to partner deviations. Experimental results across collaborative benchmarks, including an LLM collaboration task, show that the approach consistently achieves reliable cooperation with heterogeneous and unseen partners, reduces free-riding, and can yield better equilibrium outcomes than classical Nash solutions.</div>
<div class="mono" style="margin-top:8px">本文针对现有协作智能体在与新伙伴合作时表现脆弱的局限性，将问题归因于训练中的搭便车行为和策略鲁棒性不足。方法上，提出将策略风险规避作为一种原则性归纳偏置，并将其集成到多智能体强化学习（MARL）中，以训练能够适应伙伴行为偏差的智能体。在包括大语言模型协作任务在内的多个协作基准测试中，实验结果表明，该方法能持续实现与异构及未见伙伴的可靠协作，减少搭便车现象，并能产生优于经典纳什均衡的协作结果。</div>
</details>
</div>
<div class="card">
<div class="title">SPACeR: Self-Play Anchoring with Centralized Reference Models</div>
<div class="meta-line">Authors: Wei-Jer Chang, Akshay Rangesh, Kevin Joseph, Matthew Strong, Masayoshi Tomizuka, Yihan Hu, Wei Zhan</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-20T19:53:02+00:00 · Latest: 2026-02-25T02:51:23+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026. Project page: https://spacer-ai.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.18060v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.18060v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://spacer-ai.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Developing autonomous vehicles (AVs) requires not only safety and efficiency, but also realistic, human-like behaviors that are socially aware and predictable. Achieving this requires sim agent policies that are human-like, fast, and scalable in multi-agent settings. Recent progress in imitation learning with large diffusion-based or tokenized models has shown that behaviors can be captured directly from human driving data, producing realistic policies. However, these models are computationally expensive, slow during inference, and struggle to adapt in reactive, closed-loop scenarios. In contrast, self-play reinforcement learning (RL) scales efficiently and naturally captures multi-agent interactions, but it often relies on heuristics and reward shaping, and the resulting policies can diverge from human norms. We propose SPACeR, a framework that leverages a pretrained tokenized autoregressive motion model as a centralized reference policy to guide decentralized self-play. The reference model provides likelihood rewards and KL divergence, anchoring policies to the human driving distribution while preserving RL scalability. Evaluated on the Waymo Sim Agents Challenge, our method achieves competitive performance with imitation-learned policies while being up to 10x faster at inference and 50x smaller in parameter size than large generative models. In addition, we demonstrate in closed-loop ego planning evaluation tasks that our sim agents can effectively measure planner quality with fast and scalable traffic simulation, establishing a new paradigm for testing autonomous driving policies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPACeR：基于集中式参考模型的自博弈锚定框架</div>
<div class="mono" style="margin-top:8px">自动驾驶系统开发不仅需要安全与效率，还需具备符合社会预期、可预测的拟人化行为。这要求仿真智能体策略在多智能体环境中兼具拟人性、高效性与可扩展性。近期基于大规扩散模型或分词化模型的模仿学习进展表明，可直接从人类驾驶数据中提取行为模式以生成逼真策略，但这类模型计算成本高、推理速度慢，且难以适应反应式闭环场景。相比之下，自博弈强化学习能高效扩展并自然捕捉多智能体交互，但常依赖启发式规则与奖励设计，且生成策略易偏离人类行为规范。本文提出SPACeR框架，利用预训练的分词自回归运动模型作为集中式参考策略来指导去中心化自博弈。该参考模型通过似然奖励与KL散度，将策略锚定在人类驾驶分布上，同时保持强化学习的可扩展性。在Waymo仿真智能体挑战赛的评估中，本方法在推理速度上比大型生成模型快10倍、参数量减少50倍的同时，达到与模仿学习策略相当的竞争力。在闭环自我规划评估任务中，本框架生成的仿真智能体可通过快速可扩展的交通模拟有效评估规划器质量，为自动驾驶策略测试建立了新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need for autonomous vehicle (AV) simulation agents that are both human-like and computationally efficient in multi-agent settings, addressing limitations of existing imitation learning models (which are slow and struggle in reactive scenarios) and self-play reinforcement learning (which can diverge from human norms). The method, SPACeR, introduces a framework that uses a pretrained tokenized autoregressive motion model as a centralized reference policy to guide decentralized self-play RL, providing likelihood rewards and KL divergence to anchor policies to human driving distributions while maintaining scalability. Experimental results on the Waymo Sim Agents Challenge show that SPACeR achieves competitive performance with imitation-learned policies while being up to 10 times faster at inference and 50 times smaller in parameter size, and it effectively measures planner quality in closed-loop ego planning tasks, enabling fast and scalable traffic simulation for AV testing.</div>
<div class="mono" style="margin-top:8px">本文的动机是开发既拟人化又计算高效的多智能体自动驾驶仿真代理，以解决现有模仿学习模型（推理慢、在反应性场景中表现不佳）和自博弈强化学习（可能偏离人类驾驶规范）的局限性。方法上，SPACeR提出一个框架，利用预训练的令牌化自回归运动模型作为集中式参考策略来指导分散式自博弈强化学习，通过似然奖励和KL散度将策略锚定在人类驾驶分布上，同时保持可扩展性。在Waymo Sim Agents Challenge上的实验结果表明，SPACeR达到了与模仿学习策略相竞争的性能，且推理速度提升高达10倍、参数规模缩小50倍，并在闭环自我规划评估任务中有效衡量规划器质量，为自动驾驶策略测试建立了快速可扩展的交通仿真新范式。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-agent deep reinforcement learning with centralized training and decentralized execution for transportation infrastructure management</div>
<div class="meta-line">Authors: M. Saifullah, K. G. Papakonstantinou, A. Bhattacharya, S. M. Stoffels, C. P. Andriotis</div>
<div class="meta-line">First: 2024-01-23T02:52:36+00:00 · Latest: 2026-02-25T01:58:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2401.12455v2">Abs</a> · <a href="https://arxiv.org/pdf/2401.12455v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Life-cycle management of large-scale transportation systems requires determining a sequence of inspection and maintenance decisions to minimize long-term risks and costs while dealing with multiple uncertainties and constraints that lie in high-dimensional spaces. Traditional approaches have been widely applied but often suffer from limitations related to optimality, scalability, and the ability to properly handle uncertainty. Moreover, many existing methods rely on unconstrained formulations that overlook critical operational constraints. We address these issues in this work by casting the optimization problem within the framework of constrained Partially Observable Markov Decision Processes (POMDPs), which provide a robust mathematical foundation for stochastic sequential decision-making under observation uncertainties, in the presence of risk and resource limitations. To tackle the high dimensionality of state and action spaces, we propose DDMAC-CTDE, a Deep Decentralized Multi-Agent Actor-Critic (DDMAC) reinforcement learning architecture with Centralized Training and Decentralized Execution (CTDE). To demonstrate the utility of the proposed framework, we also develop a new comprehensive benchmark environment representing an existing transportation network in Virginia, U.S., with heterogeneous pavement and bridge assets undergoing nonstationary degradation. This environment incorporates multiple practical constraints related to budget limits, performance guidelines, traffic delays, and risk considerations. On this benchmark, DDMAC-CTDE consistently outperforms standard transportation management baselines, producing better policies. Together, the proposed framework and benchmark provide (i) a scalable, constraint-aware methodology, and (ii) a realistic, rigorous testbed for comprehensive evaluation of Deep Reinforcement Learning (DRL) for transportation infrastructure management.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向交通基础设施管理的集中训练与分散执行多智能体深度强化学习</div>
<div class="mono" style="margin-top:8px">大规模交通系统的全生命周期管理需要在处理高维空间中的多重不确定性与约束条件下，确定一系列检测与维护决策序列，以最小化长期风险与成本。传统方法虽广泛应用，但在最优性、可扩展性及不确定性处理能力方面常存在局限。此外，许多现有方法依赖忽略关键操作约束的无约束建模。本研究通过将优化问题置于约束部分可观测马尔可夫决策过程框架中，以应对这些挑战——该框架为存在风险与资源限制的观测不确定性下的随机序贯决策提供了坚实的数学基础。为应对状态与行动空间的高维特性，我们提出DDMAC-CTDE：一种采用集中训练与分散执行架构的深度分散多智能体行动者-评论家强化学习算法。为验证框架实用性，我们还开发了基于美国弗吉尼亚州真实交通网络的综合基准环境，包含经历非平稳退化的异质路面与桥梁资产。该环境整合了预算限制、性能标准、交通延误及风险考量等多重现实约束。在此基准测试中，DDMAC-CTDE持续超越标准交通管理基线方法，生成更优策略。所提出的框架与基准共同提供了：（1）可扩展的约束感知方法论；（2）用于全面评估深度强化学习在交通基础设施管理中应用的现实化严谨测试平台。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of traditional methods in life-cycle management of large-scale transportation systems, which struggle with optimality, scalability, uncertainty handling, and operational constraints. The authors formulate the problem as a constrained Partially Observable Markov Decision Process (POMDP) and propose DDMAC-CTDE, a deep decentralized multi-agent actor-critic reinforcement learning architecture with centralized training and decentralized execution, to manage high-dimensional state and action spaces. They develop a realistic benchmark environment based on a Virginia transportation network, incorporating practical constraints like budget and risk, and demonstrate that DDMAC-CTDE consistently outperforms standard baselines, offering a scalable, constraint-aware methodology and a rigorous testbed for deep reinforcement learning in infrastructure management.</div>
<div class="mono" style="margin-top:8px">本文针对大规模交通系统全生命周期管理中传统方法在最优性、可扩展性、不确定性处理和操作约束方面的局限性，将问题建模为约束部分可观测马尔可夫决策过程。作者提出了DDMAC-CTDE，一种采用集中训练与分散执行的深度分散多智能体演员-评论家强化学习架构，以应对高维状态和动作空间。基于美国弗吉尼亚州交通网络，他们开发了一个包含预算、风险等实际约束的逼真基准环境，实验表明DDMAC-CTDE持续优于标准基线方法，为交通基础设施管理提供了一种可扩展、考虑约束的方法论和一个严谨的深度强化学习评估测试平台。</div>
</details>
</div>
<div class="card">
<div class="title">GradAlign: Gradient-Aligned Data Selection for LLM Reinforcement Learning</div>
<div class="meta-line">Authors: Ningyuan Yang, Weihua Du, Weiwei Sun, Sean Welleck, Yiming Yang</div>
<div class="meta-line">First: 2026-02-25T01:54:50+00:00 · Latest: 2026-02-25T01:54:50+00:00</div>
<div class="meta-line">Comments: 14 pages. Preliminary work</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21492v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21492v1">PDF</a> · <a href="https://github.com/StigLidu/GradAlign">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has become a central post-training paradigm for large language models (LLMs), but its performance is highly sensitive to the quality of training problems. This sensitivity stems from the non-stationarity of RL: rollouts are generated by an evolving policy, and learning is shaped by exploration and reward feedback, unlike supervised fine-tuning (SFT) with fixed trajectories. As a result, prior work often relies on manual curation or simple heuristic filters (e.g., accuracy), which can admit incorrect or low-utility problems. We propose GradAlign, a gradient-aligned data selection method for LLM reinforcement learning that uses a small, trusted validation set to prioritize training problems whose policy gradients align with validation gradients, yielding an adaptive curriculum. We evaluate GradAlign across three challenging data regimes: unreliable reward signals, distribution imbalance, and low-utility training corpus, showing that GradAlign consistently outperforms existing baselines, underscoring the importance of directional gradient signals in navigating non-stationary policy optimization and yielding more stable training and improved final performance. We release our implementation at https://github.com/StigLidu/GradAlign</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GradAlign：面向大语言模型强化学习的梯度对齐数据选择方法</div>
<div class="mono" style="margin-top:8px">强化学习已成为大语言模型后训练的核心范式，但其性能高度依赖训练问题的质量。这种敏感性源于强化学习的非平稳性：轨迹由动态演化的策略生成，学习过程受探索和奖励反馈的塑造，这与使用固定轨迹的监督微调不同。现有方法多依赖人工筛选或简单启发式过滤（如准确率），可能引入错误或低效问题。本文提出GradAlign——一种梯度对齐的数据选择方法，通过小型可信验证集优先选择策略梯度与验证梯度方向一致的问题，形成自适应课程。我们在三种挑战性数据场景（不可靠奖励信号、分布不均衡、低效训练语料）中评估GradAlign，结果表明其持续优于现有基线，验证了方向梯度信号在非平稳策略优化中的关键作用，能实现更稳定的训练和更优的最终性能。代码已开源：https://github.com/StigLidu/GradAlign</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for GradAlign stems from the sensitivity of reinforcement learning (RL) for large language models (LLMs) to training data quality, as RL&#x27;s non-stationary nature—with evolving policies and exploration—makes it vulnerable to poor or incorrect problems when using simple heuristics. The method introduces a gradient-aligned data selection technique that leverages a small trusted validation set to prioritize training problems whose policy gradients align with validation gradients, creating an adaptive curriculum. Experimental results across challenging regimes like unreliable rewards, distribution imbalance, and low-utility corpora show that GradAlign consistently outperforms baselines, leading to more stable training and improved final performance by effectively using directional gradient signals.</div>
<div class="mono" style="margin-top:8px">GradAlign的提出动机在于，大型语言模型（LLM）的强化学习（RL）对训练数据质量高度敏感，由于RL的非平稳性（如策略演变和探索），使用简单启发式方法容易引入错误或低效问题。该方法采用梯度对齐的数据选择技术，利用一个小的可信验证集，优先选择那些策略梯度与验证梯度对齐的训练问题，从而形成自适应课程。在不可靠奖励、分布不平衡和低效用训练语料等挑战性场景下的实验结果表明，GradAlign持续优于现有基线，通过有效利用方向性梯度信号，实现了更稳定的训练和最终性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Spurious Rewards: Rethinking Training Signals in RLVR</div>
<div class="meta-line">Authors: Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, Yulia Tsvetkov, Hannaneh Hajishirzi, Pang Wei Koh, Luke Zettlemoyer</div>
<div class="meta-line">First: 2025-06-12T17:49:55+00:00 · Latest: 2026-02-25T01:06:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.10947v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.10947v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We show that reinforcement learning with verifiable rewards (RLVR) can elicit strong mathematical reasoning in certain language models even with spurious rewards that have little, no, or even negative correlation with the correct answer. For example, RLVR training with GRPO improves MATH-500 performance for Qwen2.5-Math-7B by 21.4 percentage points using randomly assigned rewards, nearly matching the 29.1-point gain from ground-truth rewards. To explain this counterintuitive observation, we show that GRPO exhibits a clipping bias from the clip term, which can amplify high-prior behaviors learned during pretraining even without informative rewards. As a case study, we identify one such behavior in Qwen2.5-Math models, which we call code reasoning -- reasoning in code without actual code execution; code-reasoning frequency increases from 65 percent to over 90 percent with spurious rewards. However, the presence of such amplifiable behaviors is highly model-dependent. In practice, spurious rewards that are effective for Qwen models often fail to produce gains for other model families, such as Llama3 or OLMo2. Our results highlight the importance of validating RL methods across diverse models rather than relying on a single de facto choice: large gains can arise on Qwen models even from random rewards that do not reflect genuine capability improvements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>虚假奖励：重新思考RLVR中的训练信号</div>
<div class="mono" style="margin-top:8px">研究表明，即使使用与正确答案相关性微弱、无关甚至负相关的虚假奖励，基于可验证奖励的强化学习（RLVR）仍能在某些语言模型中激发强大的数学推理能力。例如，使用随机分配奖励的GRPO进行RLVR训练，可将Qwen2.5-Math-7B在MATH-500上的性能提升21.4个百分点，接近使用真实奖励带来的29.1分增益。为解释这一反直觉现象，我们证明GRPO因裁剪项存在偏差，即使在没有信息性奖励的情况下，也能放大预训练期间习得的高先验行为。案例研究发现Qwen2.5-Math模型存在一种“代码推理”行为——即不实际执行代码的代码形式推理；使用虚假奖励时，该行为频率从65%提升至90%以上。但此类可放大行为高度依赖模型：对Qwen模型有效的虚假奖励，常无法在Llama3或OLMo2等其他模型家族中产生增益。本研究强调需跨多样化模型验证RL方法，而非依赖单一默认选择——即使随机奖励也可能在Qwen模型上产生显著增益，但这并不反映真实能力提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the counterintuitive finding that reinforcement learning with verifiable rewards (RLVR) can significantly enhance mathematical reasoning in certain language models even when using spurious rewards that are poorly correlated with correctness. The method, exemplified by GRPO, is shown to induce a clipping bias that amplifies high-prior behaviors learned during pretraining, such as a &#x27;code reasoning&#x27; pattern in Qwen2.5-Math models, rather than directly optimizing for task accuracy. Experimental results demonstrate that training Qwen2.5-Math-7B with randomly assigned rewards on MATH-500 yields a 21.4 percentage point performance gain, approaching the 29.1-point improvement from ground-truth rewards, while increasing code-reasoning frequency from 65% to over 90%. However, the effectiveness of such spurious rewards is highly model-dependent, as they often fail to produce gains in other model families like Llama3 or OLMo2, underscoring the need for cross-model validation of RL methods.</div>
<div class="mono" style="margin-top:8px">本文研究了一个反直觉的发现：即使使用与正确答案相关性微弱、无关甚至负相关的虚假奖励，基于可验证奖励的强化学习（RLVR）仍能显著提升某些语言模型的数学推理能力。该方法以GRPO为例，其产生的裁剪偏差会放大预训练期间习得的高先验行为（例如Qwen2.5-Math模型中的“代码推理”模式），而非直接优化任务准确性。实验结果表明，在MATH-500数据集上使用随机分配的奖励训练Qwen2.5-Math-7B，性能提升了21.4个百分点，接近使用真实奖励带来的29.1点增益，同时将代码推理频率从65%提高到90%以上。然而，这种虚假奖励的有效性高度依赖于模型，它们在Llama3或OLMo2等其他模型系列中往往无法产生增益，这强调了跨模型验证强化学习方法的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks</div>
<div class="meta-line">Authors: Hao Bai, Alexey Taymanov, Tong Zhang, Aviral Kumar, Spencer Whitehead</div>
<div class="meta-line">First: 2026-01-05T09:35:11+00:00 · Latest: 2026-02-25T00:08:59+00:00</div>
<div class="meta-line">Comments: Added link to tasks on HF</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02439v4">Abs</a> · <a href="https://arxiv.org/pdf/2601.02439v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present WebGym, the largest-to-date open-source environment for training realistic visual web agents. Real websites are non-stationary and diverse, making artificial or small-scale task sets insufficient for robust policy learning. WebGym contains nearly 300,000 tasks with rubric-based evaluations across diverse, real-world websites and difficulty levels. We train agents with a simple reinforcement learning (RL) recipe, which trains on the agent&#x27;s own interaction traces (rollouts), using task rewards as feedback to guide learning. To enable scaling RL, we speed up sampling of trajectories in WebGym by developing a high-throughput asynchronous rollout system, designed specifically for web agents. Our system achieves a 4-5x rollout speedup compared to naive implementations. Second, we scale the task set breadth, depth, and size, which results in continued performance improvement. Fine-tuning a strong base vision-language model, Qwen-3-VL-8B-Instruct, on WebGym results in an improvement in success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models such as GPT-4o and GPT-5-Thinking that achieve 27.1% and 29.8%, respectively. This improvement is substantial because our test set consists only of tasks on websites never seen during training, unlike many other prior works on training visual web agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WebGym：面向视觉网页代理的现实任务训练环境规模化构建</div>
<div class="mono" style="margin-top:8px">本文提出WebGym，这是迄今为止最大规模的开源环境，用于训练现实场景下的视觉网页代理。真实网站具有非稳态和多样性特征，使得人工或小规模任务集难以支撑鲁棒策略学习。WebGym包含近30万个任务，通过标准化评估体系覆盖多样化的真实网站及难度层级。我们采用简洁的强化学习方案训练代理：利用代理自身交互轨迹（rollout）进行训练，以任务奖励作为学习反馈。为实现强化学习的规模化，我们专门为网页代理开发了高吞吐异步轨迹采样系统，将WebGym中的轨迹采样速度较原始实现提升4-5倍。其次，通过拓展任务集的广度、深度与规模，实现了持续的性能提升。基于Qwen-3-VL-8B-Instruct强基线视觉语言模型在WebGym上进行微调后，在分布外测试集上的成功率从26.2%提升至42.9%，显著优于基于GPT-4o（27.1%）和GPT-5-Thinking（29.8%）等专有模型的代理。该提升具有实质性意义，因为我们的测试集仅包含训练阶段未见的网站任务，这与多数现有视觉网页代理训练研究形成鲜明对比。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for WebGym is to address the insufficiency of artificial or small-scale task sets for training robust visual web agents, given the non-stationary and diverse nature of real websites. The method involves creating a large-scale open-source environment with nearly 300,000 tasks across real-world websites, using a reinforcement learning recipe that trains on agent interaction traces with task rewards as feedback, and implementing a high-throughput asynchronous rollout system to speed up trajectory sampling. The main experimental results show that this system achieves a 4-5x rollout speedup compared to naive implementations, and fine-tuning the Qwen-3-VL-8B-Instruct model on WebGym improves the success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models like GPT-4o and GPT-5-Thinking.</div>
<div class="mono" style="margin-top:8px">WebGym的动机在于，由于真实网站的非平稳性和多样性，人工或小规模任务集不足以训练鲁棒的视觉网页智能体。其方法包括构建一个大规模开源环境，包含近30万个基于真实网站的任务，采用强化学习方案，利用智能体交互轨迹和任务奖励进行训练，并开发了高吞吐量的异步轨迹采样系统以加速训练。主要实验结果表明，该系统相比简单实现实现了4-5倍的轨迹采样加速，且在WebGym上微调Qwen-3-VL-8B-Instruct模型后，在未见过网站的分布外测试集上的成功率从26.2%提升至42.9%，显著优于基于GPT-4o和GPT-5-Thinking等专有模型的智能体。</div>
</details>
</div>
<div class="card">
<div class="title">Stabilizing Off-Policy Training for Long-Horizon LLM Agent via Turn-Level Importance Sampling and Clipping-Triggered Normalization</div>
<div class="meta-line">Authors: Chenliang Li, Adel Elmahdy, Alex Boyd, Zhongruo Wang, Siliang Zeng, Alfredo Garcia, Parminder Bhatia, Taha Kass-Hout, Cao Xiao, Mingyi Hong</div>
<div class="meta-line">First: 2025-11-25T05:54:02+00:00 · Latest: 2026-02-24T23:08:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.20718v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.20718v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) algorithms such as PPO and GRPO are widely used to train large language models (LLMs) for multi-turn agentic tasks. However, in off-policy training pipelines, these methods often exhibit unstable optimization dynamics and are prone to performance collapse. Through empirical analysis, we identify two fundamental sources of instability in this setting: (1)~a granularity mismatch between token-level policy optimization and turn-structured interactions, and (2) high-variance and unreliable gradient updates induced by off-policy importance sampling and inaccurate advantage estimation. To address these challenges, we propose SORL, \underline{S}tabilizing \underline{O}ff-Policy \underline{R}einforcement \underline{L}earning for Long-Horizon Agent Training. SORL introduces principled mechanisms that align policy optimization with the structure of multi-turn interactions and adaptively suppress unreliable off-policy updates, yielding more conservative and robust learning dynamics. Within this framework, we instantiate two stabilized algorithms: SO-PPO and SO-GRPO. Both algorithms are designed to mitigate gradient variance and prevent optimization collapse without requiring careful early stopping or heuristic tuning. We evaluate SO-PPO and SO-GRPO on a range of multi-turn search benchmarks, including general question answering, multi-hop question answering, and medical multiple-choice QA tasks. Experimental results show that both methods consistently prevent training instabilities and performance collapses observed in standard PPO and GRPO, maintain lower clipping ratios and more stable optimization trajectories, and achieve superior or comparable task performance. These results demonstrate that the proposed algorithm provides a practical, scalable, and general framework for stabilizing reinforcement learning in multi-turn LLM agent training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于轮次重要性采样与截断触发归一化的长程LLM智能体离线策略训练稳定化方法</div>
<div class="mono" style="margin-top:8px">强化学习算法（如PPO和GRPO）被广泛用于训练大语言模型执行多轮智能体任务。然而在离线策略训练流程中，这些方法常出现优化动态不稳定且易发生性能崩溃的问题。通过实证分析，我们识别出该场景下两个根本的不稳定来源：（1）词元级策略优化与轮次结构化交互之间的粒度失配；（2）由离线策略重要性采样和不准确优势估计引起的高方差、不可靠梯度更新。为应对这些挑战，我们提出SORL——面向长程智能体训练的稳定化离线策略强化学习框架。SORL引入原理性机制，使策略优化与多轮交互结构对齐，并自适应抑制不可靠的离线策略更新，从而产生更保守稳健的学习动态。在此框架下，我们实例化了两种稳定化算法：SO-PPO与SO-GRPO。这两种算法均旨在降低梯度方差并防止优化崩溃，且无需精细的早停策略或启发式调参。我们在多轮搜索基准测试（包括通用问答、多跳问答及医学多选题任务）上评估了SO-PPO与SO-GRPO。实验结果表明：两种方法均能持续避免标准PPO/GRPO中出现的训练不稳定与性能崩溃现象，保持更低的截断比率和更稳定的优化轨迹，并获得更优或相当的任务性能。这些结果证明所提算法为多轮LLM智能体训练中的强化学习稳定化提供了实用、可扩展的通用框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the instability and performance collapse observed in off-policy reinforcement learning methods like PPO and GRPO when training large language models for multi-turn agent tasks, attributed to a mismatch between token-level optimization and turn-structured interactions and high-variance gradient updates. To address this, the method introduces SORL, a framework that stabilizes training via turn-level importance sampling and clipping-triggered normalization, aligning policy updates with interaction granularity and adaptively suppressing unreliable gradients, leading to two instantiated algorithms: SO-PPO and SO-GRPO. Experimental results on multi-turn search benchmarks, including general and multi-hop question answering, show that these methods prevent training instabilities and collapses, maintain stable optimization trajectories with lower clipping ratios, and achieve superior or comparable task performance compared to standard baselines.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，使用PPO和GRPO等离策略强化学习方法训练大型语言模型进行多轮代理任务时，常出现优化不稳定和性能崩溃的问题，这源于令牌级策略优化与轮次结构化交互之间的不匹配以及高方差梯度更新。为解决此问题，方法提出了SORL框架，通过轮次级重要性采样和裁剪触发归一化来稳定训练，使策略更新与交互粒度对齐并自适应抑制不可靠梯度，从而实例化出SO-PPO和SO-GRPO两种算法。在包括通用问答、多跳问答和医学选择题在内的多轮搜索基准测试中，实验结果表明这些方法能防止训练不稳定和崩溃，保持较低的裁剪率和更稳定的优化轨迹，并取得优于或相当于基线方法的任务性能。</div>
</details>
</div>
<div class="card">
<div class="title">On the Structural Non-Preservation of Epistemic Behaviour under Policy Transformation</div>
<div class="meta-line">Authors: Alexander Galozy</div>
<div class="meta-line">First: 2026-02-24T22:55:21+00:00 · Latest: 2026-02-24T22:55:21+00:00</div>
<div class="meta-line">Comments: 15 pages, 3 figures. Under review at RLC 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21424v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21424v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) agents under partial observability often condition actions on internally accumulated information such as memory or inferred latent context. We formalise such information-conditioned interaction patterns as behavioural dependency: variation in action selection with respect to internal information under fixed observations. This induces a probe-relative notion of $ε$-behavioural equivalence and a within-policy behavioural distance that quantifies probe sensitivity. We establish three structural results. First, the set of policies exhibiting non-trivial behavioural dependency is not closed under convex aggregation. Second, behavioural distance contracts under convex combination. Third, we prove a sufficient local condition under which gradient ascent on a skewed mixture objective decreases behavioural distance when a dominant-mode gradient aligns with the direction of steepest contraction. Minimal bandit and partially observable gridworld experiments provide controlled witnesses of these mechanisms. In the examined settings, behavioural distance decreases under convex aggregation and under continued optimisation with skewed latent priors, and in these experiments it precedes degradation under latent prior shift. These results identify structural conditions under which probe-conditioned behavioural separation is not preserved under common policy transformations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>论策略变换下认知行为的结构非保持性</div>
<div class="mono" style="margin-top:8px">部分可观测性下的强化学习（RL）智能体常根据内部累积信息（如记忆或推断的潜在情境）选择行动。我们将此类信息依赖的交互模式形式化为行为依赖性：即在固定观测下，行动选择随内部信息的变化。这引出了探测相对性的ε-行为等价概念，以及量化探测敏感度的策略内行为距离。我们建立了三个结构性结论：一、展现非平凡行为依赖性的策略集在凸聚合下不封闭；二、行为距离在凸组合下收缩；三、我们证明了一个局部充分条件：当优势模态梯度与最速收缩方向一致时，在偏斜混合目标上的梯度上升会减小行为距离。最小赌博机与部分可观测网格世界实验为这些机制提供了受控验证。在考察场景中，行为距离在凸聚合及偏斜潜在先验持续优化下减小，且在这些实验中先于潜在先验偏移下的性能退化。这些结果明确了探测条件行为分离在常见策略变换下无法保持的结构性条件。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need to understand how reinforcement learning agents under partial observability, which condition actions on internal information like memory, preserve their epistemic behavior under policy transformations. The method formalizes this as behavioral dependency, introducing a notion of ε-behavioral equivalence and a behavioral distance metric to quantify sensitivity to internal probes. Experimental results from minimal bandit and gridworld settings show that behavioral distance contracts under convex policy aggregation and decreases during optimization with skewed latent priors, often preceding performance degradation under latent shift, thus identifying structural conditions where behavioral separation is not preserved.</div>
<div class="mono" style="margin-top:8px">本文的动机在于需要理解部分可观测性下的强化学习智能体，其基于记忆等内部信息选择行动时，如何在策略变换中保持认知行为。方法将这种行为形式化为行为依赖性，引入了ε-行为等价概念和行为距离度量来量化对内部探针的敏感性。在最小化赌博机和网格世界实验中的结果表明，行为距离在凸策略聚合下收缩，并在使用偏斜隐先验的持续优化中减小，且常先于隐先验偏移下的性能退化，从而识别了行为分离在常见策略变换中不被保持的结构性条件。</div>
</details>
</div>
<div class="card">
<div class="title">Overconfident Errors Need Stronger Correction: Asymmetric Confidence Penalties for Reinforcement Learning</div>
<div class="meta-line">Authors: Yuanda Xu, Hejian Sang, Zhengze Zhou, Ran He, Zhipeng Wang</div>
<div class="meta-line">First: 2026-02-24T22:46:43+00:00 · Latest: 2026-02-24T22:46:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21420v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21420v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has become the leading paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard RLVR algorithms suffer from a well-documented pathology: while they improve Pass@1 accuracy through sharpened sampling, they simultaneously narrow the model&#x27;s reasoning boundary and reduce generation diversity. We identify a root cause that existing methods overlook: the uniform penalization of errors. Current approaches -- whether data-filtering methods that select prompts by difficulty, or advantage normalization schemes -- treat all incorrect rollouts within a group identically. We show that this uniformity allows overconfident errors (incorrect reasoning paths that the RL process has spuriously reinforced) to persist and monopolize probability mass, ultimately suppressing valid exploratory trajectories. To address this, we propose the Asymmetric Confidence-aware Error Penalty (ACE). ACE introduces a per-rollout confidence shift metric, c_i = log(pi_theta(y_i|x) / pi_ref(y_i|x)), to dynamically modulate negative advantages. Theoretically, we demonstrate that ACE&#x27;s gradient can be decomposed into the gradient of a selective regularizer restricted to overconfident errors, plus a well-characterized residual that partially moderates the regularizer&#x27;s strength. We conduct extensive experiments fine-tuning Qwen2.5-Math-7B, Qwen3-8B-Base, and Llama-3.1-8B-Instruct on the DAPO-Math-17K dataset using GRPO and DAPO within the VERL framework. Evaluated on MATH-500 and AIME 2025, ACE composes seamlessly with existing methods and consistently improves the full Pass@k spectrum across all three model families and benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>过度自信错误需更强校正：强化学习中的非对称置信度惩罚</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）已成为增强大语言模型推理能力的主流范式。然而，标准RLVR算法存在一个公认缺陷：虽然通过锐化采样提高了Pass@1准确率，却同时收窄了模型的推理边界并降低了生成多样性。我们发现现有方法忽视了一个根本原因：对错误的均匀惩罚。当前方法——无论是按难度筛选提示的数据过滤法，还是优势值归一化方案——均对组内所有错误轨迹进行同等处理。这种均匀性使得过度自信错误（被RL过程虚假强化的错误推理路径）得以持续占据概率质量，最终压制有效的探索轨迹。为此，我们提出非对称置信感知错误惩罚（ACE）。ACE引入逐轨迹置信度偏移度量c_i = log(pi_theta(y_i|x) / pi_ref(y_i|x))，以动态调节负优势值。理论上，我们证明ACE梯度可分解为针对过度自信错误的选择性正则化器梯度，加上一个能部分调节正则化强度的特征化残差项。我们在VERL框架下使用GRPO和DAPO方法，基于DAPO-Math-17K数据集对Qwen2.5-Math-7B、Qwen3-8B-Base和Llama-3.1-8B-Instruct进行微调实验。在MATH-500和AIME 2025基准测试中，ACE与现有方法无缝兼容，并在全部三个模型系列和基准上持续提升全谱系Pass@k性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by a pathology in standard Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Models, where methods improve Pass@1 accuracy but reduce reasoning diversity by uniformly penalizing all errors, thereby allowing overconfident incorrect reasoning to persist and dominate. To address this, the authors propose the Asymmetric Confidence-aware Error Penalty (ACE), a method that uses a per-rollout confidence shift metric to dynamically modulate negative advantages, theoretically decomposing its gradient into a selective regularizer for overconfident errors and a moderating residual. Experimental results from fine-tuning models like Qwen2.5-Math-7B on the DAPO-Math-17K dataset show that ACE consistently improves the full Pass@k spectrum on benchmarks such as MATH-500 and AIME 2025, seamlessly integrating with existing methods.</div>
<div class="mono" style="margin-top:8px">本文的动机源于大型语言模型中可验证奖励的强化学习（RLVR）存在的一个固有问题：标准方法虽能通过锐化采样提高Pass@1准确率，却因对所有错误进行均匀惩罚而缩小了推理边界并降低了生成多样性，导致过度自信的错误推理持续占据主导。为解决此问题，作者提出了非对称置信感知错误惩罚（ACE），该方法通过每个生成路径的置信度偏移度量动态调整负优势值，其梯度在理论上可分解为针对过度自信错误的选择性正则项和一个起调节作用的残差项。在DAPO-Math-17K数据集上对Qwen2.5-Math-7B等模型进行微调的实验结果表明，ACE与现有方法无缝结合，在MATH-500和AIME 2025等基准测试中持续提升了所有Pass@k指标。</div>
</details>
</div>
<div class="card">
<div class="title">World Simulation with Video Foundation Models for Physical AI</div>
<div class="meta-line">Authors: NVIDIA, :, Arslan Ali, Junjie Bai, Maciej Bala, Yogesh Balaji, Aaron Blakeman, Tiffany Cai, Jiaxin Cao, Tianshi Cao, Elizabeth Cha, Yu-Wei Chao, Prithvijit Chattopadhyay, Mike Chen, Yongxin Chen, Yu Chen, Shuai Cheng, Yin Cui, Jenna Diamond, Yifan Ding, Jiaojiao Fan, Linxi Fan, Liang Feng, Francesco Ferroni, Sanja Fidler, Xiao Fu, Ruiyuan Gao, Yunhao Ge, Jinwei Gu, Aryaman Gupta, Siddharth Gururani, Imad El Hanafi, Ali Hassani, Zekun Hao, Jacob Huffman, Joel Jang, Pooya Jannaty, Jan Kautz, Grace Lam, Xuan Li, Zhaoshuo Li, Maosheng Liao, Chen-Hsuan Lin, Tsung-Yi Lin, Yen-Chen Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Yifan Lu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Seungjun Nah, Yashraj Narang, Abhijeet Panaskar, Lindsey Pavao, Trung Pham, Morteza Ramezanali, Fitsum Reda, Scott Reed, Xuanchi Ren, Haonan Shao, Yue Shen, Stella Shi, Shuran Song, Bartosz Stefaniak, Shangkun Sun, Shitao Tang, Sameena Tasmeen, Lyne Tchapmi, Wei-Cheng Tseng, Jibin Varghese, Andrew Z. Wang, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Jiashu Xu, Dinghao Yang, Xiaodong Yang, Haotian Ye, Seonghyeon Ye, Xiaohui Zeng, Jing Zhang, Qinsheng Zhang, Kaiwen Zheng, Andrew Zhu, Yuke Zhu</div>
<div class="meta-line">First: 2025-10-28T22:44:13+00:00 · Latest: 2026-02-24T21:52:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.00062v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.00062v2">PDF</a> · <a href="https://github.com/nvidia-cosmos/cosmos-predict2.5">Code1</a> · <a href="https://github.com/nvidia-cosmos/cosmos-transfer2.5">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language model, to provide richer text grounding and finer control of world simulation. Trained on 200M curated video clips and refined with reinforcement learning-based post-training, [Cosmos-Predict2.5] achieves substantial improvements over [Cosmos-Predict1] in video quality and instruction alignment, with models released at 2B and 14B scales. These capabilities enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems. We further extend the family with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and Real2Real world translation. Despite being 3.5$\times$ smaller than [Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video generation. Together, these advances establish [Cosmos-Predict2.5] and [Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To accelerate research and deployment in Physical AI, we release source code, pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open resources lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向物理AI的视频基础模型世界仿真</div>
<div class="mono" style="margin-top:8px">我们推出最新一代物理AI世界基础模型[Cosmos-Predict2.5]。该模型基于流式架构，在单一模型中统一了文本到世界、图像到世界及视频到世界的生成能力，并借助物理AI视觉语言模型[Cosmos-Reason1]实现更丰富的文本语义关联与更精细的世界仿真控制。通过2亿条精选视频片段训练及基于强化学习的后训练优化，[Cosmos-Predict2.5]在视频质量与指令对齐方面较[Cosmos-Predict1]取得显著提升，同步发布20亿与140亿参数版本。这些能力为机器人及自主系统提供更可靠的合成数据生成、策略评估与闭环仿真支持。我们进一步推出控制网络风格框架[Cosmos-Transfer2.5]以扩展模型家族，实现仿真到现实及现实到现实的世界转换。该模型体积仅为[Cosmos-Transfer1]的1/3.5，却能实现更高保真度与鲁棒性的长时序视频生成。这些进展共同确立了[Cosmos-Predict2.5]与[Cosmos-Transfer2.5]作为规模化具身智能通用工具的地位。为加速物理AI领域的研究部署，我们在NVIDIA开放模型许可下于https://github.com/nvidia-cosmos/cosmos-predict2.5 与 https://github.com/nvidia-cosmos/cosmos-transfer2.5 开源代码、预训练模型及精选基准测试，期望通过开放资源降低应用门槛，推动下一代具身智能的创新建设。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for advanced tools to scale embodied intelligence, this paper introduces Cosmos-Predict2.5, a flow-based video foundation model that unifies text, image, and video-to-world generation, and Cosmos-Transfer2.5, a control-net style framework for world translation. The method leverages a physical AI vision-language model for richer grounding and employs training on 200M video clips with reinforcement learning-based refinement. Experimental results show substantial improvements in video quality and instruction alignment over its predecessor, with released 2B and 14B models enabling reliable synthetic data generation and simulation for robotics, while Cosmos-Transfer2.5 achieves higher fidelity and robust long-horizon generation despite being significantly smaller.</div>
<div class="mono" style="margin-top:8px">为满足扩展具身智能对先进工具的需求，本文提出了基于流架构的视频基础模型Cosmos-Predict2.5，它统一了文本、图像和视频到世界的生成，以及用于世界转换的控制网络风格框架Cosmos-Transfer2.5。该方法利用物理AI视觉语言模型提供更丰富的文本基础和精细控制，并在2亿个精选视频片段上训练，结合基于强化学习的后训练进行优化。实验结果表明，相比前代模型，其在视频质量和指令对齐方面取得显著提升，发布的20亿和140亿参数模型能够为机器人学提供可靠的合成数据生成与仿真，而Cosmos-Transfer2.5尽管模型规模缩小了3.5倍，仍实现了更高保真度和鲁棒的长序列视频生成。</div>
</details>
</div>
<div class="card">
<div class="title">EExApp: GNN-Based Reinforcement Learning for Radio Unit Energy Optimization in 5G O-RAN</div>
<div class="meta-line">Authors: Jie Lu, Peihao Yan, Huacheng Zeng</div>
<div class="meta-line">First: 2026-02-09T21:17:23+00:00 · Latest: 2026-02-24T20:53:30+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE INFOCOM 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09206v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.09206v2">PDF</a> · <a href="https://github.com/EExApp/EExApp">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With over 3.5 million 5G base stations deployed globally, their collective energy consumption (projected to exceed 131 TWh annually) raises significant concerns over both operational costs and environmental impacts. In this paper, we present EExAPP, a deep reinforcement learning (DRL)-based xApp for 5G Open Radio Access Network (O-RAN) that jointly optimizes radio unit (RU) sleep scheduling and distributed unit (DU) resource slicing. EExAPP uses a dual-actor-dual-critic Proximal Policy Optimization (PPO) architecture, with dedicated actor-critic pairs targeting energy efficiency and quality-of-service (QoS) compliance. A transformer-based encoder enables scalable handling of variable user equipment (UE) populations by encoding all-UE observations into fixed-dimensional representations. To coordinate the two optimization objectives, a bipartite Graph Attention Network (GAT) is used to modulate actor updates based on both critic outputs, enabling adaptive trade-offs between power savings and QoS. We have implemented EExAPP and deployed it on a real-world 5G O-RAN testbed with live traffic, commercial RU and smartphones. Extensive over-the-air experiments and ablation studies confirm that EExAPP significantly outperforms existing methods in reducing the energy consumption of RU while maintaining QoS. The source code is available at https://github.com/EExApp/EExApp.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EExApp：基于图神经网络的强化学习在5G O-RAN无线单元能耗优化中的应用</div>
<div class="mono" style="margin-top:8px">随着全球部署超过350万个5G基站，其总体能耗（预计每年超过131太瓦时）引发了运营成本和环境影响的重大关切。本文提出EExAPP——一种基于深度强化学习的5G开放式无线接入网络xApp，可联合优化无线单元休眠调度与分布式单元资源切片。EExAPP采用双执行器-双评判器近端策略优化架构，通过专设的执行器-评判器对分别针对能效和服务质量合规性。基于Transformer的编码器通过将所有用户设备观测编码为固定维度表征，实现了对可变用户设备规模的可扩展处理。为协调两个优化目标，系统采用二分图注意力网络，依据双评判器输出调制执行器更新，从而在节能与服务质量间实现自适应权衡。我们已在承载真实流量、配备商用无线单元和智能手机的5G O-RAN现网测试平台中部署EExAPP。大量空口实验与消融研究证实，EExAPP在保障服务质量的同时，能显著超越现有方法降低无线单元能耗。源代码发布于https://github.com/EExApp/EExApp。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the substantial energy consumption and environmental impact of globally deployed 5G base stations, this paper introduces EExAPP, a deep reinforcement learning xApp for O-RAN that jointly optimizes radio unit sleep scheduling and distributed unit resource slicing. The method employs a dual-actor-dual-critic PPO architecture with a transformer encoder for scalable user equipment handling and a bipartite Graph Attention Network to adaptively balance energy efficiency and quality-of-service objectives. Experimental deployment on a real-world 5G O-RAN testbed with live traffic demonstrates that EExAPP significantly reduces radio unit energy consumption while maintaining QoS, outperforming existing methods.</div>
<div class="mono" style="margin-top:8px">本文针对全球部署的5G基站能耗巨大且影响环境的问题，提出了EExAPP，一种基于深度强化学习的O-RAN xApp，用于联合优化射频单元休眠调度和分布式单元资源切片。该方法采用双行动者-双评论者PPO架构，结合变压器编码器处理可变用户设备规模，并利用二分图注意力网络自适应权衡能效与服务质量目标。在具有实时流量的真实5G O-RAN测试平台上进行的实验表明，EExAPP在保证服务质量的同时，显著降低了射频单元能耗，性能优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Alignment-Weighted DPO: A principled reasoning approach to improve safety alignment</div>
<div class="meta-line">Authors: Mengxuan Hu, Vivek V. Datla, Anoop Kumar, Zihan Guan, Sheng Li, Alfy Samuel, Daben Liu</div>
<div class="meta-line">First: 2026-02-24T20:30:51+00:00 · Latest: 2026-02-24T20:30:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21346v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21346v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in alignment techniques such as Supervised Fine-Tuning (SFT), Reinforcement Learning from Human Feedback (RLHF), and Direct Preference Optimization (DPO) have improved the safety of large language models (LLMs). However, these LLMs remain vulnerable to jailbreak attacks that disguise harmful intent through indirect or deceptive phrasing. Using causal intervention, we empirically demonstrate that this vulnerability stems from shallow alignment mechanisms that lack deep reasoning, often rejecting harmful prompts without truly understanding why they are harmful. To mitigate this vulnerability, we propose enhancing alignment through reasoning-aware post-training. We construct and release a novel Chain-of-Thought (CoT) fine-tuning dataset that includes both utility-oriented and safety-critical prompts with step-by-step rationales. Fine-tuning on this dataset encourages models to produce principled refusals grounded in reasoning, outperforming standard SFT baselines. Furthermore, inspired by failure patterns in CoT fine-tuning, we introduce Alignment-Weighted DPO, which targets the most problematic parts of an output by assigning different preference weights to the reasoning and final-answer segments. This produces finer-grained, targeted updates than vanilla DPO and improves robustness to diverse jailbreak strategies. Extensive experiments across multiple safety and utility benchmarks show that our method consistently improves alignment robustness while maintaining overall model utility.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对齐加权DPO：一种基于原则推理的安全对齐改进方法</div>
<div class="mono" style="margin-top:8px">监督微调（SFT）、基于人类反馈的强化学习（RLHF）和直接偏好优化（DPO）等对齐技术的进展提升了大型语言模型（LLM）的安全性。然而，这些模型仍易受通过间接或欺骗性表述掩盖恶意意图的越狱攻击。通过因果干预实验，我们证明该漏洞源于缺乏深度推理的浅层对齐机制，模型常在不真正理解危害原因的情况下拒绝有害提示。为缓解此问题，我们提出通过推理感知的后训练增强对齐能力。我们构建并发布了一个新颖的思维链（CoT）微调数据集，包含具有逐步推理过程的实用导向与安全关键提示。基于该数据集的微调促使模型生成基于推理的原则性拒绝，其效果优于标准SFT基线。此外，受CoT微调中失败模式的启发，我们提出对齐加权DPO方法，通过对推理段和最终答案段分配不同偏好权重，针对输出中最成问题的部分进行优化。相比原始DPO，该方法能实现更细粒度的定向更新，并提升对多样化越狱策略的鲁棒性。在多个安全与实用基准上的广泛实验表明，我们的方法在保持模型整体效能的同时，持续提升了对齐鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the vulnerability of aligned large language models to jailbreak attacks, which exploit shallow alignment mechanisms that lack deep reasoning. To address this, the method introduces reasoning-aware post-training using a novel Chain-of-Thought fine-tuning dataset with step-by-step rationales, and proposes Alignment-Weighted DPO, which assigns preference weights to reasoning and answer segments for targeted updates. Experimental results show that this approach consistently improves alignment robustness against diverse jailbreak strategies while maintaining model utility across multiple benchmarks.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，尽管现有对齐技术提升了大型语言模型的安全性，但其仍易受越狱攻击，这源于缺乏深度推理的浅层对齐机制。方法上，通过构建包含逐步推理的思维链微调数据集进行推理感知后训练，并提出了对齐加权DPO，对推理和答案段分配偏好权重以实现针对性更新。实验结果表明，该方法在多个安全性和实用性基准测试中，能持续提升对齐鲁棒性以抵御多样越狱策略，同时保持模型整体效用。</div>
</details>
</div>
<div class="card">
<div class="title">Flow-Based Single-Step Completion for Efficient and Expressive Policy Learning</div>
<div class="meta-line">Authors: Prajwal Koirala, Cody Fleming</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-06-26T16:09:53+00:00 · Latest: 2026-02-24T20:26:25+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.21427v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.21427v3">PDF</a> · <a href="https://github.com/PrajwalKoirala/SSCP-Single-Step-Completion-Policy">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative models such as diffusion and flow-matching offer expressive policies for offline reinforcement learning (RL) by capturing rich, multimodal action distributions, but their iterative sampling introduces high inference costs and training instability due to gradient propagation across sampling steps. We propose the Single-Step Completion Policy (SSCP), a generative policy trained with an augmented flow-matching objective to predict direct completion vectors from intermediate flow samples, enabling accurate, one-shot action generation. In an off-policy actor-critic framework, SSCP combines the expressiveness of generative models with the training and inference efficiency of unimodal policies, without requiring long backpropagation chains. Our method scales effectively to offline, offline-to-online, and online RL settings, offering substantial gains in speed and adaptability over diffusion-based baselines. We further extend SSCP to goal-conditioned RL, enabling flat policies to exploit subgoal structures without explicit hierarchical inference. SSCP achieves strong results across standard offline RL and behavior cloning benchmarks, positioning it as a versatile, expressive, and efficient framework for deep RL and sequential decision-making. The code is available at https://github.com/PrajwalKoirala/SSCP-Single-Step-Completion-Policy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于流模型的单步补全策略：高效且具表达性的策略学习方法</div>
<div class="mono" style="margin-top:8px">扩散模型与流匹配等生成模型虽能通过捕捉丰富的多模态动作分布为离线强化学习提供表达性策略，但其迭代采样过程因梯度在采样步骤间的传播而带来高推理成本与训练不稳定性。本文提出单步补全策略（SSCP），这是一种采用增强流匹配目标训练的生成策略，通过预测中间流样本的直接补全向量实现精确的单步动作生成。在离轨行动者-评论者框架中，SSCP融合了生成模型的表达性与单峰策略的训练推理效率，无需长反向传播链。该方法可有效扩展至离线、离线转在线及在线强化学习场景，在速度与适应性上显著超越基于扩散的基线方法。我们进一步将SSCP扩展至目标条件强化学习，使扁平化策略能利用子目标结构而无需显式分层推理。SSCP在标准离线强化学习与行为克隆基准测试中均取得优异结果，成为深度强化学习与序列决策中兼具通用性、表达性与高效性的框架。代码发布于 https://github.com/PrajwalKoirala/SSCP-Single-Step-Completion-Policy。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the high inference costs and training instability of iterative generative models like diffusion and flow-matching in offline reinforcement learning, which capture rich action distributions but suffer from slow sampling. The proposed method, the Single-Step Completion Policy (SSCP), introduces an augmented flow-matching objective that predicts direct completion vectors from intermediate samples, enabling accurate one-shot action generation within an actor-critic framework, thus combining expressiveness with efficiency. Experimental results show that SSCP scales effectively across offline, offline-to-online, and online RL settings, achieving substantial speed gains over diffusion baselines and strong performance on standard benchmarks, while also extending successfully to goal-conditioned RL without hierarchical inference.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决离线强化学习中扩散和流匹配等迭代生成模型的高推理成本和训练不稳定问题，这些模型能捕捉丰富的动作分布但采样速度慢。所提出的方法，即单步完成策略（SSCP），引入了一种增强的流匹配目标，从中间样本预测直接完成向量，从而在演员-评论家框架内实现准确的一步动作生成，兼顾了表达能力和效率。实验结果表明，SSCP能有效扩展到离线、离线到在线和在线强化学习设置中，相比扩散基线实现了显著的速度提升，并在标准基准测试中表现出色，同时无需分层推理即可成功应用于目标条件强化学习。</div>
</details>
</div>
<div class="card">
<div class="title">EARL: Entropy-Aware RL Alignment of LLMs for Reliable RTL Code Generation</div>
<div class="meta-line">Authors: Jiahe Shi, Zhengqi Gao, Ching-Yun Ko, Duane Boning</div>
<div class="meta-line">First: 2025-11-15T05:00:07+00:00 · Latest: 2026-02-24T20:22:09+00:00</div>
<div class="meta-line">Comments: Accepted to DAC 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12033v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.12033v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large language models (LLMs) have demonstrated significant potential in hardware design automation, particularly in using natural language to synthesize Register-Transfer Level (RTL) code. Despite this progress, a gap remains between model capability and the demands of real-world RTL design, including syntax errors, functional hallucinations, and weak alignment to designer intent. Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising approach to bridge this gap, as hardware provides executable and formally checkable signals that can be used to further align model outputs with design intent. However, in long, structured RTL code sequences, not all tokens contribute equally to functional correctness, and naïvely spreading gradients across all tokens dilutes learning signals. A key insight from our entropy analysis in RTL generation is that only a small fraction of tokens (e.g., always, if, assign, posedge) exhibit high uncertainty and largely influence control flow and module structure. To address these challenges, we present EARL, an Entropy-Aware Reinforcement Learning framework for Verilog generation. EARL performs policy optimization using verifiable reward signals and introduces entropy-guided selective updates that gate policy gradients to high-entropy tokens. This approach preserves training stability and concentrates gradient updates on functionally important regions of code. Our experiments on VerilogEval and RTLLM show that EARL improves functional pass rates over prior LLM baselines by up to 14.7%, while reducing unnecessary updates and improving training stability. These results indicate that focusing RL on critical, high-uncertainty tokens enables more reliable and targeted policy improvement for structured RTL code generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EARL：基于熵感知强化学习的LLM对齐方法用于可靠RTL代码生成</div>
<div class="mono" style="margin-top:8px">大语言模型（LLM）在硬件设计自动化领域的最新进展，特别是在使用自然语言生成寄存器传输级（RTL）代码方面展现出巨大潜力。然而，模型能力与实际RTL设计需求之间仍存在差距，包括语法错误、功能幻觉以及与设计意图对齐不足等问题。基于可验证奖励的强化学习（RLVR）为弥合这一差距提供了可行路径，因为硬件可提供可执行且形式可验证的信号，用于进一步对齐模型输出与设计意图。但在长序列结构化RTL代码中，并非所有词元对功能正确性的贡献均等，简单地将梯度分散到所有词元会稀释学习信号。我们通过RTL生成的熵分析发现，仅少量关键词元（如always、if、assign、posedge）具有高不确定性且主要影响控制流和模块结构。为此，我们提出EARL——一种用于Verilog生成的熵感知强化学习框架。EARL利用可验证奖励信号进行策略优化，并引入熵引导的选择性更新机制，将策略梯度聚焦于高熵词元。该方法保持了训练稳定性，并将梯度更新集中在功能重要的代码区域。在VerilogEval和RTLLM数据集上的实验表明，EARL将功能通过率较现有LLM基线提升最高达14.7%，同时减少了不必要的更新并提升了训练稳定性。这些结果表明，将强化学习聚焦于关键高不确定性词元，能为结构化RTL代码生成带来更可靠、更具针对性的策略改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the gap between LLM capabilities and real-world RTL design demands, such as syntax errors and functional hallucinations, this paper introduces EARL, an entropy-aware reinforcement learning framework for Verilog generation. The method leverages verifiable hardware rewards and introduces entropy-guided selective updates that concentrate policy gradients on high-uncertainty tokens critical to control flow and structure, thereby preserving training stability. Experimental results on VerilogEval and RTLLM benchmarks show that EARL improves functional pass rates by up to 14.7% over prior LLM baselines while reducing unnecessary updates.</div>
<div class="mono" style="margin-top:8px">针对大型语言模型在寄存器传输级代码生成中存在语法错误、功能幻觉与设计意图对齐不足的问题，本文提出了EARL，一种用于Verilog生成的熵感知强化学习框架。该方法利用可验证的硬件奖励信号，并通过熵引导的选择性更新机制，将策略梯度集中在影响控制流和结构的高不确定性关键令牌上，从而保持训练稳定性。在VerilogEval和RTLLM基准测试上的实验表明，EARL将功能通过率较先前基线提升了最高14.7%，同时减少了不必要的更新。</div>
</details>
</div>
<div class="card">
<div class="title">Tool-R0: Self-Evolving LLM Agents for Tool-Learning from Zero Data</div>
<div class="meta-line">Authors: Emre Can Acikgoz, Cheng Qian, Jonas Hübotter, Heng Ji, Dilek Hakkani-Tür, Gokhan Tur</div>
<div class="meta-line">First: 2026-02-24T19:41:18+00:00 · Latest: 2026-02-24T19:41:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21320v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21320v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are becoming the foundation for autonomous agents that can use tools to solve complex tasks. Reinforcement learning (RL) has emerged as a common approach for injecting such agentic capabilities, but typically under tightly controlled training setups. It often depends on carefully constructed task-solution pairs and substantial human supervision, which creates a fundamental obstacle to open-ended self-evolution toward superintelligent systems. In this paper, we propose Tool-R0 framework for training general purpose tool-calling agents from scratch with self-play RL, under a zero-data assumption. Initialized from the same base LLM, Tool-R0 co-evolves a Generator and a Solver with complementary rewards: one proposes targeted challenging tasks at the other&#x27;s competence frontier and the other learns to solve them with real-world tool calls. This creates a self-evolving cycle that requires no pre-existing tasks or datasets. Evaluation on different tool-use benchmarks show that Tool-R0 yields 92.5 relative improvement over the base model and surpasses fully supervised tool-calling baselines under the same setting. Our work further provides empirical insights into self-play LLM agents by analyzing co-evolution, curriculum dynamics, and scaling behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Tool-R0：从零数据实现工具学习的自进化大语言模型智能体</div>
<div class="mono" style="margin-top:8px">大语言模型正成为能够使用工具解决复杂任务的自主智能体的基础。强化学习已成为注入此类智能体能力的常用方法，但通常在严格受控的训练设置下进行，往往依赖精心构建的任务-解决方案对和大量人工监督，这为通向超级智能系统的开放式自进化设置了根本性障碍。本文提出Tool-R0框架，在零数据假设下通过自我博弈强化学习从头训练通用工具调用智能体。Tool-R0从同一基础大语言模型初始化，通过互补奖励机制协同进化生成器与求解器：一方在对方能力边界提出针对性挑战任务，另一方学习通过真实世界工具调用来解决这些任务，从而形成无需预存任务或数据集的自进化循环。在不同工具使用基准测试中，Tool-R0相比基础模型实现92.5%的相对性能提升，并在同等设置下超越全监督工具调用基线方法。本研究还通过分析协同进化、课程动态和扩展行为，为自我博弈大语言模型智能体提供了实证见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of reinforcement learning approaches for LLM agents that rely on curated datasets and human supervision, this paper introduces Tool-R0, a framework for training general-purpose tool-calling agents from scratch using self-play reinforcement learning under a zero-data assumption. The method co-evolves a Generator and a Solver initialized from the same base LLM, where the Generator proposes challenging tasks at the Solver&#x27;s competence frontier and the Solver learns to solve them with real-world tool calls, creating a self-evolving cycle without pre-existing data. Experimental results on tool-use benchmarks demonstrate a 92.5% relative improvement over the base model and performance surpassing fully supervised baselines, with further analysis providing insights into co-evolution, curriculum dynamics, and scaling behavior.</div>
<div class="mono" style="margin-top:8px">针对依赖人工监督和精心构建数据集的强化学习方法在训练大语言模型工具调用代理上的局限性，本文提出了Tool-R0框架，旨在通过零数据假设下的自博弈强化学习，从零开始训练通用工具调用代理。该方法从同一基础大语言模型初始化一个生成器和一个求解器进行协同进化：生成器在求解器能力边界提出针对性挑战任务，求解器则学习使用真实工具调用来解决它们，从而形成一个无需预先数据即可自我演进的循环。在工具使用基准测试上的实验结果表明，该方法相比基础模型实现了92.5%的相对性能提升，并超越了同等设置下的全监督基线，同时通过对协同进化、课程动态和扩展行为的分析提供了实证见解。</div>
</details>
</div>
<div class="card">
<div class="title">Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models</div>
<div class="meta-line">Authors: Siddarth Venkatraman, Vineet Jain, Sarthak Mittal, Vedant Shah, Johan Obando-Ceron, Yoshua Bengio, Brian R. Bartoldson, Bhavya Kailkhura, Guillaume Lajoie, Glen Berseth, Nikolay Malkin, Moksh Jain</div>
<div class="meta-line">First: 2025-09-30T17:58:03+00:00 · Latest: 2026-02-24T18:58:30+00:00</div>
<div class="meta-line">Comments: 23 pages, 10 figures. Project page: https://rsa-llm.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.26626v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.26626v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://rsa-llm.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Test-time scaling methods improve the capabilities of large language models (LLMs) by increasing the amount of compute used during inference to make a prediction. Inference-time compute can be scaled in parallel by choosing among multiple independent solutions or sequentially through self-refinement. We propose Recursive Self-Aggregation (RSA), a test-time scaling method inspired by evolutionary methods that combines the benefits of both parallel and sequential scaling. Each step of RSA refines a population of candidate reasoning chains through aggregation of subsets to yield a population of improved solutions, which are then used as the candidate pool for the next iteration. Empirically, RSA delivers substantial performance gains with increasing compute budgets across diverse tasks, model families and sizes. Notably, RSA with Gemini 3 Flash attains performance near the top of the ARC-AGI-2 public leaderboard. RSA also enables Qwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning models, including DeepSeek-R1 and o3-mini (high), outperforming purely parallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning Gym, LiveCodeBench-v6, and SuperGPQA. We further propose a novel aggregation-aware reinforcement learning approach that yields significant performance gains by training the model to combine solutions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>递归自聚合解锁大型语言模型的深度思考能力</div>
<div class="mono" style="margin-top:8px">测试时扩展方法通过增加推理阶段的计算量来提升大型语言模型（LLMs）的能力。推理计算可通过并行选择多个独立解或通过自精炼进行序列化扩展。我们提出递归自聚合（RSA），这是一种受进化方法启发的测试时扩展方法，融合了并行与序列化扩展的优势。RSA的每一步通过聚合子集来精炼候选推理链种群，产生改进解种群，并作为下一轮迭代的候选池。实证表明，RSA在不同任务、模型系列和规模下，随计算预算增加均带来显著性能提升。值得注意的是，搭载Gemini 3 Flash的RSA在ARC-AGI-2公开排行榜中达到接近顶端的性能。RSA还使Qwen3-4B-Instruct-2507在AIME-25、HMMT-25、Reasoning Gym、LiveCodeBench-v6和SuperGPQA等任务中，与包括DeepSeek-R1和o3-mini（高配版）在内的大型推理模型取得竞争性表现，且优于纯并行与序列化扩展策略。我们进一步提出一种新颖的聚合感知强化学习方法，通过训练模型融合解决方案实现显著性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to enhance large language models&#x27; (LLMs) reasoning capabilities through more efficient test-time compute scaling, this paper introduces Recursive Self-Aggregation (RSA), a method that synergistically combines parallel and sequential scaling by iteratively refining a population of candidate reasoning chains through subset aggregation. The method involves recursively aggregating subsets of solutions to produce improved candidates for subsequent iterations. Experimental results demonstrate that RSA delivers substantial performance gains across diverse tasks, model families, and sizes with increasing compute budgets; notably, it enables models like Gemini 3 Flash to achieve near-top performance on the ARC-AGI-2 leaderboard and allows smaller models such as Qwen3-4B-Instruct-2507 to compete with larger reasoning models on benchmarks including AIME-25 and HMMT-25, outperforming purely parallel or sequential strategies.</div>
<div class="mono" style="margin-top:8px">本文旨在通过更高效的测试时计算扩展来提升大语言模型（LLMs）的推理能力，提出了递归自聚合（RSA）方法，该方法通过子集聚合迭代优化候选推理链种群，从而融合了并行与顺序扩展的优势。其方法核心是通过递归聚合解决方案的子集，为后续迭代生成改进的候选方案。主要实验结果表明，RSA在不同任务、模型系列和规模上，随着计算预算增加均带来显著性能提升；特别地，它使得Gemini 3 Flash在ARC-AGI-2公开排行榜上达到接近顶端的性能，并让Qwen3-4B-Instruct-2507等较小模型在AIME-25、HMMT-25等基准测试中与DeepSeek-R1等大型推理模型竞争，超越了纯并行或顺序扩展策略。</div>
</details>
</div>
<div class="card">
<div class="title">Squint: Fast Visual Reinforcement Learning for Sim-to-Real Robotics</div>
<div class="meta-line">Authors: Abdulaziz Almuzairee, Henrik I. Christensen</div>
<div class="meta-line">First: 2026-02-24T18:58:11+00:00 · Latest: 2026-02-24T18:58:11+00:00</div>
<div class="meta-line">Comments: For website and code, see https://aalmuzairee.github.io/squint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21203v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21203v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://aalmuzairee.github.io/squint">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual reinforcement learning is appealing for robotics but expensive -- off-policy methods are sample-efficient yet slow; on-policy methods parallelize well but waste samples. Recent work has shown that off-policy methods can train faster than on-policy methods in wall-clock time for state-based control. Extending this to vision remains challenging, where high-dimensional input images complicate training dynamics and introduce substantial storage and encoding overhead. To address these challenges, we introduce Squint, a visual Soft Actor Critic method that achieves faster wall-clock training than prior visual off-policy and on-policy methods. Squint achieves this via parallel simulation, a distributional critic, resolution squinting, layer normalization, a tuned update-to-data ratio, and an optimized implementation. We evaluate on the SO-101 Task Set, a new suite of eight manipulation tasks in ManiSkill3 with heavy domain randomization, and demonstrate sim-to-real transfer to a real SO-101 robot. We train policies for 15 minutes on a single RTX 3090 GPU, with most tasks converging in under 6 minutes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Squint：面向仿真到现实机器人的快速视觉强化学习</div>
<div class="mono" style="margin-top:8px">视觉强化学习在机器人领域前景广阔但成本高昂——离策略方法样本效率高但速度慢；同策略方法并行性好但样本利用率低。近期研究表明，在基于状态的控制任务中，离策略方法在挂钟时间上能比同策略方法训练得更快。但将这一优势扩展到视觉领域仍具挑战性：高维输入图像使训练动态复杂化，并带来显著的存储与编码开销。为应对这些挑战，我们提出Squint——一种视觉软演员-评论家方法，其挂钟训练速度超越现有视觉离策略与同策略方法。Squint通过并行仿真、分布式评论家、分辨率压缩、层归一化、优化的更新-数据比率及工程实现优化达成这一目标。我们在SO-101任务集（ManiSkill3中八个采用重度域随机化的操作任务套件）上评估性能，并演示了向真实SO-101机器人的仿真到现实迁移。所有策略均在单张RTX 3090 GPU上训练15分钟，其中多数任务在6分钟内收敛。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for Squint is to overcome the inefficiencies in visual reinforcement learning for robotics, where off-policy methods are sample-efficient but slow, while on-policy methods parallelize well but waste samples. The method introduces a visual Soft Actor Critic approach that accelerates wall-clock training through parallel simulation, a distributional critic, resolution squinting, layer normalization, a tuned update-to-data ratio, and an optimized implementation. Experimental results on the SO-101 Task Set, a suite of eight manipulation tasks with heavy domain randomization, show that policies can be trained in 15 minutes on a single GPU, with most tasks converging in under 6 minutes, and successful sim-to-real transfer to a real robot is demonstrated.</div>
<div class="mono" style="margin-top:8px">Squint的动机是解决机器人视觉强化学习中的效率问题，其中离策略方法样本效率高但速度慢，而在策略方法并行性好却浪费样本。该方法提出了一种视觉软演员-评论家方法，通过并行仿真、分布评论家、分辨率眯眼、层归一化、调整的更新数据比和优化实现，加速了实际训练时间。在SO-101任务集（一套包含八个操作任务且具有重度领域随机化的测试环境）上的实验结果表明，策略可在单个GPU上15分钟内完成训练，大多数任务在6分钟内收敛，并成功实现了从仿真到真实机器人的迁移。</div>
</details>
</div>
<div class="card">
<div class="title">Cooperative-Competitive Team Play of Real-World Craft Robots</div>
<div class="meta-line">Authors: Rui Zhao, Xihui Li, Yizheng Zhang, Yuzhen Liu, Zhong Zhang, Yufeng Zhang, Cheng Zhou, Zhengyou Zhang, Lei Han</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-24T17:15:37+00:00 · Latest: 2026-02-24T17:15:37+00:00</div>
<div class="meta-line">Comments: Accepted by 2026 IEEE International Conference on Robotics and Automation (ICRA 2026), Vienna, Austria</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21119v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21119v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent deep Reinforcement Learning (RL) has made significant progress in developing intelligent game-playing agents in recent years. However, the efficient training of collective robots using multi-agent RL and the transfer of learned policies to real-world applications remain open research questions. In this work, we first develop a comprehensive robotic system, including simulation, distributed learning framework, and physical robot components. We then propose and evaluate reinforcement learning techniques designed for efficient training of cooperative and competitive policies on this platform. To address the challenges of multi-agent sim-to-real transfer, we introduce Out of Distribution State Initialization (OODSI) to mitigate the impact of the sim-to-real gap. In the experiments, OODSI improves the Sim2Real performance by 20%. We demonstrate the effectiveness of our approach through experiments with a multi-robot car competitive game and a cooperative task in real-world settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>现实世界工艺机器人的合作-竞争团队博弈</div>
<div class="mono" style="margin-top:8px">近年来，多智能体深度强化学习在开发智能游戏代理方面取得了显著进展。然而，利用多智能体强化学习高效训练集体机器人，并将习得策略迁移至现实应用，仍是待解决的研究问题。本研究首先构建了一套完整的机器人系统，包括仿真环境、分布式学习框架与实体机器人组件。随后，我们提出并评估了专为该平台设计的强化学习技术，以实现合作与竞争策略的高效训练。为应对多智能体仿真到现实迁移的挑战，我们引入了分布外状态初始化方法以缓解仿真与现实间的差异。实验表明，该方法将仿真到现实的性能提升了20%。我们通过多机器人车辆竞争游戏和现实场景中的合作任务实验，验证了所提方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work is motivated by the challenges of efficiently training collective robots with multi-agent reinforcement learning (RL) and transferring learned policies to the real world. The method involves developing a full robotic system with simulation, a distributed learning framework, and hardware, and proposing RL techniques for training cooperative-competitive policies, including an Out of Distribution State Initialization (OODSI) technique to reduce the sim-to-real gap. The main experimental results show that OODSI improves Sim2Real performance by 20%, validated in real-world multi-robot car games for both competitive and cooperative tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决使用多智能体强化学习高效训练集体机器人以及将习得策略迁移到现实世界中的挑战。方法上，开发了一个包含仿真、分布式学习框架和物理机器人的完整机器人系统，并提出了用于训练合作-竞争策略的强化学习技术，特别引入了分布外状态初始化（OODSI）以减少仿真到现实的差距。主要实验结果表明，OODSI将仿真到现实的性能提升了20%，并在真实世界的多机器人汽车竞争游戏和合作任务中验证了该方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">AceGRPO: Adaptive Curriculum Enhanced Group Relative Policy Optimization for Autonomous Machine Learning Engineering</div>
<div class="meta-line">Authors: Yuzhu Cai, Zexi Liu, Xinyu Zhu, Cheng Wang, Siheng Chen</div>
<div class="meta-line">First: 2026-02-08T10:55:03+00:00 · Latest: 2026-02-24T17:14:22+00:00</div>
<div class="meta-line">Comments: 17 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07906v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.07906v2">PDF</a> · <a href="https://github.com/yuzhu-cai/AceGRPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous Machine Learning Engineering (MLE) requires agents to perform sustained, iterative optimization over long horizons. While recent LLM-based agents show promise, current prompt-based agents for MLE suffer from behavioral stagnation due to frozen parameters. Although Reinforcement Learning (RL) offers a remedy, applying it to MLE is hindered by prohibitive execution latency and inefficient data selection. Recognizing these challenges, we propose AceGRPO with two core components: (1) Evolving Data Buffer that continuously repurposes execution traces into reusable training tasks, and (2) Adaptive Sampling guided by a Learnability Potential function, which dynamically prioritizes tasks at the agent&#x27;s learning frontier to maximize learning efficiency. Leveraging AceGRPO, our trained Ace-30B model achieves a 100% valid submission rate on MLE-Bench-Lite, approaches the performance of proprietary frontier models, and outperforms larger open-source baselines (e.g., DeepSeek-V3.2), demonstrating robust capability for sustained iterative optimization. Code is available at https://github.com/yuzhu-cai/AceGRPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AceGRPO：面向自主机器学习工程的自适应课程增强型群体相对策略优化</div>
<div class="mono" style="margin-top:8px">自主机器学习工程（MLE）要求智能体在长周期内执行持续迭代优化。尽管当前基于大语言模型的智能体展现出潜力，但现有基于提示的MLE智能体因参数冻结而存在行为停滞问题。强化学习虽能提供解决方案，但其在MLE中的应用受限于高昂的执行延迟与低效的数据选择。针对这些挑战，我们提出AceGRPO框架，其包含两大核心组件：（1）演化数据缓冲池：持续将执行轨迹转化为可复用的训练任务；（2）自适应采样机制：通过可学习潜力函数动态优先处理智能体学习前沿的任务，以最大化学习效率。基于AceGRPO训练的Ace-30B模型在MLE-Bench-Lite基准上实现100%有效提交率，性能接近前沿专有模型，并超越更大规模的开源基线（如DeepSeek-V3.2），展现出持续迭代优化的强大能力。代码已开源：https://github.com/yuzhu-cai/AceGRPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of behavioral stagnation in prompt-based agents for Autonomous Machine Learning Engineering (MLE), where frozen parameters limit sustained iterative optimization. To overcome this, the authors propose AceGRPO, a method featuring an Evolving Data Buffer that repurposes execution traces into reusable tasks and an Adaptive Sampling mechanism guided by a Learnability Potential function to prioritize tasks at the learning frontier for efficiency. Experimental results show that the trained Ace-30B model achieves a 100% valid submission rate on MLE-Bench-Lite, matches the performance of proprietary frontier models, and surpasses larger open-source baselines like DeepSeek-V3.2, demonstrating robust capabilities for long-horizon optimization.</div>
<div class="mono" style="margin-top:8px">本文针对自主机器学习工程中基于提示的智能体因参数冻结导致行为停滞、难以持续迭代优化的问题，提出AceGRPO方法。该方法包含两个核心组件：一是演化数据缓冲区，将执行轨迹转化为可重用的训练任务；二是基于可学习性潜力函数的自适应采样机制，动态优先处理学习前沿的任务以提升效率。实验结果表明，训练后的Ace-30B模型在MLE-Bench-Lite上实现了100%的有效提交率，性能接近前沿专有模型，并超越了如DeepSeek-V3.2等更大的开源基线，展现出强大的持续迭代优化能力。</div>
</details>
</div>
<div class="card">
<div class="title">SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation</div>
<div class="meta-line">Authors: Kushal Kedia, Tyler Ga Wei Lum, Jeannette Bohg, C. Karen Liu</div>
<div class="meta-line">First: 2026-02-18T20:42:39+00:00 · Latest: 2026-02-24T17:10:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16863v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.16863v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ability to manipulate tools significantly expands the set of tasks a robot can perform. Yet, tool manipulation represents a challenging class of dexterity, requiring grasping thin objects, in-hand object rotations, and forceful interactions. Since collecting teleoperation data for these behaviors is challenging, sim-to-real reinforcement learning (RL) is a promising alternative. However, prior approaches typically require substantial engineering effort to model objects and tune reward functions for each task. In this work, we propose SimToolReal, taking a step towards generalizing sim-to-real RL policies for tool manipulation. Instead of focusing on a single object and task, we procedurally generate a large variety of tool-like object primitives in simulation and train a single RL policy with the universal goal of manipulating each object to random goal poses. This approach enables SimToolReal to perform general dexterous tool manipulation at test-time without any object or task-specific training. We demonstrate that SimToolReal outperforms prior retargeting and fixed-grasp methods by 37% while matching the performance of specialist RL policies trained on specific target objects and tasks. Finally, we show that SimToolReal generalizes across a diverse set of everyday tools, achieving strong zero-shot performance over 120 real-world rollouts spanning 24 tasks, 12 object instances, and 6 tool categories.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SimToolReal：一种面向零样本灵巧工具操作的物体中心策略</div>
<div class="mono" style="margin-top:8px">操作工具的能力显著扩展了机器人可执行的任务范围。然而，工具操作代表了一类具有挑战性的灵巧性任务，需要抓握细薄物体、进行手内物体旋转以及强力的交互。由于收集这些行为的遥操作数据较为困难，仿真到现实的强化学习成为一种有前景的替代方案。但以往方法通常需要大量工程努力来建模物体并为每个任务调整奖励函数。本文提出SimToolReal，旨在推动仿真到现实强化学习策略在工具操作中的泛化。该方法不再局限于单一物体和任务，而是在仿真中程序化生成大量多样化的类工具物体基元，并训练一个统一的强化学习策略，其通用目标是将每个物体操纵至随机目标姿态。这一方法使SimToolReal在测试时无需任何物体或任务特定训练即可执行通用的灵巧工具操作。实验表明，SimToolReal在性能上超越先前的重定向和固定抓取方法37%，同时与针对特定目标物体和任务训练的专用强化学习策略表现相当。最后，我们证明SimToolReal能够泛化至多种日常工具，在涵盖24个任务、12个物体实例和6个工具类别的120次真实世界测试中均展现出强大的零样本性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the difficulty of collecting real-world teleoperation data for dexterous tool manipulation, this work introduces SimToolReal, a sim-to-real reinforcement learning approach that aims for generalization. The method involves procedurally generating a diverse set of tool-like object primitives in simulation and training a single policy to manipulate these objects to random goal poses, avoiding task-specific engineering. Experimental results show that this policy outperforms prior retargeting and fixed-grasp methods by 37%, matches the performance of specialist policies trained on specific objects, and demonstrates strong zero-shot generalization across 24 real-world tasks, 12 object instances, and 6 tool categories.</div>
<div class="mono" style="margin-top:8px">本研究针对灵巧工具操作中真实遥操作数据收集困难的问题，提出了SimToolReal这一旨在实现泛化的仿真到现实强化学习方法。该方法通过在仿真中程序化生成多样化的工具状物体基元，并训练一个单一策略来将这些物体操控至随机目标姿态，从而避免了针对特定任务的设计。实验结果表明，该策略的性能优于先前的重定向和固定抓取方法37%，与针对特定物体训练的专业策略表现相当，并在24个真实世界任务、12个物体实例和6种工具类别上展现了强大的零样本泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Localized Dynamics-Aware Domain Adaption for Off-Dynamics Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Zhangjie Xia, Yu Yang, Pan Xu</div>
<div class="meta-line">First: 2026-02-24T16:32:50+00:00 · Latest: 2026-02-24T16:32:50+00:00</div>
<div class="meta-line">Comments: 33 pages, 9 figures, 11 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21072v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21072v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Off-dynamics offline reinforcement learning (RL) aims to learn a policy for a target domain using limited target data and abundant source data collected under different transition dynamics. Existing methods typically address dynamics mismatch either globally over the state space or via pointwise data filtering; these approaches can miss localized cross-domain similarities or incur high computational cost. We propose Localized Dynamics-Aware Domain Adaptation (LoDADA), which exploits localized dynamics mismatch to better reuse source data. LoDADA clusters transitions from source and target datasets and estimates cluster-level dynamics discrepancy via domain discrimination. Source transitions from clusters with small discrepancy are retained, while those from clusters with large discrepancy are filtered out. This yields a fine-grained and scalable data selection strategy that avoids overly coarse global assumptions and expensive per-sample filtering. We provide theoretical insights and extensive experiments across environments with diverse global and local dynamics shifts. Results show that LoDADA consistently outperforms state-of-the-art off-dynamics offline RL methods by better leveraging localized distribution mismatch.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向离动态离线强化学习的局部动态感知域适应方法</div>
<div class="mono" style="margin-top:8px">离动态离线强化学习旨在利用有限的目标域数据和在不同转移动态下收集的丰富源域数据，为目标域学习策略。现有方法通常在整个状态空间上全局处理动态失配，或通过逐点数据过滤；这些方法可能忽略局部跨域相似性或计算成本高昂。我们提出局部动态感知域适应（LoDADA），利用局部动态失配以更好地重用源数据。LoDADA对源和目标数据集的转移进行聚类，并通过域判别估计聚类级动态差异。保留差异较小的源转移聚类，过滤差异较大的聚类。这产生了一种细粒度且可扩展的数据选择策略，避免了过于粗略的全局假设和昂贵的逐样本过滤。我们提供了理论分析，并在具有不同全局与局部动态变化的环境中进行广泛实验。结果表明，LoDADA通过更好地利用局部分布失配，持续优于最先进的离动态离线强化学习方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of off-dynamics offline reinforcement learning, where a policy must be learned for a target domain using limited target data and abundant but dynamically mismatched source data. The proposed method, Localized Dynamics-Aware Domain Adaptation (LoDADA), clusters transitions from both datasets and estimates dynamics discrepancy at the cluster level via domain discrimination, selectively retaining source data from clusters with small discrepancy while filtering out others. Experimental results across environments with diverse dynamics shifts demonstrate that LoDADA consistently outperforms existing state-of-the-art methods by more effectively leveraging localized distribution mismatches, offering a fine-grained and scalable alternative to global or pointwise filtering approaches.</div>
<div class="mono" style="margin-top:8px">本文针对离动态离线强化学习中的挑战，即必须利用有限的目标域数据和丰富但动态不匹配的源域数据来学习目标域策略。所提出的方法——局部动态感知域适应（LoDADA），对源域和目标域的转移进行聚类，并通过域判别估计聚类级别的动态差异，选择性保留差异较小的源域数据，同时过滤掉差异较大的数据。在具有不同全局和局部动态变化的环境中的实验结果表明，LoDADA通过更有效地利用局部分布不匹配，持续优于现有的最先进方法，提供了一种比全局或逐点过滤方法更细粒度且可扩展的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Safe Reinforcement Learning for Real-World Engine Control</div>
<div class="meta-line">Authors: Julian Bedei, Lucas Koch, Kevin Badalian, Alexander Winkler, Patrick Schaber, Jakob Andert</div>
<div class="meta-line">First: 2025-01-28T01:19:05+00:00 · Latest: 2026-02-24T15:50:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.16613v2">Abs</a> · <a href="https://arxiv.org/pdf/2501.16613v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work introduces a toolchain for applying Reinforcement Learning (RL), specifically the Deep Deterministic Policy Gradient (DDPG) algorithm, in safety-critical real-world environments. As an exemplary application, transient load control is demonstrated on a single-cylinder internal combustion engine testbench in Homogeneous Charge Compression Ignition (HCCI) mode, that offers high thermal efficiency and low emissions. However, HCCI poses challenges for traditional control methods due to its nonlinear, autoregressive, and stochastic nature. RL provides a viable solution, however, safety concerns, such as excessive pressure rise rates, must be addressed when applying to HCCI. A single unsuitable control input can severely damage the engine or cause misfiring and shut down. Additionally, operating limits are not known a priori and must be determined experimentally. To mitigate these risks, real-time safety monitoring based on the k-nearest neighbor algorithm is implemented, enabling safe interaction with the testbench. The feasibility of this approach is demonstrated as the RL agent learns a control policy through interaction with the testbench. A root mean square error of 0.1374 bar is achieved for the indicated mean effective pressure, comparable to neural network-based controllers from the literature. The toolchain&#x27;s flexibility is further demonstrated by adapting the agent&#x27;s policy to increase ethanol energy shares, promoting renewable fuel use while maintaining safety. This RL approach addresses the longstanding challenge of applying RL to safety-critical real-world environments. The developed toolchain, with its adaptability and safety mechanisms, paves the way for future applicability of RL in engine testbenches and other safety-critical settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向真实世界发动机控制的安全强化学习</div>
<div class="mono" style="margin-top:8px">本研究提出了一套在安全关键的真实环境中应用强化学习（RL）的工具链，重点采用深度确定性策略梯度（DDPG）算法。以均质充量压缩点火（HCCI）模式下单缸内燃机试验台的瞬态负荷控制为例进行演示，该模式具有高热效率和低排放优势。然而，HCCI因其非线性、自回归和随机特性对传统控制方法构成挑战。强化学习虽提供可行解决方案，但在应用于HCCI时必须解决安全性问题（如压力上升率过高），单次不当控制输入可能导致发动机严重损坏、失火或停机。此外，运行极限需通过实验确定而无法预先获知。为降低风险，研究采用基于k近邻算法的实时安全监测机制，实现与试验台的安全交互。通过强化学习智能体与试验台的交互学习控制策略，验证了该方法的可行性：指示平均有效压力的均方根误差达到0.1374巴，与文献中基于神经网络的控制器性能相当。通过调整智能体策略以提高乙醇能量占比，进一步证明了工具链的灵活性，在保障安全的前提下促进可再生燃料应用。该强化学习方法攻克了将RL应用于安全关键真实环境的长期难题，所开发工具链凭借其适应性与安全机制，为强化学习在未来发动机试验台及其他安全关键场景的应用铺平道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the challenge of applying reinforcement learning (RL) to safety-critical real-world systems, exemplified by the need for robust control in a Homogeneous Charge Compression Ignition (HCCI) engine, which is efficient but highly nonlinear and stochastic. The method employs the Deep Deterministic Policy Gradient (DDPG) algorithm within a toolchain that integrates real-time safety monitoring using a k-nearest neighbor algorithm to prevent damaging control actions like excessive pressure rise rates. The main experimental results demonstrate the feasibility of this safe RL approach on a physical engine testbench, where the agent learned a control policy achieving a root mean square error of 0.1374 bar for indicated mean effective pressure, comparable to existing neural network controllers, and successfully adapted to increase ethanol fuel share while maintaining safety.</div>
<div class="mono" style="margin-top:8px">本文的动机源于将强化学习应用于安全关键型现实系统的挑战，以均质充量压燃（HCCI）发动机这一高效但高度非线性和随机性的系统需要鲁棒控制为例。方法上，该研究在工具链中采用深度确定性策略梯度（DDPG）算法，并集成基于k近邻算法的实时安全监控，以防止如压力上升率过高等可能损坏发动机的控制动作。主要实验结果表明，这种安全强化学习方法在物理发动机试验台上是可行的，智能体学习到的控制策略在指示平均有效压力上实现了0.1374 bar的均方根误差，与现有神经网络控制器性能相当，并能成功适应提高乙醇燃料比例的同时确保安全。</div>
</details>
</div>
<div class="card">
<div class="title">Do We Need Adam? Surprisingly Strong and Sparse Reinforcement Learning with SGD in LLMs</div>
<div class="meta-line">Authors: Sagnik Mukherjee, Lifan Yuan, Pavan Jayasinha, Dilek Hakkani-Tür, Hao Peng</div>
<div class="meta-line">First: 2026-02-07T23:25:26+00:00 · Latest: 2026-02-24T15:43:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07729v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.07729v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL), particularly RL from verifiable reward (RLVR), has become a crucial phase of training large language models (LLMs) and a key focus of current scaling efforts. However, optimization practices in RL largely follow those of next-token prediction stages (e.g., pretraining and supervised fine-tuning), despite fundamental differences between RL and these stages highlighted by recent work. One such practice is the use of the AdamW optimizer, which is widely adopted for training large-scale transformers despite its high memory overhead. Our analysis shows that both momentum and adaptive learning rates in AdamW are less influential in RL than in SFT, leading us to hypothesize that RL benefits less from Adam-style per-parameter adaptive learning rates and momentum. Confirming this hypothesis, our experiments demonstrate that the substantially more memory-efficient SGD, which is known to perform poorly in supervised learning of large-scale transformers, matches or even outperforms AdamW in RL for LLMs. Remarkably, full fine-tuning with SGD updates fewer than 0.02% of model parameters without any sparsity-promoting regularization, more than 1000 times fewer than AdamW. Our analysis offers potential reasons for this update sparsity. These findings provide new insights into the optimization dynamics of RL in LLMs and show that RL can be substantially more parameter-efficient than previously recognized.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>我们真的需要Adam吗？LLM中SGD实现惊人强大且稀疏的强化学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL），特别是基于可验证奖励的强化学习（RLVR），已成为训练大型语言模型（LLM）的关键阶段，也是当前规模化研究的重点。然而，尽管近期研究揭示了RL与预训练、监督微调等阶段存在本质差异，RL的优化实践仍主要沿用下一词预测阶段的方案。例如广泛用于大规模Transformer训练的AdamW优化器，虽内存开销巨大却成为默认选择。我们的分析表明，AdamW中的动量与自适应学习率在RL中的作用远小于在监督微调中，由此推测RL从Adam风格的逐参数自适应学习率与动量中获益有限。实验证实了这一假设：在LLM的RL任务中，内存效率显著更高的SGD（已知在大规模Transformer监督学习中表现欠佳）达到甚至超越了AdamW的性能。值得注意的是，SGD全参数微调仅更新不足0.02%的模型参数（未使用任何稀疏正则化），更新量比AdamW减少超1000倍。我们进一步分析了产生这种更新稀疏性的潜在原因。这些发现为理解LLM中RL的优化动力学提供了新视角，并揭示RL可能具有远超预期的参数效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates whether the widely used AdamW optimizer is necessary for reinforcement learning (RL) in large language models (LLMs), motivated by the observation that RL optimization practices often uncritically follow those from supervised training stages despite fundamental differences. The method involves analyzing the roles of momentum and adaptive learning rates in AdamW during RL, hypothesizing that RL benefits less from these features, and then experimentally comparing the memory-efficient SGD optimizer against AdamW for RL fine-tuning. The main experimental results show that SGD matches or outperforms AdamW in RL performance for LLMs while being vastly more parameter-efficient, updating fewer than 0.02% of model parameters without explicit sparsity regularization—over 1000 times sparser than AdamW—revealing that RL in LLMs can be both highly effective and remarkably sparse with simpler optimization.</div>
<div class="mono" style="margin-top:8px">本文探讨了在大型语言模型的强化学习中广泛使用的AdamW优化器是否必要，其动机在于观察到尽管强化学习与监督训练存在根本差异，但其优化实践常不加批判地沿用后者。研究方法包括分析AdamW中动量和自适应学习率在强化学习中的作用，提出强化学习从这些特性中获益较少的假设，并通过实验对比内存高效的SGD优化器与AdamW在强化学习微调中的表现。主要实验结果表明，SGD在大型语言模型的强化学习性能上匹配甚至优于AdamW，同时参数效率极高，在没有显式稀疏正则化的情况下更新的模型参数少于0.02%，比AdamW稀疏超过1000倍，这揭示了强化学习在大型语言模型中既能高效进行，又能通过更简单的优化实现显著的稀疏性。</div>
</details>
</div>
<div class="card">
<div class="title">A Survey on the Optimization of Large Language Model-based Agents</div>
<div class="meta-line">Authors: Shangheng Du, Jiabao Zhao, Jinxin Shi, Zhentao Xie, Xin Jiang, Yanhong Bai, Liang He</div>
<div class="meta-line">Venue: ACM Computing Surveys 58(9), Article 223, July 2026</div>
<div class="meta-line">First: 2025-03-16T10:09:10+00:00 · Latest: 2026-02-24T15:31:52+00:00</div>
<div class="meta-line">Comments: Published in ACM Computing Surveys, Vol. 58, No. 9, Article 223, July 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.12434v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.12434v2">PDF</a> · <a href="https://github.com/YoungDubbyDu/LLM-Agent-Optimization">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rapid development of Large Language Models (LLMs), LLM-based agents have been widely adopted in various fields, becoming essential for autonomous decision-making and interactive tasks. However, current work typically relies on prompt design or fine-tuning strategies applied to vanilla LLMs, which often leads to limited effectiveness or suboptimal performance in complex agent-related environments. Although LLM optimization techniques can improve model performance across many general tasks, they lack specialized optimization towards critical agent functionalities such as long-term planning, dynamic environmental interaction, and complex decision-making. Although numerous recent studies have explored various strategies to optimize LLM-based agents for complex agent tasks, a systematic review summarizing and comparing these methods from a holistic perspective is still lacking. In this survey, we provide a comprehensive review of LLM-based agent optimization approaches, categorizing them into parameter-driven and parameter-free methods. We first focus on parameter-driven optimization, covering fine-tuning-based optimization, reinforcement learning-based optimization, and hybrid strategies, analyzing key aspects such as trajectory data construction, fine-tuning techniques, reward function design, and optimization algorithms. Additionally, we briefly discuss parameter-free strategies that optimize agent behavior through prompt engineering and external knowledge retrieval. Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions. Our repository for related references is available at https://github.com/YoungDubbyDu/LLM-Agent-Optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型的智能体优化方法综述</div>
<div class="mono" style="margin-top:8px">随着大语言模型（LLMs）的快速发展，基于LLM的智能体已在多个领域广泛应用，成为自主决策与交互任务的关键技术。然而，现有研究多依赖针对基础LLM的提示设计或微调策略，在复杂智能体环境中常面临效果有限或性能欠佳的问题。尽管LLM优化技术能提升模型在通用任务中的表现，但缺乏对智能体核心功能（如长期规划、动态环境交互、复杂决策）的专项优化。近年来虽涌现出许多针对复杂智能体任务的LLM优化策略，但仍缺乏从整体视角系统梳理与比较这些方法的综述研究。本文全面回顾了基于LLM的智能体优化方法，将其划分为参数驱动与无参数两类：重点探讨参数驱动优化（包括基于微调、强化学习及混合策略的优化），分析轨迹数据构建、微调技术、奖励函数设计、优化算法等关键环节；同时简要讨论通过提示工程和外部知识检索实现行为优化的无参数策略。最后，总结了评估与调优所用的数据集和基准测试，回顾了LLM智能体的主要应用场景，并探讨了当前挑战与未来研究方向。相关参考文献库详见https://github.com/YoungDubbyDu/LLM-Agent-Optimization。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the widespread adoption yet suboptimal performance of Large Language Model (LLM)-based agents in complex environments, this survey systematically reviews optimization methods to enhance their capabilities in tasks like planning and decision-making. The method categorizes approaches into parameter-driven strategies, including fine-tuning and reinforcement learning, and parameter-free techniques like prompt engineering, analyzing key components such as data construction and reward design. Main experimental results and discussions are synthesized from existing studies, highlighting evaluation benchmarks, applications, and future challenges in the field.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，尽管基于大语言模型（LLM）的智能体已被广泛应用，但在复杂环境中其性能往往受限，因此需要对优化方法进行系统梳理以提升其在规划、决策等关键功能上的表现。方法上，该综述将优化策略分为参数驱动方法（如微调和强化学习）和无参数方法（如提示工程），并分析了轨迹数据构建、奖励函数设计等关键方面。主要实验成果基于现有研究总结，涵盖了评估数据集、基准测试以及实际应用，并讨论了该领域的主要挑战和未来方向。</div>
</details>
</div>
<div class="card">
<div class="title">UI-Venus-1.5 Technical Report</div>
<div class="meta-line">Authors: Venus Team, Changlong Gao, Zhangxuan Gu, Yulin Liu, Xinyu Qiu, Shuheng Shen, Yue Wen, Tianyu Xia, Zhenyu Xu, Zhengwen Zeng, Beitong Zhou, Xingran Zhou, Weizhi Chen, Sunhao Dai, Jingya Dou, Yichen Gong, Yuan Guo, Zhenlin Guo, Feng Li, Qian Li, Jinzhen Lin, Yuqi Zhou, Linchao Zhu, Liang Chen, Zhenyu Guo, Changhua Meng, Weiqiang Wang</div>
<div class="meta-line">First: 2026-02-09T18:43:40+00:00 · Latest: 2026-02-24T15:17:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09082v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.09082v2">PDF</a> · <a href="https://github.com/inclusionAI/UI-Venus">Code1</a> · <a href="https://huggingface.co/collections/inclusionAI/ui-venus">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging. In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications. The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios. Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UI-Venus-1.5技术报告</div>
<div class="mono" style="margin-top:8px">GUI代理已成为自动化数字环境交互的强大范式，但实现广泛通用性与持续强劲的任务性能仍具挑战。本报告介绍了UI-Venus-1.5，一个为稳健现实应用设计的统一端到端GUI代理。该模型系列包含两个密集变体（2B和8B）及一个专家混合变体（30B-A3B），以适应多样下游应用场景。相较于前代版本，UI-Venus-1.5引入三项关键技术进展：（1）利用30余个数据集的百亿标记进行综合中期训练，建立基础GUI语义；（2）采用全轨迹推演的在线强化学习，使训练目标与大规模环境中的长时程动态导航对齐；（3）通过模型融合构建单一统一GUI代理，将领域专用模型（基础、网页及移动端）合成为连贯检查点。广泛评估表明，UI-Venus-1.5在ScreenSpot-Pro（69.6%）、VenusBench-GD（75.0%）和AndroidWorld（77.6%）等基准测试中创下性能新纪录，显著超越先前强基线。此外，该模型在各类中文移动应用中展现出稳健导航能力，可在真实场景中有效执行用户指令。代码：https://github.com/inclusionAI/UI-Venus；模型：https://huggingface.co/collections/inclusionAI/ui-venus</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of creating GUI agents that are both broadly generalizable and highly performant for real-world automation, this report introduces UI-Venus-1.5, a unified end-to-end agent family with dense and mixture-of-experts variants. The method employs a three-part technical advance: mid-training on extensive GUI data to build foundational semantics, online reinforcement learning for long-horizon navigation alignment, and model merging to unify domain-specific expertise into a single agent. Experimental results show state-of-the-art performance on key benchmarks like ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), with demonstrated robustness in navigating Chinese mobile applications.</div>
<div class="mono" style="margin-top:8px">为应对图形用户界面（GUI）智能体在广泛通用性和强大任务性能方面仍存的挑战，本报告提出了UI-Venus-1.5，这是一个为鲁棒现实应用设计的统一端到端GUI智能体系列，包含密集型和混合专家型变体。其方法基于三项关键技术：利用海量数据的中期训练建立GUI语义基础、采用全轨迹在线强化学习以对齐长期动态导航目标，以及通过模型合并将领域专用模型融合为单一智能体。主要实验结果表明，该模型在ScreenSpot-Pro（69.6%）、VenusBench-GD（75.0%）和AndroidWorld（77.6%）等基准测试中取得了最先进的性能，并在多种中文移动应用导航中展现出强大的执行能力。</div>
</details>
</div>
<div class="card">
<div class="title">Wasserstein Barycenter Soft Actor-Critic</div>
<div class="meta-line">Authors: Zahra Shahrooei, Ali Baheri</div>
<div class="meta-line">First: 2025-06-11T20:39:50+00:00 · Latest: 2026-02-24T14:53:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.10167v4">Abs</a> · <a href="https://arxiv.org/pdf/2506.10167v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep off-policy actor-critic algorithms have emerged as the leading framework for reinforcement learning in continuous control domains. However, most of these algorithms suffer from poor sample efficiency, especially in environments with sparse rewards. In this paper, we take a step towards addressing this issue by providing a principled directed exploration strategy. We propose Wasserstein Barycenter Soft Actor-Critic (WBSAC) algorithm, which benefits from a pessimistic actor for temporal difference learning and an optimistic actor to promote exploration. This is achieved by using the Wasserstein barycenter of the pessimistic and optimistic policies as the exploration policy and adjusting the degree of exploration throughout the learning process. We compare WBSAC with state-of-the-art off-policy actor-critic algorithms and show that WBSAC is more sample-efficient on MuJoCo continuous control tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Wasserstein重心软演员-评论家算法</div>
<div class="mono" style="margin-top:8px">深度离策略演员-评论家算法已成为连续控制领域强化学习的主流框架，但多数算法样本效率低下，尤其在稀疏奖励环境中。本文通过提出一种基于原理的定向探索策略以解决该问题，引入Wasserstein重心软演员-评论家算法。该算法融合悲观演员的时间差分学习与乐观演员的探索促进机制，以悲观策略和乐观策略的Wasserstein重心作为探索策略，并在学习过程中动态调整探索强度。实验表明，在MuJoCo连续控制任务中，本算法较现有离策略演员-评论家算法具有更高的样本效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the poor sample efficiency of deep off-policy actor-critic algorithms, particularly in sparse-reward environments, by introducing a principled directed exploration strategy. The proposed Wasserstein Barycenter Soft Actor-Critic (WBSAC) method employs a dual-actor framework: a pessimistic actor for temporal difference learning and an optimistic actor to encourage exploration, with their exploration policy formed by taking the Wasserstein barycenter of the two policies and adaptively adjusting exploration intensity. Experimental results on MuJoCo continuous control tasks demonstrate that WBSAC achieves superior sample efficiency compared to state-of-the-art off-policy actor-critic algorithms.</div>
<div class="mono" style="margin-top:8px">本文针对深度离策略演员-评论家算法样本效率低的问题，特别是在稀疏奖励环境中，提出了一种有原则的定向探索策略。所提出的Wasserstein重心软演员-评论家（WBSAC）算法采用双演员框架：一个悲观演员用于时序差分学习，一个乐观演员用于促进探索，并通过计算两者策略的Wasserstein重心作为探索策略，且在整个学习过程中自适应调整探索程度。在MuJoCo连续控制任务上的实验结果表明，WBSAC相比最先进的离策略演员-评论家算法具有更高的样本效率。</div>
</details>
</div>
<div class="card">
<div class="title">PMG: Parameterized Motion Generator for Human-like Locomotion Control</div>
<div class="meta-line">Authors: Chenxi Han, Yuheng Min, Zihao Huang, Ao Hong, Hang Liu, Yi Cheng, Houde Liu</div>
<div class="meta-line">First: 2026-02-13T06:38:04+00:00 · Latest: 2026-02-24T14:34:42+00:00</div>
<div class="meta-line">Comments: Website: https://pmg-icra26.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12656v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.12656v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://pmg-icra26.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in data-driven reinforcement learning and motion tracking have substantially improved humanoid locomotion, yet critical practical challenges remain. In particular, while low-level motion tracking and trajectory-following controllers are mature, whole-body reference-guided methods are difficult to adapt to higher-level command interfaces and diverse task contexts: they require large, high-quality datasets, are brittle across speed and pose regimes, and are sensitive to robot-specific calibration. To address these limitations, we propose the Parameterized Motion Generator (PMG), a real-time motion generator grounded in an analysis of human motion structure that synthesizes reference trajectories using only a compact set of parameterized motion data together with high-dimensional control commands. Combined with an imitation-learning pipeline and an optimization-based sim-to-real motor parameter identification module, we validate the complete approach on our humanoid prototype ZERITH Z1 and show that, within a single integrated system, PMG produces natural, human-like locomotion, responds precisely to high-dimensional control inputs-including VR-based teleoperation-and enables efficient, verifiable sim-to-real transfer. Together, these results establish a practical, experimentally validated pathway toward natural and deployable humanoid control. Website: https://pmg-icra26.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PMG：参数化运动生成器——面向类人步态控制的参数化运动生成器</div>
<div class="mono" style="margin-top:8px">数据驱动的强化学习与运动追踪技术的最新进展显著提升了人形机器人的步态控制能力，但关键的实际挑战依然存在。具体而言，虽然低层级的运动追踪与轨迹跟踪控制器已较为成熟，但基于全身参考引导的方法难以适配高层级指令接口与多样化任务场景：这些方法需要大规模高质量数据集，在不同速度与姿态区间表现脆弱，且对机器人特定校准参数敏感。为突破这些局限，我们提出参数化运动生成器（PMG）——一种基于人体运动结构分析的实时运动生成器，仅需使用紧凑的参数化运动数据集配合高维度控制指令即可合成参考轨迹。结合模仿学习流程与基于优化的仿真-现实电机参数辨识模块，我们在人形机器人原型ZERITH Z1上验证了完整方案，证明PMG在单一集成系统中能生成自然类人步态，精准响应高维度控制输入（包括基于VR的遥操作），并实现高效可验证的仿真-现实迁移。这些成果共同构建了一条经过实验验证、可实际部署的自然化人形机器人控制路径。项目网站：https://pmg-icra26.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing whole-body reference-guided locomotion controllers, which are difficult to adapt to high-level commands and diverse tasks, this paper introduces the Parameterized Motion Generator (PMG). The method synthesizes reference trajectories using a compact set of parameterized motion data and high-dimensional control commands, grounded in an analysis of human motion structure, and integrates it with an imitation-learning pipeline and a sim-to-real motor parameter identification module. Experimental validation on the ZERITH Z1 humanoid shows the system produces natural, human-like locomotion, responds precisely to high-dimensional inputs like VR teleoperation, and enables efficient sim-to-real transfer, establishing a practical pathway for deployable humanoid control.</div>
<div class="mono" style="margin-top:8px">针对现有全身参考引导的仿人运动控制器难以适应高级指令和多样化任务的局限性，本文提出了参数化运动生成器（PMG）。该方法基于对人体运动结构的分析，仅使用紧凑的参数化运动数据和高维控制命令来合成参考轨迹，并将其与模仿学习流程及基于优化的仿真到现实电机参数识别模块相结合。在ZERITH Z1仿人机器人上的实验验证表明，该系统能生成自然、类人的运动，精确响应如VR遥操作等高维输入，并实现高效的仿真到现实迁移，为可部署的仿人控制建立了一条实用途径。</div>
</details>
</div>
<div class="card">
<div class="title">Statistical Inference for Temporal Difference Learning with Linear Function Approximation</div>
<div class="meta-line">Authors: Weichen Wu, Gen Li, Yuting Wei, Alessandro Rinaldo</div>
<div class="meta-line">First: 2024-10-21T15:34:44+00:00 · Latest: 2026-02-24T12:51:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.16106v5">Abs</a> · <a href="https://arxiv.org/pdf/2410.16106v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate the statistical properties of Temporal Difference (TD) learning with Polyak-Ruppert averaging, arguably one of the most widely used algorithms in reinforcement learning, for the task of estimating the parameters of the optimal linear approximation to the value function. Assuming independent samples, we make three theoretical contributions that improve upon the current state-of-the-art results: (i) we establish refined high-dimensional Berry-Esseen bounds over the class of convex sets, achieving faster rates than the best known results, and (ii) we propose and analyze a novel, computationally efficient online plug-in estimator of the asymptotic covariance matrix; (iii) we derive sharper high probability convergence guarantees that depend explicitly on the asymptotic variance and hold under weaker conditions than those adopted in the literature. These results enable the construction of confidence regions and simultaneous confidence intervals for the linear parameters of the value function approximation, with guaranteed finite-sample coverage. We demonstrate the applicability of our theoretical findings through numerical experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于线性函数逼近的时间差分学习的统计推断</div>
<div class="mono" style="margin-top:8px">本文研究了强化学习中最广泛使用的算法之一——结合Polyak-Ruppert平均的时间差分学习在估计价值函数最优线性逼近参数时的统计性质。基于独立样本假设，我们在三个理论层面改进了现有最优成果：(i) 在凸集类上建立了更精细的高维Berry-Esseen界，获得了优于已知结果的收敛速率；(ii) 提出并分析了一种计算高效的新型在线插件估计器用于渐近协方差矩阵；(iii) 推导出更敏锐的高概率收敛保证，其显式依赖于渐近方差且所需条件弱于文献现有假设。这些成果使得为价值函数逼近的线性参数构建置信域与同步置信区间成为可能，并确保有限样本覆盖性。我们通过数值实验验证了理论发现的实际适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need for reliable statistical inference in Temporal Difference (TD) learning, a core reinforcement learning algorithm, when it is used with linear function approximation to estimate value function parameters. The method involves analyzing TD learning with Polyak-Ruppert averaging under an independent sampling assumption, and its key contributions are establishing refined high-dimensional Berry-Esseen bounds for faster convergence rates, proposing a novel online plug-in estimator for the asymptotic covariance matrix, and deriving sharper high-probability convergence guarantees. The main experimental results, supported by numerical experiments, demonstrate that these theoretical advances enable the construction of confidence regions and intervals for the linear parameters with guaranteed finite-sample coverage, thereby providing a foundation for statistical inference in practical TD learning applications.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，当使用线性函数逼近来估计价值函数参数时，需要为时序差分（TD）学习这一核心强化学习算法提供可靠的统计推断。研究方法基于独立采样假设，分析了采用Polyak-Ruppert平均的TD学习，其主要理论贡献包括：建立了更精细的高维Berry-Esseen界以获得更快的收敛速率，提出并分析了一种新颖的、计算高效的在线插件估计器用于渐近协方差矩阵，以及在比文献更弱的条件下推导了依赖于渐近方差的更尖锐的高概率收敛保证。实验结果表明，这些理论进展能够为线性参数构建具有有限样本覆盖保证的置信区域和同时置信区间，从而为实际TD学习应用中的统计推断奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">From Parameters to Behaviors: Unsupervised Compression of the Policy Space</div>
<div class="meta-line">Authors: Davide Tenedini, Riccardo Zamboni, Mirco Mutti, Marcello Restelli</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-09-26T16:42:52+00:00 · Latest: 2026-02-24T12:23:51+00:00</div>
<div class="meta-line">Comments: ICLR 2026 camera ready version. Changed typo in the title</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.22566v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.22566v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite its recent successes, Deep Reinforcement Learning (DRL) is notoriously sample-inefficient. We argue that this inefficiency stems from the standard practice of optimizing policies directly in the high-dimensional and highly redundant parameter space $Θ$. This challenge is greatly compounded in multi-task settings. In this work, we develop a novel, unsupervised approach that compresses the policy parameter space $Θ$ into a low-dimensional latent space $\mathcal{Z}$. We train a generative model $g:\mathcal{Z}\toΘ$ by optimizing a behavioral reconstruction loss, which ensures that the latent space is organized by functional similarity rather than proximity in parameterization. We conjecture that the inherent dimensionality of this manifold is a function of the environment&#x27;s complexity, rather than the size of the policy network. We validate our approach in continuous control domains, showing that the parameterization of standard policy networks can be compressed up to five orders of magnitude while retaining most of its expressivity. As a byproduct, we show that the learned manifold enables task-specific adaptation via Policy Gradient operating in the latent space $\mathcal{Z}$.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从参数到行为：策略空间的无监督压缩</div>
<div class="mono" style="margin-top:8px">尽管深度强化学习（DRL）近期取得了成功，但其样本效率低下是众所周知的。我们认为这种低效源于直接在高度冗余的高维参数空间$Θ$中优化策略的标准做法，该挑战在多任务场景中尤为突出。本研究提出一种新颖的无监督方法，将策略参数空间$Θ$压缩至低维潜空间$\mathcal{Z}$。我们通过优化行为重构损失训练生成模型$g:\mathcal{Z}\toΘ$，确保潜空间按功能相似性而非参数邻近性组织。我们推测该流形的本征维度取决于环境复杂度，而非策略网络规模。在连续控制领域的验证表明，标准策略网络的参数化可被压缩高达五个数量级，同时保留大部分表达能力。作为副产品，我们证明所学流形支持通过在潜空间$\mathcal{Z}$中运行的策略梯度实现任务特定适应。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the sample inefficiency of Deep Reinforcement Learning (DRL), which stems from optimizing policies in a high-dimensional, redundant parameter space, this paper introduces an unsupervised method to compress the policy parameter space into a low-dimensional latent space. The method trains a generative model using a behavioral reconstruction loss, organizing the latent space by functional similarity rather than parameter proximity, and conjectures that the manifold&#x27;s dimensionality depends on environment complexity, not network size. Experimental results in continuous control domains demonstrate compression of policy network parameterization by up to five orders of magnitude while retaining expressivity, with the learned manifold enabling task-specific adaptation via Policy Gradient in the latent space.</div>
<div class="mono" style="margin-top:8px">本文针对深度强化学习（DRL）因在高维冗余参数空间中优化策略而导致的样本效率低下问题，提出了一种无监督方法，将策略参数空间压缩到低维潜在空间。该方法通过行为重建损失训练生成模型，使潜在空间按功能相似性而非参数邻近性组织，并推测流形维度取决于环境复杂性而非网络大小。在连续控制领域的实验结果表明，标准策略网络的参数化可压缩高达五个数量级，同时保持大部分表达能力，且学习的流形支持在潜在空间中通过策略梯度进行任务特定适应。</div>
</details>
</div>
<div class="card">
<div class="title">Regret-Guided Search Control for Efficient Learning in AlphaZero</div>
<div class="meta-line">Authors: Yun-Jui Tsai, Wei-Yu Chen, Yan-Ru Ju, Yu-Hung Chang, Ti-Rong Wu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-24T11:49:59+00:00 · Latest: 2026-02-24T11:49:59+00:00</div>
<div class="meta-line">Comments: Accepted by the Fourteenth International Conference on Learning Representations (ICLR 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20809v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20809v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://rlg.iis.sinica.edu.tw/papers/rgsc">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) agents achieve remarkable performance but remain far less learning-efficient than humans. While RL agents require extensive self-play games to extract useful signals, humans often need only a few games, improving rapidly by repeatedly revisiting states where mistakes occurred. This idea, known as search control, aims to restart from valuable states rather than always from the initial state. In AlphaZero, prior work Go-Exploit applies this idea by sampling past states from self-play or search trees, but it treats all states equally, regardless of their learning potential. We propose Regret-Guided Search Control (RGSC), which extends AlphaZero with a regret network that learns to identify high-regret states, where the agent&#x27;s evaluation diverges most from the actual outcome. These states are collected from both self-play trajectories and MCTS nodes, stored in a prioritized regret buffer, and reused as new starting positions. Across 9x9 Go, 10x10 Othello, and 11x11 Hex, RGSC outperforms AlphaZero and Go-Exploit by an average of 77 and 89 Elo, respectively. When training on a well-trained 9x9 Go model, RGSC further improves the win rate against KataGo from 69.3% to 78.2%, while both baselines show no improvement. These results demonstrate that RGSC provides an effective mechanism for search control, improving both efficiency and robustness of AlphaZero training. Our code is available at https://rlg.iis.sinica.edu.tw/papers/rgsc.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于遗憾引导的搜索控制提升AlphaZero学习效率</div>
<div class="mono" style="margin-top:8px">强化学习智能体虽能取得卓越性能，但其学习效率远低于人类。人类仅需少量对局即可通过反复复盘失误状态快速提升，而强化学习智能体需要大量自我对局才能提取有效信号。搜索控制的核心思想是从高价值状态而非初始状态重启学习。在AlphaZero框架中，现有方法Go-Exploit通过从自我对局或搜索树中采样历史状态实现该思想，但未区分状态的学习潜力。本文提出遗憾引导搜索控制方法，通过训练遗憾网络识别高遗憾状态——即智能体评估与实际结果偏差最大的状态。这些状态从自我对局轨迹和蒙特卡洛树搜索节点中收集，存储于优先级遗憾缓冲区，并作为新起始位置重复利用。在9×9围棋、10×10黑白棋和11×11六边形棋的实验中，RGSC分别以平均77和89等级分的优势超越AlphaZero和Go-Exploit。在已训练的9×9围棋模型上，RGSC将对抗KataGo的胜率从69.3%提升至78.2%，而基线方法均无改进。结果表明RGSC为搜索控制提供了有效机制，显著提升了AlphaZero训练的效率与鲁棒性。代码已开源：https://rlg.iis.sinica.edu.tw/papers/rgsc。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the inefficiency of standard reinforcement learning agents compared to human learning, which improves by revisiting mistakes, this paper introduces Regret-Guided Search Control (RGSC) to enhance AlphaZero. The method employs a regret network to identify high-regret states where the agent&#x27;s evaluation diverges from actual outcomes, collecting these from self-play and MCTS nodes into a prioritized buffer for reuse as starting positions. Experimental results across 9x9 Go, 10x10 Othello, and 11x11 Hex show RGSC outperforms AlphaZero and Go-Exploit by average Elo gains of 77 and 89, respectively, and further boosts win rates against KataGo in 9x9 Go from 69.3% to 78.2%, demonstrating improved training efficiency and robustness.</div>
<div class="mono" style="margin-top:8px">针对强化学习智能体学习效率远低于人类的问题，本文提出后悔引导搜索控制（RGSC）来改进AlphaZero，其动机是人类通过重复访问错误状态来快速提升。该方法使用后悔网络识别高后悔状态，即智能体评估与实际结果差异大的状态，从自我对弈和MCTS节点中收集这些状态并存储于优先缓冲区中，作为新的起始位置重用。在9x9围棋、10x10黑白棋和11x11六边形棋上的实验表明，RGSC平均Elo分别比AlphaZero和Go-Exploit高出77和89，并在9x9围棋中将对抗KataGo的胜率从69.3%提升至78.2%，证明了训练效率和鲁棒性的增强。</div>
</details>
</div>
<div class="card">
<div class="title">Probing Dec-POMDP Reasoning in Cooperative MARL</div>
<div class="meta-line">Authors: Kale-ab Tessera, Leonard Hinckeldey, Riccardo Zamboni, David Abel, Amos Storkey</div>
<div class="meta-line">Venue: AAMAS 2026</div>
<div class="meta-line">First: 2026-02-24T11:44:46+00:00 · Latest: 2026-02-24T11:44:46+00:00</div>
<div class="meta-line">Comments: To appear at the 25th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20804v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20804v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cooperative multi-agent reinforcement learning (MARL) is typically framed as a decentralised partially observable Markov decision process (Dec-POMDP), a setting whose hardness stems from two key challenges: partial observability and decentralised coordination. Genuinely solving such tasks requires Dec-POMDP reasoning, where agents use history to infer hidden states and coordinate based on local information. Yet it remains unclear whether popular benchmarks actually demand this reasoning or permit success via simpler strategies. We introduce a diagnostic suite combining statistically grounded performance comparisons and information-theoretic probes to audit the behavioural complexity of baseline policies (IPPO and MAPPO) across 37 scenarios spanning MPE, SMAX, Overcooked, Hanabi, and MaBrax. Our diagnostics reveal that success on these benchmarks rarely requires genuine Dec-POMDP reasoning. Reactive policies match the performance of memory-based agents in over half the scenarios, and emergent coordination frequently relies on brittle, synchronous action coupling rather than robust temporal influence. These findings suggest that some widely used benchmarks may not adequately test core Dec-POMDP assumptions under current training paradigms, potentially leading to over-optimistic assessments of progress. We release our diagnostic tooling to support more rigorous environment design and evaluation in cooperative MARL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探究合作多智能体强化学习中的分散式部分可观测马尔可夫决策过程推理</div>
<div class="mono" style="margin-top:8px">合作多智能体强化学习通常被建模为分散式部分可观测马尔可夫决策过程，其复杂性源于部分可观测性与分散式协调两大挑战。真正解决此类任务需依赖Dec-POMDP推理——智能体需利用历史信息推断隐藏状态并基于局部信息进行协调。然而，现有主流基准测试是否真正需要此类推理，抑或可通过简单策略达成目标，仍不明确。本研究开发了一套诊断工具集，结合基于统计的性能比较与信息论探针，对IPPO和MAPPO基线策略在涵盖MPE、SMAX、Overcooked、Hanabi及MaBrax的37个场景中的行为复杂度进行审计。诊断结果表明：这些基准测试的成功很少需要真正的Dec-POMDP推理。在超半数场景中，反应式策略与基于记忆的智能体表现相当；涌现的协调常依赖脆弱同步动作耦合，而非稳健的时序影响。这些发现暗示当前训练范式下，部分广泛使用的基准测试可能未能充分检验Dec-POMDP核心假设，可能导致对研究进展的过度乐观评估。我们公开诊断工具以支持合作多智能体强化学习中更严谨的环境设计与评估。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study investigates whether cooperative multi-agent reinforcement learning (MARL) benchmarks genuinely require Dec-POMDP reasoning, which involves agents using history to infer hidden states and coordinate locally, or if simpler strategies suffice. The authors develop a diagnostic suite combining performance comparisons and information-theoretic probes to analyze baseline policies like IPPO and MAPPO across 37 scenarios from environments such as MPE, SMAX, Overcooked, Hanabi, and MaBrax. Experimental results show that success on these benchmarks rarely demands true Dec-POMDP reasoning, with reactive policies matching memory-based agents in over half the scenarios, and coordination often relying on brittle, synchronous action coupling rather than robust temporal influence. This suggests current benchmarks may not adequately test core Dec-POMDP assumptions, potentially leading to over-optimistic progress assessments in cooperative MARL.</div>
<div class="mono" style="margin-top:8px">本研究探讨了合作多智能体强化学习（MARL）基准测试是否真正需要分散式部分可观测马尔可夫决策过程（Dec-POMDP）推理，即智能体利用历史推断隐藏状态并进行本地协调，还是更简单的策略就足够。作者开发了一个诊断套件，结合性能比较和信息论探针，分析了IPPO和MAPPO等基线策略在MPE、SMAX、Overcooked、Hanabi和MaBrax等环境的37个场景中的表现。实验结果表明，这些基准测试的成功很少需要真正的Dec-POMDP推理，反应式策略在一半以上的场景中与基于记忆的智能体表现相当，且协调通常依赖于脆弱的同步动作耦合而非稳健的时间影响。这表明当前基准测试可能未能充分检验Dec-POMDP的核心假设，可能导致对合作MARL进展的过度乐观评估。</div>
</details>
</div>
<div class="card">
<div class="title">GLM-5: from Vibe Coding to Agentic Engineering</div>
<div class="meta-line">Authors: GLM-5-Team, :, Aohan Zeng, Xin Lv, Zhenyu Hou, Zhengxiao Du, Qinkai Zheng, Bin Chen, Da Yin, Chendi Ge, Chenghua Huang, Chengxing Xie, Chenzheng Zhu, Congfeng Yin, Cunxiang Wang, Gengzheng Pan, Hao Zeng, Haoke Zhang, Haoran Wang, Huilong Chen, Jiajie Zhang, Jian Jiao, Jiaqi Guo, Jingsen Wang, Jingzhao Du, Jinzhu Wu, Kedong Wang, Lei Li, Lin Fan, Lucen Zhong, Mingdao Liu, Mingming Zhao, Pengfan Du, Qian Dong, Rui Lu, Shuang-Li, Shulin Cao, Song Liu, Ting Jiang, Xiaodong Chen, Xiaohan Zhang, Xuancheng Huang, Xuezhen Dong, Yabo Xu, Yao Wei, Yifan An, Yilin Niu, Yitong Zhu, Yuanhao Wen, Yukuo Cen, Yushi Bai, Zhongpei Qiao, Zihan Wang, Zikang Wang, Zilin Zhu, Ziqiang Liu, Zixuan Li, Bojie Wang, Bosi Wen, Can Huang, Changpeng Cai, Chao Yu, Chen Li, Chengwei Hu, Chenhui Zhang, Dan Zhang, Daoyan Lin, Dayong Yang, Di Wang, Ding Ai, Erle Zhu, Fangzhou Yi, Feiyu Chen, Guohong Wen, Hailong Sun, Haisha Zhao, Haiyi Hu, Hanchen Zhang, Hanrui Liu, Hanyu Zhang, Hao Peng, Hao Tai, Haobo Zhang, He Liu, Hongwei Wang, Hongxi Yan, Hongyu Ge, Huan Liu, Huanpeng Chu, Jia&#x27;ni Zhao, Jiachen Wang, Jiajing Zhao, Jiamin Ren, Jiapeng Wang, Jiaxin Zhang, Jiayi Gui, Jiayue Zhao, Jijie Li, Jing An, Jing Li, Jingwei Yuan, Jinhua Du, Jinxin Liu, Junkai Zhi, Junwen Duan, Kaiyue Zhou, Kangjian Wei, Ke Wang, Keyun Luo, Laiqiang Zhang, Leigang Sha, Liang Xu, Lindong Wu, Lintao Ding, Lu Chen, Minghao Li, Nianyi Lin, Pan Ta, Qiang Zou, Rongjun Song, Ruiqi Yang, Shangqing Tu, Shangtong Yang, Shaoxiang Wu, Shengyan Zhang, Shijie Li, Shuang Li, Shuyi Fan, Wei Qin, Wei Tian, Weining Zhang, Wenbo Yu, Wenjie Liang, Xiang Kuang, Xiangmeng Cheng, Xiangyang Li, Xiaoquan Yan, Xiaowei Hu, Xiaoying Ling, Xing Fan, Xingye Xia, Xinyuan Zhang, Xinze Zhang, Xirui Pan, Xu Zou, Xunkai Zhang, Yadi Liu, Yandong Wu, Yanfu Li, Yidong Wang, Yifan Zhu, Yijun Tan, Yilin Zhou, Yiming Pan, Ying Zhang, Yinpei Su, Yipeng Geng, Yong Yan, Yonglin Tan, Yuean Bi, Yuhan Shen, Yuhao Yang, Yujiang Li, Yunan Liu, Yunqing Wang, Yuntao Li, Yurong Wu, Yutao Zhang, Yuxi Duan, Yuxuan Zhang, Zezhen Liu, Zhengtao Jiang, Zhenhe Yan, Zheyu Zhang, Zhixiang Wei, Zhuo Chen, Zhuoer Feng, Zijun Yao, Ziwei Chai, Ziyuan Wang, Zuzhou Zhang, Bin Xu, Minlie Huang, Hongning Wang, Juanzi Li, Yuxiao Dong, Jie Tang</div>
<div class="meta-line">First: 2026-02-17T17:50:56+00:00 · Latest: 2026-02-24T10:44:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15763v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.15763v2">PDF</a> · <a href="https://github.com/zai-org/GLM-5">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GLM-5：从氛围编码到智能体工程</div>
<div class="mono" style="margin-top:8px">我们推出GLM-5，这是一款旨在将范式从氛围编码转向智能体工程的下一代基础模型。基于前代模型的智能体、推理与编码（ARC）能力，GLM-5采用DSA架构显著降低训练与推理成本，同时保持长上下文保真度。为推进模型对齐与自主性，我们构建了新型异步强化学习基础设施，通过解耦生成与训练大幅提升后训练效率。此外，我们提出创新的异步智能体强化学习算法，进一步提升强化学习质量，使模型能更有效地从复杂长程交互中学习。通过这些创新，GLM-5在主流开放基准测试中取得最先进性能。最关键的是，GLM-5在实际编码任务中展现出前所未有的能力，在处理端到端软件工程挑战方面超越以往基线。代码、模型及更多信息详见https://github.com/zai-org/GLM-5。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces GLM-5, a next-generation foundation model motivated by advancing from vibe coding to agentic engineering, aiming to enhance agentic, reasoning, and coding capabilities. The method employs DSA to reduce training and inference costs while preserving long-context fidelity, and implements an asynchronous reinforcement learning infrastructure with novel algorithms to improve post-training efficiency and learning from complex interactions. Experimental results show that GLM-5 achieves state-of-the-art performance on major open benchmarks and demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in end-to-end software engineering challenges.</div>
<div class="mono" style="margin-top:8px">本文介绍了GLM-5，这是一种新一代基础模型，其动机是从氛围编码转向代理工程，旨在提升代理、推理和编码能力。方法上采用DSA以降低训练和推理成本同时保持长上下文保真度，并实施异步强化学习基础设施及新算法，以提高后训练效率并从复杂长期交互中更有效地学习。实验结果表明，GLM-5在主要开放基准测试中达到最先进性能，并在现实世界编码任务中展现出前所未有的能力，在处理端到端软件工程挑战方面超越了先前基线。</div>
</details>
</div>
<div class="card">
<div class="title">One-Step Flow Q-Learning: Addressing the Diffusion Policy Bottleneck in Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Thanh Nguyen, Chang D. Yoo</div>
<div class="meta-line">First: 2025-08-19T15:05:55+00:00 · Latest: 2026-02-24T10:29:51+00:00</div>
<div class="meta-line">Comments: 10 pages, ICLR2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.13904v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.13904v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Q-Learning (DQL) has established diffusion policies as a high-performing paradigm for offline reinforcement learning, but its reliance on multi-step denoising for action generation renders both training and inference slow and fragile. Existing efforts to accelerate DQL toward one-step denoising typically rely on auxiliary modules or policy distillation, sacrificing either simplicity or performance. It remains unclear whether a one-step policy can be trained directly without such trade-offs. To this end, we introduce One-Step Flow Q-Learning (OFQL), a novel framework that enables effective one-step action generation during both training and inference, without auxiliary modules or distillation. OFQL reformulates the DQL policy within the Flow Matching (FM) paradigm but departs from conventional FM by learning an average velocity field that directly supports accurate one-step action generation. This design removes the need for multi-step denoising and backpropagation-through-time updates, resulting in substantially faster and more robust learning. Extensive experiments on the D4RL benchmark show that OFQL, despite generating actions in a single step, not only significantly reduces computation during both training and inference but also outperforms multi-step DQL by a large margin. Furthermore, OFQL surpasses all other baselines, achieving state-of-the-art performance in D4RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一步流Q学习：解决离线强化学习中扩散策略的瓶颈</div>
<div class="mono" style="margin-top:8px">扩散Q学习（DQL）已确立扩散策略作为离线强化学习的高性能范式，但其依赖多步去噪进行动作生成，导致训练和推断过程缓慢且脆弱。现有加速DQL实现一步去噪的方法通常依赖辅助模块或策略蒸馏，牺牲了简洁性或性能。目前尚不清楚能否在不做此类权衡的情况下直接训练一步策略。为此，我们提出一步流Q学习（OFQL），这是一种新颖的框架，能在训练和推断期间实现有效的一步动作生成，无需辅助模块或蒸馏。OFQL在流匹配（FM）范式内重构了DQL策略，但通过直接学习支持精确一步动作生成的平均速度场，突破了传统FM的限制。该设计消除了多步去噪和时序反向传播更新的需求，实现了显著更快且更稳健的学习。在D4RL基准上的大量实验表明，尽管OFQL仅通过单步生成动作，不仅大幅减少了训练和推断的计算量，还显著超越了多步DQL的性能。此外，OFQL超越了所有其他基线方法，在D4RL上达到了最先进的性能水平。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the computational inefficiency of Diffusion Q-Learning (DQL) in offline reinforcement learning, where multi-step denoising for action generation slows training and inference. The authors propose One-Step Flow Q-Learning (OFQL), a novel framework that reformulates the policy within the Flow Matching paradigm to learn an average velocity field, enabling direct one-step action generation without auxiliary modules or distillation. Experimental results on the D4RL benchmark demonstrate that OFQL not only drastically reduces computation time but also outperforms multi-step DQL and other baselines, achieving state-of-the-art performance.</div>
<div class="mono" style="margin-top:8px">本文针对离线强化学习中扩散Q学习（DQL）因多步去噪生成动作导致训练和推理缓慢的计算效率问题，提出了一步流Q学习（OFQL）这一新框架。该方法在流匹配范式下重构策略，学习平均速度场以支持直接的一步动作生成，无需辅助模块或蒸馏。在D4RL基准上的大量实验表明，OFQL不仅显著降低了计算开销，而且性能大幅超越多步DQL及其他基线方法，达到了最先进的水平。</div>
</details>
</div>
<div class="card">
<div class="title">PyVision-RL: Forging Open Agentic Vision Models via RL</div>
<div class="meta-line">Authors: Shitian Zhao, Shaoheng Lin, Ming Li, Haoquan Zhang, Wenshuo Peng, Kaipeng Zhang, Chen Wei</div>
<div class="meta-line">First: 2026-02-24T10:08:33+00:00 · Latest: 2026-02-24T10:08:33+00:00</div>
<div class="meta-line">Comments: preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20739v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20739v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PyVision-RL：通过强化学习锻造开放智能体视觉模型</div>
<div class="mono" style="margin-top:8px">面向智能体多模态模型的强化学习常面临交互崩溃问题，即模型倾向于减少工具使用和多轮推理，限制了智能体行为的优势。我们提出PyVision-RL——一个用于开放权重多模态模型的强化学习框架，能稳定训练并维持交互持续性。该方法结合过采样-过滤-排序的轨迹生成策略与累积工具奖励机制，有效防止崩溃并促进多轮工具调用。通过统一训练流程，我们开发了面向图像理解的PyVision-Image和面向视频理解的PyVision-Video。在视频推理任务中，PyVision-Video采用按需上下文构建技术，在推理过程中选择性采样任务相关帧，显著降低视觉标记消耗。实验表明，该系统在保持优异性能的同时提升了效率，证明持续交互与按需视觉处理对可扩展多模态智能体至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the problem of interaction collapse in reinforcement learning for agentic multimodal models, where models reduce tool usage and multi-turn reasoning, thereby limiting agentic benefits. The authors introduce PyVision-RL, a reinforcement learning framework designed for open-weight multimodal models, which stabilizes training and sustains interaction through an oversampling-filtering-ranking rollout strategy combined with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified pipeline, they develop PyVision-Image and PyVision-Video for image and video understanding, with PyVision-Video employing on-demand context construction to selectively sample task-relevant frames during reasoning, significantly reducing visual token usage. Experimental results demonstrate strong performance and improved efficiency, highlighting that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.</div>
<div class="mono" style="margin-top:8px">本文针对多模态智能体模型在强化学习中出现的交互崩溃问题展开研究，该问题导致模型减少工具使用和多轮推理，从而限制了智能体行为的优势。作者提出了PyVision-RL，一个面向开放权重重多模态模型的强化学习框架，通过结合过采样-过滤-排序的轨迹策略与累积工具奖励，稳定训练并维持交互，以防止崩溃并鼓励多轮工具使用。利用统一训练流程，他们开发了用于图像和视频理解的PyVision-Image和PyVision-Video模型，其中PyVision-Video采用按需上下文构建，在推理过程中选择性采样任务相关帧，显著减少了视觉令牌的使用。实验结果表明，模型表现出色且效率提升，证明了持续交互和按需视觉处理对于可扩展多模态智能体的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Fuz-RL: A Fuzzy-Guided Robust Framework for Safe Reinforcement Learning under Uncertainty</div>
<div class="meta-line">Authors: Xu Wan, Chao Yang, Cheng Yang, Jie Song, Mingyang Sun</div>
<div class="meta-line">First: 2026-02-24T09:50:17+00:00 · Latest: 2026-02-24T09:50:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20729v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20729v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safe Reinforcement Learning (RL) is crucial for achieving high performance while ensuring safety in real-world applications. However, the complex interplay of multiple uncertainty sources in real environments poses significant challenges for interpretable risk assessment and robust decision-making. To address these challenges, we propose Fuz-RL, a fuzzy measure-guided robust framework for safe RL. Specifically, our framework develops a novel fuzzy Bellman operator for estimating robust value functions using Choquet integrals. Theoretically, we prove that solving the Fuz-RL problem (in Constrained Markov Decision Process (CMDP) form) is equivalent to solving distributionally robust safe RL problems (in robust CMDP form), effectively avoiding min-max optimization. Empirical analyses on safe-control-gym and safety-gymnasium scenarios demonstrate that Fuz-RL effectively integrates with existing safe RL baselines in a model-free manner, significantly improving both safety and control performance under various types of uncertainties in observation, action, and dynamics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Fuz-RL：一种模糊引导的鲁棒框架，用于不确定性下的安全强化学习</div>
<div class="mono" style="margin-top:8px">安全强化学习（RL）在现实应用中对于在确保安全的同时实现高性能至关重要。然而，真实环境中多种不确定性源的复杂交互为可解释的风险评估和鲁棒决策带来了重大挑战。为解决这些挑战，我们提出了Fuz-RL，一种基于模糊测度引导的鲁棒框架，用于安全RL。具体而言，该框架开发了一种新颖的模糊贝尔曼算子，利用Choquet积分估计鲁棒值函数。理论上，我们证明求解Fuz-RL问题（以约束马尔可夫决策过程（CMDP）形式）等价于求解分布鲁棒安全RL问题（以鲁棒CMDP形式），有效避免了最小-最大优化。在safe-control-gym和safety-gymnasium场景上的实证分析表明，Fuz-RL能以无模型方式有效整合现有安全RL基线，在观测、动作和动力学等多种不确定性下显著提升安全性和控制性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Fuz-RL, a fuzzy-guided robust framework designed to enhance safe reinforcement learning (RL) by addressing the challenges of interpretable risk assessment and robust decision-making under multiple uncertainty sources in real environments. The method develops a novel fuzzy Bellman operator that estimates robust value functions using Choquet integrals, theoretically linking it to distributionally robust safe RL problems in Constrained Markov Decision Processes (CMDPs) and avoiding min-max optimization. Experimental results on safe-control-gym and safety-gymnasium scenarios show that Fuz-RL integrates model-free with existing safe RL baselines, significantly improving both safety and control performance under uncertainties in observation, action, and dynamics.</div>
<div class="mono" style="margin-top:8px">本文提出了Fuz-RL，一个模糊引导的鲁棒框架，旨在解决现实环境中多源不确定性带来的可解释风险评估和鲁棒决策挑战，以增强安全强化学习（RL）。该方法通过使用Choquet积分开发了一种新颖的模糊贝尔曼算子来估计鲁棒值函数，理论上将其与约束马尔可夫决策过程（CMDP）中的分布鲁棒安全RL问题等价，避免了最小-最大优化。在safe-control-gym和safety-gymnasium场景上的实验结果表明，Fuz-RL以无模型方式与现有安全RL基线有效集成，在观测、动作和动力学等多种不确定性下显著提升了安全性和控制性能。</div>
</details>
</div>
<div class="card">
<div class="title">Balancing Multiple Objectives in Urban Traffic Control with Reinforcement Learning from AI Feedback</div>
<div class="meta-line">Authors: Chenyang Zhao, Vinny Cahill, Ivana Dusparic</div>
<div class="meta-line">First: 2026-02-24T09:47:25+00:00 · Latest: 2026-02-24T09:47:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20728v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20728v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward design has been one of the central challenges for real world reinforcement learning (RL) deployment, especially in settings with multiple objectives. Preference-based RL offers an appealing alternative by learning from human preferences over pairs of behavioural outcomes. More recently, RL from AI feedback (RLAIF) has demonstrated that large language models (LLMs) can generate preference labels at scale, mitigating the reliance on human annotators. However, existing RLAIF work typically focuses only on single-objective tasks, leaving the open question of how RLAIF handles systems that involve multiple objectives. In such systems trade-offs among conflicting objectives are difficult to specify, and policies risk collapsing into optimizing for a dominant goal. In this paper, we explore the extension of the RLAIF paradigm to multi-objective self-adaptive systems. We show that multi-objective RLAIF can produce policies that yield balanced trade-offs reflecting different user priorities without laborious reward engineering. We argue that integrating RLAIF into multi-objective RL offers a scalable path toward user-aligned policy learning in domains with inherently conflicting objectives.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于人工智能反馈强化学习的多目标城市交通控制平衡策略</div>
<div class="mono" style="margin-top:8px">奖励设计一直是现实世界强化学习部署的核心挑战之一，尤其是在多目标场景中。基于偏好的强化学习通过从人类对行为结果对的偏好中学习，提供了一种有吸引力的替代方案。最近，基于人工智能反馈的强化学习证明大型语言模型能够大规模生成偏好标签，从而减少对人类标注者的依赖。然而，现有RLAIF研究通常仅关注单目标任务，对于RLAIF如何处理涉及多目标的系统仍存在开放性问题。在此类系统中，相互冲突目标间的权衡难以明确界定，策略可能陷入仅优化主导目标的困境。本文探索将RLAIF范式扩展至多目标自适应系统，证明多目标RLAIF能够生成反映不同用户优先级的平衡权衡策略，无需繁琐的奖励工程。我们认为，将RLAIF整合到多目标强化学习中，为在目标本质冲突的领域实现用户对齐的策略学习提供了可扩展的路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of designing effective reward functions for multi-objective reinforcement learning (RL) in real-world systems, such as urban traffic control, where balancing conflicting goals is difficult. The authors propose extending Reinforcement Learning from AI Feedback (RLAIF) to multi-objective settings, leveraging large language models to generate preference labels at scale instead of relying on human annotators or manual reward engineering. Their experimental results demonstrate that this multi-objective RLAIF approach can produce policies that achieve balanced trade-offs aligned with different user priorities, effectively preventing the collapse into optimizing a single dominant objective.</div>
<div class="mono" style="margin-top:8px">本文针对城市交通控制等多目标现实系统中强化学习奖励函数设计的挑战，提出将人工智能反馈的强化学习（RLAIF）扩展至多目标场景，利用大语言模型规模化生成偏好标签以替代人工标注或繁琐的奖励工程。实验结果表明，该方法能生成平衡不同用户优先级权衡的策略，有效避免了策略偏向单一主导目标的问题，为具有内在冲突目标的领域提供了一条可扩展的用户对齐策略学习路径。</div>
</details>
</div>
<div class="card">
<div class="title">Buffer Matters: Unleashing the Power of Off-Policy Reinforcement Learning in Large Language Model Reasoning</div>
<div class="meta-line">Authors: Xu Wan, Yansheng Wang, Wenqi Huang, Mingyang Sun</div>
<div class="meta-line">First: 2026-02-24T09:35:43+00:00 · Latest: 2026-02-24T09:35:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20722v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20722v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional on-policy Reinforcement Learning with Verifiable Rewards (RLVR) frameworks suffer from experience waste and reward homogeneity, which directly hinders learning efficiency on difficult samples during large language models post-training. In this paper, we introduce Batch Adaptation Policy Optimization (BAPO), an off-policy RLVR framework to improve the data efficiency in large language models post-training. It dynamically selects training batches by re-evaluating historically difficult samples and reusing high-quality ones, while holding a lower bound guarantee for policy improvement. Extensive experiments further demonstrate that BAPO achieves an average 12.5% improvement over GRPO across mathematics, planning, and visual reasoning tasks. Crucially, BAPO successfully resolves 40.7% of problems that base models consistently fail to solve.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>缓冲区的重要性：释放离线策略强化学习在大语言模型推理中的潜力</div>
<div class="mono" style="margin-top:8px">传统的基于可验证奖励的在线策略强化学习框架存在经验浪费和奖励同质化问题，这直接阻碍了大语言模型后训练阶段在困难样本上的学习效率。本文提出批量自适应策略优化，一种离线策略RLVR框架，旨在提升大语言模型后训练的数据效率。该框架通过重新评估历史困难样本并复用高质量样本动态选择训练批次，同时为策略改进提供下界保证。大量实验进一步表明，在数学、规划和视觉推理任务上，BAPO相比GRPO平均提升12.5%。关键的是，BAPO成功解决了基础模型持续无法解决的40.7%的问题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the inefficiencies of on-policy reinforcement learning with verifiable rewards (RLVR) in large language model post-training, which suffers from experience waste and reward homogeneity, this paper introduces Batch Adaptation Policy Optimization (BAPO), an off-policy RLVR framework designed to enhance data efficiency. The method dynamically selects training batches by re-evaluating historically difficult samples and reusing high-quality ones, all while maintaining a theoretical lower bound guarantee for policy improvement. Experimental results show that BAPO achieves an average 12.5% performance improvement over GRPO across mathematics, planning, and visual reasoning tasks, and crucially resolves 40.7% of problems that base models consistently fail to solve.</div>
<div class="mono" style="margin-top:8px">针对传统基于策略的强化学习与可验证奖励框架在大语言模型后训练中存在经验浪费和奖励同质化、阻碍困难样本学习效率的问题，本文提出了批量自适应策略优化方法，这是一种离策略的强化学习框架，旨在提升数据效率。该方法通过重新评估历史困难样本并重用高质量样本，动态选择训练批次，同时保证了策略改进的理论下界。实验结果表明，在数学、规划和视觉推理任务上，该方法平均性能比GRPO提升了12.5%，并成功解决了基础模型持续无法解决的40.7%的问题。</div>
</details>
</div>
<div class="card">
<div class="title">Polychromic Objectives for Reinforcement Learning</div>
<div class="meta-line">Authors: Jubayer Ibn Hamid, Ifdita Hasan Orney, Ellen Xu, Chelsea Finn, Dorsa Sadigh</div>
<div class="meta-line">First: 2025-09-29T19:32:11+00:00 · Latest: 2026-02-24T09:06:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.25424v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.25424v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning fine-tuning (RLFT) is a dominant paradigm for improving pretrained policies for downstream tasks. These pretrained policies, trained on large datasets, produce generations with a broad range of promising but unrefined behaviors. Often, a critical failure mode of RLFT arises when policies lose this diversity and collapse into a handful of easily exploitable outputs. This convergence hinders exploration, which is essential for expanding the capabilities of the pretrained policy and for amplifying the benefits of test-time compute scaling. To address this, we introduce an objective for policy gradient methods that explicitly enforces the exploration and refinement of diverse generations, which we call a polychromic objective. We then show how proximal policy optimization (PPO) can be adapted to optimize this objective. Our method (1) employs vine sampling to collect on-policy rollouts and (2) modifies the advantage function to reflect the advantage under our new objective. Experiments on BabyAI, Minigrid, and Algorithmic Creativity show that our method improves success rates by reliably solving a larger set of environment configurations and generalizes better under large perturbations. Moreover, when given multiple attempts in pass@$k$ experiments, the policy achieves substantially higher coverage, demonstrating its ability to maintain and exploit a diverse repertoire of strategies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习的多色目标</div>
<div class="mono" style="margin-top:8px">强化学习微调（RLFT）是改进下游任务预训练策略的主流范式。这些基于大规模数据集训练的预训练策略能生成广泛但未经优化的行为。RLFT的一个关键失败模式常出现在策略失去多样性、坍缩为少数易被利用的输出时，这种收敛阻碍了探索——而探索对于扩展预训练策略能力及放大测试时计算扩展的效益至关重要。为此，我们提出一种策略梯度方法目标，显式强制对多样化生成进行探索与优化，称为多色目标。我们进一步展示如何调整近端策略优化（PPO）以优化此目标：方法（1）采用藤蔓采样收集同策略轨迹，（2）修改优势函数以反映新目标下的优势。在BabyAI、Minigrid和算法创造力任务上的实验表明，本方法通过稳定解决更多环境配置提升了成功率，并在强扰动下展现更优泛化能力。此外，在pass@k实验中给予多次尝试时，策略实现了显著更高的覆盖率，证明其保持并利用多样化策略库的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue of policy collapse in reinforcement learning fine-tuning (RLFT), where pretrained policies lose behavioral diversity and become exploitable, hindering exploration and the benefits of test-time compute scaling. To mitigate this, the authors propose a polychromic objective that explicitly enforces the exploration and refinement of diverse generations, adapting proximal policy optimization (PPO) with vine sampling for on-policy rollouts and a modified advantage function. Experimental results on BabyAI, Minigrid, and Algorithmic Creativity demonstrate that the method improves success rates by solving more environment configurations, enhances generalization under perturbations, and achieves higher coverage in pass@k experiments, showing its ability to maintain and exploit diverse strategies.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习微调中策略崩溃的问题，即预训练策略失去行为多样性并变得易于利用，从而阻碍探索和测试时计算扩展的效益。为此，作者提出了一种多色目标，明确强制探索和优化多样化的生成，通过采用藤蔓采样收集在线轨迹并修改优势函数，对近端策略优化进行了适配。在BabyAI、Minigrid和Algorithmic Creativity上的实验结果表明，该方法通过解决更多环境配置提高了成功率，在强扰动下增强了泛化能力，并在pass@k实验中实现了更高的覆盖率，证明了其保持和利用多样化策略的能力。</div>
</details>
</div>
<div class="card">
<div class="title">MASPO: Unifying Gradient Utilization, Probability Mass, and Signal Reliability for Robust and Sample-Efficient LLM Reasoning</div>
<div class="meta-line">Authors: Xiaoliang Fu, Jiaye Lin, Yangyi Fang, Binbin Zheng, Chaowen Hu, Zekai Shao, Cong Qin, Lu Pan, Ke Zeng, Xunliang Cai</div>
<div class="meta-line">First: 2026-02-19T17:05:20+00:00 · Latest: 2026-02-24T08:43:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17550v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.17550v2">PDF</a> · <a href="https://github.com/VenomRose-Juri/MASPO-RL}{https://github.com/VenomRose-Juri/MASPO-RL">Code1</a> · <a href="https://github.com/VenomRose-Juri/MASPO-RL">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing Reinforcement Learning with Verifiable Rewards (RLVR) algorithms, such as GRPO, rely on rigid, uniform, and symmetric trust region mechanisms that are fundamentally misaligned with the complex optimization dynamics of Large Language Models (LLMs). In this paper, we identify three critical challenges in these methods: (1) inefficient gradient utilization caused by the binary cutoff of hard clipping, (2) insensitive probability mass arising from uniform ratio constraints that ignore the token distribution, and (3) asymmetric signal reliability stemming from the disparate credit assignment ambiguity between positive and negative samples. To bridge these gaps, we propose Mass-Adaptive Soft Policy Optimization (MASPO), a unified framework designed to harmonize these three dimensions. MASPO integrates a differentiable soft Gaussian gating to maximize gradient utility, a mass-adaptive limiter to balance exploration across the probability spectrum, and an asymmetric risk controller to align update magnitudes with signal confidence. Extensive evaluations demonstrate that MASPO serves as a robust, all-in-one RLVR solution, significantly outperforming baselines. Our code is at: \href{https://github.com/VenomRose-Juri/MASPO-RL}{https://github.com/VenomRose-Juri/MASPO-RL}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MASPO：统一梯度利用、概率质量与信号可靠性以实现鲁棒且样本高效的大语言模型推理</div>
<div class="mono" style="margin-top:8px">现有可验证奖励强化学习（RLVR）算法（如GRPO）依赖僵化、均匀且对称的信任域机制，这与大语言模型（LLM）复杂的优化动态存在根本性错配。本文指出此类方法的三大关键挑战：（1）硬截断的二元截断导致梯度利用效率低下；（2）均匀比例约束忽视词元分布，引发概率质量不敏感问题；（3）正负样本间信用分配模糊度差异导致信号可靠性不对称。为弥合这些缺陷，我们提出质量自适应软策略优化（MASPO）——一个统一协调这三个维度的框架。MASPO集成可微分软高斯门控以最大化梯度效用，通过质量自适应限幅器平衡概率谱上的探索，并采用非对称风险控制器使更新幅度与信号置信度对齐。大量实验表明，MASPO作为鲁棒的全能RLVR解决方案，显著超越基线方法。代码发布于：https://github.com/VenomRose-Juri/MASPO-RL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the limitations of existing RLVR methods like GRPO, which use rigid trust region mechanisms that are poorly suited for LLM optimization, leading to inefficient gradient use, insensitive probability mass handling, and asymmetric signal reliability. To address these issues, the method introduces MASPO, a unified framework that incorporates a soft Gaussian gating for better gradient utilization, a mass-adaptive limiter to balance probability mass, and an asymmetric risk controller to align updates with signal confidence. Experimental results show that MASPO robustly outperforms baseline methods, offering a sample-efficient and effective RLVR solution for LLM reasoning.</div>
<div class="mono" style="margin-top:8px">本文的动机源于现有RLVR方法（如GRPO）的局限性，这些方法采用僵化的信任区域机制，不适合大语言模型优化，导致梯度利用低效、概率质量处理不敏感以及信号可靠性不对称。为解决这些问题，该方法提出了MASPO这一统一框架，整合了软高斯门控以提升梯度利用效率、质量自适应限制器以平衡概率质量分布，以及非对称风险控制器以使更新幅度与信号置信度对齐。实验结果表明，MASPO在各项评估中显著优于基线方法，为大语言模型推理提供了一个鲁棒且样本高效的RLVR解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">CAMEL: Confidence-Gated Reflection for Reward Modeling</div>
<div class="meta-line">Authors: Zirui Zhu, Hailun Xu, Yang Luo, Yong Liu, Kanchan Sarkar, Kun Xu, Yang You</div>
<div class="meta-line">First: 2026-02-24T08:20:08+00:00 · Latest: 2026-02-24T08:20:08+00:00</div>
<div class="meta-line">Comments: Preprint. 13 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20670v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20670v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward models play a fundamental role in aligning large language models with human preferences. Existing methods predominantly follow two paradigms: scalar discriminative preference models, which are efficient but lack interpretability, and generative judging models, which offer richer reasoning at the cost of higher computational overhead. We observe that the log-probability margin between verdict tokens strongly correlates with prediction correctness, providing a reliable proxy for instance difficulty without additional inference cost. Building on this insight, we propose CAMEL, a confidence-gated reflection framework that performs a lightweight single-token preference decision first and selectively invokes reflection only for low-confidence instances. To induce effective self-correction, we train the model via reinforcement learning with counterfactual prefix augmentation, which exposes the model to diverse initial verdicts and encourages genuine revision. Empirically, CAMEL achieves state-of-the-art performance on three widely used reward-model benchmarks with 82.9% average accuracy, surpassing the best prior model by 3.2% and outperforming 70B-parameter models using only 14B parameters, while establishing a strictly better accuracy-efficiency Pareto frontier.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CAMEL：基于置信度门控反思的奖励建模方法</div>
<div class="mono" style="margin-top:8px">奖励模型在将大语言模型与人类偏好对齐中起基础作用。现有方法主要遵循两种范式：标量判别式偏好模型（高效但缺乏可解释性）和生成式评判模型（提供更丰富的推理但计算开销更高）。我们发现，判决词元间的对数概率差值与预测正确性高度相关，可在无需额外推理成本下为实例难度提供可靠代理。基于此，提出CAMEL——一种置信度门控反思框架，先执行轻量级单词元偏好决策，仅对低置信度实例选择性触发反思。为引导有效自校正，通过强化学习结合反事实前缀增强训练模型，使模型接触多样初始判决并促进实质性修正。实验表明，CAMEL在三个广泛使用的奖励模型基准上达到82.9%平均准确率，较先前最佳模型提升3.2%，仅用140亿参数即超越700亿参数模型，同时建立了严格更优的准确率-效率帕累托前沿。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the trade-off between the efficiency of scalar discriminative reward models and the interpretability but high cost of generative judging models, this paper introduces CAMEL, a confidence-gated reflection framework. The method leverages the observation that the log-probability margin between verdict tokens correlates with prediction correctness, enabling a lightweight initial preference decision followed by selective reflection only for low-confidence instances; it further employs reinforcement learning with counterfactual prefix augmentation to train the model for effective self-correction. Experimental results show that CAMEL achieves state-of-the-art performance with 82.9% average accuracy across three benchmarks, surpassing prior models by 3.2% and outperforming larger 70B-parameter models using only 14B parameters, while establishing a superior accuracy-efficiency Pareto frontier.</div>
<div class="mono" style="margin-top:8px">本文的动机源于标量判别式奖励模型的高效性与生成式评判模型的可解释性但高成本之间的权衡，提出了CAMEL这一置信度门控反思框架。该方法基于判决令牌间对数概率差与预测正确性相关的观察，首先进行轻量级偏好决策，然后仅对低置信度实例选择性触发反思，并通过带反事实前缀增强的强化学习训练模型以实现有效自我纠正。实验结果表明，CAMEL在三个广泛使用的奖励模型基准上取得了最先进的性能，平均准确率达82.9%，较先前最佳模型提升3.2%，仅使用140亿参数即超越700亿参数模型，同时建立了更优的准确率-效率帕累托边界。</div>
</details>
</div>
<div class="card">
<div class="title">TrajGPT-R: Generating Urban Mobility Trajectory with Reinforcement Learning-Enhanced Generative Pre-trained Transformer</div>
<div class="meta-line">Authors: Jiawei Wang, Chuang Yang, Jiawei Yong, Xiaohang Xu, Hongjun Wang, Noboru Koshizuka, Shintaro Fukushima, Ryosuke Shibasaki, Renhe Jiang</div>
<div class="meta-line">First: 2026-02-24T07:44:19+00:00 · Latest: 2026-02-24T07:44:19+00:00</div>
<div class="meta-line">Comments: TrajGPT-R is a Reinforcement Learning-Enhanced Generative Pre-trained Transformer for Mobility Trajectory Generation</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20643v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20643v1">PDF</a> · <a href="https://github.com/Wangjw6/TrajGPT_R">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mobility trajectories are essential for understanding urban dynamics and enhancing urban planning, yet access to such data is frequently hindered by privacy concerns. This research introduces a transformative framework for generating large-scale urban mobility trajectories, employing a novel application of a transformer-based model pre-trained and fine-tuned through a two-phase process. Initially, trajectory generation is conceptualized as an offline reinforcement learning (RL) problem, with a significant reduction in vocabulary space achieved during tokenization. The integration of Inverse Reinforcement Learning (IRL) allows for the capture of trajectory-wise reward signals, leveraging historical data to infer individual mobility preferences. Subsequently, the pre-trained model is fine-tuned using the constructed reward model, effectively addressing the challenges inherent in traditional RL-based autoregressive methods, such as long-term credit assignment and handling of sparse reward environments. Comprehensive evaluations on multiple datasets illustrate that our framework markedly surpasses existing models in terms of reliability and diversity. Our findings not only advance the field of urban mobility modeling but also provide a robust methodology for simulating urban data, with significant implications for traffic management and urban development planning. The implementation is publicly available at https://github.com/Wangjw6/TrajGPT_R.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TrajGPT-R：基于强化学习增强的生成式预训练Transformer的城市移动轨迹生成</div>
<div class="mono" style="margin-top:8px">移动轨迹对于理解城市动态和优化城市规划至关重要，但此类数据的获取常受隐私问题限制。本研究提出了一种生成大规模城市移动轨迹的创新框架，采用基于Transformer的模型，通过两阶段预训练与微调实现突破性应用。首先，将轨迹生成建模为离线强化学习问题，并在词元化过程中显著压缩词汇空间。通过逆强化学习的整合，该框架能捕获轨迹级奖励信号，利用历史数据推断个体移动偏好。随后，使用构建的奖励模型对预训练模型进行微调，有效解决了传统基于强化学习的自回归方法中存在的长期信用分配和稀疏奖励环境处理等固有挑战。在多个数据集上的综合评估表明，本框架在可靠性和多样性方面显著超越现有模型。研究成果不仅推动了城市移动建模领域的发展，还为城市数据模拟提供了稳健的方法论，对交通管理和城市规划具有重要启示。代码已开源：https://github.com/Wangjw6/TrajGPT_R。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for urban mobility data that is often restricted due to privacy issues, this paper proposes TrajGPT-R, a framework that generates urban mobility trajectories using a transformer-based model enhanced with reinforcement learning. The method formulates trajectory generation as an offline reinforcement learning problem, employing tokenization to reduce vocabulary size and integrating inverse reinforcement learning to infer individual preferences from historical data as reward signals; the pre-trained model is then fine-tuned with these rewards to overcome challenges like long-term credit assignment. Experimental results on multiple datasets demonstrate that the framework significantly outperforms existing models in both reliability and diversity, advancing urban mobility modeling and offering a robust tool for traffic management and urban planning.</div>
<div class="mono" style="margin-top:8px">本研究针对城市移动轨迹数据因隐私问题难以获取的挑战，提出了TrajGPT-R框架，利用强化学习增强的生成式预训练Transformer来生成城市移动轨迹。该方法将轨迹生成构建为离线强化学习问题，通过分词减少词汇空间，并结合逆强化学习从历史数据中推断个体移动偏好作为奖励信号；随后使用构建的奖励模型对预训练模型进行微调，以解决传统基于强化学习的自回归方法中长期信用分配和稀疏奖励环境等难题。在多个数据集上的综合评估表明，该框架在可靠性和多样性上显著优于现有模型，不仅推动了城市移动建模领域的发展，也为交通管理和城市规划提供了强大的数据模拟方法。</div>
</details>
</div>
<div class="card">
<div class="title">Enjoying Non-linearity in Multinomial Logistic Bandits: A Minimax-Optimal Algorithm</div>
<div class="meta-line">Authors: Pierre Boudart, Pierre Gaillard, Alessandro Rudi</div>
<div class="meta-line">First: 2025-07-07T08:18:25+00:00 · Latest: 2026-02-24T07:43:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.05306v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.05306v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider the multinomial logistic bandit problem in which a learner interacts with an environment by selecting actions to maximize expected rewards based on probabilistic feedback from multiple possible outcomes. In the binary setting, recent work has focused on understanding the impact of the non-linearity of the logistic model (Faury et al., 2020; Abeille et al., 2021). They introduced a problem-dependent constant $κ_* \geq 1$ that may be exponentially large in some problem parameters and which is captured by the derivative of the sigmoid function. It encapsulates the non-linearity and improves existing regret guarantees over $T$ rounds from $\smash{O(d\sqrt{T})}$ to $\smash{O(d\sqrt{T/κ_*})}$, where $d$ is the dimension of the parameter space. We extend their analysis to the multinomial logistic bandit framework with a finite action space, making it suitable for complex applications with more than two choices, such as reinforcement learning or recommender systems. To achieve this, we extend the definition of $ κ_* $ to the multinomial setting and propose an efficient algorithm that leverages the problem&#x27;s non-linearity. Our method yields a problem-dependent regret bound of order $ \smash{\widetilde{\mathcal{O}}( R d \sqrt{ {KT}/{κ_*}} ) } $, where $R$ denotes the norm of the vector of rewards and $K$ is the number of outcomes. This improves upon the best existing guarantees of order $ \smash{\widetilde{\mathcal{O}}( RdK \sqrt{T} )}$. Moreover, we provide a matching $\smash{ Ω(dR\sqrt{KT/κ_*})}$ lower-bound, showing that our algorithm is minimax-optimal and that our definition of $κ_*$ is optimal.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>享受多项逻辑老虎机中的非线性：一种极小极大最优算法</div>
<div class="mono" style="margin-top:8px">我们研究多项逻辑老虎机问题，其中学习者通过选择动作与环境交互，基于多个可能结果的概率反馈来最大化期望奖励。在二元设置中，近期研究聚焦于理解逻辑模型非线性的影响（Faury等人，2020；Abeille等人，2021）。他们引入了一个问题相关常数$κ_* \geq 1$，该常数在某些问题参数下可能呈指数级增长，并由sigmoid函数的导数捕捉。它封装了非线性特性，并将$T$轮内的现有遗憾保证从$\smash{O(d\sqrt{T})}$改进为$\smash{O(d\sqrt{T/κ_*})}$，其中$d$是参数空间的维度。我们将此分析扩展到具有有限动作空间的多项逻辑老虎机框架，使其适用于超过两个选择的复杂应用（如强化学习或推荐系统）。为此，我们将$κ_*$的定义扩展到多项设置，并提出一种利用问题非线性的高效算法。我们的方法产生了阶为$\smash{\widetilde{\mathcal{O}}( R d \sqrt{ {KT}/{κ_*}} ) }$的问题依赖遗憾界，其中$R$表示奖励向量的范数，$K$是结果数量。这改进了现有最优保证阶$\smash{\widetilde{\mathcal{O}}( RdK \sqrt{T} )}$。此外，我们提供了匹配的$\smash{ Ω(dR\sqrt{KT/κ_*})}$下界，证明我们的算法是极小极大最优的，且我们对$κ_*$的定义是最优的。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper extends the analysis of non-linear logistic bandits from binary to multinomial settings, motivated by the need to handle complex applications with multiple outcomes like reinforcement learning and recommender systems. The method introduces a problem-dependent constant κ_* to capture non-linearity and proposes an efficient algorithm that leverages this structure. Experimental results demonstrate a regret bound of order Õ(Rd√(KT/κ_*)), which improves upon prior Õ(RdK√T) guarantees and is shown to be minimax-optimal via a matching lower bound.</div>
<div class="mono" style="margin-top:8px">本文将非线性逻辑赌博机分析从二元设置扩展到多元设置，动机在于处理具有多个结果的复杂应用，如强化学习和推荐系统。方法引入了一个问题依赖常数κ_*来捕捉非线性，并提出了一种利用此结构的有效算法。实验结果表明，其遗憾界为Õ(Rd√(KT/κ_*))，优于现有的Õ(RdK√T)保证，并通过匹配的下界证明了该算法是最小极大最优的。</div>
</details>
</div>
<div class="card">
<div class="title">Performance Asymmetry in Model-Based Reinforcement Learning</div>
<div class="meta-line">Authors: Jing Yu Lim, Rushi Shah, Zarif Ikram, Samson Yu, Haozhe Ma, Tze-Yun Leong, Dianbo Liu</div>
<div class="meta-line">First: 2025-05-26T08:52:45+00:00 · Latest: 2026-02-24T06:45:28+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.19698v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.19698v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, Model-Based Reinforcement Learning (MBRL) have achieved super-human level performance on the Atari100k benchmark on average. However, we discover that conventional aggregates mask a major problem, Performance Asymmetry: MBRL agents dramatically outperform humans in certain tasks (Agent-Optimal tasks) while drastically underperform humans in other tasks (Human-Optimal tasks). Indeed, despite achieving SOTA in the overall mean Human-Normalized Scores (HNS), the SOTA agent scored the worst among baselines on Human-Optimal tasks, with a striking 21X performance gap between the Human-Optimal and Agent-Optimal subsets. To address this, we partition Atari100k evenly into Human-Optimal and Agent-Optimal subsets, and introduce a more balanced aggregate, Sym-HNS. Furthermore, we trace the striking Performance Asymmetry in the SOTA pixel diffusion world model to the curse of dimensionality and its prowess on high visual detail tasks (e.g. Breakout). To this end, we propose a novel latent end-to-end Joint Embedding DIffusion (JEDI) world model that achieves SOTA results in Sym-HNS, Human-Optimal tasks, and Breakout -- thus reversing the worsening Performance Asymmetry trend while improving computational efficiency and remaining competitive on the full Atari100k.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于模型的强化学习中的性能不对称现象</div>
<div class="mono" style="margin-top:8px">近期，基于模型的强化学习（MBRL）在Atari100k基准测试中平均达到了超人类水平。然而，我们发现传统聚合指标掩盖了一个重大问题——性能不对称：MBRL智能体在某些任务（智能体优势任务）中显著超越人类，而在其他任务（人类优势任务）中却大幅落后。尽管当前最优（SOTA）智能体在整体人类标准化分数（HNS）均值上达到最高水平，但在人类优势任务中的表现却是基线方法中最差的，其人类优势与智能体优势任务子集之间存在高达21倍的性能差距。为解决此问题，我们将Atari100k均匀划分为人类优势与智能体优势子集，并引入更平衡的聚合指标——对称HNS（Sym-HNS）。进一步地，我们将SOTA像素扩散世界模型中显著的性能不对称现象归因于维度灾难及其在高视觉细节任务（如《打砖块》）中的优势。为此，我们提出了一种新颖的潜在端到端联合嵌入扩散（JEDI）世界模型，该模型在Sym-HNS、人类优势任务及《打砖块》中均取得SOTA结果——从而逆转了日益加剧的性能不对称趋势，同时提升了计算效率，并在完整Atari100k基准上保持竞争力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper identifies a significant issue in Model-Based Reinforcement Learning (MBRL) called Performance Asymmetry, where state-of-the-art agents excel at some Atari tasks (Agent-Optimal) but severely underperform humans on others (Human-Optimal), a problem masked by conventional average metrics. To address this, the authors evenly partition the Atari100k benchmark and introduce a more balanced aggregate metric, Sym-HNS, and trace the asymmetry in a leading pixel diffusion world model to the curse of dimensionality and its strength in high-detail tasks like Breakout. They propose a novel latent end-to-end Joint Embedding DIffusion (JEDI) world model, which achieves state-of-the-art results on Sym-HNS and Human-Optimal tasks, reverses the asymmetry trend, improves computational efficiency, and remains competitive on the full benchmark.</div>
<div class="mono" style="margin-top:8px">本文揭示了基于模型的强化学习中一个显著问题——性能不对称，即先进智能体在某些Atari任务（智能体优势任务）上表现远超人类，而在其他任务（人类优势任务）上却严重落后，这一问题被传统平均指标所掩盖。为解决此问题，作者将Atari100k基准均匀划分为两部分，引入更平衡的聚合指标Sym-HNS，并将领先的像素扩散世界模型中的不对称性归因于维度诅咒及其在如Breakout等高视觉细节任务上的优势。他们提出了一种新颖的潜在端到端联合嵌入扩散（JEDI）世界模型，该模型在Sym-HNS和人类优势任务上取得了最先进的结果，扭转了不对称性加剧的趋势，提高了计算效率，并在完整基准上保持竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">OptiLeak: Efficient Prompt Reconstruction via Reinforcement Learning in Multi-tenant LLM Services</div>
<div class="meta-line">Authors: Longxiang Wang, Xiang Zheng, Xuhao Zhang, Yao Zhang, Ye Wu, Cong Wang</div>
<div class="meta-line">First: 2026-02-24T06:35:22+00:00 · Latest: 2026-02-24T06:35:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20595v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20595v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-tenant LLM serving frameworks widely adopt shared Key-Value caches to enhance efficiency. However, this creates side-channel vulnerabilities enabling prompt leakage attacks. Prior studies identified these attack surfaces yet focused on expanding attack vectors rather than optimizing attack performance, reporting impractically high attack costs that underestimate the true privacy risk. We propose OptiLeak, a reinforcement learning-enhanced framework that maximizes prompt reconstruction efficiency through two-stage fine-tuning. Our key insight is that domain-specific ``hard tokens&#x27;&#x27; -- terms difficult to predict yet carrying sensitive information -- can be automatically identified via likelihood ranking and used to construct preference pairs for Direct Preference Optimization, eliminating manual annotation. This enables effective preference alignment while avoiding the overfitting issues of extended supervised fine-tuning. Evaluated on three benchmarks spanning medical and financial domains, OptiLeak achieves up to $12.48\times$ reduction in average requests per token compared to baseline approaches, with consistent improvements across model scales from 3B to 14B parameters. Our findings demonstrate that cache-based prompt leakage poses a more severe threat than previously reported, underscoring the need for robust cache isolation in production deployments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OptiLeak：基于强化学习的多租户LLM服务高效提示词重构框架</div>
<div class="mono" style="margin-top:8px">多租户大语言模型服务框架广泛采用共享键值缓存以提升效率，但这会引发侧信道漏洞导致提示词泄露攻击。现有研究虽识别了攻击面，却侧重于扩展攻击向量而非优化攻击性能，所报告的不切实际的高攻击成本低估了真实隐私风险。本文提出OptiLeak——一种通过两阶段微调实现提示词重构效率最大化的强化学习增强框架。核心洞见在于：领域特定的&#x27;困难词元&#x27;（难以预测却携带敏感信息的术语）可通过似然排序自动识别，并用于构建直接偏好优化的偏好对，从而免除人工标注。该方法在实现有效偏好对齐的同时，避免了扩展监督微调导致的过拟合问题。在涵盖医疗和金融领域的三个基准测试中，OptiLeak相比基线方法实现了最高12.48倍的每词元平均请求数降低，且在3B至14B参数规模模型中均保持稳定改进。研究结果表明，基于缓存的提示词泄露威胁比既往报道更为严重，凸显了生产部署中强化缓存隔离的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the security vulnerability of prompt leakage in multi-tenant LLM services caused by shared Key-Value caches, noting that prior work overestimated attack costs and thus underestimated the risk. The proposed method, OptiLeak, uses a reinforcement learning framework enhanced by two-stage fine-tuning; it automatically identifies domain-specific &#x27;hard tokens&#x27; via likelihood ranking to create preference pairs for Direct Preference Optimization, avoiding manual annotation and overfitting. Experimental results on medical and financial benchmarks show OptiLeak reduces the average number of requests per token by up to 12.48 times compared to baselines, demonstrating a more severe threat and the need for better cache isolation.</div>
<div class="mono" style="margin-top:8px">本文针对多租户大语言模型服务中因共享键值缓存导致的提示词泄露安全漏洞展开研究，指出先前工作高估了攻击成本从而低估了风险。所提出的OptiLeak方法采用强化学习框架，并通过两阶段微调进行增强；它通过似然排序自动识别领域特定的“困难词元”，用于构建直接偏好优化的偏好对，从而避免了人工标注和过拟合问题。在医疗和金融领域的基准测试中，实验结果表明，与基线方法相比，OptiLeak将每个词元的平均请求数降低了高达12.48倍，证明了该威胁比以往报道的更为严重，凸显了在生产部署中加强缓存隔离的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training</div>
<div class="meta-line">Authors: Guobin Shen, Chenxiao Zhao, Xiang Cheng, Lei Huang, Xing Yu</div>
<div class="meta-line">First: 2026-02-11T09:48:08+00:00 · Latest: 2026-02-24T06:30:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10693v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.10693v2">PDF</a> · <a href="https://github.com/FloyedShen/VESPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VESPO：面向稳定离策略大语言模型训练的变分序列级软策略优化</div>
<div class="mono" style="margin-top:8px">训练稳定性始终是大语言模型强化学习的核心挑战。策略陈旧性、异步训练以及训练与推理引擎间的失配，均会导致行为策略偏离当前策略，引发训练崩溃风险。重要性采样为这种分布偏移提供了理论修正，但存在高方差缺陷；现有解决方案（如词元级截断和序列级归一化）缺乏统一的理论基础。本文提出变分序列级软策略优化方法。通过将方差缩减融入提案分布的变分框架，VESPO推导出可直接作用于序列级重要性权重（无需长度归一化）的闭式重塑核。数学推理基准实验表明：VESPO能在高达64倍陈旧率及完全异步执行环境下保持训练稳定，并在稠密模型与混合专家模型中均取得持续性能提升。代码已开源：https://github.com/FloyedShen/VESPO</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of training instability in reinforcement learning for large language models, which arises from policy staleness, asynchronous training, and mismatches between training and inference engines. The authors propose VESPO, a method that integrates variance reduction into a variational formulation over proposal distributions to derive a closed-form reshaping kernel for sequence-level importance weights, avoiding length normalization. Experimental results on mathematical reasoning benchmarks demonstrate that VESPO maintains stable training under high staleness ratios and asynchronous execution, achieving consistent performance improvements across both dense and Mixture-of-Experts models.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型强化学习中因策略陈旧、异步训练以及训练与推理引擎不匹配导致的训练不稳定问题展开研究。作者提出了VESPO方法，该方法将方差减少融入提案分布的变分框架中，推导出一个直接作用于序列级重要性权重的闭式重塑核，无需长度归一化。在数学推理基准测试上的实验结果表明，VESPO能在高陈旧比和完全异步执行下保持训练稳定，并在稠密模型与专家混合模型上均取得一致的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">From Logs to Language: Learning Optimal Verbalization for LLM-Based Recommendation in Production</div>
<div class="meta-line">Authors: Yucheng Shi, Ying Li, Yu Wang, Yesu Feng, Arjun Rao, Rein Houthooft, Shradha Sehgal, Jin Wang, Hao Zhen, Ninghao Liu, Linas Baltrunas</div>
<div class="meta-line">First: 2026-02-24T05:15:24+00:00 · Latest: 2026-02-24T05:15:24+00:00</div>
<div class="meta-line">Comments: Work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20558v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20558v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are promising backbones for generative recommender systems, yet a key challenge remains underexplored: verbalization, i.e., converting structured user interaction logs into effective natural language inputs. Existing methods rely on rigid templates that simply concatenate fields, yielding suboptimal representations for recommendation. We propose a data-centric framework that learns verbalization for LLM-based recommendation. Using reinforcement learning, a verbalization agent transforms raw interaction histories into optimized textual contexts, with recommendation accuracy as the training signal. This agent learns to filter noise, incorporate relevant metadata, and reorganize information to improve downstream predictions. Experiments on a large-scale industrial streaming dataset show that learned verbalization delivers up to 93% relative improvement in discovery item recommendation accuracy over template-based baselines. Further analysis reveals emergent strategies such as user interest summarization, noise removal, and syntax normalization, offering insights into effective context construction for LLM-based recommender systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从日志到语言：学习面向生产环境中基于大语言模型的推荐系统的最优文本化方法</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）作为生成式推荐系统的核心架构前景广阔，但一个关键挑战——文本化，即将结构化用户交互日志转化为有效的自然语言输入——仍未得到充分探索。现有方法依赖僵化的模板，仅简单拼接字段，导致推荐表示效果欠佳。我们提出一种以数据为中心的框架，用于学习基于大语言模型的推荐文本化方法。通过强化学习，一个文本化代理将原始交互历史转化为优化的文本上下文，并以推荐准确性作为训练信号。该代理学会过滤噪声、整合相关元数据并重组信息，以提升下游预测性能。在大规模工业流式数据集上的实验表明，学习得到的文本化方法在发现项推荐准确性上，相比基于模板的基线实现了高达93%的相对提升。进一步分析揭示了涌现的策略，如用户兴趣总结、噪声去除和句法规范化，为基于大语言模型的推荐系统提供了有效上下文构建的洞见。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that while large language models (LLMs) hold promise for generative recommender systems, the process of verbalization—converting structured user interaction logs into natural language inputs—remains a key underexplored challenge, with existing template-based methods being suboptimal. The proposed method is a data-centric framework where a reinforcement learning agent learns to transform raw interaction histories into optimized textual contexts, using recommendation accuracy as the training signal to filter noise and reorganize information. The main experimental results, from tests on a large-scale industrial streaming dataset, show that this learned verbalization delivers up to a 93% relative improvement in discovery item recommendation accuracy over template-based baselines, with analysis revealing emergent strategies like interest summarization and noise removal.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，尽管大语言模型（LLM）在生成式推荐系统中前景广阔，但如何将结构化的用户交互日志转化为有效的自然语言输入（即言语化）这一关键挑战尚未得到充分探索，现有基于模板的方法效果欠佳。所提出的方法是一个以数据为中心的框架，其中通过强化学习训练一个言语化智能体，以推荐准确性为训练信号，将原始交互历史转化为优化的文本上下文，从而学习过滤噪声并重组信息。在大型工业流式数据集上的主要实验结果表明，这种学习得到的言语化方法相比基于模板的基线，在发现项推荐准确性上实现了高达93%的相对提升，分析还揭示了如用户兴趣总结和噪声去除等新兴策略。</div>
</details>
</div>
<div class="card">
<div class="title">PCPO: Proportionate Credit Policy Optimization for Aligning Image Generation Models</div>
<div class="meta-line">Authors: Jeongjae Lee, Jong Chul Ye</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-09-30T04:43:58+00:00 · Latest: 2026-02-24T05:01:33+00:00</div>
<div class="meta-line">Comments: 35 pages, 20 figures. ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.25774v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.25774v3">PDF</a> · <a href="https://github.com/jaylee2000/pcpo/">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While reinforcement learning has advanced the alignment of text-to-image (T2I) models, state-of-the-art policy gradient methods are still hampered by training instability and high variance, hindering convergence speed and compromising image quality. Our analysis identifies a key cause of this instability: disproportionate credit assignment, in which the mathematical structure of the generative sampler produces volatile and non-proportional feedback across timesteps. To address this, we introduce Proportionate Credit Policy Optimization (PCPO), a framework that enforces proportional credit assignment through a stable objective reformulation and a principled reweighting of timesteps. This correction stabilizes the training process, leading to significantly accelerated convergence and superior image quality. The improvement in quality is a direct result of mitigating model collapse, a common failure mode in recursive training. PCPO substantially outperforms existing policy gradient baselines on all fronts, including the state-of-the-art DanceGRPO. Code is available at https://github.com/jaylee2000/pcpo/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PCPO：面向图像生成模型对齐的比例信用策略优化</div>
<div class="mono" style="margin-top:8px">尽管强化学习推动了文本到图像（T2I）模型的对齐，但最先进的策略梯度方法仍受限于训练不稳定性和高方差，阻碍收敛速度并影响图像质量。我们的分析发现其关键原因在于不成比例的信用分配：生成采样器的数学结构导致跨时间步的反馈波动剧烈且不成比例。为此，我们提出比例信用策略优化（PCPO），该框架通过稳定的目标重构和时间步的加权调整，实现比例化信用分配。这一修正稳定了训练过程，显著加速收敛并提升图像质量。质量改善直接源于缓解了递归训练中常见的模型坍塌问题。PCPO在包括最先进的DanceGRPO在内的所有基线方法上均表现优异。代码发布于 https://github.com/jaylee2000/pcpo/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the training instability and high variance in state-of-the-art reinforcement learning methods for aligning text-to-image models, which hinder convergence and image quality. The proposed method, Proportionate Credit Policy Optimization (PCPO), addresses the root cause of disproportionate credit assignment by introducing a stable objective reformulation and a principled reweighting of timesteps to enforce proportional credit. The main experimental results demonstrate that PCPO significantly accelerates convergence, improves image quality by mitigating model collapse, and substantially outperforms existing policy gradient baselines, including the state-of-the-art DanceGRPO.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于当前用于对齐文本到图像模型的强化学习方法存在训练不稳定和高方差问题，这阻碍了收敛速度并损害了图像质量。所提出的方法——比例信用策略优化（PCPO），通过引入稳定的目标重构和基于原则的时间步重加权，解决了信用分配不成比例的根本原因。主要实验结果表明，PCPO显著加速了收敛，通过缓解模型崩溃提高了图像质量，并在所有方面大幅超越了现有的策略梯度基线方法，包括最先进的DanceGRPO。</div>
</details>
</div>
<div class="card">
<div class="title">Actor-Curator: Co-adaptive Curriculum Learning via Policy-Improvement Bandits for RL Post-Training</div>
<div class="meta-line">Authors: Zhengyao Gu, Jonathan Light, Raul Astudillo, Ziyu Ye, Langzhou He, Henry Peng Zou, Wei Cheng, Santiago Paternain, Philip S. Yu, Yisong Yue</div>
<div class="meta-line">First: 2026-02-24T04:19:48+00:00 · Latest: 2026-02-24T04:19:48+00:00</div>
<div class="meta-line">Comments: 37 pages, 8 figures, 1 table. Preprint under review. Equal contribution by first two authors</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20532v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20532v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Post-training large foundation models with reinforcement learning typically relies on massive and heterogeneous datasets, making effective curriculum learning both critical and challenging. In this work, we propose ACTOR-CURATOR, a scalable and fully automated curriculum learning framework for reinforcement learning post-training of large language models (LLMs). ACTOR-CURATOR learns a neural curator that dynamically selects training problems from large problem banks by directly optimizing for expected policy performance improvement. We formulate problem selection as a non-stationary stochastic bandit problem, derive a principled loss function based on online stochastic mirror descent, and establish regret guarantees under partial feedback. Empirically, ACTOR-CURATOR consistently outperforms uniform sampling and strong curriculum baselines across a wide range of challenging reasoning benchmarks, demonstrating improved training stability and efficiency. Notably, it achieves relative gains of 28.6% on AIME2024 and 30.5% on ARC-1D over the strongest baseline and up to 80% speedup. These results suggest that ACTOR-CURATOR is a powerful and practical approach for scalable LLM post-training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Actor-Curator：基于策略改进多臂老虎机的协同自适应课程学习用于强化学习后训练</div>
<div class="mono" style="margin-top:8px">基于强化学习对大型基础模型进行后训练通常依赖海量异构数据集，这使得高效的课程学习既关键又具挑战性。本研究提出ACTOR-CURATOR，一个可扩展、全自动的课程学习框架，专用于大语言模型的强化学习后训练。该框架通过直接优化策略性能提升期望，训练一个动态从大型问题库中选择训练问题的神经策展器。我们将问题选择建模为非平稳随机多臂老虎机问题，基于在线随机镜像下降推导出理论严谨的损失函数，并在部分反馈条件下建立遗憾保证。实验表明，在多个高难度推理基准测试中，ACTOR-CURATOR持续优于均匀采样及强课程基线方法，展现出更优的训练稳定性与效率。其在AIME2024上实现28.6%、在ARC-1D上实现30.5%的相对性能提升（对比最强基线），并最高加速80%。这些结果表明ACTOR-CURATOR是可扩展大语言模型后训练的强大实用方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that post-training large foundation models with reinforcement learning often depends on massive, heterogeneous datasets, making effective curriculum learning both crucial and difficult. The method introduces ACTOR-CURATOR, a scalable and automated curriculum learning framework for reinforcement learning post-training of large language models, which learns a neural curator that dynamically selects training problems from large banks by directly optimizing for expected policy improvement, formulated as a non-stationary stochastic bandit problem with a principled loss function derived from online stochastic mirror descent and regret guarantees. The main experimental results show that ACTOR-CURATOR consistently outperforms uniform sampling and strong curriculum baselines across various challenging reasoning benchmarks, improving training stability and efficiency, with notable relative gains of 28.6% on AIME2024 and 30.5% on ARC-1D over the strongest baseline and up to 80% speedup.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，使用强化学习对大型基础模型进行后训练通常依赖于海量异构数据集，这使得有效的课程学习既关键又具有挑战性。方法上提出了ACTOR-CURATOR，这是一个可扩展且全自动的课程学习框架，用于大语言模型的强化学习后训练，它通过学习一个神经策展器，通过直接优化期望策略性能改进来动态从大型问题库中选择训练问题，并将其形式化为非平稳随机赌博机问题，基于在线随机镜像下降推导出原则性损失函数并提供了遗憾保证。主要实验结果表明，ACTOR-CURATOR在多种具有挑战性的推理基准测试中持续优于均匀采样和强课程基线，提高了训练稳定性和效率，在AIME2024和ARC-1D上相对于最强基线分别实现了28.6%和30.5%的相对提升，并达到高达80%的加速效果。</div>
</details>
</div>
<div class="card">
<div class="title">A Generalized Apprenticeship Learning Framework for Capturing Evolving Student Pedagogical Strategies</div>
<div class="meta-line">Authors: Md Mirajul Islam, Xi Yang, Adittya Soukarjya Saha, Rajesh Debnath, Min Chi</div>
<div class="meta-line">Venue: AIED 2025, LNCS 15879, Springer, pp. 393-408</div>
<div class="meta-line">First: 2026-02-24T04:08:31+00:00 · Latest: 2026-02-24T04:08:31+00:00</div>
<div class="meta-line">Comments: 16 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20527v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20527v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) and Deep Reinforcement Learning (DRL) have advanced rapidly in recent years and have been successfully applied to e-learning environments like intelligent tutoring systems (ITSs). Despite great success, the broader application of DRL to educational technologies has been limited due to major challenges such as sample inefficiency and difficulty designing the reward function. In contrast, Apprenticeship Learning (AL) uses a few expert demonstrations to infer the expert&#x27;s underlying reward functions and derive decision-making policies that generalize and replicate optimal behavior. In this work, we leverage a generalized AL framework, THEMES, to induce effective pedagogical policies by capturing the complexities of the expert student learning process, where multiple reward functions may dynamically evolve over time. We evaluate the effectiveness of THEMES against six state-of-the-art baselines, demonstrating its superior performance and highlighting its potential as a powerful alternative for inducing effective pedagogical policies and show that it can achieve high performance, with an AUC of 0.899 and a Jaccard of 0.653, using only 18 trajectories of a previous semester to predict student pedagogical decisions in a later semester.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种用于捕捉演化学生教学策略的广义学徒学习框架</div>
<div class="mono" style="margin-top:8px">近年来，强化学习（RL）与深度强化学习（DRL）发展迅速，已成功应用于智能导学系统等电子学习环境。尽管取得显著成果，但DRL在教育技术中的广泛应用仍受限于样本效率低下和奖励函数设计困难等主要挑战。相比之下，学徒学习（AL）通过少量专家示范数据，即可推断专家的潜在奖励函数，并推导出能够泛化和复现最优行为的决策策略。本研究采用广义AL框架THEMES，通过捕捉专家学生学习过程中可能随时间动态演化的多重奖励函数复杂性，推导出有效的教学策略。我们将THEMES与六种先进基线方法进行比较评估，证明了其优越性能，突显其作为推导有效教学策略的强大替代方案的潜力。实验表明，仅使用前一学期的18条轨迹数据预测后续学期的学生教学决策，THEMES即可取得AUC 0.899和Jaccard系数0.653的高性能表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenges of applying deep reinforcement learning to intelligent tutoring systems, such as sample inefficiency and reward design difficulty, this paper proposes a generalized apprenticeship learning framework called THEMES to capture evolving student pedagogical strategies. The method infers potentially dynamic expert reward functions from demonstrations to derive effective pedagogical policies. Experimental results show THEMES outperforms six state-of-the-art baselines, achieving an AUC of 0.899 and a Jaccard score of 0.653 using only 18 expert trajectories to predict future student decisions.</div>
<div class="mono" style="margin-top:8px">针对深度强化学习应用于智能导学系统时存在的样本效率低和奖励函数设计困难等挑战，本文提出了一个名为THEMES的广义学徒学习框架，旨在捕捉不断演化的学生教学策略。该方法通过从专家演示中推断可能动态变化的奖励函数，从而推导出有效的教学策略。实验结果表明，THEMES在六个先进基线模型中表现最优，仅使用18条专家轨迹预测后续学期学生决策，就取得了0.899的AUC和0.653的Jaccard分数。</div>
</details>
</div>
<div class="card">
<div class="title">KairosVL: Orchestrating Time Series and Semantics for Unified Reasoning</div>
<div class="meta-line">Authors: Haotian Si, Changhua Pei, Xiao He, Zeyan Li, Zhe Xie, Zexin Wang, Jiyao Hu, Zhaoyang Yu, Tieying Zhang, Dan Pei, Jianhui Li, Gaogang Xie</div>
<div class="meta-line">First: 2026-02-24T02:50:38+00:00 · Latest: 2026-02-24T02:50:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20494v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20494v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Driven by the increasingly complex and decision-oriented demands of time series analysis, we introduce the Semantic-Conditional Time Series Reasoning task, which extends conventional time series analysis beyond purely numerical modeling to incorporate contextual and semantic understanding. To further enhance the mode&#x27;s reasoning capabilities on complex time series problems, we propose a two-round reinforcement learning framework: the first round strengthens the mode&#x27;s perception of fundamental temporal primitives, while the second focuses on semantic-conditioned reasoning. The resulting model, KairosVL, achieves competitive performance across both synthetic and real-world tasks. Extensive experiments and ablation studies demonstrate that our framework not only boosts performance but also preserves intrinsic reasoning ability and significantly improves generalization to unseen scenarios. To summarize, our work highlights the potential of combining semantic reasoning with temporal modeling and provides a practical framework for real-world time series intelligence, which is in urgent demand.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KairosVL：时序与语义协同的统一推理框架</div>
<div class="mono" style="margin-top:8px">受日益复杂且面向决策的时序分析需求驱动，我们提出了语义条件时序推理任务，将传统时序分析从纯数值建模扩展至融合上下文与语义理解。为增强模型对复杂时序问题的推理能力，我们设计了一种两轮强化学习框架：首轮强化模型对基础时序基元的感知，次轮聚焦于语义条件推理。所构建的模型KairosVL在合成与真实任务中均取得优异性能。大量实验与消融研究表明，该框架不仅能提升性能，还可保持内在推理能力，并显著增强对未见场景的泛化性。总之，本研究揭示了语义推理与时序建模结合的潜力，并为当前亟需的现实世界时序智能提供了实用框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to move beyond purely numerical modeling in time series analysis and incorporate contextual understanding for complex decision-making, this paper introduces the Semantic-Conditional Time Series Reasoning task. The method employs a two-round reinforcement learning framework: the first round enhances perception of fundamental temporal primitives, and the second focuses on semantic-conditioned reasoning, resulting in the model KairosVL. Experimental results on synthetic and real-world tasks show that KairosVL achieves competitive performance, with ablation studies confirming that the framework boosts performance, preserves intrinsic reasoning ability, and significantly improves generalization to unseen scenarios.</div>
<div class="mono" style="margin-top:8px">本文的动机是超越传统时间序列分析的纯数值建模，融入上下文语义理解以满足复杂决策需求，为此提出了语义条件时间序列推理任务。方法上采用了两轮强化学习框架：第一轮强化模型对基本时间原语的感知，第二轮专注于语义条件推理，由此构建了KairosVL模型。在合成和真实世界任务上的实验结果表明，KairosVL取得了有竞争力的性能，消融研究证实该框架不仅提升了性能、保持了内在推理能力，还显著增强了对未见场景的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion Modulation via Environment Mechanism Modeling for Planning</div>
<div class="meta-line">Authors: Hanping Zhang, Yuhong Guo</div>
<div class="meta-line">First: 2026-02-23T23:41:22+00:00 · Latest: 2026-02-23T23:41:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20422v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20422v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models have shown promising capabilities in trajectory generation for planning in offline reinforcement learning (RL). However, conventional diffusion-based planning methods often fail to account for the fact that generating trajectories in RL requires unique consistency between transitions to ensure coherence in real environments. This oversight can result in considerable discrepancies between the generated trajectories and the underlying mechanisms of a real environment. To address this problem, we propose a novel diffusion-based planning method, termed as Diffusion Modulation via Environment Mechanism Modeling (DMEMM). DMEMM modulates diffusion model training by incorporating key RL environment mechanisms, particularly transition dynamics and reward functions. Experimental results demonstrate that DMEMM achieves state-of-the-art performance for planning with offline reinforcement learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于环境机制建模的扩散调制规划方法</div>
<div class="mono" style="margin-top:8px">扩散模型在离线强化学习规划中的轨迹生成方面展现出潜力，但传统基于扩散的规划方法常忽略强化学习轨迹生成需保持状态转移间的一致性，以确保真实环境中的连贯性。这一疏漏可能导致生成轨迹与真实环境机制存在显著偏差。为此，我们提出一种新颖的基于扩散的规划方法——基于环境机制建模的扩散调制（DMEMM）。该方法通过融入关键强化学习环境机制（特别是状态转移动态和奖励函数）来调制扩散模型的训练。实验结果表明，DMEMM在离线强化学习规划任务中实现了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the observation that conventional diffusion-based planning methods in offline reinforcement learning often neglect the necessary consistency between transitions, leading to generated trajectories that misalign with real environment mechanisms. To address this, the authors propose DMEMM, a novel method that modulates diffusion model training by explicitly incorporating key environment mechanisms such as transition dynamics and reward functions. Experimental results show that DMEMM achieves state-of-the-art performance in planning tasks within offline reinforcement learning settings.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，传统的基于扩散模型的离线强化学习规划方法常忽略状态转移间的一致性要求，导致生成的轨迹与真实环境机制存在偏差。为此，作者提出了DMEMM方法，该方法通过显式融入环境机制（如转移动态和奖励函数）来调制扩散模型的训练过程。实验结果表明，DMEMM在离线强化学习的规划任务中取得了最先进的性能。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260226_0405.html">20260226_0405</a>
<a href="archive/20260224_0355.html">20260224_0355</a>
<a href="archive/20260223_0321.html">20260223_0321</a>
<a href="archive/20260222_0327.html">20260222_0327</a>
<a href="archive/20260221_0347.html">20260221_0347</a>
<a href="archive/20260220_0349.html">20260220_0349</a>
<a href="archive/20260219_0406.html">20260219_0406</a>
<a href="archive/20260218_0358.html">20260218_0358</a>
<a href="archive/20260217_0329.html">20260217_0329</a>
<a href="archive/20260216_0321.html">20260216_0321</a>
<a href="archive/20260215_0335.html">20260215_0335</a>
<a href="archive/20260213_0416.html">20260213_0416</a>
<a href="archive/20260212_0417.html">20260212_0417</a>
<a href="archive/20260211_0421.html">20260211_0421</a>
<a href="archive/20260210_0359.html">20260210_0359</a>
<a href="archive/20260209_0321.html">20260209_0321</a>
<a href="archive/20260208_0336.html">20260208_0336</a>
<a href="archive/20260207_0400.html">20260207_0400</a>
<a href="archive/20260206_0358.html">20260206_0358</a>
<a href="archive/20260205_0400.html">20260205_0400</a>
<a href="archive/20260204_0411.html">20260204_0411</a>
<a href="archive/20260202_0320.html">20260202_0320</a>
<a href="archive/20260201_0343.html">20260201_0343</a>
<a href="archive/20260131_0357.html">20260131_0357</a>
<a href="archive/20260130_0355.html">20260130_0355</a>
<a href="archive/20260129_0353.html">20260129_0353</a>
<a href="archive/20260128_0342.html">20260128_0342</a>
<a href="archive/20260127_0320.html">20260127_0320</a>
<a href="archive/20260126_0313.html">20260126_0313</a>
<a href="archive/20260125_0321.html">20260125_0321</a>
<a href="archive/20260124_0344.html">20260124_0344</a>
<a href="archive/20260123_0345.html">20260123_0345</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0408.html">20260121_0408</a>
<a href="archive/20260120_0316.html">20260120_0316</a>
<a href="archive/20260119_0312.html">20260119_0312</a>
<a href="archive/20260118_0323.html">20260118_0323</a>
<a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
